"""
Neural Repair Model Interface for VEGA-Verified.

Provides abstract interface and implementations for neural code repair models.
Supports multiple backends: HuggingFace Transformers, API-based, and local models.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple, Callable
from enum import Enum
import os
import json
import time
import re


class ModelBackend(Enum):
    """Supported model backends."""
    HUGGINGFACE = "huggingface"
    OPENAI = "openai"
    LOCAL = "local"
    RULE_BASED = "rule_based"


@dataclass
class RepairCandidate:
    """A repair candidate generated by the model."""
    code: str
    confidence: float = 0.0
    strategy: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "code": self.code,
            "confidence": self.confidence,
            "strategy": self.strategy,
            "metadata": self.metadata,
        }


@dataclass
class ModelConfig:
    """Configuration for neural repair model."""
    model_name: str = "microsoft/unixcoder-base-nine"
    model_path: Optional[str] = None
    backend: ModelBackend = ModelBackend.RULE_BASED
    
    # Generation parameters
    max_length: int = 512
    num_beams: int = 5
    num_return_sequences: int = 5
    temperature: float = 0.7
    top_p: float = 0.95
    top_k: int = 50
    
    # API settings (for OpenAI/API backends)
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    
    # Device settings
    device: str = "cpu"  # "cpu", "cuda", "cuda:0", etc.
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ModelConfig':
        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "backend": self.backend.value,
            "max_length": self.max_length,
            "num_beams": self.num_beams,
            "temperature": self.temperature,
        }


class BaseRepairModel(ABC):
    """Abstract base class for repair models."""
    
    def __init__(self, config: Optional[ModelConfig] = None):
        self.config = config or ModelConfig()
        self.stats = {
            "total_repairs": 0,
            "successful_repairs": 0,
            "total_time_ms": 0.0,
        }
    
    @abstractmethod
    def generate(
        self,
        context: 'RepairContext',
        num_candidates: int = 5
    ) -> List[RepairCandidate]:
        """
        Generate repair candidates.
        
        Args:
            context: Repair context with code, counterexample, etc.
            num_candidates: Number of candidates to generate
            
        Returns:
            List of repair candidates
        """
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if model is available and ready."""
        pass
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get model statistics."""
        return self.stats.copy()
    
    def reset_statistics(self) -> None:
        """Reset statistics."""
        self.stats = {
            "total_repairs": 0,
            "successful_repairs": 0,
            "total_time_ms": 0.0,
        }


class TransformerRepairModel(BaseRepairModel):
    """
    HuggingFace Transformers-based repair model.
    
    Uses fine-tuned UniXcoder or similar model for code repair.
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        super().__init__(config)
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self) -> None:
        """Load the transformer model."""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            model_path = self.config.model_path or self.config.model_name
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
            
            # Move to device
            if self.config.device != "cpu":
                import torch
                if torch.cuda.is_available():
                    self.model = self.model.to(self.config.device)
        
        except ImportError:
            print("Warning: transformers not installed. Using rule-based fallback.")
            self.model = None
            self.tokenizer = None
        except Exception as e:
            print(f"Warning: Could not load model: {e}. Using rule-based fallback.")
            self.model = None
            self.tokenizer = None
    
    def is_available(self) -> bool:
        return self.model is not None and self.tokenizer is not None
    
    def generate(
        self,
        context: 'RepairContext',
        num_candidates: int = 5
    ) -> List[RepairCandidate]:
        """Generate repair candidates using transformer model."""
        start_time = time.time()
        
        if not self.is_available():
            return []
        
        # Format prompt
        prompt = self._format_prompt(context)
        
        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            max_length=self.config.max_length,
            truncation=True
        )
        
        # Move to device
        if self.config.device != "cpu" and hasattr(self.model, 'device'):
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        # Generate
        outputs = self.model.generate(
            **inputs,
            max_length=self.config.max_length,
            num_beams=self.config.num_beams,
            num_return_sequences=min(num_candidates, self.config.num_beams),
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            do_sample=True,
        )
        
        # Decode
        candidates = []
        for i, output in enumerate(outputs):
            decoded = self.tokenizer.decode(output, skip_special_tokens=True)
            repaired_code = self._extract_code(decoded, context.original_code)
            
            candidates.append(RepairCandidate(
                code=repaired_code,
                confidence=1.0 - (i * 0.1),  # Decreasing confidence by rank
                strategy="transformer_generation",
                metadata={"beam_rank": i}
            ))
        
        # Update stats
        self.stats["total_repairs"] += 1
        self.stats["total_time_ms"] += (time.time() - start_time) * 1000
        
        return candidates[:num_candidates]
    
    def _format_prompt(self, context: 'RepairContext') -> str:
        """Format repair context into prompt."""
        return f"""Fix the following code based on the counterexample.

Original code with bug:
{context.original_code}

Counterexample:
- Input: {context.counterexample.input_values if context.counterexample else 'N/A'}
- Expected: {context.counterexample.expected_output if context.counterexample else 'N/A'}
- Actual: {context.counterexample.actual_output if context.counterexample else 'N/A'}

Fault location: line {context.fault_location.line if context.fault_location else 'unknown'}

Fixed code:
"""
    
    def _extract_code(self, generated: str, original: str) -> str:
        """Extract code from generated text."""
        # Try to extract code block
        code_match = re.search(r'```(?:cpp|c\+\+)?\n?(.*?)```', generated, re.DOTALL)
        if code_match:
            return code_match.group(1).strip()
        
        # Otherwise, return generated text if it looks like code
        if '{' in generated and '}' in generated:
            return generated.strip()
        
        # Fallback to original
        return original


class APIRepairModel(BaseRepairModel):
    """
    API-based repair model (e.g., OpenAI GPT).
    
    Uses external API for code repair generation.
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        super().__init__(config)
        self.client = None
        self._setup_client()
    
    def _setup_client(self) -> None:
        """Set up API client."""
        if not self.config.api_key:
            self.config.api_key = os.environ.get("OPENAI_API_KEY")
        
        if self.config.api_key:
            try:
                import openai
                self.client = openai.OpenAI(
                    api_key=self.config.api_key,
                    base_url=self.config.api_base
                )
            except ImportError:
                self.client = None
    
    def is_available(self) -> bool:
        return self.client is not None and self.config.api_key is not None
    
    def generate(
        self,
        context: 'RepairContext',
        num_candidates: int = 5
    ) -> List[RepairCandidate]:
        """Generate repair candidates using API."""
        if not self.is_available():
            return []
        
        start_time = time.time()
        
        prompt = self._format_prompt(context)
        
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[
                    {"role": "system", "content": "You are a code repair assistant. Fix bugs in compiler backend code."},
                    {"role": "user", "content": prompt}
                ],
                n=num_candidates,
                temperature=self.config.temperature,
                max_tokens=self.config.max_length,
            )
            
            candidates = []
            for i, choice in enumerate(response.choices):
                code = self._extract_code(choice.message.content, context.original_code)
                candidates.append(RepairCandidate(
                    code=code,
                    confidence=1.0 - (i * 0.1),
                    strategy="api_generation",
                    metadata={"choice_index": i}
                ))
            
            self.stats["total_repairs"] += 1
            self.stats["total_time_ms"] += (time.time() - start_time) * 1000
            
            return candidates
        
        except Exception as e:
            print(f"API error: {e}")
            return []
    
    def _format_prompt(self, context: 'RepairContext') -> str:
        """Format repair context into API prompt."""
        return f"""Fix the bug in this C++ compiler backend code.

## Original Code
```cpp
{context.original_code}
```

## Counterexample (inputs that cause wrong behavior)
- Input values: {context.counterexample.input_values if context.counterexample else 'Unknown'}
- Expected output: {context.counterexample.expected_output if context.counterexample else 'Unknown'}
- Actual output: {context.counterexample.actual_output if context.counterexample else 'Unknown'}

## Fault Location
Line {context.fault_location.line if context.fault_location else 'Unknown'}: {context.fault_location.statement if context.fault_location else 'Unknown'}

## Task
Provide the corrected code that fixes the bug. Only output the fixed code, no explanations.

```cpp
"""
    
    def _extract_code(self, response: str, original: str) -> str:
        """Extract code from API response."""
        # Try to find code block
        code_match = re.search(r'```(?:cpp|c\+\+)?\n?(.*?)```', response, re.DOTALL)
        if code_match:
            return code_match.group(1).strip()
        
        # Clean up response
        code = response.strip()
        if code.startswith('```'):
            code = code[3:]
        if code.endswith('```'):
            code = code[:-3]
        
        return code.strip() if code else original


class _RuleBasedWrapper(BaseRepairModel):
    """
    Wrapper to make RuleBasedRepairModel compatible with BaseRepairModel interface.
    """
    
    def __init__(self, rule_model):
        super().__init__()
        self.rule_model = rule_model
    
    def is_available(self) -> bool:
        return True
    
    def generate(
        self,
        context: 'RepairContext',
        num_candidates: int = 5
    ) -> List[RepairCandidate]:
        """Generate repair candidates using rule-based model."""
        # Use the generate method which returns strings
        code_candidates = self.rule_model.generate(context, num_candidates)
        
        # Convert to RepairCandidate objects
        candidates = []
        for i, code in enumerate(code_candidates):
            candidates.append(RepairCandidate(
                code=code,
                confidence=0.6 - (i * 0.05),
                strategy=f"rule_based:{self.rule_model.last_strategy}",
                metadata={"source": "rule_based"}
            ))
        
        return candidates


class HybridRepairModel(BaseRepairModel):
    """
    Hybrid repair model combining multiple approaches.
    
    Tries models in order of preference and combines results.
    """
    
    def __init__(
        self,
        config: Optional[ModelConfig] = None,
        models: Optional[List[BaseRepairModel]] = None
    ):
        super().__init__(config)
        
        if models:
            self.models = models
        else:
            # Default model chain
            self.models = []
            
            # Try to add transformer model
            try:
                transformer = TransformerRepairModel(config)
                if transformer.is_available():
                    self.models.append(transformer)
            except Exception:
                pass
            
            # Add rule-based as fallback
            from .repair_model import RuleBasedRepairModel
            rule_model = RuleBasedRepairModel()
            self.models.append(_RuleBasedWrapper(rule_model))
    
    def is_available(self) -> bool:
        return len(self.models) > 0 and any(m.is_available() for m in self.models)
    
    def generate(
        self,
        context: 'RepairContext',
        num_candidates: int = 5
    ) -> List[RepairCandidate]:
        """Generate candidates using available models."""
        all_candidates = []
        
        for model in self.models:
            if not model.is_available():
                continue
            
            try:
                candidates = model.generate(context, num_candidates)
                all_candidates.extend(candidates)
                
                if len(all_candidates) >= num_candidates:
                    break
            except Exception as e:
                print(f"Model {model.__class__.__name__} failed: {e}")
                continue
        
        # Sort by confidence and deduplicate
        seen_codes = set()
        unique_candidates = []
        
        for c in sorted(all_candidates, key=lambda x: -x.confidence):
            if c.code not in seen_codes:
                seen_codes.add(c.code)
                unique_candidates.append(c)
        
        return unique_candidates[:num_candidates]


def create_repair_model(
    backend: ModelBackend = ModelBackend.RULE_BASED,
    config: Optional[ModelConfig] = None
) -> BaseRepairModel:
    """
    Factory function to create a repair model.
    
    Args:
        backend: Model backend to use
        config: Optional configuration
        
    Returns:
        Repair model instance
    """
    if config is None:
        config = ModelConfig(backend=backend)
    else:
        config.backend = backend
    
    if backend == ModelBackend.HUGGINGFACE:
        return TransformerRepairModel(config)
    elif backend == ModelBackend.OPENAI:
        return APIRepairModel(config)
    elif backend == ModelBackend.LOCAL:
        return TransformerRepairModel(config)
    else:
        from .repair_model import RuleBasedRepairModel
        rule_model = RuleBasedRepairModel()
        return _RuleBasedWrapper(rule_model)
