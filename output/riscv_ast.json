{
  "backend": "RISCV",
  "functions": [
    {
      "name": "expandFunctionCall",
      "return_type": "void",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "SmallVectorImpl<char>",
          "name": "&CB"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  MCInst TmpInst;\n  MCOperand Func;\n  MCRegister Ra;\n  if (MI.getOpcode() == RISCV::PseudoTAIL) {\n    Func = MI.getOperand(0);\n    Ra = RISCV::X6;\n  } else if (MI.getOpcode() == RISCV::PseudoCALLReg) {\n    Func = MI.getOperand(1);\n    Ra = MI.getOperand(0).getReg();\n  } else if (MI.getOpcode() == RISCV::PseudoCALL) {\n    Func = MI.getOperand(0);\n    Ra = RISCV::X1;\n  } else if (MI.getOpcode() == RISCV::PseudoJump) {\n    Func = MI.getOperand(1);\n    Ra = MI.getOperand(0).getReg();\n  }\n  uint32_t Binary;\n\n  assert(Func.isExpr() && \"Expected expression\");\n\n  const MCExpr *CallExpr = Func.getExpr();\n\n  // Emit AUIPC Ra, Func with R_RISCV_CALL relocation type.\n  TmpInst = MCInstBuilder(RISCV::AUIPC).addReg(Ra).addExpr(CallExpr);\n  Binary = getBinaryCodeForInstr(TmpInst, Fixups, STI);\n  support::endian::write(CB, Binary, llvm::endianness::little);\n\n  if (MI.getOpcode() == RISCV::PseudoTAIL ||\n      MI.getOpcode() == RISCV::PseudoJump)\n    // Emit JALR X0, Ra, 0\n    TmpInst = MCInstBuilder(RISCV::JALR).addReg(RISCV::X0).addReg(Ra).addImm(0);\n  else\n    // Emit JALR Ra, Ra, 0\n    TmpInst = MCInstBuilder(RISCV::JALR).addReg(Ra).addReg(Ra).addImm(0);\n  Binary = getBinaryCodeForInstr(TmpInst, Fixups, STI);\n  support::endian::write(CB, Binary, llvm::endianness::little);\n}",
      "start_line": 119,
      "end_line": 159,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getExpr",
        "addReg",
        "getOpcode",
        "getOperand",
        "addExpr",
        "write",
        "getReg",
        "addImm",
        "getBinaryCodeForInstr",
        "MCInstBuilder"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "expandTLSDESCCall",
      "return_type": "void",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "SmallVectorImpl<char>",
          "name": "&CB"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  MCOperand SrcSymbol = MI.getOperand(3);\n  assert(SrcSymbol.isExpr() &&\n         \"Expected expression as first input to TLSDESCCALL\");\n  const RISCVMCExpr *Expr = dyn_cast<RISCVMCExpr>(SrcSymbol.getExpr());\n  MCRegister Link = MI.getOperand(0).getReg();\n  MCRegister Dest = MI.getOperand(1).getReg();\n  MCRegister Imm = MI.getOperand(2).getImm();\n  Fixups.push_back(MCFixup::create(\n      0, Expr, MCFixupKind(RISCV::fixup_riscv_tlsdesc_call), MI.getLoc()));\n  MCInst Call =\n      MCInstBuilder(RISCV::JALR).addReg(Link).addReg(Dest).addImm(Imm);\n\n  uint32_t Binary = getBinaryCodeForInstr(Call, Fixups, STI);\n  support::endian::write(CB, Binary, llvm::endianness::little);\n}",
      "start_line": 161,
      "end_line": 179,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getImm",
        "getExpr",
        "getLoc",
        "addReg",
        "getOperand",
        "write",
        "getReg",
        "addImm",
        "getBinaryCodeForInstr",
        "push_back",
        "MCInstBuilder"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "expandAddTPRel",
      "return_type": "void",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "SmallVectorImpl<char>",
          "name": "&CB"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  MCOperand DestReg = MI.getOperand(0);\n  MCOperand SrcReg = MI.getOperand(1);\n  MCOperand TPReg = MI.getOperand(2);\n  assert(TPReg.isReg() && TPReg.getReg() == RISCV::X4 &&\n         \"Expected thread pointer as second input to TP-relative add\");\n\n  MCOperand SrcSymbol = MI.getOperand(3);\n  assert(SrcSymbol.isExpr() &&\n         \"Expected expression as third input to TP-relative add\");\n\n  const RISCVMCExpr *Expr = dyn_cast<RISCVMCExpr>(SrcSymbol.getExpr());\n  assert(Expr && Expr->getKind() == RISCVMCExpr::VK_RISCV_TPREL_ADD &&\n         \"Expected tprel_add relocation on TP-relative symbol\");\n\n  // Emit the correct tprel_add relocation for the symbol.\n  Fixups.push_back(MCFixup::create(\n      0, Expr, MCFixupKind(RISCV::fixup_riscv_tprel_add), MI.getLoc()));\n\n  // Emit fixup_riscv_relax for tprel_add where the relax feature is enabled.\n  if (STI.hasFeature(RISCV::FeatureRelax)) {\n    const MCConstantExpr *Dummy = MCConstantExpr::create(0, Ctx);\n    Fixups.push_back(MCFixup::create(\n        0, Dummy, MCFixupKind(RISCV::fixup_riscv_relax), MI.getLoc()));\n  }\n\n  // Emit a normal ADD instruction with the given operands.\n  MCInst TmpInst = MCInstBuilder(RISCV::ADD)\n                       .addOperand(DestReg)\n                       .addOperand(SrcReg)\n                       .addOperand(TPReg);\n  uint32_t Binary = getBinaryCodeForInstr(TmpInst, Fixups, STI);\n  support::endian::write(CB, Binary, llvm::endianness::little);\n}",
      "start_line": 182,
      "end_line": 218,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getExpr",
        "addOperand",
        "getLoc",
        "create",
        "getOperand",
        "write",
        "getReg",
        "getBinaryCodeForInstr",
        "push_back",
        "MCInstBuilder"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getInvertedBranchOp",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "BrOp"
        }
      ],
      "body": "{\n  switch (BrOp) {\n  default:\n    llvm_unreachable(\"Unexpected branch opcode!\");\n  case RISCV::PseudoLongBEQ:\n    return RISCV::BNE;\n  case RISCV::PseudoLongBNE:\n    return RISCV::BEQ;\n  case RISCV::PseudoLongBLT:\n    return RISCV::BGE;\n  case RISCV::PseudoLongBGE:\n    return RISCV::BLT;\n  case RISCV::PseudoLongBLTU:\n    return RISCV::BGEU;\n  case RISCV::PseudoLongBGEU:\n    return RISCV::BLTU;\n  }\n}",
      "start_line": 220,
      "end_line": 237,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getInvertedBranchOp",
          "condition": "BrOp",
          "cases": [
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "expandLongCondBr",
      "return_type": "void",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "SmallVectorImpl<char>",
          "name": "&CB"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  MCRegister SrcReg1 = MI.getOperand(0).getReg();\n  MCRegister SrcReg2 = MI.getOperand(1).getReg();\n  MCOperand SrcSymbol = MI.getOperand(2);\n  unsigned Opcode = MI.getOpcode();\n  bool IsEqTest =\n      Opcode == RISCV::PseudoLongBNE || Opcode == RISCV::PseudoLongBEQ;\n\n  bool UseCompressedBr = false;\n  if (IsEqTest && (STI.hasFeature(RISCV::FeatureStdExtC) ||\n                   STI.hasFeature(RISCV::FeatureStdExtZca))) {\n    if (RISCV::X8 <= SrcReg1.id() && SrcReg1.id() <= RISCV::X15 &&\n        SrcReg2.id() == RISCV::X0) {\n      UseCompressedBr = true;\n    } else if (RISCV::X8 <= SrcReg2.id() && SrcReg2.id() <= RISCV::X15 &&\n               SrcReg1.id() == RISCV::X0) {\n      std::swap(SrcReg1, SrcReg2);\n      UseCompressedBr = true;\n    }\n  }\n\n  uint32_t Offset;\n  if (UseCompressedBr) {\n    unsigned InvOpc =\n        Opcode == RISCV::PseudoLongBNE ? RISCV::C_BEQZ : RISCV::C_BNEZ;\n    MCInst TmpInst = MCInstBuilder(InvOpc).addReg(SrcReg1).addImm(6);\n    uint16_t Binary = getBinaryCodeForInstr(TmpInst, Fixups, STI);\n    support::endian::write<uint16_t>(CB, Binary, llvm::endianness::little);\n    Offset = 2;\n  } else {\n    unsigned InvOpc = getInvertedBranchOp(Opcode);\n    MCInst TmpInst =\n        MCInstBuilder(InvOpc).addReg(SrcReg1).addReg(SrcReg2).addImm(8);\n    uint32_t Binary = getBinaryCodeForInstr(TmpInst, Fixups, STI);\n    support::endian::write(CB, Binary, llvm::endianness::little);\n    Offset = 4;\n  }\n\n  // Emit an unconditional jump to the destination.\n  MCInst TmpInst =\n      MCInstBuilder(RISCV::JAL).addReg(RISCV::X0).addOperand(SrcSymbol);\n  uint32_t Binary = getBinaryCodeForInstr(TmpInst, Fixups, STI);\n  support::endian::write(CB, Binary, llvm::endianness::little);\n\n  Fixups.clear();\n  if (SrcSymbol.isExpr()) {\n    Fixups.push_back(MCFixup::create(Offset, SrcSymbol.getExpr(),\n                                     MCFixupKind(RISCV::fixup_riscv_jal),\n                                     MI.getLoc()));\n  }\n}",
      "start_line": 241,
      "end_line": 294,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "clear",
        "MCFixupKind",
        "swap",
        "addOperand",
        "getLoc",
        "getOpcode",
        "addReg",
        "getInvertedBranchOp",
        "getOperand",
        "write",
        "MCInstBuilder",
        "id",
        "getReg",
        "addImm",
        "getBinaryCodeForInstr",
        "push_back",
        "hasFeature"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "encodeInstruction",
      "return_type": "void",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "SmallVectorImpl<char>",
          "name": "&CB"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  const MCInstrDesc &Desc = MCII.get(MI.getOpcode());\n  // Get byte count of instruction.\n  unsigned Size = Desc.getSize();\n\n  // RISCVInstrInfo::getInstSizeInBytes expects that the total size of the\n  // expanded instructions for each pseudo is correct in the Size field of the\n  // tablegen definition for the pseudo.\n  switch (MI.getOpcode()) {\n  default:\n    break;\n  case RISCV::PseudoCALLReg:\n  case RISCV::PseudoCALL:\n  case RISCV::PseudoTAIL:\n  case RISCV::PseudoJump:\n    expandFunctionCall(MI, CB, Fixups, STI);\n    MCNumEmitted += 2;\n    return;\n  case RISCV::PseudoAddTPRel:\n    expandAddTPRel(MI, CB, Fixups, STI);\n    MCNumEmitted += 1;\n    return;\n  case RISCV::PseudoLongBEQ:\n  case RISCV::PseudoLongBNE:\n  case RISCV::PseudoLongBLT:\n  case RISCV::PseudoLongBGE:\n  case RISCV::PseudoLongBLTU:\n  case RISCV::PseudoLongBGEU:\n    expandLongCondBr(MI, CB, Fixups, STI);\n    MCNumEmitted += 2;\n    return;\n  case RISCV::PseudoTLSDESCCall:\n    expandTLSDESCCall(MI, CB, Fixups, STI);\n    MCNumEmitted += 1;\n    return;\n  }\n\n  switch (Size) {\n  default:\n    llvm_unreachable(\"Unhandled encodeInstruction length!\");\n  case 2: {\n    uint16_t Bits = getBinaryCodeForInstr(MI, Fixups, STI);\n    support::endian::write<uint16_t>(CB, Bits, llvm::endianness::little);\n    break;\n  }\n  case 4: {\n    uint32_t Bits = getBinaryCodeForInstr(MI, Fixups, STI);\n    support::endian::write(CB, Bits, llvm::endianness::little);\n    break;\n  }\n  }\n\n  ++MCNumEmitted; // Keep track of the # of mi's emitted.\n}",
      "start_line": 296,
      "end_line": 352,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "encodeInstruction",
          "condition": "Size",
          "cases": [
            {
              "label": "2",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "4",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getSize",
        "expandFunctionCall",
        "expandTLSDESCCall",
        "expandLongCondBr",
        "get",
        "expandAddTPRel",
        "write",
        "getBinaryCodeForInstr",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getMachineOpValue",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "const MCOperand",
          "name": "&MO"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n\n  if (MO.isReg())\n    return Ctx.getRegisterInfo()->getEncodingValue(MO.getReg());\n\n  if (MO.isImm())\n    return static_cast<unsigned>(MO.getImm());\n\n  llvm_unreachable(\"Unhandled expression!\");\n  return 0;\n}",
      "start_line": 354,
      "end_line": 367,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getImm",
        "getRegisterInfo",
        "llvm_unreachable",
        "getEncodingValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getImmOpValueAsr1",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "unsigned",
          "name": "OpNo"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  const MCOperand &MO = MI.getOperand(OpNo);\n\n  if (MO.isImm()) {\n    unsigned Res = MO.getImm();\n    assert((Res & 1) == 0 && \"LSB is non-zero\");\n    return Res >> 1;\n  }\n\n  return getImmOpValue(MI, OpNo, Fixups, STI);\n}",
      "start_line": 369,
      "end_line": 382,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand",
        "getImm",
        "getImmOpValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getImmOpValue",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "unsigned",
          "name": "OpNo"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  bool EnableRelax = STI.hasFeature(RISCV::FeatureRelax);\n  const MCOperand &MO = MI.getOperand(OpNo);\n\n  MCInstrDesc const &Desc = MCII.get(MI.getOpcode());\n  unsigned MIFrm = RISCVII::getFormat(Desc.TSFlags);\n\n  // If the destination is an immediate, there is nothing to do.\n  if (MO.isImm())\n    return MO.getImm();\n\n  assert(MO.isExpr() &&\n         \"getImmOpValue expects only expressions or immediates\");\n  const MCExpr *Expr = MO.getExpr();\n  MCExpr::ExprKind Kind = Expr->getKind();\n  RISCV::Fixups FixupKind = RISCV::fixup_riscv_invalid;\n  bool RelaxCandidate = false;\n  if (Kind == MCExpr::Target) {\n    const RISCVMCExpr *RVExpr = cast<RISCVMCExpr>(Expr);\n\n    switch (RVExpr->getKind()) {\n    case RISCVMCExpr::VK_RISCV_None:\n    case RISCVMCExpr::VK_RISCV_Invalid:\n    case RISCVMCExpr::VK_RISCV_32_PCREL:\n      llvm_unreachable(\"Unhandled fixup kind!\");\n    case RISCVMCExpr::VK_RISCV_TPREL_ADD:\n      // tprel_add is only used to indicate that a relocation should be emitted\n      // for an add instruction used in TP-relative addressing. It should not be\n      // expanded as if representing an actual instruction operand and so to\n      // encounter it here is an error.\n      llvm_unreachable(\n          \"VK_RISCV_TPREL_ADD should not represent an instruction operand\");\n    case RISCVMCExpr::VK_RISCV_LO:\n      if (MIFrm == RISCVII::InstFormatI)\n        FixupKind = RISCV::fixup_riscv_lo12_i;\n      else if (MIFrm == RISCVII::InstFormatS)\n        FixupKind = RISCV::fixup_riscv_lo12_s;\n      else\n        llvm_unreachable(\"VK_RISCV_LO used with unexpected instruction format\");\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_HI:\n      FixupKind = RISCV::fixup_riscv_hi20;\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_PCREL_LO:\n      if (MIFrm == RISCVII::InstFormatI)\n        FixupKind = RISCV::fixup_riscv_pcrel_lo12_i;\n      else if (MIFrm == RISCVII::InstFormatS)\n        FixupKind = RISCV::fixup_riscv_pcrel_lo12_s;\n      else\n        llvm_unreachable(\n            \"VK_RISCV_PCREL_LO used with unexpected instruction format\");\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_PCREL_HI:\n      FixupKind = RISCV::fixup_riscv_pcrel_hi20;\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_GOT_HI:\n      FixupKind = RISCV::fixup_riscv_got_hi20;\n      break;\n    case RISCVMCExpr::VK_RISCV_TPREL_LO:\n      if (MIFrm == RISCVII::InstFormatI)\n        FixupKind = RISCV::fixup_riscv_tprel_lo12_i;\n      else if (MIFrm == RISCVII::InstFormatS)\n        FixupKind = RISCV::fixup_riscv_tprel_lo12_s;\n      else\n        llvm_unreachable(\n            \"VK_RISCV_TPREL_LO used with unexpected instruction format\");\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_TPREL_HI:\n      FixupKind = RISCV::fixup_riscv_tprel_hi20;\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_TLS_GOT_HI:\n      FixupKind = RISCV::fixup_riscv_tls_got_hi20;\n      break;\n    case RISCVMCExpr::VK_RISCV_TLS_GD_HI:\n      FixupKind = RISCV::fixup_riscv_tls_gd_hi20;\n      break;\n    case RISCVMCExpr::VK_RISCV_CALL:\n      FixupKind = RISCV::fixup_riscv_call;\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_CALL_PLT:\n      FixupKind = RISCV::fixup_riscv_call_plt;\n      RelaxCandidate = true;\n      break;\n    case RISCVMCExpr::VK_RISCV_TLSDESC_HI:\n      FixupKind = RISCV::fixup_riscv_tlsdesc_hi20;\n      break;\n    case RISCVMCExpr::VK_RISCV_TLSDESC_LOAD_LO:\n      FixupKind = RISCV::fixup_riscv_tlsdesc_load_lo12;\n      break;\n    case RISCVMCExpr::VK_RISCV_TLSDESC_ADD_LO:\n      FixupKind = RISCV::fixup_riscv_tlsdesc_add_lo12;\n      break;\n    case RISCVMCExpr::VK_RISCV_TLSDESC_CALL:\n      FixupKind = RISCV::fixup_riscv_tlsdesc_call;\n      break;\n    }\n  } else if ((Kind == MCExpr::SymbolRef &&\n                 cast<MCSymbolRefExpr>(Expr)->getKind() ==\n                     MCSymbolRefExpr::VK_None) ||\n             Kind == MCExpr::Binary) {\n    // FIXME: Sub kind binary exprs have chance of underflow.\n    if (MIFrm == RISCVII::InstFormatJ) {\n      FixupKind = RISCV::fixup_riscv_jal;\n    } else if (MIFrm == RISCVII::InstFormatB) {\n      FixupKind = RISCV::fixup_riscv_branch;\n    } else if (MIFrm == RISCVII::InstFormatCJ) {\n      FixupKind = RISCV::fixup_riscv_rvc_jump;\n    } else if (MIFrm == RISCVII::InstFormatCB) {\n      FixupKind = RISCV::fixup_riscv_rvc_branch;\n    } else if (MIFrm == RISCVII::InstFormatI) {\n      FixupKind = RISCV::fixup_riscv_12_i;\n    }\n  }\n\n  assert(FixupKind != RISCV::fixup_riscv_invalid && \"Unhandled expression!\");\n\n  Fixups.push_back(\n      MCFixup::create(0, Expr, MCFixupKind(FixupKind), MI.getLoc()));\n  ++MCNumFixups;\n\n  // Ensure an R_RISCV_RELAX relocation will be emitted if linker relaxation is\n  // enabled and the current fixup will result in a relocation that may be\n  // relaxed.\n  if (EnableRelax && RelaxCandidate) {\n    const MCConstantExpr *Dummy = MCConstantExpr::create(0, Ctx);\n    Fixups.push_back(\n    MCFixup::create(0, Dummy, MCFixupKind(RISCV::fixup_riscv_relax),\n                    MI.getLoc()));\n    ++MCNumFixups;\n  }\n\n  return 0;\n}",
      "start_line": 384,
      "end_line": 525,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getImm",
        "getFormat",
        "getExpr",
        "getLoc",
        "create",
        "get",
        "getKind",
        "getOperand",
        "llvm_unreachable",
        "push_back",
        "hasFeature"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "MIFrm ==",
          "name": "RISCVII::InstFormatB"
        }
      ],
      "body": "{\n      FixupKind = RISCV::fixup_riscv_branch;\n    }",
      "start_line": 496,
      "end_line": 498,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "MIFrm ==",
          "name": "RISCVII::InstFormatCJ"
        }
      ],
      "body": "{\n      FixupKind = RISCV::fixup_riscv_rvc_jump;\n    }",
      "start_line": 498,
      "end_line": 500,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "MIFrm ==",
          "name": "RISCVII::InstFormatCB"
        }
      ],
      "body": "{\n      FixupKind = RISCV::fixup_riscv_rvc_branch;\n    }",
      "start_line": 500,
      "end_line": 502,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "MIFrm ==",
          "name": "RISCVII::InstFormatI"
        }
      ],
      "body": "{\n      FixupKind = RISCV::fixup_riscv_12_i;\n    }",
      "start_line": 502,
      "end_line": 504,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVMaskReg",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "unsigned",
          "name": "OpNo"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  MCOperand MO = MI.getOperand(OpNo);\n  assert(MO.isReg() && \"Expected a register.\");\n\n  switch (MO.getReg()) {\n  default:\n    llvm_unreachable(\"Invalid mask register.\");\n  case RISCV::V0:\n    return 0;\n  case RISCV::NoRegister:\n    return 1;\n  }\n}",
      "start_line": 527,
      "end_line": 541,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRlistOpValue",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "unsigned",
          "name": "OpNo"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  const MCOperand &MO = MI.getOperand(OpNo);\n  assert(MO.isImm() && \"Rlist operand must be immediate\");\n  auto Imm = MO.getImm();\n  assert(Imm >= 4 && \"EABI is currently not implemented\");\n  return Imm;\n}",
      "start_line": 543,
      "end_line": 551,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand",
        "getImm"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRegReg",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&MI"
        },
        {
          "type": "unsigned",
          "name": "OpNo"
        },
        {
          "type": "SmallVectorImpl<MCFixup>",
          "name": "&Fixups"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  const MCOperand &MO = MI.getOperand(OpNo);\n  const MCOperand &MO1 = MI.getOperand(OpNo + 1);\n  assert(MO.isReg() && MO1.isReg() && \"Expected registers.\");\n\n  unsigned Op = Ctx.getRegisterInfo()->getEncodingValue(MO.getReg());\n  unsigned Op1 = Ctx.getRegisterInfo()->getEncodingValue(MO1.getReg());\n\n  return Op | Op1 << 5;\n}",
      "start_line": 553,
      "end_line": 564,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand",
        "getRegisterInfo",
        "isReg",
        "getEncodingValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getFixupKindInfo",
      "return_type": "MCFixupKindInfo &",
      "parameters": [
        {
          "type": "MCFixupKind",
          "name": "Kind"
        }
      ],
      "body": "{\n  const static MCFixupKindInfo Infos[] = {\n      // This table *must* be in the order that the fixup_* kinds are defined in\n      // RISCVFixupKinds.h.\n      //\n      // name                      offset bits  flags\n      {\"fixup_riscv_hi20\", 12, 20, 0},\n      {\"fixup_riscv_lo12_i\", 20, 12, 0},\n      {\"fixup_riscv_12_i\", 20, 12, 0},\n      {\"fixup_riscv_lo12_s\", 0, 32, 0},\n      {\"fixup_riscv_pcrel_hi20\", 12, 20,\n       MCFixupKindInfo::FKF_IsPCRel | MCFixupKindInfo::FKF_IsTarget},\n      {\"fixup_riscv_pcrel_lo12_i\", 20, 12,\n       MCFixupKindInfo::FKF_IsPCRel | MCFixupKindInfo::FKF_IsTarget},\n      {\"fixup_riscv_pcrel_lo12_s\", 0, 32,\n       MCFixupKindInfo::FKF_IsPCRel | MCFixupKindInfo::FKF_IsTarget},\n      {\"fixup_riscv_got_hi20\", 12, 20, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_tprel_hi20\", 12, 20, 0},\n      {\"fixup_riscv_tprel_lo12_i\", 20, 12, 0},\n      {\"fixup_riscv_tprel_lo12_s\", 0, 32, 0},\n      {\"fixup_riscv_tprel_add\", 0, 0, 0},\n      {\"fixup_riscv_tls_got_hi20\", 12, 20, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_tls_gd_hi20\", 12, 20, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_jal\", 12, 20, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_branch\", 0, 32, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_rvc_jump\", 2, 11, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_rvc_branch\", 0, 16, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_call\", 0, 64, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_call_plt\", 0, 64, MCFixupKindInfo::FKF_IsPCRel},\n      {\"fixup_riscv_relax\", 0, 0, 0},\n      {\"fixup_riscv_align\", 0, 0, 0},\n\n      {\"fixup_riscv_tlsdesc_hi20\", 12, 20,\n       MCFixupKindInfo::FKF_IsPCRel | MCFixupKindInfo::FKF_IsTarget},\n      {\"fixup_riscv_tlsdesc_load_lo12\", 20, 12, 0},\n      {\"fixup_riscv_tlsdesc_add_lo12\", 20, 12, 0},\n      {\"fixup_riscv_tlsdesc_call\", 0, 0, 0},\n  };\n  static_assert((std::size(Infos)) == RISCV::NumTargetFixupKinds,\n                \"Not all fixup kinds added to Infos array\");\n\n  // Fixup kinds from .reloc directive are like R_RISCV_NONE. They\n  // do not require any extra processing.\n  if (Kind >= FirstLiteralRelocationKind)\n    return MCAsmBackend::getFixupKindInfo(FK_NONE);\n\n  if (Kind < FirstTargetFixupKind)\n    return MCAsmBackend::getFixupKindInfo(Kind);\n\n  assert(unsigned(Kind - FirstTargetFixupKind) < getNumFixupKinds() &&\n         \"Invalid kind!\");\n  return Infos[Kind - FirstTargetFixupKind];\n}",
      "start_line": 57,
      "end_line": 110,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getFixupKindInfo",
        "static_assert",
        "getNumFixupKinds"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldForceRelocation",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MCAssembler",
          "name": "&Asm"
        },
        {
          "type": "const MCFixup",
          "name": "&Fixup"
        },
        {
          "type": "const MCValue",
          "name": "&Target"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "*STI"
        }
      ],
      "body": "{\n  if (Fixup.getKind() >= FirstLiteralRelocationKind)\n    return true;\n  switch (Fixup.getTargetKind()) {\n  default:\n    break;\n  case FK_Data_1:\n  case FK_Data_2:\n  case FK_Data_4:\n  case FK_Data_8:\n  case FK_Data_leb128:\n    if (Target.isAbsolute())\n      return false;\n    break;\n  case RISCV::fixup_riscv_got_hi20:\n  case RISCV::fixup_riscv_tls_got_hi20:\n  case RISCV::fixup_riscv_tls_gd_hi20:\n  case RISCV::fixup_riscv_tlsdesc_hi20:\n    return true;\n  }\n\n  return STI->hasFeature(RISCV::FeatureRelax) || ForceRelocs;\n}",
      "start_line": 115,
      "end_line": 140,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasFeature"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "fixupNeedsRelaxationAdvanced",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MCFixup",
          "name": "&Fixup"
        },
        {
          "type": "bool",
          "name": "Resolved"
        },
        {
          "type": "uint64_t",
          "name": "Value"
        },
        {
          "type": "const MCRelaxableFragment",
          "name": "*DF"
        },
        {
          "type": "const MCAsmLayout",
          "name": "&Layout"
        },
        {
          "type": "const bool",
          "name": "WasForced"
        }
      ],
      "body": "{\n  if (!RelaxBranches)\n    return false;\n\n  int64_t Offset = int64_t(Value);\n  unsigned Kind = Fixup.getTargetKind();\n\n  // Return true if the symbol is actually unresolved.\n  // Resolved could be always false when shouldForceRelocation return true.\n  // We use !WasForced to indicate that the symbol is unresolved and not forced\n  // by shouldForceRelocation.\n  if (!Resolved && !WasForced)\n    return true;\n\n  switch (Kind) {\n  default:\n    return false;\n  case RISCV::fixup_riscv_rvc_branch:\n    // For compressed branch instructions the immediate must be\n    // in the range [-256, 254].\n    return Offset > 254 || Offset < -256;\n  case RISCV::fixup_riscv_rvc_jump:\n    // For compressed jump instructions the immediate must be\n    // in the range [-2048, 2046].\n    return Offset > 2046 || Offset < -2048;\n  case RISCV::fixup_riscv_branch:\n    // For conditional branch instructions the immediate must be\n    // in the range [-4096, 4095].\n    return !isInt<13>(Offset);\n  }\n}",
      "start_line": 142,
      "end_line": 177,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "fixupNeedsRelaxationAdvanced",
          "condition": "Kind",
          "cases": [
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "false"
        }
      ],
      "calls": [
        "int64_t",
        "getTargetKind"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "relaxInstruction",
      "return_type": "void",
      "parameters": [
        {
          "type": "MCInst",
          "name": "&Inst"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  MCInst Res;\n  switch (Inst.getOpcode()) {\n  default:\n    llvm_unreachable(\"Opcode not expected!\");\n  case RISCV::C_BEQZ:\n  case RISCV::C_BNEZ:\n  case RISCV::C_J:\n  case RISCV::C_JAL: {\n    bool Success = RISCVRVC::uncompress(Res, Inst, STI);\n    assert(Success && \"Can't uncompress instruction\");\n    (void)Success;\n    break;\n  }\n  case RISCV::BEQ:\n  case RISCV::BNE:\n  case RISCV::BLT:\n  case RISCV::BGE:\n  case RISCV::BLTU:\n  case RISCV::BGEU:\n    Res.setOpcode(getRelaxedOpcode(Inst.getOpcode()));\n    Res.addOperand(Inst.getOperand(0));\n    Res.addOperand(Inst.getOperand(1));\n    Res.addOperand(Inst.getOperand(2));\n    break;\n  }\n  Inst = std::move(Res);\n}",
      "start_line": 179,
      "end_line": 207,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "setOpcode",
        "move",
        "addOperand",
        "uncompress",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "relaxDwarfLineAddr",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MCDwarfLineAddrFragment",
          "name": "&DF"
        },
        {
          "type": "MCAsmLayout",
          "name": "&Layout"
        },
        {
          "type": "bool",
          "name": "&WasRelaxed"
        }
      ],
      "body": "{\n  MCContext &C = Layout.getAssembler().getContext();\n\n  int64_t LineDelta = DF.getLineDelta();\n  const MCExpr &AddrDelta = DF.getAddrDelta();\n  SmallVectorImpl<char> &Data = DF.getContents();\n  SmallVectorImpl<MCFixup> &Fixups = DF.getFixups();\n  size_t OldSize = Data.size();\n\n  int64_t Value;\n  bool IsAbsolute = AddrDelta.evaluateKnownAbsolute(Value, Layout);\n  assert(IsAbsolute && \"CFA with invalid expression\");\n  (void)IsAbsolute;\n\n  Data.clear();\n  Fixups.clear();\n  raw_svector_ostream OS(Data);\n\n  // INT64_MAX is a signal that this is actually a DW_LNE_end_sequence.\n  if (LineDelta != INT64_MAX) {\n    OS << uint8_t(dwarf::DW_LNS_advance_line);\n    encodeSLEB128(LineDelta, OS);\n  }\n\n  unsigned Offset;\n  std::pair<MCFixupKind, MCFixupKind> Fixup;\n\n  // According to the DWARF specification, the `DW_LNS_fixed_advance_pc` opcode\n  // takes a single unsigned half (unencoded) operand. The maximum encodable\n  // value is therefore 65535.  Set a conservative upper bound for relaxation.\n  if (Value > 60000) {\n    unsigned PtrSize = C.getAsmInfo()->getCodePointerSize();\n\n    OS << uint8_t(dwarf::DW_LNS_extended_op);\n    encodeULEB128(PtrSize + 1, OS);\n\n    OS << uint8_t(dwarf::DW_LNE_set_address);\n    Offset = OS.tell();\n    assert((PtrSize == 4 || PtrSize == 8) && \"Unexpected pointer size\");\n    Fixup = RISCV::getRelocPairForSize(PtrSize);\n    OS.write_zeros(PtrSize);\n  } else {\n    OS << uint8_t(dwarf::DW_LNS_fixed_advance_pc);\n    Offset = OS.tell();\n    Fixup = RISCV::getRelocPairForSize(2);\n    support::endian::write<uint16_t>(OS, 0, llvm::endianness::little);\n  }\n\n  const MCBinaryExpr &MBE = cast<MCBinaryExpr>(AddrDelta);\n  Fixups.push_back(MCFixup::create(Offset, MBE.getLHS(), std::get<0>(Fixup)));\n  Fixups.push_back(MCFixup::create(Offset, MBE.getRHS(), std::get<1>(Fixup)));\n\n  if (LineDelta == INT64_MAX) {\n    OS << uint8_t(dwarf::DW_LNS_extended_op);\n    OS << uint8_t(1);\n    OS << uint8_t(dwarf::DW_LNE_end_sequence);\n  } else {\n    OS << uint8_t(dwarf::DW_LNS_copy);\n  }\n\n  WasRelaxed = OldSize != Data.size();\n  return true;\n}",
      "start_line": 209,
      "end_line": 273,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "uint8_t",
        "getCodePointerSize",
        "half",
        "encodeULEB128",
        "getContents",
        "size",
        "push_back",
        "encodeSLEB128",
        "getAddrDelta",
        "getFixups",
        "evaluateKnownAbsolute",
        "getAsmInfo",
        "write_zeros",
        "getLineDelta",
        "OS",
        "clear",
        "tell",
        "getAssembler",
        "getRelocPairForSize"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "relaxDwarfCFA",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MCDwarfCallFrameFragment",
          "name": "&DF"
        },
        {
          "type": "MCAsmLayout",
          "name": "&Layout"
        },
        {
          "type": "bool",
          "name": "&WasRelaxed"
        }
      ],
      "body": "{\n  const MCExpr &AddrDelta = DF.getAddrDelta();\n  SmallVectorImpl<char> &Data = DF.getContents();\n  SmallVectorImpl<MCFixup> &Fixups = DF.getFixups();\n  size_t OldSize = Data.size();\n\n  int64_t Value;\n  if (AddrDelta.evaluateAsAbsolute(Value, Layout.getAssembler()))\n    return false;\n  bool IsAbsolute = AddrDelta.evaluateKnownAbsolute(Value, Layout);\n  assert(IsAbsolute && \"CFA with invalid expression\");\n  (void)IsAbsolute;\n\n  Data.clear();\n  Fixups.clear();\n  raw_svector_ostream OS(Data);\n\n  assert(\n      Layout.getAssembler().getContext().getAsmInfo()->getMinInstAlignment() ==\n          1 &&\n      \"expected 1-byte alignment\");\n  if (Value == 0) {\n    WasRelaxed = OldSize != Data.size();\n    return true;\n  }\n\n  auto AddFixups = [&Fixups, &AddrDelta](unsigned Offset,\n                                         std::pair<unsigned, unsigned> Fixup) {\n    const MCBinaryExpr &MBE = cast<MCBinaryExpr>(AddrDelta);\n    Fixups.push_back(\n        MCFixup::create(Offset, MBE.getLHS(),\n                        static_cast<MCFixupKind>(FirstLiteralRelocationKind +\n                                                 std::get<0>(Fixup))));\n    Fixups.push_back(\n        MCFixup::create(Offset, MBE.getRHS(),\n                        static_cast<MCFixupKind>(FirstLiteralRelocationKind +\n                                                 std::get<1>(Fixup))));\n  };\n\n  if (isUIntN(6, Value)) {\n    OS << uint8_t(dwarf::DW_CFA_advance_loc);\n    AddFixups(0, {ELF::R_RISCV_SET6, ELF::R_RISCV_SUB6});\n  } else if (isUInt<8>(Value)) {\n    OS << uint8_t(dwarf::DW_CFA_advance_loc1);\n    support::endian::write<uint8_t>(OS, 0, llvm::endianness::little);\n    AddFixups(1, {ELF::R_RISCV_SET8, ELF::R_RISCV_SUB8});\n  } else if (isUInt<16>(Value)) {\n    OS << uint8_t(dwarf::DW_CFA_advance_loc2);\n    support::endian::write<uint16_t>(OS, 0, llvm::endianness::little);\n    AddFixups(1, {ELF::R_RISCV_SET16, ELF::R_RISCV_SUB16});\n  } else if (isUInt<32>(Value)) {\n    OS << uint8_t(dwarf::DW_CFA_advance_loc4);\n    support::endian::write<uint32_t>(OS, 0, llvm::endianness::little);\n    AddFixups(1, {ELF::R_RISCV_SET32, ELF::R_RISCV_SUB32});\n  } else {\n    llvm_unreachable(\"unsupported CFA encoding\");\n  }\n\n  WasRelaxed = OldSize != Data.size();\n  return true;\n}",
      "start_line": 275,
      "end_line": 337,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "clear",
        "getAddrDelta",
        "getFixups",
        "evaluateKnownAbsolute",
        "getMinInstAlignment",
        "getAsmInfo",
        "AddFixups",
        "uint8_t",
        "getContents",
        "size",
        "llvm_unreachable",
        "push_back",
        "OS"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRelaxedOpcode",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Op"
        }
      ],
      "body": "{\n  switch (Op) {\n  default:\n    return Op;\n  case RISCV::C_BEQZ:\n    return RISCV::BEQ;\n  case RISCV::C_BNEZ:\n    return RISCV::BNE;\n  case RISCV::C_J:\n  case RISCV::C_JAL: // fall through.\n    return RISCV::JAL;\n  case RISCV::BEQ:\n    return RISCV::PseudoLongBEQ;\n  case RISCV::BNE:\n    return RISCV::PseudoLongBNE;\n  case RISCV::BLT:\n    return RISCV::PseudoLongBLT;\n  case RISCV::BGE:\n    return RISCV::PseudoLongBGE;\n  case RISCV::BLTU:\n    return RISCV::PseudoLongBLTU;\n  case RISCV::BGEU:\n    return RISCV::PseudoLongBGEU;\n  }\n}",
      "start_line": 354,
      "end_line": 378,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "getRelaxedOpcode",
          "condition": "Op",
          "cases": [
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "Op"
        }
      ],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "mayNeedRelaxation",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MCInst",
          "name": "&Inst"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "&STI"
        }
      ],
      "body": "{\n  return getRelaxedOpcode(Inst.getOpcode()) != Inst.getOpcode();\n}",
      "start_line": 380,
      "end_line": 383,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getRelaxedOpcode",
        "getOpcode"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "writeNopData",
      "return_type": "bool",
      "parameters": [
        {
          "type": "raw_ostream",
          "name": "&OS"
        },
        {
          "type": "uint64_t",
          "name": "Count"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "*STI"
        }
      ],
      "body": "{\n  // We mostly follow binutils' convention here: align to even boundary with a\n  // 0-fill padding.  We emit up to 1 2-byte nop, though we use c.nop if RVC is\n  // enabled or 0-fill otherwise.  The remainder is now padded with 4-byte nops.\n\n  // Instructions always are at even addresses.  We must be in a data area or\n  // be unaligned due to some other reason.\n  if (Count % 2) {\n    OS.write(\"\\0\", 1);\n    Count -= 1;\n  }\n\n  bool UseCompressedNop = STI->hasFeature(RISCV::FeatureStdExtC) ||\n                          STI->hasFeature(RISCV::FeatureStdExtZca);\n  // The canonical nop on RVC is c.nop.\n  if (Count % 4 == 2) {\n    OS.write(UseCompressedNop ? \"\\x01\\0\" : \"\\0\\0\", 2);\n    Count -= 2;\n  }\n\n  // The canonical nop on RISC-V is addi x0, x0, 0.\n  for (; Count >= 4; Count -= 4)\n    OS.write(\"\\x13\\0\\0\\0\", 4);\n\n  return true;\n}",
      "start_line": 385,
      "end_line": 411,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "write",
        "hasFeature"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "adjustFixupValue",
      "return_type": "uint64_t",
      "parameters": [
        {
          "type": "const MCFixup",
          "name": "&Fixup"
        },
        {
          "type": "uint64_t",
          "name": "Value"
        },
        {
          "type": "MCContext",
          "name": "&Ctx"
        }
      ],
      "body": "{\n  switch (Fixup.getTargetKind()) {\n  default:\n    llvm_unreachable(\"Unknown fixup kind!\");\n  case RISCV::fixup_riscv_got_hi20:\n  case RISCV::fixup_riscv_tls_got_hi20:\n  case RISCV::fixup_riscv_tls_gd_hi20:\n  case RISCV::fixup_riscv_tlsdesc_hi20:\n    llvm_unreachable(\"Relocation should be unconditionally forced\\n\");\n  case FK_Data_1:\n  case FK_Data_2:\n  case FK_Data_4:\n  case FK_Data_8:\n  case FK_Data_leb128:\n    return Value;\n  case RISCV::fixup_riscv_lo12_i:\n  case RISCV::fixup_riscv_pcrel_lo12_i:\n  case RISCV::fixup_riscv_tprel_lo12_i:\n  case RISCV::fixup_riscv_tlsdesc_load_lo12:\n    return Value & 0xfff;\n  case RISCV::fixup_riscv_12_i:\n    if (!isInt<12>(Value)) {\n      Ctx.reportError(Fixup.getLoc(),\n                      \"operand must be a constant 12-bit integer\");\n    }\n    return Value & 0xfff;\n  case RISCV::fixup_riscv_lo12_s:\n  case RISCV::fixup_riscv_pcrel_lo12_s:\n  case RISCV::fixup_riscv_tprel_lo12_s:\n    return (((Value >> 5) & 0x7f) << 25) | ((Value & 0x1f) << 7);\n  case RISCV::fixup_riscv_hi20:\n  case RISCV::fixup_riscv_pcrel_hi20:\n  case RISCV::fixup_riscv_tprel_hi20:\n    // Add 1 if bit 11 is 1, to compensate for low 12 bits being negative.\n    return ((Value + 0x800) >> 12) & 0xfffff;\n  case RISCV::fixup_riscv_jal: {\n    if (!isInt<21>(Value))\n      Ctx.reportError(Fixup.getLoc(), \"fixup value out of range\");\n    if (Value & 0x1)\n      Ctx.reportError(Fixup.getLoc(), \"fixup value must be 2-byte aligned\");\n    // Need to produce imm[19|10:1|11|19:12] from the 21-bit Value.\n    unsigned Sbit = (Value >> 20) & 0x1;\n    unsigned Hi8 = (Value >> 12) & 0xff;\n    unsigned Mid1 = (Value >> 11) & 0x1;\n    unsigned Lo10 = (Value >> 1) & 0x3ff;\n    // Inst{31} = Sbit;\n    // Inst{30-21} = Lo10;\n    // Inst{20} = Mid1;\n    // Inst{19-12} = Hi8;\n    Value = (Sbit << 19) | (Lo10 << 9) | (Mid1 << 8) | Hi8;\n    return Value;\n  }\n  case RISCV::fixup_riscv_branch: {\n    if (!isInt<13>(Value))\n      Ctx.reportError(Fixup.getLoc(), \"fixup value out of range\");\n    if (Value & 0x1)\n      Ctx.reportError(Fixup.getLoc(), \"fixup value must be 2-byte aligned\");\n    // Need to extract imm[12], imm[10:5], imm[4:1], imm[11] from the 13-bit\n    // Value.\n    unsigned Sbit = (Value >> 12) & 0x1;\n    unsigned Hi1 = (Value >> 11) & 0x1;\n    unsigned Mid6 = (Value >> 5) & 0x3f;\n    unsigned Lo4 = (Value >> 1) & 0xf;\n    // Inst{31} = Sbit;\n    // Inst{30-25} = Mid6;\n    // Inst{11-8} = Lo4;\n    // Inst{7} = Hi1;\n    Value = (Sbit << 31) | (Mid6 << 25) | (Lo4 << 8) | (Hi1 << 7);\n    return Value;\n  }\n  case RISCV::fixup_riscv_call:\n  case RISCV::fixup_riscv_call_plt: {\n    // Jalr will add UpperImm with the sign-extended 12-bit LowerImm,\n    // we need to add 0x800ULL before extract upper bits to reflect the\n    // effect of the sign extension.\n    uint64_t UpperImm = (Value + 0x800ULL) & 0xfffff000ULL;\n    uint64_t LowerImm = Value & 0xfffULL;\n    return UpperImm | ((LowerImm << 20) << 32);\n  }\n  case RISCV::fixup_riscv_rvc_jump: {\n    if (!isInt<12>(Value))\n      Ctx.reportError(Fixup.getLoc(), \"fixup value out of range\");\n    // Need to produce offset[11|4|9:8|10|6|7|3:1|5] from the 11-bit Value.\n    unsigned Bit11  = (Value >> 11) & 0x1;\n    unsigned Bit4   = (Value >> 4) & 0x1;\n    unsigned Bit9_8 = (Value >> 8) & 0x3;\n    unsigned Bit10  = (Value >> 10) & 0x1;\n    unsigned Bit6   = (Value >> 6) & 0x1;\n    unsigned Bit7   = (Value >> 7) & 0x1;\n    unsigned Bit3_1 = (Value >> 1) & 0x7;\n    unsigned Bit5   = (Value >> 5) & 0x1;\n    Value = (Bit11 << 10) | (Bit4 << 9) | (Bit9_8 << 7) | (Bit10 << 6) |\n            (Bit6 << 5) | (Bit7 << 4) | (Bit3_1 << 1) | Bit5;\n    return Value;\n  }\n  case RISCV::fixup_riscv_rvc_branch: {\n    if (!isInt<9>(Value))\n      Ctx.reportError(Fixup.getLoc(), \"fixup value out of range\");\n    // Need to produce offset[8|4:3], [reg 3 bit], offset[7:6|2:1|5]\n    unsigned Bit8   = (Value >> 8) & 0x1;\n    unsigned Bit7_6 = (Value >> 6) & 0x3;\n    unsigned Bit5   = (Value >> 5) & 0x1;\n    unsigned Bit4_3 = (Value >> 3) & 0x3;\n    unsigned Bit2_1 = (Value >> 1) & 0x3;\n    Value = (Bit8 << 12) | (Bit4_3 << 10) | (Bit7_6 << 5) | (Bit2_1 << 3) |\n            (Bit5 << 2);\n    return Value;\n  }\n\n  }\n}",
      "start_line": 413,
      "end_line": 524,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "reportError",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "evaluateTargetFixup",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MCAssembler",
          "name": "&Asm"
        },
        {
          "type": "const MCAsmLayout",
          "name": "&Layout"
        },
        {
          "type": "const MCFixup",
          "name": "&Fixup"
        },
        {
          "type": "const MCFragment",
          "name": "*DF"
        },
        {
          "type": "const MCValue",
          "name": "&Target"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "*STI"
        },
        {
          "type": "uint64_t",
          "name": "&Value"
        },
        {
          "type": "bool",
          "name": "&WasForced"
        }
      ],
      "body": "{\n  const MCFixup *AUIPCFixup;\n  const MCFragment *AUIPCDF;\n  MCValue AUIPCTarget;\n  switch (Fixup.getTargetKind()) {\n  default:\n    llvm_unreachable(\"Unexpected fixup kind!\");\n  case RISCV::fixup_riscv_tlsdesc_hi20:\n  case RISCV::fixup_riscv_pcrel_hi20:\n    AUIPCFixup = &Fixup;\n    AUIPCDF = DF;\n    AUIPCTarget = Target;\n    break;\n  case RISCV::fixup_riscv_pcrel_lo12_i:\n  case RISCV::fixup_riscv_pcrel_lo12_s: {\n    AUIPCFixup = cast<RISCVMCExpr>(Fixup.getValue())->getPCRelHiFixup(&AUIPCDF);\n    if (!AUIPCFixup) {\n      Asm.getContext().reportError(Fixup.getLoc(),\n                                   \"could not find corresponding %pcrel_hi\");\n      return true;\n    }\n\n    // MCAssembler::evaluateFixup will emit an error for this case when it sees\n    // the %pcrel_hi, so don't duplicate it when also seeing the %pcrel_lo.\n    const MCExpr *AUIPCExpr = AUIPCFixup->getValue();\n    if (!AUIPCExpr->evaluateAsRelocatable(AUIPCTarget, &Layout, AUIPCFixup))\n      return true;\n    break;\n  }\n  }\n\n  if (!AUIPCTarget.getSymA() || AUIPCTarget.getSymB())\n    return false;\n\n  const MCSymbolRefExpr *A = AUIPCTarget.getSymA();\n  const MCSymbol &SA = A->getSymbol();\n  if (A->getKind() != MCSymbolRefExpr::VK_None || SA.isUndefined())\n    return false;\n\n  auto *Writer = Asm.getWriterPtr();\n  if (!Writer)\n    return false;\n\n  bool IsResolved = Writer->isSymbolRefDifferenceFullyResolvedImpl(\n      Asm, SA, *AUIPCDF, false, true);\n  if (!IsResolved)\n    return false;\n\n  Value = Layout.getSymbolOffset(SA) + AUIPCTarget.getConstant();\n  Value -= Layout.getFragmentOffset(AUIPCDF) + AUIPCFixup->getOffset();\n\n  if (shouldForceRelocation(Asm, *AUIPCFixup, AUIPCTarget, STI)) {\n    WasForced = true;\n    return false;\n  }\n\n  return true;\n}",
      "start_line": 526,
      "end_line": 586,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "getSymbol",
        "getValue",
        "getOffset",
        "isSymbolRefDifferenceFullyResolvedImpl",
        "getSymB",
        "getWriterPtr",
        "getConstant",
        "isUndefined",
        "getSymbolOffset",
        "getFragmentOffset",
        "getPCRelHiFixup",
        "getSymA",
        "reportError",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "handleAddSubRelocations",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MCAsmLayout",
          "name": "&Layout"
        },
        {
          "type": "const MCFragment",
          "name": "&F"
        },
        {
          "type": "const MCFixup",
          "name": "&Fixup"
        },
        {
          "type": "const MCValue",
          "name": "&Target"
        },
        {
          "type": "uint64_t",
          "name": "&FixedValue"
        }
      ],
      "body": "{\n  uint64_t FixedValueA, FixedValueB;\n  unsigned TA = 0, TB = 0;\n  switch (Fixup.getKind()) {\n  case llvm::FK_Data_1:\n    TA = ELF::R_RISCV_ADD8;\n    TB = ELF::R_RISCV_SUB8;\n    break;\n  case llvm::FK_Data_2:\n    TA = ELF::R_RISCV_ADD16;\n    TB = ELF::R_RISCV_SUB16;\n    break;\n  case llvm::FK_Data_4:\n    TA = ELF::R_RISCV_ADD32;\n    TB = ELF::R_RISCV_SUB32;\n    break;\n  case llvm::FK_Data_8:\n    TA = ELF::R_RISCV_ADD64;\n    TB = ELF::R_RISCV_SUB64;\n    break;\n  case llvm::FK_Data_leb128:\n    TA = ELF::R_RISCV_SET_ULEB128;\n    TB = ELF::R_RISCV_SUB_ULEB128;\n    break;\n  default:\n    llvm_unreachable(\"unsupported fixup size\");\n  }\n  MCValue A = MCValue::get(Target.getSymA(), nullptr, Target.getConstant());\n  MCValue B = MCValue::get(Target.getSymB());\n  auto FA = MCFixup::create(\n      Fixup.getOffset(), nullptr,\n      static_cast<MCFixupKind>(FirstLiteralRelocationKind + TA));\n  auto FB = MCFixup::create(\n      Fixup.getOffset(), nullptr,\n      static_cast<MCFixupKind>(FirstLiteralRelocationKind + TB));\n  auto &Asm = Layout.getAssembler();\n  Asm.getWriter().recordRelocation(Asm, Layout, &F, FA, A, FixedValueA);\n  Asm.getWriter().recordRelocation(Asm, Layout, &F, FB, B, FixedValueB);\n  FixedValue = FixedValueA - FixedValueB;\n  return true;\n}",
      "start_line": 588,
      "end_line": 632,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "recordRelocation",
        "create",
        "getConstant",
        "get",
        "getAssembler",
        "getWriter",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "applyFixup",
      "return_type": "void",
      "parameters": [
        {
          "type": "const MCAssembler",
          "name": "&Asm"
        },
        {
          "type": "const MCFixup",
          "name": "&Fixup"
        },
        {
          "type": "const MCValue",
          "name": "&Target"
        },
        {
          "type": "MutableArrayRef<char>",
          "name": "Data"
        },
        {
          "type": "uint64_t",
          "name": "Value"
        },
        {
          "type": "bool",
          "name": "IsResolved"
        },
        {
          "type": "const MCSubtargetInfo",
          "name": "*STI"
        }
      ],
      "body": "{\n  MCFixupKind Kind = Fixup.getKind();\n  if (Kind >= FirstLiteralRelocationKind)\n    return;\n  MCContext &Ctx = Asm.getContext();\n  MCFixupKindInfo Info = getFixupKindInfo(Kind);\n  if (!Value)\n    return; // Doesn't change encoding.\n  // Apply any target-specific value adjustments.\n  Value = adjustFixupValue(Fixup, Value, Ctx);\n\n  // Shift the value into position.\n  Value <<= Info.TargetOffset;\n\n  unsigned Offset = Fixup.getOffset();\n  unsigned NumBytes = alignTo(Info.TargetSize + Info.TargetOffset, 8) / 8;\n\n  assert(Offset + NumBytes <= Data.size() && \"Invalid fixup offset!\");\n\n  // For each byte of the fragment that the fixup touches, mask in the\n  // bits from the fixup value.\n  for (unsigned i = 0; i != NumBytes; ++i) {\n    Data[Offset + i] |= uint8_t((Value >> (i * 8)) & 0xff);\n  }\n}",
      "start_line": 634,
      "end_line": 662,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "getOffset",
        "adjustFixupValue",
        "getKind",
        "getFixupKindInfo",
        "alignTo",
        "uint8_t"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldInsertExtraNopBytesForCodeAlign",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MCAlignFragment",
          "name": "&AF"
        },
        {
          "type": "unsigned",
          "name": "&Size"
        }
      ],
      "body": "{\n  // Calculate Nops Size only when linker relaxation enabled.\n  const MCSubtargetInfo *STI = AF.getSubtargetInfo();\n  if (!STI->hasFeature(RISCV::FeatureRelax))\n    return false;\n\n  bool UseCompressedNop = STI->hasFeature(RISCV::FeatureStdExtC) ||\n                          STI->hasFeature(RISCV::FeatureStdExtZca);\n  unsigned MinNopLen = UseCompressedNop ? 2 : 4;\n\n  if (AF.getAlignment() <= MinNopLen) {\n    return false;\n  } else {\n    Size = AF.getAlignment().value() - MinNopLen;\n    return true;\n  }\n}",
      "start_line": 668,
      "end_line": 685,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "value",
        "getAlignment",
        "getSubtargetInfo",
        "hasFeature"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldInsertFixupForCodeAlign",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MCAssembler",
          "name": "&Asm"
        },
        {
          "type": "const MCAsmLayout",
          "name": "&Layout"
        },
        {
          "type": "MCAlignFragment",
          "name": "&AF"
        }
      ],
      "body": "{\n  // Insert the fixup only when linker relaxation enabled.\n  const MCSubtargetInfo *STI = AF.getSubtargetInfo();\n  if (!STI->hasFeature(RISCV::FeatureRelax))\n    return false;\n\n  // Calculate total Nops we need to insert. If there are none to insert\n  // then simply return.\n  unsigned Count;\n  if (!shouldInsertExtraNopBytesForCodeAlign(AF, Count) || (Count == 0))\n    return false;\n\n  MCContext &Ctx = Asm.getContext();\n  const MCExpr *Dummy = MCConstantExpr::create(0, Ctx);\n  // Create fixup_riscv_align fixup.\n  MCFixup Fixup =\n      MCFixup::create(0, Dummy, MCFixupKind(RISCV::fixup_riscv_align), SMLoc());\n\n  uint64_t FixedValue = 0;\n  MCValue NopBytes = MCValue::get(Count);\n\n  Asm.getWriter().recordRelocation(Asm, Layout, &AF, Fixup, NopBytes,\n                                   FixedValue);\n\n  return true;\n}",
      "start_line": 692,
      "end_line": 719,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "recordRelocation",
        "create",
        "get",
        "getWriter",
        "SMLoc",
        "getSubtargetInfo"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVAsmBackend.cpp",
      "backend": "RISCV"
    },
    {
      "name": "needsRelocateWithSymbol",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MCValue",
          "name": "&Val"
        },
        {
          "type": "const MCSymbol",
          "name": "&Sym"
        },
        {
          "type": "unsigned",
          "name": "Type"
        }
      ],
      "body": "{\n    // TODO: this is very conservative, update once RISC-V psABI requirements\n    //       are clarified.\n    return true;\n  }",
      "start_line": 30,
      "end_line": 35,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVELFObjectWriter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRelocType",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "MCContext",
          "name": "&Ctx"
        },
        {
          "type": "const MCValue",
          "name": "&Target"
        },
        {
          "type": "const MCFixup",
          "name": "&Fixup"
        },
        {
          "type": "bool",
          "name": "IsPCRel"
        }
      ],
      "body": "{\n  const MCExpr *Expr = Fixup.getValue();\n  // Determine the type of the relocation\n  unsigned Kind = Fixup.getTargetKind();\n  if (Kind >= FirstLiteralRelocationKind)\n    return Kind - FirstLiteralRelocationKind;\n  if (IsPCRel) {\n    switch (Kind) {\n    default:\n      Ctx.reportError(Fixup.getLoc(), \"unsupported relocation type\");\n      return ELF::R_RISCV_NONE;\n    case FK_Data_4:\n    case FK_PCRel_4:\n      return Target.getAccessVariant() == MCSymbolRefExpr::VK_PLT\n                 ? ELF::R_RISCV_PLT32\n                 : ELF::R_RISCV_32_PCREL;\n    case RISCV::fixup_riscv_pcrel_hi20:\n      return ELF::R_RISCV_PCREL_HI20;\n    case RISCV::fixup_riscv_pcrel_lo12_i:\n      return ELF::R_RISCV_PCREL_LO12_I;\n    case RISCV::fixup_riscv_pcrel_lo12_s:\n      return ELF::R_RISCV_PCREL_LO12_S;\n    case RISCV::fixup_riscv_got_hi20:\n      return ELF::R_RISCV_GOT_HI20;\n    case RISCV::fixup_riscv_tls_got_hi20:\n      return ELF::R_RISCV_TLS_GOT_HI20;\n    case RISCV::fixup_riscv_tls_gd_hi20:\n      return ELF::R_RISCV_TLS_GD_HI20;\n    case RISCV::fixup_riscv_tlsdesc_hi20:\n      return ELF::R_RISCV_TLSDESC_HI20;\n    case RISCV::fixup_riscv_tlsdesc_load_lo12:\n      return ELF::R_RISCV_TLSDESC_LOAD_LO12;\n    case RISCV::fixup_riscv_tlsdesc_add_lo12:\n      return ELF::R_RISCV_TLSDESC_ADD_LO12;\n    case RISCV::fixup_riscv_tlsdesc_call:\n      return ELF::R_RISCV_TLSDESC_CALL;\n    case RISCV::fixup_riscv_jal:\n      return ELF::R_RISCV_JAL;\n    case RISCV::fixup_riscv_branch:\n      return ELF::R_RISCV_BRANCH;\n    case RISCV::fixup_riscv_rvc_jump:\n      return ELF::R_RISCV_RVC_JUMP;\n    case RISCV::fixup_riscv_rvc_branch:\n      return ELF::R_RISCV_RVC_BRANCH;\n    case RISCV::fixup_riscv_call:\n      return ELF::R_RISCV_CALL_PLT;\n    case RISCV::fixup_riscv_call_plt:\n      return ELF::R_RISCV_CALL_PLT;\n    }\n  }\n\n  switch (Kind) {\n  default:\n    Ctx.reportError(Fixup.getLoc(), \"unsupported relocation type\");\n    return ELF::R_RISCV_NONE;\n  case RISCV::fixup_riscv_tlsdesc_load_lo12:\n    return ELF::R_RISCV_TLSDESC_LOAD_LO12;\n  case RISCV::fixup_riscv_tlsdesc_add_lo12:\n    return ELF::R_RISCV_TLSDESC_ADD_LO12;\n  case RISCV::fixup_riscv_tlsdesc_call:\n    return ELF::R_RISCV_TLSDESC_CALL;\n\n  case FK_Data_1:\n    Ctx.reportError(Fixup.getLoc(), \"1-byte data relocations not supported\");\n    return ELF::R_RISCV_NONE;\n  case FK_Data_2:\n    Ctx.reportError(Fixup.getLoc(), \"2-byte data relocations not supported\");\n    return ELF::R_RISCV_NONE;\n  case FK_Data_4:\n    if (Expr->getKind() == MCExpr::Target &&\n        cast<RISCVMCExpr>(Expr)->getKind() == RISCVMCExpr::VK_RISCV_32_PCREL)\n      return ELF::R_RISCV_32_PCREL;\n    if (Target.getSymA()->getKind() == MCSymbolRefExpr::VK_GOTPCREL)\n      return ELF::R_RISCV_GOT32_PCREL;\n    return ELF::R_RISCV_32;\n  case FK_Data_8:\n    return ELF::R_RISCV_64;\n  case RISCV::fixup_riscv_hi20:\n    return ELF::R_RISCV_HI20;\n  case RISCV::fixup_riscv_lo12_i:\n    return ELF::R_RISCV_LO12_I;\n  case RISCV::fixup_riscv_lo12_s:\n    return ELF::R_RISCV_LO12_S;\n  case RISCV::fixup_riscv_tprel_hi20:\n    return ELF::R_RISCV_TPREL_HI20;\n  case RISCV::fixup_riscv_tprel_lo12_i:\n    return ELF::R_RISCV_TPREL_LO12_I;\n  case RISCV::fixup_riscv_tprel_lo12_s:\n    return ELF::R_RISCV_TPREL_LO12_S;\n  case RISCV::fixup_riscv_tprel_add:\n    return ELF::R_RISCV_TPREL_ADD;\n  case RISCV::fixup_riscv_relax:\n    return ELF::R_RISCV_RELAX;\n  case RISCV::fixup_riscv_align:\n    return ELF::R_RISCV_ALIGN;\n  }\n}",
      "start_line": 49,
      "end_line": 148,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "getRelocType",
          "condition": "Kind",
          "cases": [
            {
              "label": "FK_Data_4",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "FK_PCRel_4",
              "return": "Target.getAccessVariant() == MCSymbolRefExpr::VK_PLT\n                 ? ELF::R_RISCV_PLT32\n                 : ELF::R_RISCV_32_PCREL",
              "fallthrough": false
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "getRelocType",
          "condition": "Kind",
          "cases": [
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "FK_Data_1",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "FK_Data_2",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "FK_Data_4",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "FK_Data_8",
              "return": "ELF::R_RISCV_64",
              "fallthrough": false
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCV",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getAccessVariant",
        "getValue",
        "getKind",
        "getTargetKind",
        "reportError"
      ],
      "source_file": "llvm/lib/Target/RISCV/MCTargetDesc/RISCVELFObjectWriter.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getSetCCResultType",
      "return_type": "EVT",
      "parameters": [
        {
          "type": "const DataLayout",
          "name": "&DL"
        },
        {
          "type": "LLVMContext",
          "name": "&Context"
        },
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  if (!VT.isVector())\n    return getPointerTy(DL);\n  if (Subtarget.hasVInstructions() &&\n      (VT.isScalableVector() || Subtarget.useRVVForFixedLengthVectors()))\n    return EVT::getVectorVT(Context, MVT::i1, VT.getVectorElementCount());\n  return VT.changeVectorElementTypeToInteger();\n}",
      "start_line": 1418,
      "end_line": 1427,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "changeVectorElementTypeToInteger",
        "useRVVForFixedLengthVectors",
        "getVectorVT",
        "isScalableVector",
        "getPointerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVPExplicitVectorLengthTy",
      "return_type": "MVT",
      "parameters": [],
      "body": "{\n  return Subtarget.getXLenVT();\n}",
      "start_line": 1429,
      "end_line": 1431,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getXLenVT"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldExpandGetVectorLength",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "TripCountVT"
        },
        {
          "type": "unsigned",
          "name": "VF"
        },
        {
          "type": "bool",
          "name": "IsScalable"
        }
      ],
      "body": "{\n  if (!Subtarget.hasVInstructions())\n    return true;\n\n  if (!IsScalable)\n    return true;\n\n  if (TripCountVT != MVT::i32 && TripCountVT != Subtarget.getXLenVT())\n    return true;\n\n  // Don't allow VF=1 if those types are't legal.\n  if (VF < RISCV::RVVBitsPerBlock / Subtarget.getELen())\n    return true;\n\n  // VLEN=32 support is incomplete.\n  if (Subtarget.getRealMinVLen() < RISCV::RVVBitsPerBlock)\n    return true;\n\n  // The maximum VF is for the smallest element width with LMUL=8.\n  // VF must be a power of 2.\n  unsigned MaxVF = (RISCV::RVVBitsPerBlock / 8) * 8;\n  return VF > MaxVF || !isPowerOf2_32(VF);\n}",
      "start_line": 1434,
      "end_line": 1458,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isPowerOf2_32"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTgtMemIntrinsic",
      "return_type": "bool",
      "parameters": [
        {
          "type": "IntrinsicInfo",
          "name": "&Info"
        },
        {
          "type": "const CallInst",
          "name": "&I"
        },
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "unsigned",
          "name": "Intrinsic"
        }
      ],
      "body": "{\n  auto &DL = I.getModule()->getDataLayout();\n\n  auto SetRVVLoadStoreInfo = [&](unsigned PtrOp, bool IsStore,\n                                 bool IsUnitStrided) {\n    Info.opc = IsStore ? ISD::INTRINSIC_VOID : ISD::INTRINSIC_W_CHAIN;\n    Info.ptrVal = I.getArgOperand(PtrOp);\n    Type *MemTy;\n    if (IsStore) {\n      // Store value is the first operand.\n      MemTy = I.getArgOperand(0)->getType();\n    } else {\n      // Use return type. If it's segment load, return type is a struct.\n      MemTy = I.getType();\n      if (MemTy->isStructTy())\n        MemTy = MemTy->getStructElementType(0);\n    }\n    if (!IsUnitStrided)\n      MemTy = MemTy->getScalarType();\n\n    Info.memVT = getValueType(DL, MemTy);\n    Info.align = Align(DL.getTypeSizeInBits(MemTy->getScalarType()) / 8);\n    Info.size = MemoryLocation::UnknownSize;\n    Info.flags |=\n        IsStore ? MachineMemOperand::MOStore : MachineMemOperand::MOLoad;\n    return true;\n  };\n\n  if (I.getMetadata(LLVMContext::MD_nontemporal) != nullptr)\n    Info.flags |= MachineMemOperand::MONonTemporal;\n\n  Info.flags |= RISCVTargetLowering::getTargetMMOFlags(I);\n  switch (Intrinsic) {\n  default:\n    return false;\n  case Intrinsic::riscv_masked_atomicrmw_xchg_i32:\n  case Intrinsic::riscv_masked_atomicrmw_add_i32:\n  case Intrinsic::riscv_masked_atomicrmw_sub_i32:\n  case Intrinsic::riscv_masked_atomicrmw_nand_i32:\n  case Intrinsic::riscv_masked_atomicrmw_max_i32:\n  case Intrinsic::riscv_masked_atomicrmw_min_i32:\n  case Intrinsic::riscv_masked_atomicrmw_umax_i32:\n  case Intrinsic::riscv_masked_atomicrmw_umin_i32:\n  case Intrinsic::riscv_masked_cmpxchg_i32:\n    Info.opc = ISD::INTRINSIC_W_CHAIN;\n    Info.memVT = MVT::i32;\n    Info.ptrVal = I.getArgOperand(0);\n    Info.offset = 0;\n    Info.align = Align(4);\n    Info.flags = MachineMemOperand::MOLoad | MachineMemOperand::MOStore |\n                 MachineMemOperand::MOVolatile;\n    return true;\n  case Intrinsic::riscv_masked_strided_load:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ 1, /*IsStore*/ false,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_masked_strided_store:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ 1, /*IsStore*/ true,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_seg2_load:\n  case Intrinsic::riscv_seg3_load:\n  case Intrinsic::riscv_seg4_load:\n  case Intrinsic::riscv_seg5_load:\n  case Intrinsic::riscv_seg6_load:\n  case Intrinsic::riscv_seg7_load:\n  case Intrinsic::riscv_seg8_load:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ 0, /*IsStore*/ false,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_seg2_store:\n  case Intrinsic::riscv_seg3_store:\n  case Intrinsic::riscv_seg4_store:\n  case Intrinsic::riscv_seg5_store:\n  case Intrinsic::riscv_seg6_store:\n  case Intrinsic::riscv_seg7_store:\n  case Intrinsic::riscv_seg8_store:\n    // Operands are (vec, ..., vec, ptr, vl)\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 2,\n                               /*IsStore*/ true,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vle:\n  case Intrinsic::riscv_vle_mask:\n  case Intrinsic::riscv_vleff:\n  case Intrinsic::riscv_vleff_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ 1,\n                               /*IsStore*/ false,\n                               /*IsUnitStrided*/ true);\n  case Intrinsic::riscv_vse:\n  case Intrinsic::riscv_vse_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ 1,\n                               /*IsStore*/ true,\n                               /*IsUnitStrided*/ true);\n  case Intrinsic::riscv_vlse:\n  case Intrinsic::riscv_vlse_mask:\n  case Intrinsic::riscv_vloxei:\n  case Intrinsic::riscv_vloxei_mask:\n  case Intrinsic::riscv_vluxei:\n  case Intrinsic::riscv_vluxei_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ 1,\n                               /*IsStore*/ false,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vsse:\n  case Intrinsic::riscv_vsse_mask:\n  case Intrinsic::riscv_vsoxei:\n  case Intrinsic::riscv_vsoxei_mask:\n  case Intrinsic::riscv_vsuxei:\n  case Intrinsic::riscv_vsuxei_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ 1,\n                               /*IsStore*/ true,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vlseg2:\n  case Intrinsic::riscv_vlseg3:\n  case Intrinsic::riscv_vlseg4:\n  case Intrinsic::riscv_vlseg5:\n  case Intrinsic::riscv_vlseg6:\n  case Intrinsic::riscv_vlseg7:\n  case Intrinsic::riscv_vlseg8:\n  case Intrinsic::riscv_vlseg2ff:\n  case Intrinsic::riscv_vlseg3ff:\n  case Intrinsic::riscv_vlseg4ff:\n  case Intrinsic::riscv_vlseg5ff:\n  case Intrinsic::riscv_vlseg6ff:\n  case Intrinsic::riscv_vlseg7ff:\n  case Intrinsic::riscv_vlseg8ff:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 2,\n                               /*IsStore*/ false,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vlseg2_mask:\n  case Intrinsic::riscv_vlseg3_mask:\n  case Intrinsic::riscv_vlseg4_mask:\n  case Intrinsic::riscv_vlseg5_mask:\n  case Intrinsic::riscv_vlseg6_mask:\n  case Intrinsic::riscv_vlseg7_mask:\n  case Intrinsic::riscv_vlseg8_mask:\n  case Intrinsic::riscv_vlseg2ff_mask:\n  case Intrinsic::riscv_vlseg3ff_mask:\n  case Intrinsic::riscv_vlseg4ff_mask:\n  case Intrinsic::riscv_vlseg5ff_mask:\n  case Intrinsic::riscv_vlseg6ff_mask:\n  case Intrinsic::riscv_vlseg7ff_mask:\n  case Intrinsic::riscv_vlseg8ff_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 4,\n                               /*IsStore*/ false,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vlsseg2:\n  case Intrinsic::riscv_vlsseg3:\n  case Intrinsic::riscv_vlsseg4:\n  case Intrinsic::riscv_vlsseg5:\n  case Intrinsic::riscv_vlsseg6:\n  case Intrinsic::riscv_vlsseg7:\n  case Intrinsic::riscv_vlsseg8:\n  case Intrinsic::riscv_vloxseg2:\n  case Intrinsic::riscv_vloxseg3:\n  case Intrinsic::riscv_vloxseg4:\n  case Intrinsic::riscv_vloxseg5:\n  case Intrinsic::riscv_vloxseg6:\n  case Intrinsic::riscv_vloxseg7:\n  case Intrinsic::riscv_vloxseg8:\n  case Intrinsic::riscv_vluxseg2:\n  case Intrinsic::riscv_vluxseg3:\n  case Intrinsic::riscv_vluxseg4:\n  case Intrinsic::riscv_vluxseg5:\n  case Intrinsic::riscv_vluxseg6:\n  case Intrinsic::riscv_vluxseg7:\n  case Intrinsic::riscv_vluxseg8:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 3,\n                               /*IsStore*/ false,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vlsseg2_mask:\n  case Intrinsic::riscv_vlsseg3_mask:\n  case Intrinsic::riscv_vlsseg4_mask:\n  case Intrinsic::riscv_vlsseg5_mask:\n  case Intrinsic::riscv_vlsseg6_mask:\n  case Intrinsic::riscv_vlsseg7_mask:\n  case Intrinsic::riscv_vlsseg8_mask:\n  case Intrinsic::riscv_vloxseg2_mask:\n  case Intrinsic::riscv_vloxseg3_mask:\n  case Intrinsic::riscv_vloxseg4_mask:\n  case Intrinsic::riscv_vloxseg5_mask:\n  case Intrinsic::riscv_vloxseg6_mask:\n  case Intrinsic::riscv_vloxseg7_mask:\n  case Intrinsic::riscv_vloxseg8_mask:\n  case Intrinsic::riscv_vluxseg2_mask:\n  case Intrinsic::riscv_vluxseg3_mask:\n  case Intrinsic::riscv_vluxseg4_mask:\n  case Intrinsic::riscv_vluxseg5_mask:\n  case Intrinsic::riscv_vluxseg6_mask:\n  case Intrinsic::riscv_vluxseg7_mask:\n  case Intrinsic::riscv_vluxseg8_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 5,\n                               /*IsStore*/ false,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vsseg2:\n  case Intrinsic::riscv_vsseg3:\n  case Intrinsic::riscv_vsseg4:\n  case Intrinsic::riscv_vsseg5:\n  case Intrinsic::riscv_vsseg6:\n  case Intrinsic::riscv_vsseg7:\n  case Intrinsic::riscv_vsseg8:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 2,\n                               /*IsStore*/ true,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vsseg2_mask:\n  case Intrinsic::riscv_vsseg3_mask:\n  case Intrinsic::riscv_vsseg4_mask:\n  case Intrinsic::riscv_vsseg5_mask:\n  case Intrinsic::riscv_vsseg6_mask:\n  case Intrinsic::riscv_vsseg7_mask:\n  case Intrinsic::riscv_vsseg8_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 3,\n                               /*IsStore*/ true,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vssseg2:\n  case Intrinsic::riscv_vssseg3:\n  case Intrinsic::riscv_vssseg4:\n  case Intrinsic::riscv_vssseg5:\n  case Intrinsic::riscv_vssseg6:\n  case Intrinsic::riscv_vssseg7:\n  case Intrinsic::riscv_vssseg8:\n  case Intrinsic::riscv_vsoxseg2:\n  case Intrinsic::riscv_vsoxseg3:\n  case Intrinsic::riscv_vsoxseg4:\n  case Intrinsic::riscv_vsoxseg5:\n  case Intrinsic::riscv_vsoxseg6:\n  case Intrinsic::riscv_vsoxseg7:\n  case Intrinsic::riscv_vsoxseg8:\n  case Intrinsic::riscv_vsuxseg2:\n  case Intrinsic::riscv_vsuxseg3:\n  case Intrinsic::riscv_vsuxseg4:\n  case Intrinsic::riscv_vsuxseg5:\n  case Intrinsic::riscv_vsuxseg6:\n  case Intrinsic::riscv_vsuxseg7:\n  case Intrinsic::riscv_vsuxseg8:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 3,\n                               /*IsStore*/ true,\n                               /*IsUnitStrided*/ false);\n  case Intrinsic::riscv_vssseg2_mask:\n  case Intrinsic::riscv_vssseg3_mask:\n  case Intrinsic::riscv_vssseg4_mask:\n  case Intrinsic::riscv_vssseg5_mask:\n  case Intrinsic::riscv_vssseg6_mask:\n  case Intrinsic::riscv_vssseg7_mask:\n  case Intrinsic::riscv_vssseg8_mask:\n  case Intrinsic::riscv_vsoxseg2_mask:\n  case Intrinsic::riscv_vsoxseg3_mask:\n  case Intrinsic::riscv_vsoxseg4_mask:\n  case Intrinsic::riscv_vsoxseg5_mask:\n  case Intrinsic::riscv_vsoxseg6_mask:\n  case Intrinsic::riscv_vsoxseg7_mask:\n  case Intrinsic::riscv_vsoxseg8_mask:\n  case Intrinsic::riscv_vsuxseg2_mask:\n  case Intrinsic::riscv_vsuxseg3_mask:\n  case Intrinsic::riscv_vsuxseg4_mask:\n  case Intrinsic::riscv_vsuxseg5_mask:\n  case Intrinsic::riscv_vsuxseg6_mask:\n  case Intrinsic::riscv_vsuxseg7_mask:\n  case Intrinsic::riscv_vsuxseg8_mask:\n    return SetRVVLoadStoreInfo(/*PtrOp*/ I.arg_size() - 4,\n                               /*IsStore*/ true,\n                               /*IsUnitStrided*/ false);\n  }\n}",
      "start_line": 1460,
      "end_line": 1722,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "getTgtMemIntrinsic",
          "condition": "Intrinsic",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "false"
        }
      ],
      "calls": [
        "getModule",
        "getStructElementType",
        "getArgOperand",
        "getTargetMMOFlags",
        "getValueType",
        "getDataLayout",
        "Align",
        "getType",
        "SetRVVLoadStoreInfo",
        "are",
        "getScalarType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isLegalAddressingMode",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const DataLayout",
          "name": "&DL"
        },
        {
          "type": "const AddrMode",
          "name": "&AM"
        },
        {
          "type": "Type",
          "name": "*Ty"
        },
        {
          "type": "unsigned",
          "name": "AS"
        },
        {
          "type": "Instruction",
          "name": "*I"
        }
      ],
      "body": "{\n  // No global is ever allowed as a base.\n  if (AM.BaseGV)\n    return false;\n\n  // RVV instructions only support register addressing.\n  if (Subtarget.hasVInstructions() && isa<VectorType>(Ty))\n    return AM.HasBaseReg && AM.Scale == 0 && !AM.BaseOffs;\n\n  // Require a 12-bit signed offset.\n  if (!isInt<12>(AM.BaseOffs))\n    return false;\n\n  switch (AM.Scale) {\n  case 0: // \"r+i\" or just \"i\", depending on HasBaseReg.\n    break;\n  case 1:\n    if (!AM.HasBaseReg) // allow \"r+i\".\n      break;\n    return false; // disallow \"r+r\" or \"r+r+i\".\n  default:\n    return false;\n  }\n\n  return true;\n}",
      "start_line": 1724,
      "end_line": 1752,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "isLegalAddressingMode",
          "condition": "AM.Scale",
          "cases": [
            {
              "label": "0",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "1",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "false"
        }
      ],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isLegalICmpImmediate",
      "return_type": "bool",
      "parameters": [
        {
          "type": "int64_t",
          "name": "Imm"
        }
      ],
      "body": "{\n  return isInt<12>(Imm);\n}",
      "start_line": 1754,
      "end_line": 1756,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isLegalAddImmediate",
      "return_type": "bool",
      "parameters": [
        {
          "type": "int64_t",
          "name": "Imm"
        }
      ],
      "body": "{\n  return isInt<12>(Imm);\n}",
      "start_line": 1758,
      "end_line": 1760,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isTruncateFree",
      "return_type": "bool",
      "parameters": [
        {
          "type": "Type",
          "name": "*SrcTy"
        },
        {
          "type": "Type",
          "name": "*DstTy"
        }
      ],
      "body": "{\n  if (Subtarget.is64Bit() || !SrcTy->isIntegerTy() || !DstTy->isIntegerTy())\n    return false;\n  unsigned SrcBits = SrcTy->getPrimitiveSizeInBits();\n  unsigned DestBits = DstTy->getPrimitiveSizeInBits();\n  return (SrcBits == 64 && DestBits == 32);\n}",
      "start_line": 1767,
      "end_line": 1773,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getPrimitiveSizeInBits",
        "isIntegerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isTruncateFree",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "SrcVT"
        },
        {
          "type": "EVT",
          "name": "DstVT"
        }
      ],
      "body": "{\n  // We consider i64->i32 free on RV64 since we have good selection of W\n  // instructions that make promoting operations back to i64 free in many cases.\n  if (SrcVT.isVector() || DstVT.isVector() || !SrcVT.isInteger() ||\n      !DstVT.isInteger())\n    return false;\n  unsigned SrcBits = SrcVT.getSizeInBits();\n  unsigned DestBits = DstVT.getSizeInBits();\n  return (SrcBits == 64 && DestBits == 32);\n}",
      "start_line": 1775,
      "end_line": 1784,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isVector",
        "isInteger",
        "getSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isZExtFree",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Val"
        },
        {
          "type": "EVT",
          "name": "VT2"
        }
      ],
      "body": "{\n  // Zexts are free if they can be combined with a load.\n  // Don't advertise i32->i64 zextload as being free for RV64. It interacts\n  // poorly with type legalization of compares preferring sext.\n  if (auto *LD = dyn_cast<LoadSDNode>(Val)) {\n    EVT MemVT = LD->getMemoryVT();\n    if ((MemVT == MVT::i8 || MemVT == MVT::i16) &&\n        (LD->getExtensionType() == ISD::NON_EXTLOAD ||\n         LD->getExtensionType() == ISD::ZEXTLOAD))\n      return true;\n  }\n\n  return TargetLowering::isZExtFree(Val, VT2);\n}",
      "start_line": 1786,
      "end_line": 1799,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMemoryVT",
        "isZExtFree",
        "getExtensionType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isSExtCheaperThanZExt",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "SrcVT"
        },
        {
          "type": "EVT",
          "name": "DstVT"
        }
      ],
      "body": "{\n  return Subtarget.is64Bit() && SrcVT == MVT::i32 && DstVT == MVT::i64;\n}",
      "start_line": 1801,
      "end_line": 1803,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "is64Bit"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "signExtendConstant",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const ConstantInt",
          "name": "*CI"
        }
      ],
      "body": "{\n  return Subtarget.is64Bit() && CI->getType()->isIntegerTy(32);\n}",
      "start_line": 1805,
      "end_line": 1807,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "is64Bit",
        "getType",
        "isIntegerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isCheapToSpeculateCttz",
      "return_type": "bool",
      "parameters": [
        {
          "type": "Type",
          "name": "*Ty"
        }
      ],
      "body": "{\n  return Subtarget.hasStdExtZbb() || Subtarget.hasVendorXCVbitmanip();\n}",
      "start_line": 1809,
      "end_line": 1811,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZbb",
        "hasVendorXCVbitmanip"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isCheapToSpeculateCtlz",
      "return_type": "bool",
      "parameters": [
        {
          "type": "Type",
          "name": "*Ty"
        }
      ],
      "body": "{\n  return Subtarget.hasStdExtZbb() || Subtarget.hasVendorXTHeadBb() ||\n         Subtarget.hasVendorXCVbitmanip();\n}",
      "start_line": 1813,
      "end_line": 1816,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZbb",
        "hasVendorXCVbitmanip",
        "hasVendorXTHeadBb"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isMaskAndCmp0FoldingBeneficial",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const Instruction",
          "name": "&AndI"
        }
      ],
      "body": "{\n  // We expect to be able to match a bit extraction instruction if the Zbs\n  // extension is supported and the mask is a power of two. However, we\n  // conservatively return false if the mask would fit in an ANDI instruction,\n  // on the basis that it's possible the sinking+duplication of the AND in\n  // CodeGenPrepare triggered by this hook wouldn't decrease the instruction\n  // count and would increase code size (e.g. ANDI+BNEZ => BEXTI+BNEZ).\n  if (!Subtarget.hasStdExtZbs() && !Subtarget.hasVendorXTHeadBs())\n    return false;\n  ConstantInt *Mask = dyn_cast<ConstantInt>(AndI.getOperand(1));\n  if (!Mask)\n    return false;\n  return !Mask->getValue().isSignedIntN(12) && Mask->getValue().isPowerOf2();\n}",
      "start_line": 1818,
      "end_line": 1832,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasVendorXTHeadBs",
        "isSignedIntN",
        "getValue",
        "isPowerOf2",
        "getOperand",
        "size"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasAndNotCompare",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Y"
        }
      ],
      "body": "{\n  EVT VT = Y.getValueType();\n\n  // FIXME: Support vectors once we have tests.\n  if (VT.isVector())\n    return false;\n\n  return (Subtarget.hasStdExtZbb() || Subtarget.hasStdExtZbkb()) &&\n         !isa<ConstantSDNode>(Y);\n}",
      "start_line": 1834,
      "end_line": 1843,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZbkb",
        "getValueType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasBitTest",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "X"
        },
        {
          "type": "SDValue",
          "name": "Y"
        }
      ],
      "body": "{\n  // Zbs provides BEXT[_I], which can be used with SEQZ/SNEZ as a bit test.\n  if (Subtarget.hasStdExtZbs())\n    return X.getValueType().isScalarInteger();\n  auto *C = dyn_cast<ConstantSDNode>(Y);\n  // XTheadBs provides th.tst (similar to bexti), if Y is a constant\n  if (Subtarget.hasVendorXTHeadBs())\n    return C != nullptr;\n  // We can use ANDI+SEQZ/SNEZ as a bit test. Y contains the bit position.\n  return C && C->getAPIntValue().ule(10);\n}",
      "start_line": 1845,
      "end_line": 1855,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "ule",
        "getAPIntValue",
        "tst",
        "isScalarInteger",
        "getValueType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldFoldSelectWithIdentityConstant",
      "return_type": "bool",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        },
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  // Only enable for rvv.\n  if (!VT.isVector() || !Subtarget.hasVInstructions())\n    return false;\n\n  if (VT.isFixedLengthVector() && !isTypeLegal(VT))\n    return false;\n\n  return true;\n}",
      "start_line": 1857,
      "end_line": 1867,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasVInstructions",
        "isTypeLegal"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldConvertConstantLoadToIntImm",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const APInt",
          "name": "&Imm"
        },
        {
          "type": "Type",
          "name": "*Ty"
        }
      ],
      "body": "{\n  assert(Ty->isIntegerTy());\n\n  unsigned BitSize = Ty->getIntegerBitWidth();\n  if (BitSize > Subtarget.getXLen())\n    return false;\n\n  // Fast path, assume 32-bit immediates are cheap.\n  int64_t Val = Imm.getSExtValue();\n  if (isInt<32>(Val))\n    return true;\n\n  // A constant pool entry may be more aligned thant he load we're trying to\n  // replace. If we don't support unaligned scalar mem, prefer the constant\n  // pool.\n  // TODO: Can the caller pass down the alignment?\n  if (!Subtarget.hasFastUnalignedAccess() &&\n      !Subtarget.enableUnalignedScalarMem())\n    return true;\n\n  // Prefer to keep the load if it would require many instructions.\n  // This uses the same threshold we use for constant pools but doesn't\n  // check useConstantPoolForLargeInts.\n  // TODO: Should we keep the load only when we're definitely going to emit a\n  // constant pool?\n\n  RISCVMatInt::InstSeq Seq = RISCVMatInt::generateInstSeq(Val, Subtarget);\n  return Seq.size() <= Subtarget.getMaxBuildIntsCost();\n}",
      "start_line": 1869,
      "end_line": 1898,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "generateInstSeq",
        "enableUnalignedScalarMem",
        "getMaxBuildIntsCost",
        "size",
        "getIntegerBitWidth",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "canSplatOperand",
      "return_type": "bool",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        },
        {
          "type": "int",
          "name": "Operand"
        }
      ],
      "body": "{\n  switch (Opcode) {\n  case Instruction::Add:\n  case Instruction::Sub:\n  case Instruction::Mul:\n  case Instruction::And:\n  case Instruction::Or:\n  case Instruction::Xor:\n  case Instruction::FAdd:\n  case Instruction::FSub:\n  case Instruction::FMul:\n  case Instruction::FDiv:\n  case Instruction::ICmp:\n  case Instruction::FCmp:\n    return true;\n  case Instruction::Shl:\n  case Instruction::LShr:\n  case Instruction::AShr:\n  case Instruction::UDiv:\n  case Instruction::SDiv:\n  case Instruction::URem:\n  case Instruction::SRem:\n    return Operand == 1;\n  default:\n    return false;\n  }\n}",
      "start_line": 1924,
      "end_line": 1950,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "canSplatOperand",
          "condition": "Opcode",
          "cases": [
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Instruction",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "false"
        }
      ],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "canSplatOperand",
      "return_type": "bool",
      "parameters": [
        {
          "type": "Instruction",
          "name": "*I"
        },
        {
          "type": "int",
          "name": "Operand"
        }
      ],
      "body": "{\n  if (!I->getType()->isVectorTy() || !Subtarget.hasVInstructions())\n    return false;\n\n  if (canSplatOperand(I->getOpcode(), Operand))\n    return true;\n\n  auto *II = dyn_cast<IntrinsicInst>(I);\n  if (!II)\n    return false;\n\n  switch (II->getIntrinsicID()) {\n  case Intrinsic::fma:\n  case Intrinsic::vp_fma:\n    return Operand == 0 || Operand == 1;\n  case Intrinsic::vp_shl:\n  case Intrinsic::vp_lshr:\n  case Intrinsic::vp_ashr:\n  case Intrinsic::vp_udiv:\n  case Intrinsic::vp_sdiv:\n  case Intrinsic::vp_urem:\n  case Intrinsic::vp_srem:\n    return Operand == 1;\n    // These intrinsics are commutative.\n  case Intrinsic::vp_add:\n  case Intrinsic::vp_mul:\n  case Intrinsic::vp_and:\n  case Intrinsic::vp_or:\n  case Intrinsic::vp_xor:\n  case Intrinsic::vp_fadd:\n  case Intrinsic::vp_fmul:\n  case Intrinsic::vp_icmp:\n  case Intrinsic::vp_fcmp:\n    // These intrinsics have 'vr' versions.\n  case Intrinsic::vp_sub:\n  case Intrinsic::vp_fsub:\n  case Intrinsic::vp_fdiv:\n    return Operand == 0 || Operand == 1;\n  default:\n    return false;\n  }\n}",
      "start_line": 1953,
      "end_line": 1994,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isVectorTy",
        "hasVInstructions"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldSinkOperands",
      "return_type": "bool",
      "parameters": [
        {
          "type": "Instruction",
          "name": "*I"
        },
        {
          "type": "SmallVectorImpl<Use *>",
          "name": "&Ops"
        }
      ],
      "body": "{\n  using namespace llvm::PatternMatch;\n\n  if (!I->getType()->isVectorTy() || !Subtarget.hasVInstructions())\n    return false;\n\n  for (auto OpIdx : enumerate(I->operands())) {\n    if (!canSplatOperand(I, OpIdx.index()))\n      continue;\n\n    Instruction *Op = dyn_cast<Instruction>(OpIdx.value().get());\n    // Make sure we are not already sinking this operand\n    if (!Op || any_of(Ops, [&](Use *U) { return U->get() == Op; }))\n      continue;\n\n    // We are looking for a splat that can be sunk.\n    if (!match(Op, m_Shuffle(m_InsertElt(m_Undef(), m_Value(), m_ZeroInt()),\n                             m_Undef(), m_ZeroMask())))\n      continue;\n\n    // Don't sink i1 splats.\n    if (cast<VectorType>(Op->getType())->getElementType()->isIntegerTy(1))\n      continue;\n\n    // All uses of the shuffle should be sunk to avoid duplicating it across gpr\n    // and vector registers\n    for (Use &U : Op->uses()) {\n      Instruction *Insn = cast<Instruction>(U.getUser());\n      if (!canSplatOperand(Insn, U.getOperandNo()))\n        return false;\n    }\n\n    Ops.push_back(&Op->getOperandUse(0));\n    Ops.push_back(&OpIdx.value());\n  }\n  return true;\n}",
      "start_line": 1999,
      "end_line": 2036,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "value",
        "isVectorTy",
        "m_Undef",
        "getElementType",
        "m_Value",
        "hasVInstructions",
        "get",
        "push_back",
        "m_ZeroInt",
        "getUser",
        "m_ZeroMask",
        "isIntegerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "operand",
      "parameters": [
        {
          "type": "!Op ||",
          "name": "any_of(Ops"
        },
        {
          "type": "[&](Use",
          "name": "*U"
        }
      ],
      "body": "{ return U->get() == Op; }",
      "start_line": 2011,
      "end_line": 2012,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "get"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldScalarizeBinop",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "VecOp"
        }
      ],
      "body": "{\n  unsigned Opc = VecOp.getOpcode();\n\n  // Assume target opcodes can't be scalarized.\n  // TODO - do we have any exceptions?\n  if (Opc >= ISD::BUILTIN_OP_END)\n    return false;\n\n  // If the vector op is not supported, try to convert to scalar.\n  EVT VecVT = VecOp.getValueType();\n  if (!isOperationLegalOrCustomOrPromote(Opc, VecVT))\n    return true;\n\n  // If the vector op is supported, but the scalar op is not, the transform may\n  // not be worthwhile.\n  // Permit a vector binary operation can be converted to scalar binary\n  // operation which is custom lowered with illegal type.\n  EVT ScalarVT = VecVT.getScalarType();\n  return isOperationLegalOrCustomOrPromote(Opc, ScalarVT) ||\n         isOperationCustom(Opc, ScalarVT);\n}",
      "start_line": 2038,
      "end_line": 2058,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOpcode",
        "isOperationCustom",
        "getValueType",
        "isOperationLegalOrCustomOrPromote",
        "getScalarType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isOffsetFoldingLegal",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const GlobalAddressSDNode",
          "name": "*GA"
        }
      ],
      "body": "{\n  // In order to maximise the opportunity for common subexpression elimination,\n  // keep a separate ADD node for the global address offset instead of folding\n  // it in the global address node. Later peephole optimisations may choose to\n  // fold it back in when profitable.\n  return false;\n}",
      "start_line": 2060,
      "end_line": 2067,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "VT ==",
          "name": "MVT::f32"
        }
      ],
      "body": "{\n    IsSupportedVT = true;\n  }",
      "start_line": 2084,
      "end_line": 2086,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "VT ==",
          "name": "MVT::f64"
        }
      ],
      "body": "{\n    assert(Subtarget.hasStdExtD() && \"Expect D extension\");\n    IsSupportedVT = true;\n  }",
      "start_line": 2086,
      "end_line": 2089,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isFPImmLegal",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const APFloat",
          "name": "&Imm"
        },
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "bool",
          "name": "ForCodeSize"
        }
      ],
      "body": "{\n  bool IsLegalVT = false;\n  if (VT == MVT::f16)\n    IsLegalVT = Subtarget.hasStdExtZfhminOrZhinxmin();\n  else if (VT == MVT::f32)\n    IsLegalVT = Subtarget.hasStdExtFOrZfinx();\n  else if (VT == MVT::f64)\n    IsLegalVT = Subtarget.hasStdExtDOrZdinx();\n  else if (VT == MVT::bf16)\n    IsLegalVT = Subtarget.hasStdExtZfbfmin();\n\n  if (!IsLegalVT)\n    return false;\n\n  if (getLegalZfaFPImm(Imm, VT).first >= 0)\n    return true;\n\n  // Cannot create a 64 bit floating-point immediate value for rv32.\n  if (Subtarget.getXLen() < VT.getScalarSizeInBits()) {\n    // td can handle +0.0 or -0.0 already.\n    // -0.0 can be created by fmv + fneg.\n    return Imm.isZero();\n  }\n\n  // Special case: fmv + fneg\n  if (Imm.isNegZero())\n    return true;\n\n  // Building an integer and then converting requires a fmv at the end of\n  // the integer sequence.\n  const int Cost =\n      1 + RISCVMatInt::getIntMatCost(Imm.bitcastToAPInt(), Subtarget.getXLen(),\n                                     Subtarget);\n  return Cost <= FPImmCost;\n}",
      "start_line": 2102,
      "end_line": 2137,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtDOrZdinx",
        "hasStdExtZfhminOrZhinxmin",
        "isZero",
        "getIntMatCost",
        "hasStdExtFOrZfinx",
        "hasStdExtZfbfmin",
        "getXLen",
        "getScalarSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isExtractSubvectorCheap",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "ResVT"
        },
        {
          "type": "EVT",
          "name": "SrcVT"
        },
        {
          "type": "unsigned",
          "name": "Index"
        }
      ],
      "body": "{\n  if (!isOperationLegalOrCustom(ISD::EXTRACT_SUBVECTOR, ResVT))\n    return false;\n\n  // Only support extracting a fixed from a fixed vector for now.\n  if (ResVT.isScalableVector() || SrcVT.isScalableVector())\n    return false;\n\n  unsigned ResElts = ResVT.getVectorNumElements();\n  unsigned SrcElts = SrcVT.getVectorNumElements();\n\n  // Convervatively only handle extracting half of a vector.\n  // TODO: Relax this.\n  if ((ResElts * 2) != SrcElts)\n    return false;\n\n  // The smallest type we can slide is i8.\n  // TODO: We can extract index 0 from a mask vector without a slide.\n  if (ResVT.getVectorElementType() == MVT::i1)\n    return false;\n\n  // Slide can support arbitrary index, but we only treat vslidedown.vi as\n  // cheap.\n  if (Index >= 32)\n    return false;\n\n  // TODO: We can do arbitrary slidedowns, but for now only support extracting\n  // the upper half of a vector until we have more test coverage.\n  return Index == 0 || Index == ResElts;\n}",
      "start_line": 2140,
      "end_line": 2170,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getVectorNumElements",
        "isScalableVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRegisterTypeForCallingConv",
      "return_type": "MVT",
      "parameters": [
        {
          "type": "LLVMContext",
          "name": "&Context"
        },
        {
          "type": "CallingConv::ID",
          "name": "CC"
        },
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  // Use f32 to pass f16 if it is legal and Zfh/Zfhmin is not enabled.\n  // We might still end up using a GPR but that will be decided based on ABI.\n  if (VT == MVT::f16 && Subtarget.hasStdExtFOrZfinx() &&\n      !Subtarget.hasStdExtZfhminOrZhinxmin())\n    return MVT::f32;\n\n  MVT PartVT = TargetLowering::getRegisterTypeForCallingConv(Context, CC, VT);\n\n  if (RV64LegalI32 && Subtarget.is64Bit() && PartVT == MVT::i32)\n    return MVT::i64;\n\n  return PartVT;\n}",
      "start_line": 2172,
      "end_line": 2187,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZfhminOrZhinxmin",
        "getRegisterTypeForCallingConv"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getNumRegistersForCallingConv",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "LLVMContext",
          "name": "&Context"
        },
        {
          "type": "CallingConv::ID",
          "name": "CC"
        },
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  // Use f32 to pass f16 if it is legal and Zfh/Zfhmin is not enabled.\n  // We might still end up using a GPR but that will be decided based on ABI.\n  if (VT == MVT::f16 && Subtarget.hasStdExtFOrZfinx() &&\n      !Subtarget.hasStdExtZfhminOrZhinxmin())\n    return 1;\n\n  return TargetLowering::getNumRegistersForCallingConv(Context, CC, VT);\n}",
      "start_line": 2189,
      "end_line": 2199,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZfhminOrZhinxmin",
        "getNumRegistersForCallingConv"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVectorTypeBreakdownForCallingConv",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "LLVMContext",
          "name": "&Context"
        },
        {
          "type": "CallingConv::ID",
          "name": "CC"
        },
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "EVT",
          "name": "&IntermediateVT"
        },
        {
          "type": "unsigned",
          "name": "&NumIntermediates"
        },
        {
          "type": "MVT",
          "name": "&RegisterVT"
        }
      ],
      "body": "{\n  unsigned NumRegs = TargetLowering::getVectorTypeBreakdownForCallingConv(\n      Context, CC, VT, IntermediateVT, NumIntermediates, RegisterVT);\n\n  if (RV64LegalI32 && Subtarget.is64Bit() && IntermediateVT == MVT::i32)\n    IntermediateVT = MVT::i64;\n\n  if (RV64LegalI32 && Subtarget.is64Bit() && RegisterVT == MVT::i32)\n    RegisterVT = MVT::i64;\n\n  return NumRegs;\n}",
      "start_line": 2201,
      "end_line": 2214,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getVectorTypeBreakdownForCallingConv"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "translateSetCCForBranch",
      "return_type": "void",
      "parameters": [
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SDValue",
          "name": "&LHS"
        },
        {
          "type": "SDValue",
          "name": "&RHS"
        },
        {
          "type": "ISD::CondCode",
          "name": "&CC"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  // If this is a single bit test that can't be handled by ANDI, shift the\n  // bit to be tested to the MSB and perform a signed compare with 0.\n  if (isIntEqualitySetCC(CC) && isNullConstant(RHS) &&\n      LHS.getOpcode() == ISD::AND && LHS.hasOneUse() &&\n      isa<ConstantSDNode>(LHS.getOperand(1))) {\n    uint64_t Mask = LHS.getConstantOperandVal(1);\n    if ((isPowerOf2_64(Mask) || isMask_64(Mask)) && !isInt<12>(Mask)) {\n      unsigned ShAmt = 0;\n      if (isPowerOf2_64(Mask)) {\n        CC = CC == ISD::SETEQ ? ISD::SETGE : ISD::SETLT;\n        ShAmt = LHS.getValueSizeInBits() - 1 - Log2_64(Mask);\n      } else {\n        ShAmt = LHS.getValueSizeInBits() - llvm::bit_width(Mask);\n      }\n\n      LHS = LHS.getOperand(0);\n      if (ShAmt != 0)\n        LHS = DAG.getNode(ISD::SHL, DL, LHS.getValueType(), LHS,\n                          DAG.getConstant(ShAmt, DL, LHS.getValueType()));\n      return;\n    }\n  }\n\n  if (auto *RHSC = dyn_cast<ConstantSDNode>(RHS)) {\n    int64_t C = RHSC->getSExtValue();\n    switch (CC) {\n    default: break;\n    case ISD::SETGT:\n      // Convert X > -1 to X >= 0.\n      if (C == -1) {\n        RHS = DAG.getConstant(0, DL, RHS.getValueType());\n        CC = ISD::SETGE;\n        return;\n      }\n      break;\n    case ISD::SETLT:\n      // Convert X < 1 to 0 >= X.\n      if (C == 1) {\n        RHS = LHS;\n        LHS = DAG.getConstant(0, DL, RHS.getValueType());\n        CC = ISD::SETGE;\n        return;\n      }\n      break;\n    }\n  }\n\n  switch (CC) {\n  default:\n    break;\n  case ISD::SETGT:\n  case ISD::SETLE:\n  case ISD::SETUGT:\n  case ISD::SETULE:\n    CC = ISD::getSetCCSwappedOperands(CC);\n    std::swap(LHS, RHS);\n    break;\n  }\n}",
      "start_line": 2220,
      "end_line": 2280,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "translateSetCCForBranch",
          "condition": "CC",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "translateSetCCForBranch",
          "condition": "CC",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "hasOneUse",
        "getConstantOperandVal",
        "swap",
        "bit_width",
        "getOpcode",
        "Log2_64",
        "getValueSizeInBits",
        "getConstant",
        "isMask_64",
        "getOperand",
        "isNullConstant",
        "getSExtValue",
        "getSetCCSwappedOperands",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getLMUL",
      "return_type": "VLMUL",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  assert(VT.isScalableVector() && \"Expecting a scalable vector type\");\n  unsigned KnownSize = VT.getSizeInBits().getKnownMinValue();\n  if (VT.getVectorElementType() == MVT::i1)\n    KnownSize *= 8;\n\n  switch (KnownSize) {\n  default:\n    llvm_unreachable(\"Invalid LMUL.\");\n  case 8:\n    return RISCVII::VLMUL::LMUL_F8;\n  case 16:\n    return RISCVII::VLMUL::LMUL_F4;\n  case 32:\n    return RISCVII::VLMUL::LMUL_F2;\n  case 64:\n    return RISCVII::VLMUL::LMUL_1;\n  case 128:\n    return RISCVII::VLMUL::LMUL_2;\n  case 256:\n    return RISCVII::VLMUL::LMUL_4;\n  case 512:\n    return RISCVII::VLMUL::LMUL_8;\n  }\n}",
      "start_line": 2282,
      "end_line": 2306,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getLMUL",
          "condition": "KnownSize",
          "cases": [
            {
              "label": "8",
              "return": "RISCVII::VLMUL::LMUL_F8",
              "fallthrough": false
            },
            {
              "label": "16",
              "return": "RISCVII::VLMUL::LMUL_F4",
              "fallthrough": false
            },
            {
              "label": "32",
              "return": "RISCVII::VLMUL::LMUL_F2",
              "fallthrough": false
            },
            {
              "label": "64",
              "return": "RISCVII::VLMUL::LMUL_1",
              "fallthrough": false
            },
            {
              "label": "128",
              "return": "RISCVII::VLMUL::LMUL_2",
              "fallthrough": false
            },
            {
              "label": "256",
              "return": "RISCVII::VLMUL::LMUL_4",
              "fallthrough": false
            },
            {
              "label": "512",
              "return": "RISCVII::VLMUL::LMUL_8",
              "fallthrough": false
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable",
        "getSizeInBits",
        "getKnownMinValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRegClassIDForLMUL",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "RISCVII::VLMUL",
          "name": "LMul"
        }
      ],
      "body": "{\n  switch (LMul) {\n  default:\n    llvm_unreachable(\"Invalid LMUL.\");\n  case RISCVII::VLMUL::LMUL_F8:\n  case RISCVII::VLMUL::LMUL_F4:\n  case RISCVII::VLMUL::LMUL_F2:\n  case RISCVII::VLMUL::LMUL_1:\n    return RISCV::VRRegClassID;\n  case RISCVII::VLMUL::LMUL_2:\n    return RISCV::VRM2RegClassID;\n  case RISCVII::VLMUL::LMUL_4:\n    return RISCV::VRM4RegClassID;\n  case RISCVII::VLMUL::LMUL_8:\n    return RISCV::VRM8RegClassID;\n  }\n}",
      "start_line": 2308,
      "end_line": 2324,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getRegClassIDForLMUL",
          "condition": "LMul",
          "cases": [
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getSubregIndexByMVT",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "unsigned",
          "name": "Index"
        }
      ],
      "body": "{\n  RISCVII::VLMUL LMUL = getLMUL(VT);\n  if (LMUL == RISCVII::VLMUL::LMUL_F8 ||\n      LMUL == RISCVII::VLMUL::LMUL_F4 ||\n      LMUL == RISCVII::VLMUL::LMUL_F2 ||\n      LMUL == RISCVII::VLMUL::LMUL_1) {\n    static_assert(RISCV::sub_vrm1_7 == RISCV::sub_vrm1_0 + 7,\n                  \"Unexpected subreg numbering\");\n    return RISCV::sub_vrm1_0 + Index;\n  }\n  if (LMUL == RISCVII::VLMUL::LMUL_2) {\n    static_assert(RISCV::sub_vrm2_3 == RISCV::sub_vrm2_0 + 3,\n                  \"Unexpected subreg numbering\");\n    return RISCV::sub_vrm2_0 + Index;\n  }\n  if (LMUL == RISCVII::VLMUL::LMUL_4) {\n    static_assert(RISCV::sub_vrm4_1 == RISCV::sub_vrm4_0 + 1,\n                  \"Unexpected subreg numbering\");\n    return RISCV::sub_vrm4_0 + Index;\n  }\n  llvm_unreachable(\"Invalid vector type.\");\n}",
      "start_line": 2326,
      "end_line": 2347,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getLMUL",
        "static_assert",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRegClassIDForVecVT",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  if (VT.getVectorElementType() == MVT::i1)\n    return RISCV::VRRegClassID;\n  return getRegClassIDForLMUL(getLMUL(VT));\n}",
      "start_line": 2349,
      "end_line": 2353,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getRegClassIDForLMUL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "mergeStoresAfterLegalization",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  return !Subtarget.useRVVForFixedLengthVectors() ||\n         (VT.isFixedLengthVector() && VT.getVectorElementType() == MVT::i1);\n}",
      "start_line": 2393,
      "end_line": 2396,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "useRVVForFixedLengthVectors",
        "isFixedLengthVector",
        "getVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isLegalElementTypeForRVV",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "ScalarTy"
        }
      ],
      "body": "{\n  if (!ScalarTy.isSimple())\n    return false;\n  switch (ScalarTy.getSimpleVT().SimpleTy) {\n  case MVT::iPTR:\n    return Subtarget.is64Bit() ? Subtarget.hasVInstructionsI64() : true;\n  case MVT::i8:\n  case MVT::i16:\n  case MVT::i32:\n    return true;\n  case MVT::i64:\n    return Subtarget.hasVInstructionsI64();\n  case MVT::f16:\n    return Subtarget.hasVInstructionsF16();\n  case MVT::f32:\n    return Subtarget.hasVInstructionsF32();\n  case MVT::f64:\n    return Subtarget.hasVInstructionsF64();\n  default:\n    return false;\n  }\n}",
      "start_line": 2398,
      "end_line": 2419,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasVInstructionsI64",
        "is64Bit",
        "hasVInstructionsF64",
        "hasVInstructionsF32",
        "hasVInstructionsF16"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineRepeatedFPDivisors",
      "return_type": "unsigned",
      "parameters": [],
      "body": "{\n  return NumRepeatedDivisors;\n}",
      "start_line": 2422,
      "end_line": 2424,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVLOperand",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        }
      ],
      "body": "{\n  assert((Op.getOpcode() == ISD::INTRINSIC_WO_CHAIN ||\n          Op.getOpcode() == ISD::INTRINSIC_W_CHAIN) &&\n         \"Unexpected opcode\");\n  bool HasChain = Op.getOpcode() == ISD::INTRINSIC_W_CHAIN;\n  unsigned IntNo = Op.getConstantOperandVal(HasChain ? 1 : 0);\n  const RISCVVIntrinsicsTable::RISCVVIntrinsicInfo *II =\n      RISCVVIntrinsicsTable::getRISCVVIntrinsicInfo(IntNo);\n  if (!II)\n    return SDValue();\n  return Op.getOperand(II->VLOperand + 1 + HasChain);\n}",
      "start_line": 2426,
      "end_line": 2437,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "getConstantOperandVal",
        "getOpcode",
        "getRISCVVIntrinsicInfo",
        "getOperand"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "useRVVForFixedLengthVectorVT",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert(VT.isFixedLengthVector() && \"Expected a fixed length vector type!\");\n  if (!Subtarget.useRVVForFixedLengthVectors())\n    return false;\n\n  // We only support a set of vector types with a consistent maximum fixed size\n  // across all supported vector element types to avoid legalization issues.\n  // Therefore -- since the largest is v1024i8/v512i16/etc -- the largest\n  // fixed-length vector type we support is 1024 bytes.\n  if (VT.getFixedSizeInBits() > 1024 * 8)\n    return false;\n\n  unsigned MinVLen = Subtarget.getRealMinVLen();\n\n  MVT EltVT = VT.getVectorElementType();\n\n  // Don't use RVV for vectors we cannot scalarize if required.\n  switch (EltVT.SimpleTy) {\n  // i1 is supported but has different rules.\n  default:\n    return false;\n  case MVT::i1:\n    // Masks can only use a single register.\n    if (VT.getVectorNumElements() > MinVLen)\n      return false;\n    MinVLen /= 8;\n    break;\n  case MVT::i8:\n  case MVT::i16:\n  case MVT::i32:\n    break;\n  case MVT::i64:\n    if (!Subtarget.hasVInstructionsI64())\n      return false;\n    break;\n  case MVT::f16:\n    if (!Subtarget.hasVInstructionsF16Minimal())\n      return false;\n    break;\n  case MVT::f32:\n    if (!Subtarget.hasVInstructionsF32())\n      return false;\n    break;\n  case MVT::f64:\n    if (!Subtarget.hasVInstructionsF64())\n      return false;\n    break;\n  }\n\n  // Reject elements larger than ELEN.\n  if (EltVT.getSizeInBits() > Subtarget.getELen())\n    return false;\n\n  unsigned LMul = divideCeil(VT.getSizeInBits(), MinVLen);\n  // Don't use RVV for types that don't fit.\n  if (LMul > Subtarget.getMaxLMULForFixedLengthVectors())\n    return false;\n\n  // TODO: Perhaps an artificial restriction, but worth having whilst getting\n  // the base fixed length RVV support in place.\n  if (!VT.isPow2VectorType())\n    return false;\n\n  return true;\n}",
      "start_line": 2439,
      "end_line": 2504,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "useRVVForFixedLengthVectorVT",
          "condition": "EltVT.SimpleTy",
          "cases": [
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "false"
        }
      ],
      "calls": [
        "getRealMinVLen",
        "divideCeil",
        "getELen",
        "getVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "useRVVForFixedLengthVectorVT",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  return ::useRVVForFixedLengthVectorVT(VT, Subtarget);\n}",
      "start_line": 2506,
      "end_line": 2508,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "useRVVForFixedLengthVectorVT"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getContainerForFixedLengthVector",
      "return_type": "MVT",
      "parameters": [
        {
          "type": "const TargetLowering",
          "name": "&TLI"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // This may be called before legal types are setup.\n  assert(((VT.isFixedLengthVector() && TLI.isTypeLegal(VT)) ||\n          useRVVForFixedLengthVectorVT(VT, Subtarget)) &&\n         \"Expected legal fixed length vector!\");\n\n  unsigned MinVLen = Subtarget.getRealMinVLen();\n  unsigned MaxELen = Subtarget.getELen();\n\n  MVT EltVT = VT.getVectorElementType();\n  switch (EltVT.SimpleTy) {\n  default:\n    llvm_unreachable(\"unexpected element type for RVV container\");\n  case MVT::i1:\n  case MVT::i8:\n  case MVT::i16:\n  case MVT::i32:\n  case MVT::i64:\n  case MVT::f16:\n  case MVT::f32:\n  case MVT::f64: {\n    // We prefer to use LMUL=1 for VLEN sized types. Use fractional lmuls for\n    // narrower types. The smallest fractional LMUL we support is 8/ELEN. Within\n    // each fractional LMUL we support SEW between 8 and LMUL*ELEN.\n    unsigned NumElts =\n        (VT.getVectorNumElements() * RISCV::RVVBitsPerBlock) / MinVLen;\n    NumElts = std::max(NumElts, RISCV::RVVBitsPerBlock / MaxELen);\n    assert(isPowerOf2_32(NumElts) && \"Expected power of 2 NumElts\");\n    return MVT::getScalableVectorVT(EltVT, NumElts);\n  }\n  }\n}",
      "start_line": 2511,
      "end_line": 2543,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "getContainerForFixedLengthVector",
          "condition": "EltVT.SimpleTy",
          "cases": [
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "useRVVForFixedLengthVectorVT",
        "isTypeLegal",
        "getRealMinVLen",
        "getScalableVectorVT",
        "getVectorNumElements",
        "getELen",
        "getVectorElementType",
        "max",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getContainerForFixedLengthVector",
      "return_type": "MVT",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  return getContainerForFixedLengthVector(DAG.getTargetLoweringInfo(), VT,\n                                          Subtarget);\n}",
      "start_line": 2545,
      "end_line": 2549,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContainerForFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getContainerForFixedLengthVector",
      "return_type": "MVT",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  return ::getContainerForFixedLengthVector(*this, VT, getSubtarget());\n}",
      "start_line": 2551,
      "end_line": 2553,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContainerForFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "convertToScalableVector",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "V"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert(VT.isScalableVector() &&\n         \"Expected to convert into a scalable vector!\");\n  assert(V.getValueType().isFixedLengthVector() &&\n         \"Expected a fixed length vector operand!\");\n  SDLoc DL(V);\n  SDValue Zero = DAG.getConstant(0, DL, Subtarget.getXLenVT());\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, DAG.getUNDEF(VT), V, Zero);\n}",
      "start_line": 2556,
      "end_line": 2565,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNode",
        "getConstant",
        "isFixedLengthVector",
        "DL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "convertFromScalableVector",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "V"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert(VT.isFixedLengthVector() &&\n         \"Expected to convert into a fixed length vector!\");\n  assert(V.getValueType().isScalableVector() &&\n         \"Expected a scalable vector operand!\");\n  SDLoc DL(V);\n  SDValue Zero = DAG.getConstant(0, DL, Subtarget.getXLenVT());\n  return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, V, Zero);\n}",
      "start_line": 2568,
      "end_line": 2577,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNode",
        "isScalableVector",
        "getConstant",
        "DL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getMaskTypeFor",
      "return_type": "MVT",
      "parameters": [
        {
          "type": "MVT",
          "name": "VecVT"
        }
      ],
      "body": "{\n  assert(VecVT.isVector());\n  ElementCount EC = VecVT.getVectorElementCount();\n  return MVT::getVectorVT(MVT::i1, EC);\n}",
      "start_line": 2582,
      "end_line": 2586,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getVectorElementCount",
        "getVectorVT"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getAllOnesMask",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "MVT",
          "name": "VecVT"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MVT MaskVT = getMaskTypeFor(VecVT);\n  return DAG.getNode(RISCVISD::VMSET_VL, DL, MaskVT, VL);\n}",
      "start_line": 2590,
      "end_line": 2594,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMaskTypeFor",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVLOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "uint64_t",
          "name": "NumElts"
        },
        {
          "type": "MVT",
          "name": "ContainerVT"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // If we know the exact VLEN, and our VL is exactly equal to VLMAX,\n  // canonicalize the representation.  InsertVSETVLI will pick the immediate\n  // encoding later if profitable.\n  const auto [MinVLMAX, MaxVLMAX] =\n      RISCVTargetLowering::computeVLMAXBounds(ContainerVT, Subtarget);\n  if (MinVLMAX == MaxVLMAX && NumElts == MinVLMAX)\n    return DAG.getRegister(RISCV::X0, Subtarget.getXLenVT());\n\n  return DAG.getConstant(NumElts, DL, Subtarget.getXLenVT());\n}",
      "start_line": 2596,
      "end_line": 2607,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getConstant",
        "getRegister",
        "computeVLMAXBounds"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "computeVLMax",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "MVT",
          "name": "VecVT"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  assert(VecVT.isScalableVector() && \"Expected scalable vector\");\n  return DAG.getElementCount(DL, Subtarget.getXLenVT(),\n                             VecVT.getVectorElementCount());\n}",
      "start_line": 2641,
      "end_line": 2646,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getVectorElementCount",
        "getElementCount"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldExpandBuildVectorWithShuffles",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "unsigned",
          "name": "DefinedValues"
        }
      ],
      "body": "{\n  return false;\n}",
      "start_line": 2675,
      "end_line": 2678,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getLMULCost",
      "return_type": "InstructionCost",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  // TODO: Here assume reciprocal throughput is 1 for LMUL_1, it is\n  // implementation-defined.\n  if (!VT.isVector())\n    return InstructionCost::getInvalid();\n  unsigned DLenFactor = Subtarget.getDLenFactor();\n  unsigned Cost;\n  if (VT.isScalableVector()) {\n    unsigned LMul;\n    bool Fractional;\n    std::tie(LMul, Fractional) =\n        RISCVVType::decodeVLMUL(RISCVTargetLowering::getLMUL(VT));\n    if (Fractional)\n      Cost = LMul <= DLenFactor ? (DLenFactor / LMul) : 1;\n    else\n      Cost = (LMul * DLenFactor);\n  } else {\n    Cost = divideCeil(VT.getSizeInBits(), Subtarget.getRealMinVLen() / DLenFactor);\n  }\n  return Cost;\n}",
      "start_line": 2680,
      "end_line": 2700,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "divideCeil",
        "getInvalid",
        "getRealMinVLen",
        "tie",
        "getDLenFactor",
        "decodeVLMUL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVRGatherVVCost",
      "return_type": "InstructionCost",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  return getLMULCost(VT) * getLMULCost(VT);\n}",
      "start_line": 2706,
      "end_line": 2708,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLMULCost"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVRGatherVICost",
      "return_type": "InstructionCost",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  return getLMULCost(VT);\n}",
      "start_line": 2713,
      "end_line": 2715,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLMULCost"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVSlideVXCost",
      "return_type": "InstructionCost",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  return getLMULCost(VT);\n}",
      "start_line": 2721,
      "end_line": 2723,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLMULCost"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVSlideVICost",
      "return_type": "InstructionCost",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  return getLMULCost(VT);\n}",
      "start_line": 2729,
      "end_line": 2731,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLMULCost"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFP_TO_INT_SAT",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // RISC-V FP-to-int conversions saturate to the destination register size, but\n  // don't produce 0 for nan. We can use a conversion instruction and fix the\n  // nan case with a compare and a select.\n  SDValue Src = Op.getOperand(0);\n\n  MVT DstVT = Op.getSimpleValueType();\n  EVT SatVT = cast<VTSDNode>(Op.getOperand(1))->getVT();\n\n  bool IsSigned = Op.getOpcode() == ISD::FP_TO_SINT_SAT;\n\n  if (!DstVT.isVector()) {\n    // For bf16 or for f16 in absense of Zfh, promote to f32, then saturate\n    // the result.\n    if ((Src.getValueType() == MVT::f16 && !Subtarget.hasStdExtZfhOrZhinx()) ||\n        Src.getValueType() == MVT::bf16) {\n      Src = DAG.getNode(ISD::FP_EXTEND, SDLoc(Op), MVT::f32, Src);\n    }\n\n    unsigned Opc;\n    if (SatVT == DstVT)\n      Opc = IsSigned ? RISCVISD::FCVT_X : RISCVISD::FCVT_XU;\n    else if (DstVT == MVT::i64 && SatVT == MVT::i32)\n      Opc = IsSigned ? RISCVISD::FCVT_W_RV64 : RISCVISD::FCVT_WU_RV64;\n    else\n      return SDValue();\n    // FIXME: Support other SatVTs by clamping before or after the conversion.\n\n    SDLoc DL(Op);\n    SDValue FpToInt = DAG.getNode(\n        Opc, DL, DstVT, Src,\n        DAG.getTargetConstant(RISCVFPRndMode::RTZ, DL, Subtarget.getXLenVT()));\n\n    if (Opc == RISCVISD::FCVT_WU_RV64)\n      FpToInt = DAG.getZeroExtendInReg(FpToInt, DL, MVT::i32);\n\n    SDValue ZeroInt = DAG.getConstant(0, DL, DstVT);\n    return DAG.getSelectCC(DL, Src, Src, ZeroInt, FpToInt,\n                           ISD::CondCode::SETUO);\n  }\n\n  // Vectors.\n\n  MVT DstEltVT = DstVT.getVectorElementType();\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT SrcEltVT = SrcVT.getVectorElementType();\n  unsigned SrcEltSize = SrcEltVT.getSizeInBits();\n  unsigned DstEltSize = DstEltVT.getSizeInBits();\n\n  // Only handle saturating to the destination type.\n  if (SatVT != DstEltVT)\n    return SDValue();\n\n  // FIXME: Don't support narrowing by more than 1 steps for now.\n  if (SrcEltSize > (2 * DstEltSize))\n    return SDValue();\n\n  MVT DstContainerVT = DstVT;\n  MVT SrcContainerVT = SrcVT;\n  if (DstVT.isFixedLengthVector()) {\n    DstContainerVT = getContainerForFixedLengthVector(DAG, DstVT, Subtarget);\n    SrcContainerVT = getContainerForFixedLengthVector(DAG, SrcVT, Subtarget);\n    assert(DstContainerVT.getVectorElementCount() ==\n               SrcContainerVT.getVectorElementCount() &&\n           \"Expected same element count\");\n    Src = convertToScalableVector(SrcContainerVT, Src, DAG, Subtarget);\n  }\n\n  SDLoc DL(Op);\n\n  auto [Mask, VL] = getDefaultVLOps(DstVT, DstContainerVT, DL, DAG, Subtarget);\n\n  SDValue IsNan = DAG.getNode(RISCVISD::SETCC_VL, DL, Mask.getValueType(),\n                              {Src, Src, DAG.getCondCode(ISD::SETNE),\n                               DAG.getUNDEF(Mask.getValueType()), Mask, VL});\n\n  // Need to widen by more than 1 step, promote the FP type, then do a widening\n  // convert.\n  if (DstEltSize > (2 * SrcEltSize)) {\n    assert(SrcContainerVT.getVectorElementType() == MVT::f16 && \"Unexpected VT!\");\n    MVT InterVT = SrcContainerVT.changeVectorElementType(MVT::f32);\n    Src = DAG.getNode(RISCVISD::FP_EXTEND_VL, DL, InterVT, Src, Mask, VL);\n  }\n\n  unsigned RVVOpc =\n      IsSigned ? RISCVISD::VFCVT_RTZ_X_F_VL : RISCVISD::VFCVT_RTZ_XU_F_VL;\n  SDValue Res = DAG.getNode(RVVOpc, DL, DstContainerVT, Src, Mask, VL);\n\n  SDValue SplatZero = DAG.getNode(\n      RISCVISD::VMV_V_X_VL, DL, DstContainerVT, DAG.getUNDEF(DstContainerVT),\n      DAG.getConstant(0, DL, Subtarget.getXLenVT()), VL);\n  Res = DAG.getNode(RISCVISD::VMERGE_VL, DL, DstContainerVT, IsNan, SplatZero,\n                    Res, DAG.getUNDEF(DstContainerVT), VL);\n\n  if (DstVT.isFixedLengthVector())\n    Res = convertFromScalableVector(DstVT, Res, DAG, Subtarget);\n\n  return Res;\n}",
      "start_line": 2733,
      "end_line": 2832,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getZeroExtendInReg",
        "getSelectCC",
        "convertFromScalableVector",
        "getSizeInBits",
        "getVT",
        "getUNDEF",
        "getVectorElementType",
        "getVectorElementCount",
        "SDValue",
        "getOpcode",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getOperand",
        "getCondCode",
        "changeVectorElementType",
        "hasStdExtZfhOrZhinx",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "matchRoundingOp",
      "return_type": "RoundingMode",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opc"
        }
      ],
      "body": "{\n  switch (Opc) {\n  case ISD::FROUNDEVEN:\n  case ISD::STRICT_FROUNDEVEN:\n  case ISD::VP_FROUNDEVEN:\n    return RISCVFPRndMode::RNE;\n  case ISD::FTRUNC:\n  case ISD::STRICT_FTRUNC:\n  case ISD::VP_FROUNDTOZERO:\n    return RISCVFPRndMode::RTZ;\n  case ISD::FFLOOR:\n  case ISD::STRICT_FFLOOR:\n  case ISD::VP_FFLOOR:\n    return RISCVFPRndMode::RDN;\n  case ISD::FCEIL:\n  case ISD::STRICT_FCEIL:\n  case ISD::VP_FCEIL:\n    return RISCVFPRndMode::RUP;\n  case ISD::FROUND:\n  case ISD::STRICT_FROUND:\n  case ISD::VP_FROUND:\n    return RISCVFPRndMode::RMM;\n  case ISD::FRINT:\n    return RISCVFPRndMode::DYN;\n  }\n\n  return RISCVFPRndMode::Invalid;\n}",
      "start_line": 2834,
      "end_line": 2861,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "matchRoundingOp",
          "condition": "Opc",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorFTRUNC_FCEIL_FFLOOR_FROUND",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  assert(VT.isVector() && \"Unexpected type\");\n\n  SDLoc DL(Op);\n\n  SDValue Src = Op.getOperand(0);\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n    Src = convertToScalableVector(ContainerVT, Src, DAG, Subtarget);\n  }\n\n  SDValue Mask, VL;\n  if (Op->isVPOpcode()) {\n    Mask = Op.getOperand(1);\n    if (VT.isFixedLengthVector())\n      Mask = convertToScalableVector(getMaskTypeFor(ContainerVT), Mask, DAG,\n                                     Subtarget);\n    VL = Op.getOperand(2);\n  } else {\n    std::tie(Mask, VL) = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n  }\n\n  // Freeze the source since we are increasing the number of uses.\n  Src = DAG.getFreeze(Src);\n\n  // We do the conversion on the absolute value and fix the sign at the end.\n  SDValue Abs = DAG.getNode(RISCVISD::FABS_VL, DL, ContainerVT, Src, Mask, VL);\n\n  // Determine the largest integer that can be represented exactly. This and\n  // values larger than it don't have any fractional bits so don't need to\n  // be converted.\n  const fltSemantics &FltSem = DAG.EVTToAPFloatSemantics(ContainerVT);\n  unsigned Precision = APFloat::semanticsPrecision(FltSem);\n  APFloat MaxVal = APFloat(FltSem);\n  MaxVal.convertFromAPInt(APInt::getOneBitSet(Precision, Precision - 1),\n                          /*IsSigned*/ false, APFloat::rmNearestTiesToEven);\n  SDValue MaxValNode =\n      DAG.getConstantFP(MaxVal, DL, ContainerVT.getVectorElementType());\n  SDValue MaxValSplat = DAG.getNode(RISCVISD::VFMV_V_F_VL, DL, ContainerVT,\n                                    DAG.getUNDEF(ContainerVT), MaxValNode, VL);\n\n  // If abs(Src) was larger than MaxVal or nan, keep it.\n  MVT SetccVT = MVT::getVectorVT(MVT::i1, ContainerVT.getVectorElementCount());\n  Mask =\n      DAG.getNode(RISCVISD::SETCC_VL, DL, SetccVT,\n                  {Abs, MaxValSplat, DAG.getCondCode(ISD::SETOLT),\n                   Mask, Mask, VL});\n\n  // Truncate to integer and convert back to FP.\n  MVT IntVT = ContainerVT.changeVectorElementTypeToInteger();\n  MVT XLenVT = Subtarget.getXLenVT();\n  SDValue Truncated;\n\n  switch (Op.getOpcode()) {\n  default:\n    llvm_unreachable(\"Unexpected opcode\");\n  case ISD::FCEIL:\n  case ISD::VP_FCEIL:\n  case ISD::FFLOOR:\n  case ISD::VP_FFLOOR:\n  case ISD::FROUND:\n  case ISD::FROUNDEVEN:\n  case ISD::VP_FROUND:\n  case ISD::VP_FROUNDEVEN:\n  case ISD::VP_FROUNDTOZERO: {\n    RISCVFPRndMode::RoundingMode FRM = matchRoundingOp(Op.getOpcode());\n    assert(FRM != RISCVFPRndMode::Invalid);\n    Truncated = DAG.getNode(RISCVISD::VFCVT_RM_X_F_VL, DL, IntVT, Src, Mask,\n                            DAG.getTargetConstant(FRM, DL, XLenVT), VL);\n    break;\n  }\n  case ISD::FTRUNC:\n    Truncated = DAG.getNode(RISCVISD::VFCVT_RTZ_X_F_VL, DL, IntVT, Src,\n                            Mask, VL);\n    break;\n  case ISD::FRINT:\n  case ISD::VP_FRINT:\n    Truncated = DAG.getNode(RISCVISD::VFCVT_X_F_VL, DL, IntVT, Src, Mask, VL);\n    break;\n  case ISD::FNEARBYINT:\n  case ISD::VP_FNEARBYINT:\n    Truncated = DAG.getNode(RISCVISD::VFROUND_NOEXCEPT_VL, DL, ContainerVT, Src,\n                            Mask, VL);\n    break;\n  }\n\n  // VFROUND_NOEXCEPT_VL includes SINT_TO_FP_VL.\n  if (Truncated.getOpcode() != RISCVISD::VFROUND_NOEXCEPT_VL)\n    Truncated = DAG.getNode(RISCVISD::SINT_TO_FP_VL, DL, ContainerVT, Truncated,\n                            Mask, VL);\n\n  // Restore the original sign so that -0.0 is preserved.\n  Truncated = DAG.getNode(RISCVISD::FCOPYSIGN_VL, DL, ContainerVT, Truncated,\n                          Src, Src, Mask, VL);\n\n  if (!VT.isFixedLengthVector())\n    return Truncated;\n\n  return convertFromScalableVector(VT, Truncated, DAG, Subtarget);\n}",
      "start_line": 2867,
      "end_line": 2971,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "changeVectorElementTypeToInteger",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getFreeze",
        "getConstantFP",
        "APFloat",
        "semanticsPrecision",
        "getVectorVT",
        "matchRoundingOp",
        "getContainerForFixedLengthVector",
        "getOperand",
        "tie",
        "abs",
        "DL",
        "convertFromAPInt",
        "getXLenVT",
        "llvm_unreachable",
        "getNode",
        "EVTToAPFloatSemantics"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorStrictFTRUNC_FCEIL_FFLOOR_FROUND",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue Chain = Op.getOperand(0);\n  SDValue Src = Op.getOperand(1);\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n    Src = convertToScalableVector(ContainerVT, Src, DAG, Subtarget);\n  }\n\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  // Freeze the source since we are increasing the number of uses.\n  Src = DAG.getFreeze(Src);\n\n  // Covert sNan to qNan by executing x + x for all unordered elemenet x in Src.\n  MVT MaskVT = Mask.getSimpleValueType();\n  SDValue Unorder = DAG.getNode(RISCVISD::STRICT_FSETCC_VL, DL,\n                                DAG.getVTList(MaskVT, MVT::Other),\n                                {Chain, Src, Src, DAG.getCondCode(ISD::SETUNE),\n                                 DAG.getUNDEF(MaskVT), Mask, VL});\n  Chain = Unorder.getValue(1);\n  Src = DAG.getNode(RISCVISD::STRICT_FADD_VL, DL,\n                    DAG.getVTList(ContainerVT, MVT::Other),\n                    {Chain, Src, Src, DAG.getUNDEF(ContainerVT), Unorder, VL});\n  Chain = Src.getValue(1);\n\n  // We do the conversion on the absolute value and fix the sign at the end.\n  SDValue Abs = DAG.getNode(RISCVISD::FABS_VL, DL, ContainerVT, Src, Mask, VL);\n\n  // Determine the largest integer that can be represented exactly. This and\n  // values larger than it don't have any fractional bits so don't need to\n  // be converted.\n  const fltSemantics &FltSem = DAG.EVTToAPFloatSemantics(ContainerVT);\n  unsigned Precision = APFloat::semanticsPrecision(FltSem);\n  APFloat MaxVal = APFloat(FltSem);\n  MaxVal.convertFromAPInt(APInt::getOneBitSet(Precision, Precision - 1),\n                          /*IsSigned*/ false, APFloat::rmNearestTiesToEven);\n  SDValue MaxValNode =\n      DAG.getConstantFP(MaxVal, DL, ContainerVT.getVectorElementType());\n  SDValue MaxValSplat = DAG.getNode(RISCVISD::VFMV_V_F_VL, DL, ContainerVT,\n                                    DAG.getUNDEF(ContainerVT), MaxValNode, VL);\n\n  // If abs(Src) was larger than MaxVal or nan, keep it.\n  Mask = DAG.getNode(\n      RISCVISD::SETCC_VL, DL, MaskVT,\n      {Abs, MaxValSplat, DAG.getCondCode(ISD::SETOLT), Mask, Mask, VL});\n\n  // Truncate to integer and convert back to FP.\n  MVT IntVT = ContainerVT.changeVectorElementTypeToInteger();\n  MVT XLenVT = Subtarget.getXLenVT();\n  SDValue Truncated;\n\n  switch (Op.getOpcode()) {\n  default:\n    llvm_unreachable(\"Unexpected opcode\");\n  case ISD::STRICT_FCEIL:\n  case ISD::STRICT_FFLOOR:\n  case ISD::STRICT_FROUND:\n  case ISD::STRICT_FROUNDEVEN: {\n    RISCVFPRndMode::RoundingMode FRM = matchRoundingOp(Op.getOpcode());\n    assert(FRM != RISCVFPRndMode::Invalid);\n    Truncated = DAG.getNode(\n        RISCVISD::STRICT_VFCVT_RM_X_F_VL, DL, DAG.getVTList(IntVT, MVT::Other),\n        {Chain, Src, Mask, DAG.getTargetConstant(FRM, DL, XLenVT), VL});\n    break;\n  }\n  case ISD::STRICT_FTRUNC:\n    Truncated =\n        DAG.getNode(RISCVISD::STRICT_VFCVT_RTZ_X_F_VL, DL,\n                    DAG.getVTList(IntVT, MVT::Other), Chain, Src, Mask, VL);\n    break;\n  case ISD::STRICT_FNEARBYINT:\n    Truncated = DAG.getNode(RISCVISD::STRICT_VFROUND_NOEXCEPT_VL, DL,\n                            DAG.getVTList(ContainerVT, MVT::Other), Chain, Src,\n                            Mask, VL);\n    break;\n  }\n  Chain = Truncated.getValue(1);\n\n  // VFROUND_NOEXCEPT_VL includes SINT_TO_FP_VL.\n  if (Op.getOpcode() != ISD::STRICT_FNEARBYINT) {\n    Truncated = DAG.getNode(RISCVISD::STRICT_SINT_TO_FP_VL, DL,\n                            DAG.getVTList(ContainerVT, MVT::Other), Chain,\n                            Truncated, Mask, VL);\n    Chain = Truncated.getValue(1);\n  }\n\n  // Restore the original sign so that -0.0 is preserved.\n  Truncated = DAG.getNode(RISCVISD::FCOPYSIGN_VL, DL, ContainerVT, Truncated,\n                          Src, Src, Mask, VL);\n\n  if (VT.isFixedLengthVector())\n    Truncated = convertFromScalableVector(VT, Truncated, DAG, Subtarget);\n  return DAG.getMergeValues({Truncated, Chain}, DL);\n}",
      "start_line": 2976,
      "end_line": 3075,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "changeVectorElementTypeToInteger",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getFreeze",
        "getConstantFP",
        "APFloat",
        "semanticsPrecision",
        "getUNDEF",
        "matchRoundingOp",
        "getValue",
        "getContainerForFixedLengthVector",
        "getOperand",
        "getCondCode",
        "getTargetConstant",
        "abs",
        "getMergeValues",
        "DL",
        "convertFromAPInt",
        "getXLenVT",
        "llvm_unreachable",
        "getNode",
        "EVTToAPFloatSemantics"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFTRUNC_FCEIL_FFLOOR_FROUND",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  if (VT.isVector())\n    return lowerVectorFTRUNC_FCEIL_FFLOOR_FROUND(Op, DAG, Subtarget);\n\n  if (DAG.shouldOptForSize())\n    return SDValue();\n\n  SDLoc DL(Op);\n  SDValue Src = Op.getOperand(0);\n\n  // Create an integer the size of the mantissa with the MSB set. This and all\n  // values larger than it don't have any fractional bits so don't need to be\n  // converted.\n  const fltSemantics &FltSem = DAG.EVTToAPFloatSemantics(VT);\n  unsigned Precision = APFloat::semanticsPrecision(FltSem);\n  APFloat MaxVal = APFloat(FltSem);\n  MaxVal.convertFromAPInt(APInt::getOneBitSet(Precision, Precision - 1),\n                          /*IsSigned*/ false, APFloat::rmNearestTiesToEven);\n  SDValue MaxValNode = DAG.getConstantFP(MaxVal, DL, VT);\n\n  RISCVFPRndMode::RoundingMode FRM = matchRoundingOp(Op.getOpcode());\n  return DAG.getNode(RISCVISD::FROUND, DL, VT, Src, MaxValNode,\n                     DAG.getTargetConstant(FRM, DL, Subtarget.getXLenVT()));\n}",
      "start_line": 3077,
      "end_line": 3103,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getConstantFP",
        "matchRoundingOp",
        "APFloat",
        "lowerVectorFTRUNC_FCEIL_FFLOOR_FROUND",
        "getSimpleValueType",
        "semanticsPrecision",
        "getOperand",
        "DL",
        "convertFromAPInt",
        "getNode",
        "EVTToAPFloatSemantics"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorXRINT",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  assert(VT.isVector() && \"Unexpected type\");\n\n  SDLoc DL(Op);\n  SDValue Src = Op.getOperand(0);\n  MVT ContainerVT = VT;\n\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n    Src = convertToScalableVector(ContainerVT, Src, DAG, Subtarget);\n  }\n\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n  SDValue Truncated =\n      DAG.getNode(RISCVISD::VFCVT_X_F_VL, DL, ContainerVT, Src, Mask, VL);\n\n  if (!VT.isFixedLengthVector())\n    return Truncated;\n\n  return convertFromScalableVector(VT, Truncated, DAG, Subtarget);\n}",
      "start_line": 3106,
      "end_line": 3128,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getOperand",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVSlidedown",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "Merge"
        },
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SDValue",
          "name": "Offset"
        },
        {
          "type": "SDValue",
          "name": "Mask"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "unsigned Policy =",
          "name": "RISCVII::TAIL_UNDISTURBED_MASK_UNDISTURBED"
        }
      ],
      "body": "{\n  if (Merge.isUndef())\n    Policy = RISCVII::TAIL_AGNOSTIC | RISCVII::MASK_AGNOSTIC;\n  SDValue PolicyOp = DAG.getTargetConstant(Policy, DL, Subtarget.getXLenVT());\n  SDValue Ops[] = {Merge, Op, Offset, Mask, VL, PolicyOp};\n  return DAG.getNode(RISCVISD::VSLIDEDOWN_VL, DL, VT, Ops);\n}",
      "start_line": 3130,
      "end_line": 3140,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVSlideup",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "Merge"
        },
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SDValue",
          "name": "Offset"
        },
        {
          "type": "SDValue",
          "name": "Mask"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "unsigned Policy =",
          "name": "RISCVII::TAIL_UNDISTURBED_MASK_UNDISTURBED"
        }
      ],
      "body": "{\n  if (Merge.isUndef())\n    Policy = RISCVII::TAIL_AGNOSTIC | RISCVII::MASK_AGNOSTIC;\n  SDValue PolicyOp = DAG.getTargetConstant(Policy, DL, Subtarget.getXLenVT());\n  SDValue Ops[] = {Merge, Op, Offset, Mask, VL, PolicyOp};\n  return DAG.getNode(RISCVISD::VSLIDEUP_VL, DL, VT, Ops);\n}",
      "start_line": 3142,
      "end_line": 3152,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getLMUL1VT",
      "return_type": "MVT",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        }
      ],
      "body": "{\n  assert(VT.getVectorElementType().getSizeInBits() <= 64 &&\n         \"Unexpected vector MVT\");\n  return MVT::getScalableVectorVT(\n      VT.getVectorElementType(),\n      RISCV::RVVBitsPerBlock / VT.getVectorElementType().getSizeInBits());\n}",
      "start_line": 3154,
      "end_line": 3160,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getScalableVectorVT",
        "getSizeInBits",
        "getVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "matchSplatAsGather",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "SplatVal"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (SplatVal.getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n    return SDValue();\n  SDValue Vec = SplatVal.getOperand(0);\n  // Only perform this optimization on vectors of the same size for simplicity.\n  // Don't perform this optimization for i1 vectors.\n  // FIXME: Support i1 vectors, maybe by promoting to i8?\n  if (Vec.getValueType() != VT || VT.getVectorElementType() == MVT::i1)\n    return SDValue();\n  SDValue Idx = SplatVal.getOperand(1);\n  // The index must be a legal type.\n  if (Idx.getValueType() != Subtarget.getXLenVT())\n    return SDValue();\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n    Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n  }\n\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  SDValue Gather = DAG.getNode(RISCVISD::VRGATHER_VX_VL, DL, ContainerVT, Vec,\n                               Idx, DAG.getUNDEF(ContainerVT), Mask, VL);\n\n  if (!VT.isFixedLengthVector())\n    return Gather;\n\n  return convertFromScalableVector(VT, Gather, DAG, Subtarget);\n}",
      "start_line": 3303,
      "end_line": 3334,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getDefaultVLOps",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "getVectorElementType",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerBuildVectorViaDominantValues",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  assert(VT.isFixedLengthVector() && \"Unexpected vector!\");\n\n  MVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n\n  SDLoc DL(Op);\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  unsigned NumElts = Op.getNumOperands();\n\n  SDValue DominantValue;\n  unsigned MostCommonCount = 0;\n  DenseMap<SDValue, unsigned> ValueCounts;\n  unsigned NumUndefElts =\n      count_if(Op->op_values(), [](const SDValue &V) { return V.isUndef(); });\n\n  // Track the number of scalar loads we know we'd be inserting, estimated as\n  // any non-zero floating-point constant. Other kinds of element are either\n  // already in registers or are materialized on demand. The threshold at which\n  // a vector load is more desirable than several scalar materializion and\n  // vector-insertion instructions is not known.\n  unsigned NumScalarLoads = 0;\n\n  for (SDValue V : Op->op_values()) {\n    if (V.isUndef())\n      continue;\n\n    ValueCounts.insert(std::make_pair(V, 0));\n    unsigned &Count = ValueCounts[V];\n    if (0 == Count)\n      if (auto *CFP = dyn_cast<ConstantFPSDNode>(V))\n        NumScalarLoads += !CFP->isExactlyValue(+0.0);\n\n    // Is this value dominant? In case of a tie, prefer the highest element as\n    // it's cheaper to insert near the beginning of a vector than it is at the\n    // end.\n    if (++Count >= MostCommonCount) {\n      DominantValue = V;\n      MostCommonCount = Count;\n    }\n  }\n\n  assert(DominantValue && \"Not expecting an all-undef BUILD_VECTOR\");\n  unsigned NumDefElts = NumElts - NumUndefElts;\n  unsigned DominantValueCountThreshold = NumDefElts <= 2 ? 0 : NumDefElts - 2;\n\n  // Don't perform this optimization when optimizing for size, since\n  // materializing elements and inserting them tends to cause code bloat.\n  if (!DAG.shouldOptForSize() && NumScalarLoads < NumElts &&\n      (NumElts != 2 || ISD::isBuildVectorOfConstantSDNodes(Op.getNode())) &&\n      ((MostCommonCount > DominantValueCountThreshold) ||\n       (ValueCounts.size() <= Log2_32(NumDefElts)))) {\n    // Start by splatting the most common element.\n    SDValue Vec = DAG.getSplatBuildVector(VT, DL, DominantValue);\n\n    DenseSet<SDValue> Processed{DominantValue};\n\n    // We can handle an insert into the last element (of a splat) via\n    // v(f)slide1down.  This is slightly better than the vslideup insert\n    // lowering as it avoids the need for a vector group temporary.  It\n    // is also better than using vmerge.vx as it avoids the need to\n    // materialize the mask in a vector register.\n    if (SDValue LastOp = Op->getOperand(Op->getNumOperands() - 1);\n        !LastOp.isUndef() && ValueCounts[LastOp] == 1 &&\n        LastOp != DominantValue) {\n      Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n      auto OpCode =\n        VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL : RISCVISD::VSLIDE1DOWN_VL;\n      if (!VT.isFloatingPoint())\n        LastOp = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, LastOp);\n      Vec = DAG.getNode(OpCode, DL, ContainerVT, DAG.getUNDEF(ContainerVT), Vec,\n                        LastOp, Mask, VL);\n      Vec = convertFromScalableVector(VT, Vec, DAG, Subtarget);\n      Processed.insert(LastOp);\n    }\n\n    MVT SelMaskTy = VT.changeVectorElementType(MVT::i1);\n    for (const auto &OpIdx : enumerate(Op->ops())) {\n      const SDValue &V = OpIdx.value();\n      if (V.isUndef() || !Processed.insert(V).second)\n        continue;\n      if (ValueCounts[V] == 1) {\n        Vec = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT, Vec, V,\n                          DAG.getConstant(OpIdx.index(), DL, XLenVT));\n      } else {\n        // Blend in all instances of this value using a VSELECT, using a\n        // mask where each bit signals whether that element is the one\n        // we're after.\n        SmallVector<SDValue> Ops;\n        transform(Op->op_values(), std::back_inserter(Ops), [&](SDValue V1) {\n          return DAG.getConstant(V == V1, DL, XLenVT);\n        });\n        Vec = DAG.getNode(ISD::VSELECT, DL, VT,\n                          DAG.getBuildVector(SelMaskTy, DL, Ops),\n                          DAG.getSplatBuildVector(VT, DL, V), Vec);\n      }\n    }\n\n    return Vec;\n  }\n\n  return SDValue();\n}",
      "start_line": 3345,
      "end_line": 3450,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "isExactlyValue",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "Log2_32",
        "isFloatingPoint",
        "getNumOperands",
        "v",
        "back_inserter",
        "count_if",
        "isUndef",
        "size",
        "value",
        "SDValue",
        "isBuildVectorOfConstantSDNodes",
        "getContainerForFixedLengthVector",
        "element",
        "insert",
        "changeVectorElementType",
        "getSplatBuildVector",
        "transform",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerBuildVectorOfConstants",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  assert(VT.isFixedLengthVector() && \"Unexpected vector!\");\n\n  MVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n\n  SDLoc DL(Op);\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  unsigned NumElts = Op.getNumOperands();\n\n  if (VT.getVectorElementType() == MVT::i1) {\n    if (ISD::isBuildVectorAllZeros(Op.getNode())) {\n      SDValue VMClr = DAG.getNode(RISCVISD::VMCLR_VL, DL, ContainerVT, VL);\n      return convertFromScalableVector(VT, VMClr, DAG, Subtarget);\n    }\n\n    if (ISD::isBuildVectorAllOnes(Op.getNode())) {\n      SDValue VMSet = DAG.getNode(RISCVISD::VMSET_VL, DL, ContainerVT, VL);\n      return convertFromScalableVector(VT, VMSet, DAG, Subtarget);\n    }\n\n    // Lower constant mask BUILD_VECTORs via an integer vector type, in\n    // scalar integer chunks whose bit-width depends on the number of mask\n    // bits and XLEN.\n    // First, determine the most appropriate scalar integer type to use. This\n    // is at most XLenVT, but may be shrunk to a smaller vector element type\n    // according to the size of the final vector - use i8 chunks rather than\n    // XLenVT if we're producing a v8i1. This results in more consistent\n    // codegen across RV32 and RV64.\n    unsigned NumViaIntegerBits = std::clamp(NumElts, 8u, Subtarget.getXLen());\n    NumViaIntegerBits = std::min(NumViaIntegerBits, Subtarget.getELen());\n    // If we have to use more than one INSERT_VECTOR_ELT then this\n    // optimization is likely to increase code size; avoid peforming it in\n    // such a case. We can use a load from a constant pool in this case.\n    if (DAG.shouldOptForSize() && NumElts > NumViaIntegerBits)\n      return SDValue();\n    // Now we can create our integer vector type. Note that it may be larger\n    // than the resulting mask type: v4i1 would use v1i8 as its integer type.\n    unsigned IntegerViaVecElts = divideCeil(NumElts, NumViaIntegerBits);\n    MVT IntegerViaVecVT =\n      MVT::getVectorVT(MVT::getIntegerVT(NumViaIntegerBits),\n                       IntegerViaVecElts);\n\n    uint64_t Bits = 0;\n    unsigned BitPos = 0, IntegerEltIdx = 0;\n    SmallVector<SDValue, 8> Elts(IntegerViaVecElts);\n\n    for (unsigned I = 0; I < NumElts;) {\n      SDValue V = Op.getOperand(I);\n      bool BitValue = !V.isUndef() && V->getAsZExtVal();\n      Bits |= ((uint64_t)BitValue << BitPos);\n      ++BitPos;\n      ++I;\n\n      // Once we accumulate enough bits to fill our scalar type or process the\n      // last element, insert into our vector and clear our accumulated data.\n      if (I % NumViaIntegerBits == 0 || I == NumElts) {\n        if (NumViaIntegerBits <= 32)\n          Bits = SignExtend64<32>(Bits);\n        SDValue Elt = DAG.getConstant(Bits, DL, XLenVT);\n        Elts[IntegerEltIdx] = Elt;\n        Bits = 0;\n        BitPos = 0;\n        IntegerEltIdx++;\n      }\n    }\n\n    SDValue Vec = DAG.getBuildVector(IntegerViaVecVT, DL, Elts);\n\n    if (NumElts < NumViaIntegerBits) {\n      // If we're producing a smaller vector than our minimum legal integer\n      // type, bitcast to the equivalent (known-legal) mask type, and extract\n      // our final mask.\n      assert(IntegerViaVecVT == MVT::v1i8 && \"Unexpected mask vector type\");\n      Vec = DAG.getBitcast(MVT::v8i1, Vec);\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Vec,\n                        DAG.getConstant(0, DL, XLenVT));\n    } else {\n      // Else we must have produced an integer type with the same size as the\n      // mask type; bitcast for the final result.\n      assert(VT.getSizeInBits() == IntegerViaVecVT.getSizeInBits());\n      Vec = DAG.getBitcast(VT, Vec);\n    }\n\n    return Vec;\n  }\n\n  if (SDValue Splat = cast<BuildVectorSDNode>(Op)->getSplatValue()) {\n    unsigned Opc = VT.isFloatingPoint() ? RISCVISD::VFMV_V_F_VL\n                                        : RISCVISD::VMV_V_X_VL;\n    if (!VT.isFloatingPoint())\n      Splat = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Splat);\n    Splat =\n        DAG.getNode(Opc, DL, ContainerVT, DAG.getUNDEF(ContainerVT), Splat, VL);\n    return convertFromScalableVector(VT, Splat, DAG, Subtarget);\n  }\n\n  // Try and match index sequences, which we can lower to the vid instruction\n  // with optional modifications. An all-undef vector is matched by\n  // getSplatValue, above.\n  if (auto SimpleVID = isSimpleVIDSequence(Op, Op.getScalarValueSizeInBits())) {\n    int64_t StepNumerator = SimpleVID->StepNumerator;\n    unsigned StepDenominator = SimpleVID->StepDenominator;\n    int64_t Addend = SimpleVID->Addend;\n\n    assert(StepNumerator != 0 && \"Invalid step\");\n    bool Negate = false;\n    int64_t SplatStepVal = StepNumerator;\n    unsigned StepOpcode = ISD::MUL;\n    // Exclude INT64_MIN to avoid passing it to std::abs. We won't optimize it\n    // anyway as the shift of 63 won't fit in uimm5.\n    if (StepNumerator != 1 && StepNumerator != INT64_MIN &&\n        isPowerOf2_64(std::abs(StepNumerator))) {\n      Negate = StepNumerator < 0;\n      StepOpcode = ISD::SHL;\n      SplatStepVal = Log2_64(std::abs(StepNumerator));\n    }\n\n    // Only emit VIDs with suitably-small steps/addends. We use imm5 is a\n    // threshold since it's the immediate value many RVV instructions accept.\n    // There is no vmul.vi instruction so ensure multiply constant can fit in\n    // a single addi instruction.\n    if (((StepOpcode == ISD::MUL && isInt<12>(SplatStepVal)) ||\n         (StepOpcode == ISD::SHL && isUInt<5>(SplatStepVal))) &&\n        isPowerOf2_32(StepDenominator) &&\n        (SplatStepVal >= 0 || StepDenominator == 1) && isInt<5>(Addend)) {\n      MVT VIDVT =\n          VT.isFloatingPoint() ? VT.changeVectorElementTypeToInteger() : VT;\n      MVT VIDContainerVT =\n          getContainerForFixedLengthVector(DAG, VIDVT, Subtarget);\n      SDValue VID = DAG.getNode(RISCVISD::VID_VL, DL, VIDContainerVT, Mask, VL);\n      // Convert right out of the scalable type so we can use standard ISD\n      // nodes for the rest of the computation. If we used scalable types with\n      // these, we'd lose the fixed-length vector info and generate worse\n      // vsetvli code.\n      VID = convertFromScalableVector(VIDVT, VID, DAG, Subtarget);\n      if ((StepOpcode == ISD::MUL && SplatStepVal != 1) ||\n          (StepOpcode == ISD::SHL && SplatStepVal != 0)) {\n        SDValue SplatStep = DAG.getConstant(SplatStepVal, DL, VIDVT);\n        VID = DAG.getNode(StepOpcode, DL, VIDVT, VID, SplatStep);\n      }\n      if (StepDenominator != 1) {\n        SDValue SplatStep =\n            DAG.getConstant(Log2_64(StepDenominator), DL, VIDVT);\n        VID = DAG.getNode(ISD::SRL, DL, VIDVT, VID, SplatStep);\n      }\n      if (Addend != 0 || Negate) {\n        SDValue SplatAddend = DAG.getConstant(Addend, DL, VIDVT);\n        VID = DAG.getNode(Negate ? ISD::SUB : ISD::ADD, DL, VIDVT, SplatAddend,\n                          VID);\n      }\n      if (VT.isFloatingPoint()) {\n        // TODO: Use vfwcvt to reduce register pressure.\n        VID = DAG.getNode(ISD::SINT_TO_FP, DL, VT, VID);\n      }\n      return VID;\n    }\n  }\n\n  // For very small build_vectors, use a single scalar insert of a constant.\n  // TODO: Base this on constant rematerialization cost, not size.\n  const unsigned EltBitSize = VT.getScalarSizeInBits();\n  if (VT.getSizeInBits() <= 32 &&\n      ISD::isBuildVectorOfConstantSDNodes(Op.getNode())) {\n    MVT ViaIntVT = MVT::getIntegerVT(VT.getSizeInBits());\n    assert((ViaIntVT == MVT::i16 || ViaIntVT == MVT::i32) &&\n           \"Unexpected sequence type\");\n    // If we can use the original VL with the modified element type, this\n    // means we only have a VTYPE toggle, not a VL toggle.  TODO: Should this\n    // be moved into InsertVSETVLI?\n    unsigned ViaVecLen =\n      (Subtarget.getRealMinVLen() >= VT.getSizeInBits() * NumElts) ? NumElts : 1;\n    MVT ViaVecVT = MVT::getVectorVT(ViaIntVT, ViaVecLen);\n\n    uint64_t EltMask = maskTrailingOnes<uint64_t>(EltBitSize);\n    uint64_t SplatValue = 0;\n    // Construct the amalgamated value at this larger vector type.\n    for (const auto &OpIdx : enumerate(Op->op_values())) {\n      const auto &SeqV = OpIdx.value();\n      if (!SeqV.isUndef())\n        SplatValue |=\n            ((SeqV->getAsZExtVal() & EltMask) << (OpIdx.index() * EltBitSize));\n    }\n\n    // On RV64, sign-extend from 32 to 64 bits where possible in order to\n    // achieve better constant materializion.\n    if (Subtarget.is64Bit() && ViaIntVT == MVT::i32)\n      SplatValue = SignExtend64<32>(SplatValue);\n\n    SDValue Vec = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, ViaVecVT,\n                              DAG.getUNDEF(ViaVecVT),\n                              DAG.getConstant(SplatValue, DL, XLenVT),\n                              DAG.getConstant(0, DL, XLenVT));\n    if (ViaVecLen != 1)\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL,\n                        MVT::getVectorVT(ViaIntVT, 1), Vec,\n                        DAG.getConstant(0, DL, XLenVT));\n    return DAG.getBitcast(VT, Vec);\n  }\n\n\n  // Attempt to detect \"hidden\" splats, which only reveal themselves as splats\n  // when re-interpreted as a vector with a larger element type. For example,\n  //   v4i16 = build_vector i16 0, i16 1, i16 0, i16 1\n  // could be instead splat as\n  //   v2i32 = build_vector i32 0x00010000, i32 0x00010000\n  // TODO: This optimization could also work on non-constant splats, but it\n  // would require bit-manipulation instructions to construct the splat value.\n  SmallVector<SDValue> Sequence;\n  const auto *BV = cast<BuildVectorSDNode>(Op);\n  if (VT.isInteger() && EltBitSize < Subtarget.getELen() &&\n      ISD::isBuildVectorOfConstantSDNodes(Op.getNode()) &&\n      BV->getRepeatedSequence(Sequence) &&\n      (Sequence.size() * EltBitSize) <= Subtarget.getELen()) {\n    unsigned SeqLen = Sequence.size();\n    MVT ViaIntVT = MVT::getIntegerVT(EltBitSize * SeqLen);\n    assert((ViaIntVT == MVT::i16 || ViaIntVT == MVT::i32 ||\n            ViaIntVT == MVT::i64) &&\n           \"Unexpected sequence type\");\n\n    // If we can use the original VL with the modified element type, this\n    // means we only have a VTYPE toggle, not a VL toggle.  TODO: Should this\n    // be moved into InsertVSETVLI?\n    const unsigned RequiredVL = NumElts / SeqLen;\n    const unsigned ViaVecLen =\n      (Subtarget.getRealMinVLen() >= ViaIntVT.getSizeInBits() * NumElts) ?\n      NumElts : RequiredVL;\n    MVT ViaVecVT = MVT::getVectorVT(ViaIntVT, ViaVecLen);\n\n    unsigned EltIdx = 0;\n    uint64_t EltMask = maskTrailingOnes<uint64_t>(EltBitSize);\n    uint64_t SplatValue = 0;\n    // Construct the amalgamated value which can be splatted as this larger\n    // vector type.\n    for (const auto &SeqV : Sequence) {\n      if (!SeqV.isUndef())\n        SplatValue |=\n            ((SeqV->getAsZExtVal() & EltMask) << (EltIdx * EltBitSize));\n      EltIdx++;\n    }\n\n    // On RV64, sign-extend from 32 to 64 bits where possible in order to\n    // achieve better constant materializion.\n    if (Subtarget.is64Bit() && ViaIntVT == MVT::i32)\n      SplatValue = SignExtend64<32>(SplatValue);\n\n    // Since we can't introduce illegal i64 types at this stage, we can only\n    // perform an i64 splat on RV32 if it is its own sign-extended value. That\n    // way we can use RVV instructions to splat.\n    assert((ViaIntVT.bitsLE(XLenVT) ||\n            (!Subtarget.is64Bit() && ViaIntVT == MVT::i64)) &&\n           \"Unexpected bitcast sequence\");\n    if (ViaIntVT.bitsLE(XLenVT) || isInt<32>(SplatValue)) {\n      SDValue ViaVL =\n          DAG.getConstant(ViaVecVT.getVectorNumElements(), DL, XLenVT);\n      MVT ViaContainerVT =\n          getContainerForFixedLengthVector(DAG, ViaVecVT, Subtarget);\n      SDValue Splat =\n          DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ViaContainerVT,\n                      DAG.getUNDEF(ViaContainerVT),\n                      DAG.getConstant(SplatValue, DL, XLenVT), ViaVL);\n      Splat = convertFromScalableVector(ViaVecVT, Splat, DAG, Subtarget);\n      if (ViaVecLen != RequiredVL)\n        Splat = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL,\n                            MVT::getVectorVT(ViaIntVT, RequiredVL), Splat,\n                            DAG.getConstant(0, DL, XLenVT));\n      return DAG.getBitcast(VT, Splat);\n    }\n  }\n\n  // If the number of signbits allows, see if we can lower as a <N x i8>.\n  // Our main goal here is to reduce LMUL (and thus work) required to\n  // build the constant, but we will also narrow if the resulting\n  // narrow vector is known to materialize cheaply.\n  // TODO: We really should be costing the smaller vector.  There are\n  // profitable cases this misses.\n  if (EltBitSize > 8 && VT.isInteger() &&\n      (NumElts <= 4 || VT.getSizeInBits() > Subtarget.getRealMinVLen())) {\n    unsigned SignBits = DAG.ComputeNumSignBits(Op);\n    if (EltBitSize - SignBits < 8) {\n      SDValue Source = DAG.getBuildVector(VT.changeVectorElementType(MVT::i8),\n                                          DL, Op->ops());\n      Source = convertToScalableVector(ContainerVT.changeVectorElementType(MVT::i8),\n                                       Source, DAG, Subtarget);\n      SDValue Res = DAG.getNode(RISCVISD::VSEXT_VL, DL, ContainerVT, Source, Mask, VL);\n      return convertFromScalableVector(VT, Res, DAG, Subtarget);\n    }\n  }\n\n  if (SDValue Res = lowerBuildVectorViaDominantValues(Op, DAG, Subtarget))\n    return Res;\n\n  // For constant vectors, use generic constant pool lowering.  Otherwise,\n  // we'd have to materialize constants in GPRs just to move them into the\n  // vector.\n  return SDValue();\n}",
      "start_line": 3452,
      "end_line": 3751,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getAsZExtVal",
        "changeVectorElementTypeToInteger",
        "getRealMinVLen",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "is64Bit",
        "getBitcast",
        "getSizeInBits",
        "ComputeNumSignBits",
        "getScalarSizeInBits",
        "isFloatingPoint",
        "getNumOperands",
        "min",
        "Elts",
        "equivalent",
        "isUndef",
        "getSplatValue",
        "isPowerOf2_32",
        "getVectorVT",
        "getBuildVector",
        "size",
        "value",
        "SDValue",
        "divideCeil",
        "isBuildVectorOfConstantSDNodes",
        "getContainerForFixedLengthVector",
        "index",
        "getRepeatedSequence",
        "ops",
        "getOperand",
        "getELen",
        "LMUL",
        "clamp",
        "Log2_64",
        "getIntegerVT",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerBUILD_VECTOR",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  assert(VT.isFixedLengthVector() && \"Unexpected vector!\");\n\n  if (ISD::isBuildVectorOfConstantSDNodes(Op.getNode()) ||\n      ISD::isBuildVectorOfConstantFPSDNodes(Op.getNode()))\n    return lowerBuildVectorOfConstants(Op, DAG, Subtarget);\n\n  MVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n\n  SDLoc DL(Op);\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  if (VT.getVectorElementType() == MVT::i1) {\n    // A BUILD_VECTOR can be lowered as a SETCC. For each fixed-length mask\n    // vector type, we have a legal equivalently-sized i8 type, so we can use\n    // that.\n    MVT WideVecVT = VT.changeVectorElementType(MVT::i8);\n    SDValue VecZero = DAG.getConstant(0, DL, WideVecVT);\n\n    SDValue WideVec;\n    if (SDValue Splat = cast<BuildVectorSDNode>(Op)->getSplatValue()) {\n      // For a splat, perform a scalar truncate before creating the wider\n      // vector.\n      Splat = DAG.getNode(ISD::AND, DL, Splat.getValueType(), Splat,\n                          DAG.getConstant(1, DL, Splat.getValueType()));\n      WideVec = DAG.getSplatBuildVector(WideVecVT, DL, Splat);\n    } else {\n      SmallVector<SDValue, 8> Ops(Op->op_values());\n      WideVec = DAG.getBuildVector(WideVecVT, DL, Ops);\n      SDValue VecOne = DAG.getConstant(1, DL, WideVecVT);\n      WideVec = DAG.getNode(ISD::AND, DL, WideVecVT, WideVec, VecOne);\n    }\n\n    return DAG.getSetCC(DL, VT, WideVec, VecZero, ISD::SETNE);\n  }\n\n  if (SDValue Splat = cast<BuildVectorSDNode>(Op)->getSplatValue()) {\n    if (auto Gather = matchSplatAsGather(Splat, VT, DL, DAG, Subtarget))\n      return Gather;\n    unsigned Opc = VT.isFloatingPoint() ? RISCVISD::VFMV_V_F_VL\n                                        : RISCVISD::VMV_V_X_VL;\n    if (!VT.isFloatingPoint())\n      Splat = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Splat);\n    Splat =\n        DAG.getNode(Opc, DL, ContainerVT, DAG.getUNDEF(ContainerVT), Splat, VL);\n    return convertFromScalableVector(VT, Splat, DAG, Subtarget);\n  }\n\n  if (SDValue Res = lowerBuildVectorViaDominantValues(Op, DAG, Subtarget))\n    return Res;\n\n  // If we're compiling for an exact VLEN value, we can split our work per\n  // register in the register group.\n  const unsigned MinVLen = Subtarget.getRealMinVLen();\n  const unsigned MaxVLen = Subtarget.getRealMaxVLen();\n  if (MinVLen == MaxVLen && VT.getSizeInBits().getKnownMinValue() > MinVLen) {\n    MVT ElemVT = VT.getVectorElementType();\n    unsigned ElemsPerVReg = MinVLen / ElemVT.getFixedSizeInBits();\n    EVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n    MVT OneRegVT = MVT::getVectorVT(ElemVT, ElemsPerVReg);\n    MVT M1VT = getContainerForFixedLengthVector(DAG, OneRegVT, Subtarget);\n    assert(M1VT == getLMUL1VT(M1VT));\n\n    // The following semantically builds up a fixed length concat_vector\n    // of the component build_vectors.  We eagerly lower to scalable and\n    // insert_subvector here to avoid DAG combining it back to a large\n    // build_vector.\n    SmallVector<SDValue> BuildVectorOps(Op->op_begin(), Op->op_end());\n    unsigned NumOpElts = M1VT.getVectorMinNumElements();\n    SDValue Vec = DAG.getUNDEF(ContainerVT);\n    for (unsigned i = 0; i < VT.getVectorNumElements(); i += ElemsPerVReg) {\n      auto OneVRegOfOps = ArrayRef(BuildVectorOps).slice(i, ElemsPerVReg);\n      SDValue SubBV =\n          DAG.getNode(ISD::BUILD_VECTOR, DL, OneRegVT, OneVRegOfOps);\n      SubBV = convertToScalableVector(M1VT, SubBV, DAG, Subtarget);\n      unsigned InsertIdx = (i / ElemsPerVReg) * NumOpElts;\n      Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, ContainerVT, Vec, SubBV,\n                        DAG.getVectorIdxConstant(InsertIdx, DL));\n    }\n    return convertFromScalableVector(VT, Vec, DAG, Subtarget);\n  }\n\n  // Cap the cost at a value linear to the number of elements in the vector.\n  // The default lowering is to use the stack.  The vector store + scalar loads\n  // is linear in VL.  However, at high lmuls vslide1down and vslidedown end up\n  // being (at least) linear in LMUL.  As a result, using the vslidedown\n  // lowering for every element ends up being VL*LMUL..\n  // TODO: Should we be directly costing the stack alternative?  Doing so might\n  // give us a more accurate upper bound.\n  InstructionCost LinearBudget = VT.getVectorNumElements() * 2;\n\n  // TODO: unify with TTI getSlideCost.\n  InstructionCost PerSlideCost = 1;\n  switch (RISCVTargetLowering::getLMUL(ContainerVT)) {\n  default: break;\n  case RISCVII::VLMUL::LMUL_2:\n    PerSlideCost = 2;\n    break;\n  case RISCVII::VLMUL::LMUL_4:\n    PerSlideCost = 4;\n    break;\n  case RISCVII::VLMUL::LMUL_8:\n    PerSlideCost = 8;\n    break;\n  }\n\n  // TODO: Should we be using the build instseq then cost + evaluate scheme\n  // we use for integer constants here?\n  unsigned UndefCount = 0;\n  for (const SDValue &V : Op->ops()) {\n    if (V.isUndef()) {\n      UndefCount++;\n      continue;\n    }\n    if (UndefCount) {\n      LinearBudget -= PerSlideCost;\n      UndefCount = 0;\n    }\n    LinearBudget -= PerSlideCost;\n  }\n  if (UndefCount) {\n    LinearBudget -= PerSlideCost;\n  }\n\n  if (LinearBudget < 0)\n    return SDValue();\n\n  assert((!VT.isFloatingPoint() ||\n          VT.getVectorElementType().getSizeInBits() <= Subtarget.getFLen()) &&\n         \"Illegal type which will result in reserved encoding\");\n\n  const unsigned Policy = RISCVII::TAIL_AGNOSTIC | RISCVII::MASK_AGNOSTIC;\n\n  SDValue Vec;\n  UndefCount = 0;\n  for (SDValue V : Op->ops()) {\n    if (V.isUndef()) {\n      UndefCount++;\n      continue;\n    }\n\n    // Start our sequence with a TA splat in the hopes that hardware is able to\n    // recognize there's no dependency on the prior value of our temporary\n    // register.\n    if (!Vec) {\n      Vec = DAG.getSplatVector(VT, DL, V);\n      Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n      UndefCount = 0;\n      continue;\n    }\n\n    if (UndefCount) {\n      const SDValue Offset = DAG.getConstant(UndefCount, DL, Subtarget.getXLenVT());\n      Vec = getVSlidedown(DAG, Subtarget, DL, ContainerVT, DAG.getUNDEF(ContainerVT),\n                          Vec, Offset, Mask, VL, Policy);\n      UndefCount = 0;\n    }\n    auto OpCode =\n      VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL : RISCVISD::VSLIDE1DOWN_VL;\n    if (!VT.isFloatingPoint())\n      V = DAG.getNode(ISD::ANY_EXTEND, DL, Subtarget.getXLenVT(), V);\n    Vec = DAG.getNode(OpCode, DL, ContainerVT, DAG.getUNDEF(ContainerVT), Vec,\n                      V, Mask, VL);\n  }\n  if (UndefCount) {\n    const SDValue Offset = DAG.getConstant(UndefCount, DL, Subtarget.getXLenVT());\n    Vec = getVSlidedown(DAG, Subtarget, DL, ContainerVT, DAG.getUNDEF(ContainerVT),\n                        Vec, Offset, Mask, VL, Policy);\n  }\n  return convertFromScalableVector(VT, Vec, DAG, Subtarget);\n}",
      "start_line": 3753,
      "end_line": 3927,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "being",
        "getRealMaxVLen",
        "getRealMinVLen",
        "getSplatVector",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "isBuildVectorOfConstantFPSDNodes",
        "getKnownMinValue",
        "getSizeInBits",
        "getFLen",
        "op_end",
        "lowerBuildVectorOfConstants",
        "isFloatingPoint",
        "getVectorMinNumElements",
        "getUNDEF",
        "ArrayRef",
        "getSplatValue",
        "getVectorNumElements",
        "getVectorElementType",
        "getVectorVT",
        "getBuildVector",
        "slice",
        "getFixedSizeInBits",
        "SDValue",
        "getSetCC",
        "getContainerForFixedLengthVector",
        "getVSlidedown",
        "BuildVectorOps",
        "changeVectorElementType",
        "getSplatBuildVector",
        "Ops",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "splatPartsI64WithVL",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "Passthru"
        },
        {
          "type": "SDValue",
          "name": "Lo"
        },
        {
          "type": "SDValue",
          "name": "Hi"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  if (!Passthru)\n    Passthru = DAG.getUNDEF(VT);\n  if (isa<ConstantSDNode>(Lo) && isa<ConstantSDNode>(Hi)) {\n    int32_t LoC = cast<ConstantSDNode>(Lo)->getSExtValue();\n    int32_t HiC = cast<ConstantSDNode>(Hi)->getSExtValue();\n    // If Hi constant is all the same sign bit as Lo, lower this as a custom\n    // node in order to try and match RVV vector/scalar instructions.\n    if ((LoC >> 31) == HiC)\n      return DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, Passthru, Lo, VL);\n\n    // If vl is equal to VLMAX or fits in 4 bits and Hi constant is equal to Lo,\n    // we could use vmv.v.x whose EEW = 32 to lower it. This allows us to use\n    // vlmax vsetvli or vsetivli to change the VL.\n    // FIXME: Support larger constants?\n    // FIXME: Support non-constant VLs by saturating?\n    if (LoC == HiC) {\n      SDValue NewVL;\n      if (isAllOnesConstant(VL) ||\n          (isa<RegisterSDNode>(VL) &&\n           cast<RegisterSDNode>(VL)->getReg() == RISCV::X0))\n        NewVL = DAG.getRegister(RISCV::X0, MVT::i32);\n      else if (isa<ConstantSDNode>(VL) && isUInt<4>(VL->getAsZExtVal()))\n        NewVL = DAG.getNode(ISD::ADD, DL, VL.getValueType(), VL, VL);\n\n      if (NewVL) {\n        MVT InterVT =\n            MVT::getVectorVT(MVT::i32, VT.getVectorElementCount() * 2);\n        auto InterVec = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, InterVT,\n                                    DAG.getUNDEF(InterVT), Lo,\n                                    DAG.getRegister(RISCV::X0, MVT::i32));\n        return DAG.getNode(ISD::BITCAST, DL, VT, InterVec);\n      }\n    }\n  }\n\n  // Detect cases where Hi is (SRA Lo, 31) which means Hi is Lo sign extended.\n  if (Hi.getOpcode() == ISD::SRA && Hi.getOperand(0) == Lo &&\n      isa<ConstantSDNode>(Hi.getOperand(1)) &&\n      Hi.getConstantOperandVal(1) == 31)\n    return DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, Passthru, Lo, VL);\n\n  // If the hi bits of the splat are undefined, then it's fine to just splat Lo\n  // even if it might be sign extended.\n  if (Hi.isUndef())\n    return DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, Passthru, Lo, VL);\n\n  // Fall back to a stack store and stride x0 vector load.\n  return DAG.getNode(RISCVISD::SPLAT_VECTOR_SPLIT_I64_VL, DL, VT, Passthru, Lo,\n                     Hi, VL);\n}",
      "start_line": 3929,
      "end_line": 3981,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getConstantOperandVal",
        "getAsZExtVal",
        "getRegister",
        "getUNDEF",
        "getOperand",
        "getVectorVT",
        "is",
        "getReg",
        "getSExtValue",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "splatSplitI64WithVL",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "Passthru"
        },
        {
          "type": "SDValue",
          "name": "Scalar"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  assert(Scalar.getValueType() == MVT::i64 && \"Unexpected VT!\");\n  SDValue Lo, Hi;\n  std::tie(Lo, Hi) = DAG.SplitScalar(Scalar, DL, MVT::i32, MVT::i32);\n  return splatPartsI64WithVL(DL, VT, Passthru, Lo, Hi, VL, DAG);\n}",
      "start_line": 3986,
      "end_line": 3993,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "splatPartsI64WithVL",
        "SplitScalar",
        "tie"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerScalarSplat",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Passthru"
        },
        {
          "type": "SDValue",
          "name": "Scalar"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  bool HasPassthru = Passthru && !Passthru.isUndef();\n  if (!HasPassthru && !Passthru)\n    Passthru = DAG.getUNDEF(VT);\n  if (VT.isFloatingPoint())\n    return DAG.getNode(RISCVISD::VFMV_V_F_VL, DL, VT, Passthru, Scalar, VL);\n\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  // Simplest case is that the operand needs to be promoted to XLenVT.\n  if (Scalar.getValueType().bitsLE(XLenVT)) {\n    // If the operand is a constant, sign extend to increase our chances\n    // of being able to use a .vi instruction. ANY_EXTEND would become a\n    // a zero extend and the simm5 check in isel would fail.\n    // FIXME: Should we ignore the upper bits in isel instead?\n    unsigned ExtOpc =\n        isa<ConstantSDNode>(Scalar) ? ISD::SIGN_EXTEND : ISD::ANY_EXTEND;\n    Scalar = DAG.getNode(ExtOpc, DL, XLenVT, Scalar);\n    return DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, Passthru, Scalar, VL);\n  }\n\n  assert(XLenVT == MVT::i32 && Scalar.getValueType() == MVT::i64 &&\n         \"Unexpected scalar for splat lowering!\");\n\n  if (isOneConstant(VL) && isNullConstant(Scalar))\n    return DAG.getNode(RISCVISD::VMV_S_X_VL, DL, VT, Passthru,\n                       DAG.getConstant(0, DL, XLenVT), VL);\n\n  // Otherwise use the more complicated splatting algorithm.\n  return splatSplitI64WithVL(DL, VT, Passthru, Scalar, VL, DAG);\n}",
      "start_line": 3998,
      "end_line": 4030,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "bitsLE",
        "isUndef",
        "getUNDEF",
        "isNullConstant",
        "getXLenVT",
        "splatSplitI64WithVL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerScalarInsert",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Scalar"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert(VT.isScalableVector() && \"Expect VT is scalable vector type.\");\n\n  const MVT XLenVT = Subtarget.getXLenVT();\n  SDValue Passthru = DAG.getUNDEF(VT);\n\n  if (Scalar.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n      isNullConstant(Scalar.getOperand(1))) {\n    SDValue ExtractedVal = Scalar.getOperand(0);\n    // The element types must be the same.\n    if (ExtractedVal.getValueType().getVectorElementType() ==\n        VT.getVectorElementType()) {\n      MVT ExtractedVT = ExtractedVal.getSimpleValueType();\n      MVT ExtractedContainerVT = ExtractedVT;\n      if (ExtractedContainerVT.isFixedLengthVector()) {\n        ExtractedContainerVT = getContainerForFixedLengthVector(\n            DAG, ExtractedContainerVT, Subtarget);\n        ExtractedVal = convertToScalableVector(ExtractedContainerVT,\n                                               ExtractedVal, DAG, Subtarget);\n      }\n      if (ExtractedContainerVT.bitsLE(VT))\n        return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, Passthru,\n                           ExtractedVal, DAG.getConstant(0, DL, XLenVT));\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, ExtractedVal,\n                         DAG.getConstant(0, DL, XLenVT));\n    }\n  }\n\n\n  if (VT.isFloatingPoint())\n    return DAG.getNode(RISCVISD::VFMV_S_F_VL, DL, VT,\n                       DAG.getUNDEF(VT), Scalar, VL);\n\n  // Avoid the tricky legalization cases by falling back to using the\n  // splat code which already handles it gracefully.\n  if (!Scalar.getValueType().bitsLE(XLenVT))\n    return lowerScalarSplat(DAG.getUNDEF(VT), Scalar,\n                            DAG.getConstant(1, DL, XLenVT),\n                            VT, DL, DAG, Subtarget);\n\n  // If the operand is a constant, sign extend to increase our chances\n  // of being able to use a .vi instruction. ANY_EXTEND would become a\n  // a zero extend and the simm5 check in isel would fail.\n  // FIXME: Should we ignore the upper bits in isel instead?\n  unsigned ExtOpc =\n    isa<ConstantSDNode>(Scalar) ? ISD::SIGN_EXTEND : ISD::ANY_EXTEND;\n  Scalar = DAG.getNode(ExtOpc, DL, XLenVT, Scalar);\n  return DAG.getNode(RISCVISD::VMV_S_X_VL, DL, VT,\n                     DAG.getUNDEF(VT), Scalar, VL);\n}",
      "start_line": 4036,
      "end_line": 4087,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "bitsLE",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "getUNDEF",
        "getOperand",
        "isNullConstant",
        "getVectorElementType",
        "lowerScalarSplat",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isDeinterleaveShuffle",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "MVT",
          "name": "ContainerVT"
        },
        {
          "type": "SDValue",
          "name": "V1"
        },
        {
          "type": "SDValue",
          "name": "V2"
        },
        {
          "type": "ArrayRef<int>",
          "name": "Mask"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // Need to be able to widen the vector.\n  if (VT.getScalarSizeInBits() >= Subtarget.getELen())\n    return false;\n\n  // Both input must be extracts.\n  if (V1.getOpcode() != ISD::EXTRACT_SUBVECTOR ||\n      V2.getOpcode() != ISD::EXTRACT_SUBVECTOR)\n    return false;\n\n  // Extracting from the same source.\n  SDValue Src = V1.getOperand(0);\n  if (Src != V2.getOperand(0))\n    return false;\n\n  // Src needs to have twice the number of elements.\n  if (Src.getValueType().getVectorNumElements() != (Mask.size() * 2))\n    return false;\n\n  // The extracts must extract the two halves of the source.\n  if (V1.getConstantOperandVal(1) != 0 ||\n      V2.getConstantOperandVal(1) != Mask.size())\n    return false;\n\n  // First index must be the first even or odd element from V1.\n  if (Mask[0] != 0 && Mask[0] != 1)\n    return false;\n\n  // The others must increase by 2 each time.\n  // TODO: Support undef elements?\n  for (unsigned i = 1; i != Mask.size(); ++i)\n    if (Mask[i] != Mask[i - 1] + 2)\n      return false;\n\n  return true;\n}",
      "start_line": 4096,
      "end_line": 4133,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getConstantOperandVal",
        "getOpcode",
        "getOperand",
        "getVectorNumElements",
        "getELen",
        "size"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isInterleaveShuffle",
      "return_type": "bool",
      "parameters": [
        {
          "type": "ArrayRef<int>",
          "name": "Mask"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "int",
          "name": "&EvenSrc"
        },
        {
          "type": "int",
          "name": "&OddSrc"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // We need to be able to widen elements to the next larger integer type.\n  if (VT.getScalarSizeInBits() >= Subtarget.getELen())\n    return false;\n\n  int Size = Mask.size();\n  int NumElts = VT.getVectorNumElements();\n  assert(Size == (int)NumElts && \"Unexpected mask size\");\n\n  SmallVector<unsigned, 2> StartIndexes;\n  if (!ShuffleVectorInst::isInterleaveMask(Mask, 2, Size * 2, StartIndexes))\n    return false;\n\n  EvenSrc = StartIndexes[0];\n  OddSrc = StartIndexes[1];\n\n  // One source should be low half of first vector.\n  if (EvenSrc != 0 && OddSrc != 0)\n    return false;\n\n  // Subvectors will be subtracted from either at the start of the two input\n  // vectors, or at the start and middle of the first vector if it's an unary\n  // interleave.\n  // In both cases, HalfNumElts will be extracted.\n  // We need to ensure that the extract indices are 0 or HalfNumElts otherwise\n  // we'll create an illegal extract_subvector.\n  // FIXME: We could support other values using a slidedown first.\n  int HalfNumElts = NumElts / 2;\n  return ((EvenSrc % HalfNumElts) == 0) && ((OddSrc % HalfNumElts) == 0);\n}",
      "start_line": 4141,
      "end_line": 4171,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getVectorNumElements",
        "size",
        "getELen"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isElementRotate",
      "return_type": "int",
      "parameters": [
        {
          "type": "int",
          "name": "&LoSrc"
        },
        {
          "type": "int",
          "name": "&HiSrc"
        },
        {
          "type": "ArrayRef<int>",
          "name": "Mask"
        }
      ],
      "body": "{\n  int Size = Mask.size();\n\n  // We need to detect various ways of spelling a rotation:\n  //   [11, 12, 13, 14, 15,  0,  1,  2]\n  //   [-1, 12, 13, 14, -1, -1,  1, -1]\n  //   [-1, -1, -1, -1, -1, -1,  1,  2]\n  //   [ 3,  4,  5,  6,  7,  8,  9, 10]\n  //   [-1,  4,  5,  6, -1, -1,  9, -1]\n  //   [-1,  4,  5,  6, -1, -1, -1, -1]\n  int Rotation = 0;\n  LoSrc = -1;\n  HiSrc = -1;\n  for (int i = 0; i != Size; ++i) {\n    int M = Mask[i];\n    if (M < 0)\n      continue;\n\n    // Determine where a rotate vector would have started.\n    int StartIdx = i - (M % Size);\n    // The identity rotation isn't interesting, stop.\n    if (StartIdx == 0)\n      return -1;\n\n    // If we found the tail of a vector the rotation must be the missing\n    // front. If we found the head of a vector, it must be how much of the\n    // head.\n    int CandidateRotation = StartIdx < 0 ? -StartIdx : Size - StartIdx;\n\n    if (Rotation == 0)\n      Rotation = CandidateRotation;\n    else if (Rotation != CandidateRotation)\n      // The rotations don't match, so we can't match this mask.\n      return -1;\n\n    // Compute which value this mask is pointing at.\n    int MaskSrc = M < Size ? 0 : 1;\n\n    // Compute which of the two target values this index should be assigned to.\n    // This reflects whether the high elements are remaining or the low elemnts\n    // are remaining.\n    int &TargetSrc = StartIdx < 0 ? HiSrc : LoSrc;\n\n    // Either set up this value if we've not encountered it before, or check\n    // that it remains consistent.\n    if (TargetSrc < 0)\n      TargetSrc = MaskSrc;\n    else if (TargetSrc != MaskSrc)\n      // This may be a rotation, but it pulls from the inputs in some\n      // unsupported interleaving.\n      return -1;\n  }\n\n  // Check that we successfully analyzed the mask, and normalize the results.\n  assert(Rotation != 0 && \"Failed to locate a viable rotation!\");\n  assert((LoSrc >= 0 || HiSrc >= 0) &&\n         \"Failed to find a rotated input vector!\");\n\n  return Rotation;\n}",
      "start_line": 4185,
      "end_line": 4244,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "size"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getDeinterleaveViaVNSRL",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "Src"
        },
        {
          "type": "bool",
          "name": "EvenElts"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  // The result is a vector of type <m x n x ty>\n  MVT ContainerVT = VT;\n  // Convert fixed vectors to scalable if needed\n  if (ContainerVT.isFixedLengthVector()) {\n    assert(Src.getSimpleValueType().isFixedLengthVector());\n    ContainerVT = getContainerForFixedLengthVector(DAG, ContainerVT, Subtarget);\n\n    // The source is a vector of type <m x n*2 x ty>\n    MVT SrcContainerVT =\n        MVT::getVectorVT(ContainerVT.getVectorElementType(),\n                         ContainerVT.getVectorElementCount() * 2);\n    Src = convertToScalableVector(SrcContainerVT, Src, DAG, Subtarget);\n  }\n\n  auto [TrueMask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  // Bitcast the source vector from <m x n*2 x ty> -> <m x n x ty*2>\n  // This also converts FP to int.\n  unsigned EltBits = ContainerVT.getScalarSizeInBits();\n  MVT WideSrcContainerVT = MVT::getVectorVT(\n      MVT::getIntegerVT(EltBits * 2), ContainerVT.getVectorElementCount());\n  Src = DAG.getBitcast(WideSrcContainerVT, Src);\n\n  // The integer version of the container type.\n  MVT IntContainerVT = ContainerVT.changeVectorElementTypeToInteger();\n\n  // If we want even elements, then the shift amount is 0. Otherwise, shift by\n  // the original element size.\n  unsigned Shift = EvenElts ? 0 : EltBits;\n  SDValue SplatShift = DAG.getNode(\n      RISCVISD::VMV_V_X_VL, DL, IntContainerVT, DAG.getUNDEF(ContainerVT),\n      DAG.getConstant(Shift, DL, Subtarget.getXLenVT()), VL);\n  SDValue Res =\n      DAG.getNode(RISCVISD::VNSRL_VL, DL, IntContainerVT, Src, SplatShift,\n                  DAG.getUNDEF(IntContainerVT), TrueMask, VL);\n  // Cast back to FP if needed.\n  Res = DAG.getBitcast(ContainerVT, Res);\n\n  if (VT.isFixedLengthVector())\n    Res = convertFromScalableVector(VT, Res, DAG, Subtarget);\n  return Res;\n}",
      "start_line": 4251,
      "end_line": 4296,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "changeVectorElementTypeToInteger",
        "getNode",
        "convertToScalableVector",
        "getConstant",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getBitcast",
        "getVectorVT",
        "getVectorElementCount",
        "isFixedLengthVector",
        "getScalarSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_SHUFFLEAsVSlidedown",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "V1"
        },
        {
          "type": "SDValue",
          "name": "V2"
        },
        {
          "type": "ArrayRef<int>",
          "name": "Mask"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  auto findNonEXTRACT_SUBVECTORParent =\n      [](SDValue Parent) -> std::pair<SDValue, uint64_t> {\n    uint64_t Offset = 0;\n    while (Parent.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n           // EXTRACT_SUBVECTOR can be used to extract a fixed-width vector from\n           // a scalable vector. But we don't want to match the case.\n           Parent.getOperand(0).getSimpleValueType().isFixedLengthVector()) {\n      Offset += Parent.getConstantOperandVal(1);\n      Parent = Parent.getOperand(0);\n    }\n    return std::make_pair(Parent, Offset);\n  };\n\n  auto [V1Src, V1IndexOffset] = findNonEXTRACT_SUBVECTORParent(V1);\n  auto [V2Src, V2IndexOffset] = findNonEXTRACT_SUBVECTORParent(V2);\n\n  // Extracting from the same source.\n  SDValue Src = V1Src;\n  if (Src != V2Src)\n    return SDValue();\n\n  // Rebuild mask because Src may be from multiple EXTRACT_SUBVECTORs.\n  SmallVector<int, 16> NewMask(Mask);\n  for (size_t i = 0; i != NewMask.size(); ++i) {\n    if (NewMask[i] == -1)\n      continue;\n\n    if (static_cast<size_t>(NewMask[i]) < NewMask.size()) {\n      NewMask[i] = NewMask[i] + V1IndexOffset;\n    } else {\n      // Minus NewMask.size() is needed. Otherwise, the b case would be\n      // <5,6,7,12> instead of <5,6,7,8>.\n      NewMask[i] = NewMask[i] - NewMask.size() + V2IndexOffset;\n    }\n  }\n\n  // First index must be known and non-zero. It will be used as the slidedown\n  // amount.\n  if (NewMask[0] <= 0)\n    return SDValue();\n\n  // NewMask is also continuous.\n  for (unsigned i = 1; i != NewMask.size(); ++i)\n    if (NewMask[i - 1] + 1 != NewMask[i])\n      return SDValue();\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT ContainerVT = getContainerForFixedLengthVector(DAG, SrcVT, Subtarget);\n  auto [TrueMask, VL] = getDefaultVLOps(SrcVT, ContainerVT, DL, DAG, Subtarget);\n  SDValue Slidedown =\n      getVSlidedown(DAG, Subtarget, DL, ContainerVT, DAG.getUNDEF(ContainerVT),\n                    convertToScalableVector(ContainerVT, Src, DAG, Subtarget),\n                    DAG.getConstant(NewMask[0], DL, XLenVT), TrueMask, VL);\n  return DAG.getNode(\n      ISD::EXTRACT_SUBVECTOR, DL, VT,\n      convertFromScalableVector(SrcVT, Slidedown, DAG, Subtarget),\n      DAG.getConstant(0, DL, XLenVT));\n}",
      "start_line": 4310,
      "end_line": 4373,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getDefaultVLOps",
        "getConstantOperandVal",
        "getXLenVT",
        "findNonEXTRACT_SUBVECTORParent",
        "getNode",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "make_pair",
        "getContainerForFixedLengthVector",
        "getVSlidedown",
        "getOperand",
        "NewMask",
        "size",
        "isFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_SHUFFLEAsVSlideup",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "V1"
        },
        {
          "type": "SDValue",
          "name": "V2"
        },
        {
          "type": "ArrayRef<int>",
          "name": "Mask"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  unsigned NumElts = VT.getVectorNumElements();\n  int NumSubElts, Index;\n  if (!ShuffleVectorInst::isInsertSubvectorMask(Mask, NumElts, NumSubElts,\n                                                Index))\n    return SDValue();\n\n  bool OpsSwapped = Mask[Index] < (int)NumElts;\n  SDValue InPlace = OpsSwapped ? V2 : V1;\n  SDValue ToInsert = OpsSwapped ? V1 : V2;\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  MVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n  auto TrueMask = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).first;\n  // We slide up by the index that the subvector is being inserted at, and set\n  // VL to the index + the number of elements being inserted.\n  unsigned Policy = RISCVII::TAIL_UNDISTURBED_MASK_UNDISTURBED | RISCVII::MASK_AGNOSTIC;\n  // If the we're adding a suffix to the in place vector, i.e. inserting right\n  // up to the very end of it, then we don't actually care about the tail.\n  if (NumSubElts + Index >= (int)NumElts)\n    Policy |= RISCVII::TAIL_AGNOSTIC;\n\n  InPlace = convertToScalableVector(ContainerVT, InPlace, DAG, Subtarget);\n  ToInsert = convertToScalableVector(ContainerVT, ToInsert, DAG, Subtarget);\n  SDValue VL = DAG.getConstant(NumSubElts + Index, DL, XLenVT);\n\n  SDValue Res;\n  // If we're inserting into the lowest elements, use a tail undisturbed\n  // vmv.v.v.\n  if (Index == 0)\n    Res = DAG.getNode(RISCVISD::VMV_V_V_VL, DL, ContainerVT, InPlace, ToInsert,\n                      VL);\n  else\n    Res = getVSlideup(DAG, Subtarget, DL, ContainerVT, InPlace, ToInsert,\n                      DAG.getConstant(Index, DL, XLenVT), TrueMask, VL, Policy);\n  return convertFromScalableVector(VT, Res, DAG, Subtarget);\n}",
      "start_line": 4387,
      "end_line": 4427,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getDefaultVLOps",
        "convertToScalableVector",
        "getConstant",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getVectorNumElements",
        "getXLenVT",
        "getVSlideup",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_SHUFFLEAsVSlide1",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "MVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "V1"
        },
        {
          "type": "SDValue",
          "name": "V2"
        },
        {
          "type": "ArrayRef<int>",
          "name": "Mask"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  bool OpsSwapped = false;\n  if (!isa<BuildVectorSDNode>(V1)) {\n    if (!isa<BuildVectorSDNode>(V2))\n      return SDValue();\n    std::swap(V1, V2);\n    OpsSwapped = true;\n  }\n  SDValue Splat = cast<BuildVectorSDNode>(V1)->getSplatValue();\n  if (!Splat)\n    return SDValue();\n\n  // Return true if the mask could describe a slide of Mask.size() - 1\n  // elements from concat_vector(V1, V2)[Base:] to [Offset:].\n  auto isSlideMask = [](ArrayRef<int> Mask, unsigned Base, int Offset) {\n    const unsigned S = (Offset > 0) ? 0 : -Offset;\n    const unsigned E = Mask.size() - ((Offset > 0) ? Offset : 0);\n    for (unsigned i = S; i != E; ++i)\n      if (Mask[i] >= 0 && (unsigned)Mask[i] != Base + i + Offset)\n        return false;\n    return true;\n  };\n\n  const unsigned NumElts = VT.getVectorNumElements();\n  bool IsVSlidedown = isSlideMask(Mask, OpsSwapped ? 0 : NumElts, 1);\n  if (!IsVSlidedown && !isSlideMask(Mask, OpsSwapped ? 0 : NumElts, -1))\n    return SDValue();\n\n  const int InsertIdx = Mask[IsVSlidedown ? (NumElts - 1) : 0];\n  // Inserted lane must come from splat, undef scalar is legal but not profitable.\n  if (InsertIdx < 0 || InsertIdx / NumElts != (unsigned)OpsSwapped)\n    return SDValue();\n\n  MVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n  auto [TrueMask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n  auto OpCode = IsVSlidedown ?\n    (VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL : RISCVISD::VSLIDE1DOWN_VL) :\n    (VT.isFloatingPoint() ? RISCVISD::VFSLIDE1UP_VL : RISCVISD::VSLIDE1UP_VL);\n  if (!VT.isFloatingPoint())\n    Splat = DAG.getNode(ISD::ANY_EXTEND, DL, Subtarget.getXLenVT(), Splat);\n  auto Vec = DAG.getNode(OpCode, DL, ContainerVT,\n                         DAG.getUNDEF(ContainerVT),\n                         convertToScalableVector(ContainerVT, V2, DAG, Subtarget),\n                         Splat, TrueMask, VL);\n  return convertFromScalableVector(VT, Vec, DAG, Subtarget);\n}",
      "start_line": 4431,
      "end_line": 4480,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getDefaultVLOps",
        "swap",
        "isFloatingPoint",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getSplatValue",
        "getVectorNumElements",
        "isSlideMask",
        "size",
        "getNode",
        "concat_vector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getWideningInterleave",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "EvenV"
        },
        {
          "type": "SDValue",
          "name": "OddV"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT VecVT = EvenV.getSimpleValueType();\n  MVT VecContainerVT = VecVT; // <vscale x n x ty>\n  // Convert fixed vectors to scalable if needed\n  if (VecContainerVT.isFixedLengthVector()) {\n    VecContainerVT = getContainerForFixedLengthVector(DAG, VecVT, Subtarget);\n    EvenV = convertToScalableVector(VecContainerVT, EvenV, DAG, Subtarget);\n    OddV = convertToScalableVector(VecContainerVT, OddV, DAG, Subtarget);\n  }\n\n  assert(VecVT.getScalarSizeInBits() < Subtarget.getELen());\n\n  // We're working with a vector of the same size as the resulting\n  // interleaved vector, but with half the number of elements and\n  // twice the SEW (Hence the restriction on not using the maximum\n  // ELEN)\n  MVT WideVT =\n      MVT::getVectorVT(MVT::getIntegerVT(VecVT.getScalarSizeInBits() * 2),\n                       VecVT.getVectorElementCount());\n  MVT WideContainerVT = WideVT; // <vscale x n x ty*2>\n  if (WideContainerVT.isFixedLengthVector())\n    WideContainerVT = getContainerForFixedLengthVector(DAG, WideVT, Subtarget);\n\n  // Bitcast the input vectors to integers in case they are FP\n  VecContainerVT = VecContainerVT.changeTypeToInteger();\n  EvenV = DAG.getBitcast(VecContainerVT, EvenV);\n  OddV = DAG.getBitcast(VecContainerVT, OddV);\n\n  auto [Mask, VL] = getDefaultVLOps(VecVT, VecContainerVT, DL, DAG, Subtarget);\n  SDValue Passthru = DAG.getUNDEF(WideContainerVT);\n\n  SDValue Interleaved;\n  if (Subtarget.hasStdExtZvbb()) {\n    // Interleaved = (OddV << VecVT.getScalarSizeInBits()) + EvenV.\n    SDValue OffsetVec =\n        DAG.getSplatVector(VecContainerVT, DL,\n                           DAG.getConstant(VecVT.getScalarSizeInBits(), DL,\n                                           Subtarget.getXLenVT()));\n    Interleaved = DAG.getNode(RISCVISD::VWSLL_VL, DL, WideContainerVT, OddV,\n                              OffsetVec, Passthru, Mask, VL);\n    Interleaved = DAG.getNode(RISCVISD::VWADDU_W_VL, DL, WideContainerVT,\n                              Interleaved, EvenV, Passthru, Mask, VL);\n  } else {\n    // Widen EvenV and OddV with 0s and add one copy of OddV to EvenV with\n    // vwaddu.vv\n    Interleaved = DAG.getNode(RISCVISD::VWADDU_VL, DL, WideContainerVT, EvenV,\n                              OddV, Passthru, Mask, VL);\n\n    // Then get OddV * by 2^(VecVT.getScalarSizeInBits() - 1)\n    SDValue AllOnesVec = DAG.getSplatVector(\n        VecContainerVT, DL, DAG.getAllOnesConstant(DL, Subtarget.getXLenVT()));\n    SDValue OddsMul = DAG.getNode(RISCVISD::VWMULU_VL, DL, WideContainerVT,\n                                  OddV, AllOnesVec, Passthru, Mask, VL);\n\n    // Add the two together so we get\n    //   (OddV * 0xff...ff) + (OddV + EvenV)\n    // = (OddV * 0x100...00) + EvenV\n    // = (OddV << VecVT.getScalarSizeInBits()) + EvenV\n    // Note the ADD_VL and VLMULU_VL should get selected as vwmaccu.vx\n    Interleaved = DAG.getNode(RISCVISD::ADD_VL, DL, WideContainerVT,\n                              Interleaved, OddsMul, Passthru, Mask, VL);\n  }\n\n  // Bitcast from <vscale x n * ty*2> to <vscale x 2*n x ty>\n  MVT ResultContainerVT = MVT::getVectorVT(\n      VecVT.getVectorElementType(), // Make sure to use original type\n      VecContainerVT.getVectorElementCount().multiplyCoefficientBy(2));\n  Interleaved = DAG.getBitcast(ResultContainerVT, Interleaved);\n\n  // Convert back to a fixed vector if needed\n  MVT ResultVT =\n      MVT::getVectorVT(VecVT.getVectorElementType(),\n                       VecVT.getVectorElementCount().multiplyCoefficientBy(2));\n  if (ResultVT.isFixedLengthVector())\n    Interleaved =\n        convertFromScalableVector(ResultVT, Interleaved, DAG, Subtarget);\n\n  return Interleaved;\n}",
      "start_line": 4485,
      "end_line": 4565,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getXLenVT",
        "getNode",
        "getSplatVector",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getUNDEF",
        "multiplyCoefficientBy",
        "getBitcast",
        "getELen",
        "getVectorVT",
        "changeTypeToInteger",
        "getVectorElementCount",
        "SEW",
        "getScalarSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerBitreverseShuffle",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "ShuffleVectorSDNode",
          "name": "*SVN"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDLoc DL(SVN);\n  MVT VT = SVN->getSimpleValueType(0);\n  SDValue V = SVN->getOperand(0);\n  unsigned NumElts = VT.getVectorNumElements();\n\n  assert(VT.getVectorElementType() == MVT::i1);\n\n  if (!ShuffleVectorInst::isReverseMask(SVN->getMask(),\n                                        SVN->getMask().size()) ||\n      !SVN->getOperand(1).isUndef())\n    return SDValue();\n\n  unsigned ViaEltSize = std::max((uint64_t)8, PowerOf2Ceil(NumElts));\n  EVT ViaVT = EVT::getVectorVT(\n      *DAG.getContext(), EVT::getIntegerVT(*DAG.getContext(), ViaEltSize), 1);\n  EVT ViaBitVT =\n      EVT::getVectorVT(*DAG.getContext(), MVT::i1, ViaVT.getScalarSizeInBits());\n\n  // If we don't have zvbb or the larger element type > ELEN, the operation will\n  // be illegal.\n  if (!Subtarget.getTargetLowering()->isOperationLegalOrCustom(ISD::BITREVERSE,\n                                                               ViaVT) ||\n      !Subtarget.getTargetLowering()->isTypeLegal(ViaBitVT))\n    return SDValue();\n\n  // If the bit vector doesn't fit exactly into the larger element type, we need\n  // to insert it into the larger vector and then shift up the reversed bits\n  // afterwards to get rid of the gap introduced.\n  if (ViaEltSize > NumElts)\n    V = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, ViaBitVT, DAG.getUNDEF(ViaBitVT),\n                    V, DAG.getVectorIdxConstant(0, DL));\n\n  SDValue Res =\n      DAG.getNode(ISD::BITREVERSE, DL, ViaVT, DAG.getBitcast(ViaVT, V));\n\n  // Shift up the reversed bits if the vector didn't exactly fit into the larger\n  // element type.\n  if (ViaEltSize > NumElts)\n    Res = DAG.getNode(ISD::SRL, DL, ViaVT, Res,\n                      DAG.getConstant(ViaEltSize - NumElts, DL, ViaVT));\n\n  Res = DAG.getBitcast(ViaBitVT, Res);\n\n  if (ViaEltSize > NumElts)\n    Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Res,\n                      DAG.getVectorIdxConstant(0, DL));\n  return Res;\n}",
      "start_line": 4569,
      "end_line": 4619,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getSimpleValueType",
        "getBitcast",
        "getScalarSizeInBits",
        "getMask",
        "isUndef",
        "isOperationLegalOrCustom",
        "getVectorIdxConstant",
        "getVectorNumElements",
        "getVectorVT",
        "max",
        "size",
        "SDValue",
        "isTypeLegal",
        "getOperand",
        "PowerOf2Ceil",
        "getTargetLowering",
        "getIntegerVT",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_SHUFFLEAsRotate",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "ShuffleVectorSDNode",
          "name": "*SVN"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDLoc DL(SVN);\n\n  EVT VT = SVN->getValueType(0);\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n  unsigned NumSubElts, RotateAmt;\n  if (!ShuffleVectorInst::isBitRotateMask(SVN->getMask(), EltSizeInBits, 2,\n                                          NumElts, NumSubElts, RotateAmt))\n    return SDValue();\n  MVT RotateVT = MVT::getVectorVT(MVT::getIntegerVT(EltSizeInBits * NumSubElts),\n                                  NumElts / NumSubElts);\n\n  // We might have a RotateVT that isn't legal, e.g. v4i64 on zve32x.\n  if (!Subtarget.getTargetLowering()->isTypeLegal(RotateVT))\n    return SDValue();\n\n  SDValue Op = DAG.getBitcast(RotateVT, SVN->getOperand(0));\n\n  SDValue Rotate;\n  // A rotate of an i16 by 8 bits either direction is equivalent to a byteswap,\n  // so canonicalize to vrev8.\n  if (RotateVT.getScalarType() == MVT::i16 && RotateAmt == 8)\n    Rotate = DAG.getNode(ISD::BSWAP, DL, RotateVT, Op);\n  else\n    Rotate = DAG.getNode(ISD::ROTL, DL, RotateVT, Op,\n                         DAG.getConstant(RotateAmt, DL, RotateVT));\n\n  return DAG.getBitcast(VT, Rotate);\n}",
      "start_line": 4624,
      "end_line": 4655,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "isTypeLegal",
        "getNode",
        "getValueType",
        "getVectorNumElements",
        "getBitcast",
        "DL",
        "getVectorVT",
        "getScalarSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerShuffleViaVRegSplitting",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "ShuffleVectorSDNode",
          "name": "*SVN"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDLoc DL(SVN);\n  MVT VT = SVN->getSimpleValueType(0);\n  SDValue V1 = SVN->getOperand(0);\n  SDValue V2 = SVN->getOperand(1);\n  ArrayRef<int> Mask = SVN->getMask();\n  unsigned NumElts = VT.getVectorNumElements();\n\n  // If we don't know exact data layout, not much we can do.  If this\n  // is already m1 or smaller, no point in splitting further.\n  const unsigned MinVLen = Subtarget.getRealMinVLen();\n  const unsigned MaxVLen = Subtarget.getRealMaxVLen();\n  if (MinVLen != MaxVLen || VT.getSizeInBits().getFixedValue() <= MinVLen)\n    return SDValue();\n\n  MVT ElemVT = VT.getVectorElementType();\n  unsigned ElemsPerVReg = MinVLen / ElemVT.getFixedSizeInBits();\n  unsigned VRegsPerSrc = NumElts / ElemsPerVReg;\n\n  SmallVector<std::pair<int, SmallVector<int>>>\n    OutMasks(VRegsPerSrc, {-1, {}});\n\n  // Check if our mask can be done as a 1-to-1 mapping from source\n  // to destination registers in the group without needing to\n  // write each destination more than once.\n  for (unsigned DstIdx = 0; DstIdx < Mask.size(); DstIdx++) {\n    int DstVecIdx = DstIdx / ElemsPerVReg;\n    int DstSubIdx = DstIdx % ElemsPerVReg;\n    int SrcIdx = Mask[DstIdx];\n    if (SrcIdx < 0 || (unsigned)SrcIdx >= 2 * NumElts)\n      continue;\n    int SrcVecIdx = SrcIdx / ElemsPerVReg;\n    int SrcSubIdx = SrcIdx % ElemsPerVReg;\n    if (OutMasks[DstVecIdx].first == -1)\n      OutMasks[DstVecIdx].first = SrcVecIdx;\n    if (OutMasks[DstVecIdx].first != SrcVecIdx)\n      // Note: This case could easily be handled by keeping track of a chain\n      // of source values and generating two element shuffles below.  This is\n      // less an implementation question, and more a profitability one.\n      return SDValue();\n\n    OutMasks[DstVecIdx].second.resize(ElemsPerVReg, -1);\n    OutMasks[DstVecIdx].second[DstSubIdx] = SrcSubIdx;\n  }\n\n  EVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n  MVT OneRegVT = MVT::getVectorVT(ElemVT, ElemsPerVReg);\n  MVT M1VT = getContainerForFixedLengthVector(DAG, OneRegVT, Subtarget);\n  assert(M1VT == getLMUL1VT(M1VT));\n  unsigned NumOpElts = M1VT.getVectorMinNumElements();\n  SDValue Vec = DAG.getUNDEF(ContainerVT);\n  // The following semantically builds up a fixed length concat_vector\n  // of the component shuffle_vectors.  We eagerly lower to scalable here\n  // to avoid DAG combining it back to a large shuffle_vector again.\n  V1 = convertToScalableVector(ContainerVT, V1, DAG, Subtarget);\n  V2 = convertToScalableVector(ContainerVT, V2, DAG, Subtarget);\n  for (unsigned DstVecIdx = 0 ; DstVecIdx < OutMasks.size(); DstVecIdx++) {\n    auto &[SrcVecIdx, SrcSubMask] = OutMasks[DstVecIdx];\n    if (SrcVecIdx == -1)\n      continue;\n    unsigned ExtractIdx = (SrcVecIdx % VRegsPerSrc) * NumOpElts;\n    SDValue SrcVec = (unsigned)SrcVecIdx >= VRegsPerSrc ? V2 : V1;\n    SDValue SubVec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, M1VT, SrcVec,\n                                 DAG.getVectorIdxConstant(ExtractIdx, DL));\n    SubVec = convertFromScalableVector(OneRegVT, SubVec, DAG, Subtarget);\n    SubVec = DAG.getVectorShuffle(OneRegVT, DL, SubVec, SubVec, SrcSubMask);\n    SubVec = convertToScalableVector(M1VT, SubVec, DAG, Subtarget);\n    unsigned InsertIdx = DstVecIdx * NumOpElts;\n    Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, ContainerVT, Vec, SubVec,\n                      DAG.getVectorIdxConstant(InsertIdx, DL));\n  }\n  return convertFromScalableVector(VT, Vec, DAG, Subtarget);\n}",
      "start_line": 4660,
      "end_line": 4734,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getRealMaxVLen",
        "getRealMinVLen",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getMask",
        "getVectorMinNumElements",
        "getVectorShuffle",
        "getUNDEF",
        "getVectorNumElements",
        "getVectorElementType",
        "getVectorVT",
        "getFixedSizeInBits",
        "getFixedValue",
        "SDValue",
        "getContainerForFixedLengthVector",
        "getOperand",
        "resize",
        "OutMasks",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_SHUFFLE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue V1 = Op.getOperand(0);\n  SDValue V2 = Op.getOperand(1);\n  SDLoc DL(Op);\n  MVT XLenVT = Subtarget.getXLenVT();\n  MVT VT = Op.getSimpleValueType();\n  unsigned NumElts = VT.getVectorNumElements();\n  ShuffleVectorSDNode *SVN = cast<ShuffleVectorSDNode>(Op.getNode());\n\n  if (VT.getVectorElementType() == MVT::i1) {\n    // Lower to a vror.vi of a larger element type if possible before we promote\n    // i1s to i8s.\n    if (SDValue V = lowerVECTOR_SHUFFLEAsRotate(SVN, DAG, Subtarget))\n      return V;\n    if (SDValue V = lowerBitreverseShuffle(SVN, DAG, Subtarget))\n      return V;\n\n    // Promote i1 shuffle to i8 shuffle.\n    MVT WidenVT = MVT::getVectorVT(MVT::i8, VT.getVectorElementCount());\n    V1 = DAG.getNode(ISD::ZERO_EXTEND, DL, WidenVT, V1);\n    V2 = V2.isUndef() ? DAG.getUNDEF(WidenVT)\n                      : DAG.getNode(ISD::ZERO_EXTEND, DL, WidenVT, V2);\n    SDValue Shuffled = DAG.getVectorShuffle(WidenVT, DL, V1, V2, SVN->getMask());\n    return DAG.getSetCC(DL, VT, Shuffled, DAG.getConstant(0, DL, WidenVT),\n                        ISD::SETNE);\n  }\n\n  MVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n\n  auto [TrueMask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  if (SVN->isSplat()) {\n    const int Lane = SVN->getSplatIndex();\n    if (Lane >= 0) {\n      MVT SVT = VT.getVectorElementType();\n\n      // Turn splatted vector load into a strided load with an X0 stride.\n      SDValue V = V1;\n      // Peek through CONCAT_VECTORS as VectorCombine can concat a vector\n      // with undef.\n      // FIXME: Peek through INSERT_SUBVECTOR, EXTRACT_SUBVECTOR, bitcasts?\n      int Offset = Lane;\n      if (V.getOpcode() == ISD::CONCAT_VECTORS) {\n        int OpElements =\n            V.getOperand(0).getSimpleValueType().getVectorNumElements();\n        V = V.getOperand(Offset / OpElements);\n        Offset %= OpElements;\n      }\n\n      // We need to ensure the load isn't atomic or volatile.\n      if (ISD::isNormalLoad(V.getNode()) && cast<LoadSDNode>(V)->isSimple()) {\n        auto *Ld = cast<LoadSDNode>(V);\n        Offset *= SVT.getStoreSize();\n        SDValue NewAddr = DAG.getMemBasePlusOffset(\n            Ld->getBasePtr(), TypeSize::getFixed(Offset), DL);\n\n        // If this is SEW=64 on RV32, use a strided load with a stride of x0.\n        if (SVT.isInteger() && SVT.bitsGT(XLenVT)) {\n          SDVTList VTs = DAG.getVTList({ContainerVT, MVT::Other});\n          SDValue IntID =\n              DAG.getTargetConstant(Intrinsic::riscv_vlse, DL, XLenVT);\n          SDValue Ops[] = {Ld->getChain(),\n                           IntID,\n                           DAG.getUNDEF(ContainerVT),\n                           NewAddr,\n                           DAG.getRegister(RISCV::X0, XLenVT),\n                           VL};\n          SDValue NewLoad = DAG.getMemIntrinsicNode(\n              ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops, SVT,\n              DAG.getMachineFunction().getMachineMemOperand(\n                  Ld->getMemOperand(), Offset, SVT.getStoreSize()));\n          DAG.makeEquivalentMemoryOrdering(Ld, NewLoad);\n          return convertFromScalableVector(VT, NewLoad, DAG, Subtarget);\n        }\n\n        // Otherwise use a scalar load and splat. This will give the best\n        // opportunity to fold a splat into the operation. ISel can turn it into\n        // the x0 strided load if we aren't able to fold away the select.\n        if (SVT.isFloatingPoint())\n          V = DAG.getLoad(SVT, DL, Ld->getChain(), NewAddr,\n                          Ld->getPointerInfo().getWithOffset(Offset),\n                          Ld->getOriginalAlign(),\n                          Ld->getMemOperand()->getFlags());\n        else\n          V = DAG.getExtLoad(ISD::SEXTLOAD, DL, XLenVT, Ld->getChain(), NewAddr,\n                             Ld->getPointerInfo().getWithOffset(Offset), SVT,\n                             Ld->getOriginalAlign(),\n                             Ld->getMemOperand()->getFlags());\n        DAG.makeEquivalentMemoryOrdering(Ld, V);\n\n        unsigned Opc =\n            VT.isFloatingPoint() ? RISCVISD::VFMV_V_F_VL : RISCVISD::VMV_V_X_VL;\n        SDValue Splat =\n            DAG.getNode(Opc, DL, ContainerVT, DAG.getUNDEF(ContainerVT), V, VL);\n        return convertFromScalableVector(VT, Splat, DAG, Subtarget);\n      }\n\n      V1 = convertToScalableVector(ContainerVT, V1, DAG, Subtarget);\n      assert(Lane < (int)NumElts && \"Unexpected lane!\");\n      SDValue Gather = DAG.getNode(RISCVISD::VRGATHER_VX_VL, DL, ContainerVT,\n                                   V1, DAG.getConstant(Lane, DL, XLenVT),\n                                   DAG.getUNDEF(ContainerVT), TrueMask, VL);\n      return convertFromScalableVector(VT, Gather, DAG, Subtarget);\n    }\n  }\n\n  // For exact VLEN m2 or greater, try to split to m1 operations if we\n  // can split cleanly.\n  if (SDValue V = lowerShuffleViaVRegSplitting(SVN, DAG, Subtarget))\n    return V;\n\n  ArrayRef<int> Mask = SVN->getMask();\n\n  if (SDValue V =\n          lowerVECTOR_SHUFFLEAsVSlide1(DL, VT, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  if (SDValue V =\n          lowerVECTOR_SHUFFLEAsVSlidedown(DL, VT, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // A bitrotate will be one instruction on Zvkb, so try to lower to it first if\n  // available.\n  if (Subtarget.hasStdExtZvkb())\n    if (SDValue V = lowerVECTOR_SHUFFLEAsRotate(SVN, DAG, Subtarget))\n      return V;\n\n  // Lower rotations to a SLIDEDOWN and a SLIDEUP. One of the source vectors may\n  // be undef which can be handled with a single SLIDEDOWN/UP.\n  int LoSrc, HiSrc;\n  int Rotation = isElementRotate(LoSrc, HiSrc, Mask);\n  if (Rotation > 0) {\n    SDValue LoV, HiV;\n    if (LoSrc >= 0) {\n      LoV = LoSrc == 0 ? V1 : V2;\n      LoV = convertToScalableVector(ContainerVT, LoV, DAG, Subtarget);\n    }\n    if (HiSrc >= 0) {\n      HiV = HiSrc == 0 ? V1 : V2;\n      HiV = convertToScalableVector(ContainerVT, HiV, DAG, Subtarget);\n    }\n\n    // We found a rotation. We need to slide HiV down by Rotation. Then we need\n    // to slide LoV up by (NumElts - Rotation).\n    unsigned InvRotate = NumElts - Rotation;\n\n    SDValue Res = DAG.getUNDEF(ContainerVT);\n    if (HiV) {\n      // Even though we could use a smaller VL, don't to avoid a vsetivli\n      // toggle.\n      Res = getVSlidedown(DAG, Subtarget, DL, ContainerVT, Res, HiV,\n                          DAG.getConstant(Rotation, DL, XLenVT), TrueMask, VL);\n    }\n    if (LoV)\n      Res = getVSlideup(DAG, Subtarget, DL, ContainerVT, Res, LoV,\n                        DAG.getConstant(InvRotate, DL, XLenVT), TrueMask, VL,\n                        RISCVII::TAIL_AGNOSTIC);\n\n    return convertFromScalableVector(VT, Res, DAG, Subtarget);\n  }\n\n  // If this is a deinterleave and we can widen the vector, then we can use\n  // vnsrl to deinterleave.\n  if (isDeinterleaveShuffle(VT, ContainerVT, V1, V2, Mask, Subtarget)) {\n    return getDeinterleaveViaVNSRL(DL, VT, V1.getOperand(0), Mask[0] == 0,\n                                   Subtarget, DAG);\n  }\n\n  if (SDValue V =\n          lowerVECTOR_SHUFFLEAsVSlideup(DL, VT, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // Detect an interleave shuffle and lower to\n  // (vmaccu.vx (vwaddu.vx lohalf(V1), lohalf(V2)), lohalf(V2), (2^eltbits - 1))\n  int EvenSrc, OddSrc;\n  if (isInterleaveShuffle(Mask, VT, EvenSrc, OddSrc, Subtarget)) {\n    // Extract the halves of the vectors.\n    MVT HalfVT = VT.getHalfNumVectorElementsVT();\n\n    int Size = Mask.size();\n    SDValue EvenV, OddV;\n    assert(EvenSrc >= 0 && \"Undef source?\");\n    EvenV = (EvenSrc / Size) == 0 ? V1 : V2;\n    EvenV = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, HalfVT, EvenV,\n                        DAG.getConstant(EvenSrc % Size, DL, XLenVT));\n\n    assert(OddSrc >= 0 && \"Undef source?\");\n    OddV = (OddSrc / Size) == 0 ? V1 : V2;\n    OddV = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, HalfVT, OddV,\n                       DAG.getConstant(OddSrc % Size, DL, XLenVT));\n\n    return getWideningInterleave(EvenV, OddV, DL, DAG, Subtarget);\n  }\n\n  // Detect shuffles which can be re-expressed as vector selects; these are\n  // shuffles in which each element in the destination is taken from an element\n  // at the corresponding index in either source vectors.\n  bool IsSelect = all_of(enumerate(Mask), [&](const auto &MaskIdx) {\n    int MaskIndex = MaskIdx.value();\n    return MaskIndex < 0 || MaskIdx.index() == (unsigned)MaskIndex % NumElts;\n  });\n\n  assert(!V1.isUndef() && \"Unexpected shuffle canonicalization\");\n\n  // By default we preserve the original operand order, and use a mask to\n  // select LHS as true and RHS as false. However, since RVV vector selects may\n  // feature splats but only on the LHS, we may choose to invert our mask and\n  // instead select between RHS and LHS.\n  bool SwapOps = DAG.isSplatValue(V2) && !DAG.isSplatValue(V1);\n\n  if (IsSelect) {\n    // Now construct the mask that will be used by the vselect operation.\n    SmallVector<SDValue> MaskVals;\n    for (int MaskIndex : Mask) {\n      bool SelectMaskVal = (MaskIndex < (int)NumElts) ^ SwapOps;\n      MaskVals.push_back(DAG.getConstant(SelectMaskVal, DL, XLenVT));\n    }\n\n    if (SwapOps)\n      std::swap(V1, V2);\n\n    assert(MaskVals.size() == NumElts && \"Unexpected select-like shuffle\");\n    MVT MaskVT = MVT::getVectorVT(MVT::i1, NumElts);\n    SDValue SelectMask = DAG.getBuildVector(MaskVT, DL, MaskVals);\n    return DAG.getNode(ISD::VSELECT, DL, VT, SelectMask, V1, V2);\n  }\n\n  // We might be able to express the shuffle as a bitrotate. But even if we\n  // don't have Zvkb and have to expand, the expanded sequence of approx. 2\n  // shifts and a vor will have a higher throughput than a vrgather.\n  if (SDValue V = lowerVECTOR_SHUFFLEAsRotate(SVN, DAG, Subtarget))\n    return V;\n\n  if (VT.getScalarSizeInBits() == 8 && VT.getVectorNumElements() > 256) {\n    // On such a large vector we're unable to use i8 as the index type.\n    // FIXME: We could promote the index to i16 and use vrgatherei16, but that\n    // may involve vector splitting if we're already at LMUL=8, or our\n    // user-supplied maximum fixed-length LMUL.\n    return SDValue();\n  }\n\n  // As a backup, shuffles can be lowered via a vrgather instruction, possibly\n  // merged with a second vrgather.\n  SmallVector<SDValue> GatherIndicesLHS, GatherIndicesRHS;\n\n  // Keep a track of which non-undef indices are used by each LHS/RHS shuffle\n  // half.\n  DenseMap<int, unsigned> LHSIndexCounts, RHSIndexCounts;\n\n  SmallVector<SDValue> MaskVals;\n\n  // Now construct the mask that will be used by the blended vrgather operation.\n  // Cconstruct the appropriate indices into each vector.\n  for (int MaskIndex : Mask) {\n    bool SelectMaskVal = (MaskIndex < (int)NumElts) ^ !SwapOps;\n    MaskVals.push_back(DAG.getConstant(SelectMaskVal, DL, XLenVT));\n    bool IsLHSOrUndefIndex = MaskIndex < (int)NumElts;\n    GatherIndicesLHS.push_back(IsLHSOrUndefIndex && MaskIndex >= 0\n                               ? DAG.getConstant(MaskIndex, DL, XLenVT)\n                               : DAG.getUNDEF(XLenVT));\n    GatherIndicesRHS.push_back(\n                               IsLHSOrUndefIndex ? DAG.getUNDEF(XLenVT)\n                               : DAG.getConstant(MaskIndex - NumElts, DL, XLenVT));\n    if (IsLHSOrUndefIndex && MaskIndex >= 0)\n      ++LHSIndexCounts[MaskIndex];\n    if (!IsLHSOrUndefIndex)\n      ++RHSIndexCounts[MaskIndex - NumElts];\n  }\n\n  if (SwapOps) {\n    std::swap(V1, V2);\n    std::swap(GatherIndicesLHS, GatherIndicesRHS);\n  }\n\n  assert(MaskVals.size() == NumElts && \"Unexpected select-like shuffle\");\n  MVT MaskVT = MVT::getVectorVT(MVT::i1, NumElts);\n  SDValue SelectMask = DAG.getBuildVector(MaskVT, DL, MaskVals);\n\n  unsigned GatherVXOpc = RISCVISD::VRGATHER_VX_VL;\n  unsigned GatherVVOpc = RISCVISD::VRGATHER_VV_VL;\n  MVT IndexVT = VT.changeTypeToInteger();\n  // Since we can't introduce illegal index types at this stage, use i16 and\n  // vrgatherei16 if the corresponding index type for plain vrgather is greater\n  // than XLenVT.\n  if (IndexVT.getScalarType().bitsGT(XLenVT)) {\n    GatherVVOpc = RISCVISD::VRGATHEREI16_VV_VL;\n    IndexVT = IndexVT.changeVectorElementType(MVT::i16);\n  }\n\n  // If the mask allows, we can do all the index computation in 16 bits.  This\n  // requires less work and less register pressure at high LMUL, and creates\n  // smaller constants which may be cheaper to materialize.\n  if (IndexVT.getScalarType().bitsGT(MVT::i16) && isUInt<16>(NumElts - 1) &&\n      (IndexVT.getSizeInBits() / Subtarget.getRealMinVLen()) > 1) {\n    GatherVVOpc = RISCVISD::VRGATHEREI16_VV_VL;\n    IndexVT = IndexVT.changeVectorElementType(MVT::i16);\n  }\n\n  MVT IndexContainerVT =\n      ContainerVT.changeVectorElementType(IndexVT.getScalarType());\n\n  SDValue Gather;\n  // TODO: This doesn't trigger for i64 vectors on RV32, since there we\n  // encounter a bitcasted BUILD_VECTOR with low/high i32 values.\n  if (SDValue SplatValue = DAG.getSplatValue(V1, /*LegalTypes*/ true)) {\n    Gather = lowerScalarSplat(SDValue(), SplatValue, VL, ContainerVT, DL, DAG,\n                              Subtarget);\n  } else {\n    V1 = convertToScalableVector(ContainerVT, V1, DAG, Subtarget);\n    // If only one index is used, we can use a \"splat\" vrgather.\n    // TODO: We can splat the most-common index and fix-up any stragglers, if\n    // that's beneficial.\n    if (LHSIndexCounts.size() == 1) {\n      int SplatIndex = LHSIndexCounts.begin()->getFirst();\n      Gather = DAG.getNode(GatherVXOpc, DL, ContainerVT, V1,\n                           DAG.getConstant(SplatIndex, DL, XLenVT),\n                           DAG.getUNDEF(ContainerVT), TrueMask, VL);\n    } else {\n      SDValue LHSIndices = DAG.getBuildVector(IndexVT, DL, GatherIndicesLHS);\n      LHSIndices =\n          convertToScalableVector(IndexContainerVT, LHSIndices, DAG, Subtarget);\n\n      Gather = DAG.getNode(GatherVVOpc, DL, ContainerVT, V1, LHSIndices,\n                           DAG.getUNDEF(ContainerVT), TrueMask, VL);\n    }\n  }\n\n  // If a second vector operand is used by this shuffle, blend it in with an\n  // additional vrgather.\n  if (!V2.isUndef()) {\n    V2 = convertToScalableVector(ContainerVT, V2, DAG, Subtarget);\n\n    MVT MaskContainerVT = ContainerVT.changeVectorElementType(MVT::i1);\n    SelectMask =\n        convertToScalableVector(MaskContainerVT, SelectMask, DAG, Subtarget);\n\n    // If only one index is used, we can use a \"splat\" vrgather.\n    // TODO: We can splat the most-common index and fix-up any stragglers, if\n    // that's beneficial.\n    if (RHSIndexCounts.size() == 1) {\n      int SplatIndex = RHSIndexCounts.begin()->getFirst();\n      Gather = DAG.getNode(GatherVXOpc, DL, ContainerVT, V2,\n                           DAG.getConstant(SplatIndex, DL, XLenVT), Gather,\n                           SelectMask, VL);\n    } else {\n      SDValue RHSIndices = DAG.getBuildVector(IndexVT, DL, GatherIndicesRHS);\n      RHSIndices =\n          convertToScalableVector(IndexContainerVT, RHSIndices, DAG, Subtarget);\n      Gather = DAG.getNode(GatherVVOpc, DL, ContainerVT, V2, RHSIndices, Gather,\n                           SelectMask, VL);\n    }\n  }\n\n  return convertFromScalableVector(VT, Gather, DAG, Subtarget);\n}",
      "start_line": 4736,
      "end_line": 5091,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getChain",
        "lohalf",
        "getRealMinVLen",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getSizeInBits",
        "bitsGT",
        "isSimple",
        "getOriginalAlign",
        "getHalfNumVectorElementsVT",
        "isFloatingPoint",
        "getMask",
        "isElementRotate",
        "getFixed",
        "getFlags",
        "vx",
        "getWideningInterleave",
        "isUndef",
        "getUNDEF",
        "getVectorShuffle",
        "getVectorNumElements",
        "getMemIntrinsicNode",
        "getVectorElementType",
        "getVectorVT",
        "makeEquivalentMemoryOrdering",
        "getBuildVector",
        "changeTypeToInteger",
        "lowerScalarSplat",
        "size",
        "isSplatValue",
        "getMachineMemOperand",
        "push_back",
        "begin",
        "value",
        "getMemOperand",
        "SDValue",
        "getSetCC",
        "getExtLoad",
        "getVTList",
        "swap",
        "getLoad",
        "getWithOffset",
        "getContainerForFixedLengthVector",
        "getRegister",
        "getVSlidedown",
        "index",
        "getOperand",
        "getFirst",
        "by",
        "getDeinterleaveViaVNSRL",
        "getStoreSize",
        "changeVectorElementType",
        "getVSlideup",
        "all_of",
        "getMemBasePlusOffset",
        "DL",
        "getXLenVT",
        "getSplatIndex",
        "getPointerInfo",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isShuffleMaskLegal",
      "return_type": "bool",
      "parameters": [
        {
          "type": "ArrayRef<int>",
          "name": "M"
        },
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  // Support splats for any type. These should type legalize well.\n  if (ShuffleVectorSDNode::isSplatMask(M.data(), VT))\n    return true;\n\n  // Only support legal VTs for other shuffles for now.\n  if (!isTypeLegal(VT))\n    return false;\n\n  MVT SVT = VT.getSimpleVT();\n\n  // Not for i1 vectors.\n  if (SVT.getScalarType() == MVT::i1)\n    return false;\n\n  int Dummy1, Dummy2;\n  return (isElementRotate(Dummy1, Dummy2, M) > 0) ||\n         isInterleaveShuffle(M, SVT, Dummy1, Dummy2, Subtarget);\n}",
      "start_line": 5093,
      "end_line": 5111,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getSimpleVT",
        "isInterleaveShuffle"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerCTLZ_CTTZ_ZERO_UNDEF",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  unsigned EltSize = VT.getScalarSizeInBits();\n  SDValue Src = Op.getOperand(0);\n  SDLoc DL(Op);\n  MVT ContainerVT = VT;\n\n  SDValue Mask, VL;\n  if (Op->isVPOpcode()) {\n    Mask = Op.getOperand(1);\n    if (VT.isFixedLengthVector())\n      Mask = convertToScalableVector(getMaskTypeFor(ContainerVT), Mask, DAG,\n                                     Subtarget);\n    VL = Op.getOperand(2);\n  }\n\n  // We choose FP type that can represent the value if possible. Otherwise, we\n  // use rounding to zero conversion for correct exponent of the result.\n  // TODO: Use f16 for i8 when possible?\n  MVT FloatEltVT = (EltSize >= 32) ? MVT::f64 : MVT::f32;\n  if (!isTypeLegal(MVT::getVectorVT(FloatEltVT, VT.getVectorElementCount())))\n    FloatEltVT = MVT::f32;\n  MVT FloatVT = MVT::getVectorVT(FloatEltVT, VT.getVectorElementCount());\n\n  // Legal types should have been checked in the RISCVTargetLowering\n  // constructor.\n  // TODO: Splitting may make sense in some cases.\n  assert(DAG.getTargetLoweringInfo().isTypeLegal(FloatVT) &&\n         \"Expected legal float type!\");\n\n  // For CTTZ_ZERO_UNDEF, we need to extract the lowest set bit using X & -X.\n  // The trailing zero count is equal to log2 of this single bit value.\n  if (Op.getOpcode() == ISD::CTTZ_ZERO_UNDEF) {\n    SDValue Neg = DAG.getNegative(Src, DL, VT);\n    Src = DAG.getNode(ISD::AND, DL, VT, Src, Neg);\n  } else if (Op.getOpcode() == ISD::VP_CTTZ_ZERO_UNDEF) {\n    SDValue Neg = DAG.getNode(ISD::VP_SUB, DL, VT, DAG.getConstant(0, DL, VT),\n                              Src, Mask, VL);\n    Src = DAG.getNode(ISD::VP_AND, DL, VT, Src, Neg, Mask, VL);\n  }\n\n  // We have a legal FP type, convert to it.\n  SDValue FloatVal;\n  if (FloatVT.bitsGT(VT)) {\n    if (Op->isVPOpcode())\n      FloatVal = DAG.getNode(ISD::VP_UINT_TO_FP, DL, FloatVT, Src, Mask, VL);\n    else\n      FloatVal = DAG.getNode(ISD::UINT_TO_FP, DL, FloatVT, Src);\n  } else {\n    // Use RTZ to avoid rounding influencing exponent of FloatVal.\n    if (VT.isFixedLengthVector()) {\n      ContainerVT = getContainerForFixedLengthVector(VT);\n      Src = convertToScalableVector(ContainerVT, Src, DAG, Subtarget);\n    }\n    if (!Op->isVPOpcode())\n      std::tie(Mask, VL) = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n    SDValue RTZRM =\n        DAG.getTargetConstant(RISCVFPRndMode::RTZ, DL, Subtarget.getXLenVT());\n    MVT ContainerFloatVT =\n        MVT::getVectorVT(FloatEltVT, ContainerVT.getVectorElementCount());\n    FloatVal = DAG.getNode(RISCVISD::VFCVT_RM_F_XU_VL, DL, ContainerFloatVT,\n                           Src, Mask, RTZRM, VL);\n    if (VT.isFixedLengthVector())\n      FloatVal = convertFromScalableVector(FloatVT, FloatVal, DAG, Subtarget);\n  }\n  // Bitcast to integer and shift the exponent to the LSB.\n  EVT IntVT = FloatVT.changeVectorElementTypeToInteger();\n  SDValue Bitcast = DAG.getBitcast(IntVT, FloatVal);\n  unsigned ShiftAmt = FloatEltVT == MVT::f64 ? 52 : 23;\n\n  SDValue Exp;\n  // Restore back to original type. Truncation after SRL is to generate vnsrl.\n  if (Op->isVPOpcode()) {\n    Exp = DAG.getNode(ISD::VP_LSHR, DL, IntVT, Bitcast,\n                      DAG.getConstant(ShiftAmt, DL, IntVT), Mask, VL);\n    Exp = DAG.getVPZExtOrTrunc(DL, VT, Exp, Mask, VL);\n  } else {\n    Exp = DAG.getNode(ISD::SRL, DL, IntVT, Bitcast,\n                      DAG.getConstant(ShiftAmt, DL, IntVT));\n    if (IntVT.bitsLT(VT))\n      Exp = DAG.getNode(ISD::ZERO_EXTEND, DL, VT, Exp);\n    else if (IntVT.bitsGT(VT))\n      Exp = DAG.getNode(ISD::TRUNCATE, DL, VT, Exp);\n  }\n\n  // The exponent contains log2 of the value in biased form.\n  unsigned ExponentBias = FloatEltVT == MVT::f64 ? 1023 : 127;\n  // For trailing zeros, we just need to subtract the bias.\n  if (Op.getOpcode() == ISD::CTTZ_ZERO_UNDEF)\n    return DAG.getNode(ISD::SUB, DL, VT, Exp,\n                       DAG.getConstant(ExponentBias, DL, VT));\n  if (Op.getOpcode() == ISD::VP_CTTZ_ZERO_UNDEF)\n    return DAG.getNode(ISD::VP_SUB, DL, VT, Exp,\n                       DAG.getConstant(ExponentBias, DL, VT), Mask, VL);\n\n  // For leading zeros, we need to remove the bias and convert from log2 to\n  // leading zeros. We can do this by subtracting from (Bias + (EltSize - 1)).\n  unsigned Adjust = ExponentBias + (EltSize - 1);\n  SDValue Res;\n  if (Op->isVPOpcode())\n    Res = DAG.getNode(ISD::VP_SUB, DL, VT, DAG.getConstant(Adjust, DL, VT), Exp,\n                      Mask, VL);\n  else\n    Res = DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(Adjust, DL, VT), Exp);\n\n  // The above result with zero input equals to Adjust which is greater than\n  // EltSize. Hence, we can do min(Res, EltSize) for CTLZ.\n  if (Op.getOpcode() == ISD::CTLZ)\n    Res = DAG.getNode(ISD::UMIN, DL, VT, Res, DAG.getConstant(EltSize, DL, VT));\n  else if (Op.getOpcode() == ISD::VP_CTLZ)\n    Res = DAG.getNode(ISD::VP_UMIN, DL, VT, Res,\n                      DAG.getConstant(EltSize, DL, VT), Mask, VL);\n  return Res;\n}",
      "start_line": 5115,
      "end_line": 5230,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "changeVectorElementTypeToInteger",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getBitcast",
        "getVPZExtOrTrunc",
        "getScalarSizeInBits",
        "getNegative",
        "min",
        "getVectorVT",
        "isTypeLegal",
        "getContainerForFixedLengthVector",
        "getOperand",
        "tie",
        "DL",
        "from",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "expandUnalignedRVVLoad",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  auto *Load = cast<LoadSDNode>(Op);\n  assert(Load && Load->getMemoryVT().isVector() && \"Expected vector load\");\n\n  if (allowsMemoryAccessForAlignment(*DAG.getContext(), DAG.getDataLayout(),\n                                     Load->getMemoryVT(),\n                                     *Load->getMemOperand()))\n    return SDValue();\n\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  unsigned EltSizeBits = VT.getScalarSizeInBits();\n  assert((EltSizeBits == 16 || EltSizeBits == 32 || EltSizeBits == 64) &&\n         \"Unexpected unaligned RVV load type\");\n  MVT NewVT =\n      MVT::getVectorVT(MVT::i8, VT.getVectorElementCount() * (EltSizeBits / 8));\n  assert(NewVT.isValid() &&\n         \"Expecting equally-sized RVV vector types to be legal\");\n  SDValue L = DAG.getLoad(NewVT, DL, Load->getChain(), Load->getBasePtr(),\n                          Load->getPointerInfo(), Load->getOriginalAlign(),\n                          Load->getMemOperand()->getFlags());\n  return DAG.getMergeValues({DAG.getBitcast(VT, L), L.getValue(1)}, DL);\n}",
      "start_line": 5236,
      "end_line": 5259,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getMemOperand",
        "getMergeValues",
        "getBasePtr",
        "getValue",
        "getMemoryVT",
        "getLoad",
        "getFlags",
        "getSimpleValueType",
        "isVector",
        "getDataLayout",
        "DL",
        "getVectorVT",
        "getPointerInfo",
        "getScalarSizeInBits",
        "getOriginalAlign"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "expandUnalignedRVVStore",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  auto *Store = cast<StoreSDNode>(Op);\n  assert(Store && Store->getValue().getValueType().isVector() &&\n         \"Expected vector store\");\n\n  if (allowsMemoryAccessForAlignment(*DAG.getContext(), DAG.getDataLayout(),\n                                     Store->getMemoryVT(),\n                                     *Store->getMemOperand()))\n    return SDValue();\n\n  SDLoc DL(Op);\n  SDValue StoredVal = Store->getValue();\n  MVT VT = StoredVal.getSimpleValueType();\n  unsigned EltSizeBits = VT.getScalarSizeInBits();\n  assert((EltSizeBits == 16 || EltSizeBits == 32 || EltSizeBits == 64) &&\n         \"Unexpected unaligned RVV store type\");\n  MVT NewVT =\n      MVT::getVectorVT(MVT::i8, VT.getVectorElementCount() * (EltSizeBits / 8));\n  assert(NewVT.isValid() &&\n         \"Expecting equally-sized RVV vector types to be legal\");\n  StoredVal = DAG.getBitcast(NewVT, StoredVal);\n  return DAG.getStore(Store->getChain(), DL, StoredVal, Store->getBasePtr(),\n                      Store->getPointerInfo(), Store->getOriginalAlign(),\n                      Store->getMemOperand()->getFlags());\n}",
      "start_line": 5265,
      "end_line": 5290,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMemOperand",
        "SDValue",
        "getValue",
        "getBasePtr",
        "getMemoryVT",
        "getStore",
        "getFlags",
        "getSimpleValueType",
        "getValueType",
        "isVector",
        "getDataLayout",
        "getBitcast",
        "DL",
        "getVectorVT",
        "getPointerInfo",
        "getScalarSizeInBits",
        "getOriginalAlign"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerConstant",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert(Op.getValueType() == MVT::i64 && \"Unexpected VT\");\n\n  int64_t Imm = cast<ConstantSDNode>(Op)->getSExtValue();\n\n  // All simm32 constants should be handled by isel.\n  // NOTE: The getMaxBuildIntsCost call below should return a value >= 2 making\n  // this check redundant, but small immediates are common so this check\n  // should have better compile time.\n  if (isInt<32>(Imm))\n    return Op;\n\n  // We only need to cost the immediate, if constant pool lowering is enabled.\n  if (!Subtarget.useConstantPoolForLargeInts())\n    return Op;\n\n  RISCVMatInt::InstSeq Seq = RISCVMatInt::generateInstSeq(Imm, Subtarget);\n  if (Seq.size() <= Subtarget.getMaxBuildIntsCost())\n    return Op;\n\n  // Optimizations below are disabled for opt size. If we're optimizing for\n  // size, use a constant pool.\n  if (DAG.shouldOptForSize())\n    return SDValue();\n\n  // Special case. See if we can build the constant as (ADD (SLLI X, C), X) do\n  // that if it will avoid a constant pool.\n  // It will require an extra temporary register though.\n  // If we have Zba we can use (ADD_UW X, (SLLI X, 32)) to handle cases where\n  // low and high 32 bits are the same and bit 31 and 63 are set.\n  unsigned ShiftAmt, AddOpc;\n  RISCVMatInt::InstSeq SeqLo =\n      RISCVMatInt::generateTwoRegInstSeq(Imm, Subtarget, ShiftAmt, AddOpc);\n  if (!SeqLo.empty() && (SeqLo.size() + 2) <= Subtarget.getMaxBuildIntsCost())\n    return Op;\n\n  return SDValue();\n}",
      "start_line": 5292,
      "end_line": 5330,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "generateInstSeq",
        "generateTwoRegInstSeq",
        "getMaxBuildIntsCost",
        "use",
        "size",
        "as",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerATOMIC_FENCE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDLoc dl(Op);\n  AtomicOrdering FenceOrdering =\n      static_cast<AtomicOrdering>(Op.getConstantOperandVal(1));\n  SyncScope::ID FenceSSID =\n      static_cast<SyncScope::ID>(Op.getConstantOperandVal(2));\n\n  if (Subtarget.hasStdExtZtso()) {\n    // The only fence that needs an instruction is a sequentially-consistent\n    // cross-thread fence.\n    if (FenceOrdering == AtomicOrdering::SequentiallyConsistent &&\n        FenceSSID == SyncScope::System)\n      return Op;\n\n    // MEMBARRIER is a compiler barrier; it codegens to a no-op.\n    return DAG.getNode(ISD::MEMBARRIER, dl, MVT::Other, Op.getOperand(0));\n  }\n\n  // singlethread fences only synchronize with signal handlers on the same\n  // thread and thus only need to preserve instruction order, not actually\n  // enforce memory ordering.\n  if (FenceSSID == SyncScope::SingleThread)\n    // MEMBARRIER is a compiler barrier; it codegens to a no-op.\n    return DAG.getNode(ISD::MEMBARRIER, dl, MVT::Other, Op.getOperand(0));\n\n  return Op;\n}",
      "start_line": 5332,
      "end_line": 5359,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNode",
        "getConstantOperandVal",
        "dl"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerIS_FPCLASS",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n  unsigned Check = Op.getConstantOperandVal(1);\n  unsigned TDCMask = 0;\n  if (Check & fcSNan)\n    TDCMask |= RISCV::FPMASK_Signaling_NaN;\n  if (Check & fcQNan)\n    TDCMask |= RISCV::FPMASK_Quiet_NaN;\n  if (Check & fcPosInf)\n    TDCMask |= RISCV::FPMASK_Positive_Infinity;\n  if (Check & fcNegInf)\n    TDCMask |= RISCV::FPMASK_Negative_Infinity;\n  if (Check & fcPosNormal)\n    TDCMask |= RISCV::FPMASK_Positive_Normal;\n  if (Check & fcNegNormal)\n    TDCMask |= RISCV::FPMASK_Negative_Normal;\n  if (Check & fcPosSubnormal)\n    TDCMask |= RISCV::FPMASK_Positive_Subnormal;\n  if (Check & fcNegSubnormal)\n    TDCMask |= RISCV::FPMASK_Negative_Subnormal;\n  if (Check & fcPosZero)\n    TDCMask |= RISCV::FPMASK_Positive_Zero;\n  if (Check & fcNegZero)\n    TDCMask |= RISCV::FPMASK_Negative_Zero;\n\n  bool IsOneBitMask = isPowerOf2_32(TDCMask);\n\n  SDValue TDCMaskV = DAG.getConstant(TDCMask, DL, XLenVT);\n\n  if (VT.isVector()) {\n    SDValue Op0 = Op.getOperand(0);\n    MVT VT0 = Op.getOperand(0).getSimpleValueType();\n\n    if (VT.isScalableVector()) {\n      MVT DstVT = VT0.changeVectorElementTypeToInteger();\n      auto [Mask, VL] = getDefaultScalableVLOps(VT0, DL, DAG, Subtarget);\n      if (Op.getOpcode() == ISD::VP_IS_FPCLASS) {\n        Mask = Op.getOperand(2);\n        VL = Op.getOperand(3);\n      }\n      SDValue FPCLASS = DAG.getNode(RISCVISD::FCLASS_VL, DL, DstVT, Op0, Mask,\n                                    VL, Op->getFlags());\n      if (IsOneBitMask)\n        return DAG.getSetCC(DL, VT, FPCLASS,\n                            DAG.getConstant(TDCMask, DL, DstVT),\n                            ISD::CondCode::SETEQ);\n      SDValue AND = DAG.getNode(ISD::AND, DL, DstVT, FPCLASS,\n                                DAG.getConstant(TDCMask, DL, DstVT));\n      return DAG.getSetCC(DL, VT, AND, DAG.getConstant(0, DL, DstVT),\n                          ISD::SETNE);\n    }\n\n    MVT ContainerVT0 = getContainerForFixedLengthVector(VT0);\n    MVT ContainerVT = getContainerForFixedLengthVector(VT);\n    MVT ContainerDstVT = ContainerVT0.changeVectorElementTypeToInteger();\n    auto [Mask, VL] = getDefaultVLOps(VT0, ContainerVT0, DL, DAG, Subtarget);\n    if (Op.getOpcode() == ISD::VP_IS_FPCLASS) {\n      Mask = Op.getOperand(2);\n      MVT MaskContainerVT =\n          getContainerForFixedLengthVector(Mask.getSimpleValueType());\n      Mask = convertToScalableVector(MaskContainerVT, Mask, DAG, Subtarget);\n      VL = Op.getOperand(3);\n    }\n    Op0 = convertToScalableVector(ContainerVT0, Op0, DAG, Subtarget);\n\n    SDValue FPCLASS = DAG.getNode(RISCVISD::FCLASS_VL, DL, ContainerDstVT, Op0,\n                                  Mask, VL, Op->getFlags());\n\n    TDCMaskV = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerDstVT,\n                           DAG.getUNDEF(ContainerDstVT), TDCMaskV, VL);\n    if (IsOneBitMask) {\n      SDValue VMSEQ =\n          DAG.getNode(RISCVISD::SETCC_VL, DL, ContainerVT,\n                      {FPCLASS, TDCMaskV, DAG.getCondCode(ISD::SETEQ),\n                       DAG.getUNDEF(ContainerVT), Mask, VL});\n      return convertFromScalableVector(VT, VMSEQ, DAG, Subtarget);\n    }\n    SDValue AND = DAG.getNode(RISCVISD::AND_VL, DL, ContainerDstVT, FPCLASS,\n                              TDCMaskV, DAG.getUNDEF(ContainerDstVT), Mask, VL);\n\n    SDValue SplatZero = DAG.getConstant(0, DL, XLenVT);\n    SplatZero = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerDstVT,\n                            DAG.getUNDEF(ContainerDstVT), SplatZero, VL);\n\n    SDValue VMSNE = DAG.getNode(RISCVISD::SETCC_VL, DL, ContainerVT,\n                                {AND, SplatZero, DAG.getCondCode(ISD::SETNE),\n                                 DAG.getUNDEF(ContainerVT), Mask, VL});\n    return convertFromScalableVector(VT, VMSNE, DAG, Subtarget);\n  }\n\n  SDValue FCLASS = DAG.getNode(RISCVISD::FCLASS, DL, XLenVT, Op.getOperand(0));\n  SDValue AND = DAG.getNode(ISD::AND, DL, XLenVT, FCLASS, TDCMaskV);\n  SDValue Res = DAG.getSetCC(DL, XLenVT, AND, DAG.getConstant(0, DL, XLenVT),\n                             ISD::CondCode::SETNE);\n  return DAG.getNode(ISD::TRUNCATE, DL, VT, Res);\n}",
      "start_line": 5361,
      "end_line": 5459,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getSetCC",
        "getConstantOperandVal",
        "changeVectorElementTypeToInteger",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getUNDEF",
        "getOperand",
        "isPowerOf2_32",
        "DL",
        "getXLenVT",
        "getDefaultScalableVLOps",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFMAXIMUM_FMINIMUM",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  SDValue X = Op.getOperand(0);\n  SDValue Y = Op.getOperand(1);\n\n  if (!VT.isVector()) {\n    MVT XLenVT = Subtarget.getXLenVT();\n\n    // If X is a nan, replace Y with X. If Y is a nan, replace X with Y. This\n    // ensures that when one input is a nan, the other will also be a nan\n    // allowing the nan to propagate. If both inputs are nan, this will swap the\n    // inputs which is harmless.\n\n    SDValue NewY = Y;\n    if (!Op->getFlags().hasNoNaNs() && !DAG.isKnownNeverNaN(X)) {\n      SDValue XIsNonNan = DAG.getSetCC(DL, XLenVT, X, X, ISD::SETOEQ);\n      NewY = DAG.getSelect(DL, VT, XIsNonNan, Y, X);\n    }\n\n    SDValue NewX = X;\n    if (!Op->getFlags().hasNoNaNs() && !DAG.isKnownNeverNaN(Y)) {\n      SDValue YIsNonNan = DAG.getSetCC(DL, XLenVT, Y, Y, ISD::SETOEQ);\n      NewX = DAG.getSelect(DL, VT, YIsNonNan, X, Y);\n    }\n\n    unsigned Opc =\n        Op.getOpcode() == ISD::FMAXIMUM ? RISCVISD::FMAX : RISCVISD::FMIN;\n    return DAG.getNode(Opc, DL, VT, NewX, NewY);\n  }\n\n  // Check no NaNs before converting to fixed vector scalable.\n  bool XIsNeverNan = Op->getFlags().hasNoNaNs() || DAG.isKnownNeverNaN(X);\n  bool YIsNeverNan = Op->getFlags().hasNoNaNs() || DAG.isKnownNeverNaN(Y);\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);\n    X = convertToScalableVector(ContainerVT, X, DAG, Subtarget);\n    Y = convertToScalableVector(ContainerVT, Y, DAG, Subtarget);\n  }\n\n  SDValue Mask, VL;\n  if (Op->isVPOpcode()) {\n    Mask = Op.getOperand(2);\n    if (VT.isFixedLengthVector())\n      Mask = convertToScalableVector(getMaskTypeFor(ContainerVT), Mask, DAG,\n                                     Subtarget);\n    VL = Op.getOperand(3);\n  } else {\n    std::tie(Mask, VL) = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n  }\n\n  SDValue NewY = Y;\n  if (!XIsNeverNan) {\n    SDValue XIsNonNan = DAG.getNode(RISCVISD::SETCC_VL, DL, Mask.getValueType(),\n                                    {X, X, DAG.getCondCode(ISD::SETOEQ),\n                                     DAG.getUNDEF(ContainerVT), Mask, VL});\n    NewY = DAG.getNode(RISCVISD::VMERGE_VL, DL, ContainerVT, XIsNonNan, Y, X,\n                       DAG.getUNDEF(ContainerVT), VL);\n  }\n\n  SDValue NewX = X;\n  if (!YIsNeverNan) {\n    SDValue YIsNonNan = DAG.getNode(RISCVISD::SETCC_VL, DL, Mask.getValueType(),\n                                    {Y, Y, DAG.getCondCode(ISD::SETOEQ),\n                                     DAG.getUNDEF(ContainerVT), Mask, VL});\n    NewX = DAG.getNode(RISCVISD::VMERGE_VL, DL, ContainerVT, YIsNonNan, X, Y,\n                       DAG.getUNDEF(ContainerVT), VL);\n  }\n\n  unsigned Opc =\n      Op.getOpcode() == ISD::FMAXIMUM || Op->getOpcode() == ISD::VP_FMAXIMUM\n          ? RISCVISD::VFMAX_VL\n          : RISCVISD::VFMIN_VL;\n  SDValue Res = DAG.getNode(Opc, DL, ContainerVT, NewX, NewY,\n                            DAG.getUNDEF(ContainerVT), Mask, VL);\n  if (VT.isFixedLengthVector())\n    Res = convertFromScalableVector(VT, Res, DAG, Subtarget);\n  return Res;\n}",
      "start_line": 5463,
      "end_line": 5545,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getSetCC",
        "getSelect",
        "getOpcode",
        "getFlags",
        "convertToScalableVector",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getUNDEF",
        "getOperand",
        "hasNoNaNs",
        "DL",
        "tie",
        "getXLenVT",
        "getCondCode",
        "getNode",
        "isKnownNeverNaN"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRISCVVLOp",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        }
      ],
      "body": "{\n#define OP_CASE(NODE)                                                          \\\n  case ISD::NODE:                                                              \\\n    return RISCVISD::NODE##_VL;\n#define VP_CASE(NODE)                                                          \\\n  case ISD::VP_##NODE:                                                         \\\n    return RISCVISD::NODE##_VL;\n  // clang-format off\n  switch (Op.getOpcode()) {\n  default:\n    llvm_unreachable(\"don't have RISC-V specified VL op for this SDNode\");\n  OP_CASE(ADD)\n  OP_CASE(SUB)\n  OP_CASE(MUL)\n  OP_CASE(MULHS)\n  OP_CASE(MULHU)\n  OP_CASE(SDIV)\n  OP_CASE(SREM)\n  OP_CASE(UDIV)\n  OP_CASE(UREM)\n  OP_CASE(SHL)\n  OP_CASE(SRA)\n  OP_CASE(SRL)\n  OP_CASE(ROTL)\n  OP_CASE(ROTR)\n  OP_CASE(BSWAP)\n  OP_CASE(CTTZ)\n  OP_CASE(CTLZ)\n  OP_CASE(CTPOP)\n  OP_CASE(BITREVERSE)\n  OP_CASE(SADDSAT)\n  OP_CASE(UADDSAT)\n  OP_CASE(SSUBSAT)\n  OP_CASE(USUBSAT)\n  OP_CASE(AVGFLOORU)\n  OP_CASE(AVGCEILU)\n  OP_CASE(FADD)\n  OP_CASE(FSUB)\n  OP_CASE(FMUL)\n  OP_CASE(FDIV)\n  OP_CASE(FNEG)\n  OP_CASE(FABS)\n  OP_CASE(FSQRT)\n  OP_CASE(SMIN)\n  OP_CASE(SMAX)\n  OP_CASE(UMIN)\n  OP_CASE(UMAX)\n  OP_CASE(STRICT_FADD)\n  OP_CASE(STRICT_FSUB)\n  OP_CASE(STRICT_FMUL)\n  OP_CASE(STRICT_FDIV)\n  OP_CASE(STRICT_FSQRT)\n  VP_CASE(ADD)        // VP_ADD\n  VP_CASE(SUB)        // VP_SUB\n  VP_CASE(MUL)        // VP_MUL\n  VP_CASE(SDIV)       // VP_SDIV\n  VP_CASE(SREM)       // VP_SREM\n  VP_CASE(UDIV)       // VP_UDIV\n  VP_CASE(UREM)       // VP_UREM\n  VP_CASE(SHL)        // VP_SHL\n  VP_CASE(FADD)       // VP_FADD\n  VP_CASE(FSUB)       // VP_FSUB\n  VP_CASE(FMUL)       // VP_FMUL\n  VP_CASE(FDIV)       // VP_FDIV\n  VP_CASE(FNEG)       // VP_FNEG\n  VP_CASE(FABS)       // VP_FABS\n  VP_CASE(SMIN)       // VP_SMIN\n  VP_CASE(SMAX)       // VP_SMAX\n  VP_CASE(UMIN)       // VP_UMIN\n  VP_CASE(UMAX)       // VP_UMAX\n  VP_CASE(FCOPYSIGN)  // VP_FCOPYSIGN\n  VP_CASE(SETCC)      // VP_SETCC\n  VP_CASE(SINT_TO_FP) // VP_SINT_TO_FP\n  VP_CASE(UINT_TO_FP) // VP_UINT_TO_FP\n  VP_CASE(BITREVERSE) // VP_BITREVERSE\n  VP_CASE(BSWAP)      // VP_BSWAP\n  VP_CASE(CTLZ)       // VP_CTLZ\n  VP_CASE(CTTZ)       // VP_CTTZ\n  VP_CASE(CTPOP)      // VP_CTPOP\n  case ISD::CTLZ_ZERO_UNDEF:\n  case ISD::VP_CTLZ_ZERO_UNDEF:\n    return RISCVISD::CTLZ_VL;\n  case ISD::CTTZ_ZERO_UNDEF:\n  case ISD::VP_CTTZ_ZERO_UNDEF:\n    return RISCVISD::CTTZ_VL;\n  case ISD::FMA:\n  case ISD::VP_FMA:\n    return RISCVISD::VFMADD_VL;\n  case ISD::STRICT_FMA:\n    return RISCVISD::STRICT_VFMADD_VL;\n  case ISD::AND:\n  case ISD::VP_AND:\n    if (Op.getSimpleValueType().getVectorElementType() == MVT::i1)\n      return RISCVISD::VMAND_VL;\n    return RISCVISD::AND_VL;\n  case ISD::OR:\n  case ISD::VP_OR:\n    if (Op.getSimpleValueType().getVectorElementType() == MVT::i1)\n      return RISCVISD::VMOR_VL;\n    return RISCVISD::OR_VL;\n  case ISD::XOR:\n  case ISD::VP_XOR:\n    if (Op.getSimpleValueType().getVectorElementType() == MVT::i1)\n      return RISCVISD::VMXOR_VL;\n    return RISCVISD::XOR_VL;\n  case ISD::VP_SELECT:\n  case ISD::VP_MERGE:\n    return RISCVISD::VMERGE_VL;\n  case ISD::VP_ASHR:\n    return RISCVISD::SRA_VL;\n  case ISD::VP_LSHR:\n    return RISCVISD::SRL_VL;\n  case ISD::VP_SQRT:\n    return RISCVISD::FSQRT_VL;\n  case ISD::VP_SIGN_EXTEND:\n    return RISCVISD::VSEXT_VL;\n  case ISD::VP_ZERO_EXTEND:\n    return RISCVISD::VZEXT_VL;\n  case ISD::VP_FP_TO_SINT:\n    return RISCVISD::VFCVT_RTZ_X_F_VL;\n  case ISD::VP_FP_TO_UINT:\n    return RISCVISD::VFCVT_RTZ_XU_F_VL;\n  case ISD::FMINNUM:\n  case ISD::VP_FMINNUM:\n    return RISCVISD::VFMIN_VL;\n  case ISD::FMAXNUM:\n  case ISD::VP_FMAXNUM:\n    return RISCVISD::VFMAX_VL;\n  }\n  // clang-format on\n#undef OP_CASE\n#undef VP_CASE\n}",
      "start_line": 5548,
      "end_line": 5680,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "OP_CASE",
        "llvm_unreachable",
        "VP_CASE",
        "getVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasMergeOp",
      "return_type": "bool",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        }
      ],
      "body": "{\n  assert(Opcode > RISCVISD::FIRST_NUMBER &&\n         Opcode <= RISCVISD::LAST_RISCV_STRICTFP_OPCODE &&\n         \"not a RISC-V target specific op\");\n  static_assert(RISCVISD::LAST_VL_VECTOR_OP - RISCVISD::FIRST_VL_VECTOR_OP ==\n                    126 &&\n                RISCVISD::LAST_RISCV_STRICTFP_OPCODE -\n                        ISD::FIRST_TARGET_STRICTFP_OPCODE ==\n                    21 &&\n                \"adding target specific op should update this function\");\n  if (Opcode >= RISCVISD::ADD_VL && Opcode <= RISCVISD::VFMAX_VL)\n    return true;\n  if (Opcode == RISCVISD::FCOPYSIGN_VL)\n    return true;\n  if (Opcode >= RISCVISD::VWMUL_VL && Opcode <= RISCVISD::VFWSUB_W_VL)\n    return true;\n  if (Opcode == RISCVISD::SETCC_VL)\n    return true;\n  if (Opcode >= RISCVISD::STRICT_FADD_VL && Opcode <= RISCVISD::STRICT_FDIV_VL)\n    return true;\n  if (Opcode == RISCVISD::VMERGE_VL)\n    return true;\n  return false;\n}",
      "start_line": 5683,
      "end_line": 5706,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "static_assert"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasMaskOp",
      "return_type": "bool",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        }
      ],
      "body": "{\n  assert(Opcode > RISCVISD::FIRST_NUMBER &&\n         Opcode <= RISCVISD::LAST_RISCV_STRICTFP_OPCODE &&\n         \"not a RISC-V target specific op\");\n  static_assert(RISCVISD::LAST_VL_VECTOR_OP - RISCVISD::FIRST_VL_VECTOR_OP ==\n                    126 &&\n                RISCVISD::LAST_RISCV_STRICTFP_OPCODE -\n                        ISD::FIRST_TARGET_STRICTFP_OPCODE ==\n                    21 &&\n                \"adding target specific op should update this function\");\n  if (Opcode >= RISCVISD::TRUNCATE_VECTOR_VL && Opcode <= RISCVISD::SETCC_VL)\n    return true;\n  if (Opcode >= RISCVISD::VRGATHER_VX_VL && Opcode <= RISCVISD::VFIRST_VL)\n    return true;\n  if (Opcode >= RISCVISD::STRICT_FADD_VL &&\n      Opcode <= RISCVISD::STRICT_VFROUND_NOEXCEPT_VL)\n    return true;\n  return false;\n}",
      "start_line": 5709,
      "end_line": 5727,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "static_assert"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SplitVectorOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  auto [LoVT, HiVT] = DAG.GetSplitDestVTs(Op.getValueType());\n  SDLoc DL(Op);\n\n  SmallVector<SDValue, 4> LoOperands(Op.getNumOperands());\n  SmallVector<SDValue, 4> HiOperands(Op.getNumOperands());\n\n  for (unsigned j = 0; j != Op.getNumOperands(); ++j) {\n    if (!Op.getOperand(j).getValueType().isVector()) {\n      LoOperands[j] = Op.getOperand(j);\n      HiOperands[j] = Op.getOperand(j);\n      continue;\n    }\n    std::tie(LoOperands[j], HiOperands[j]) =\n        DAG.SplitVector(Op.getOperand(j), DL);\n  }\n\n  SDValue LoRes =\n      DAG.getNode(Op.getOpcode(), DL, LoVT, LoOperands, Op->getFlags());\n  SDValue HiRes =\n      DAG.getNode(Op.getOpcode(), DL, HiVT, HiOperands, Op->getFlags());\n\n  return DAG.getNode(ISD::CONCAT_VECTORS, DL, Op.getValueType(), LoRes, HiRes);\n}",
      "start_line": 5729,
      "end_line": 5752,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "LoOperands",
        "getFlags",
        "getValueType",
        "GetSplitDestVTs",
        "isVector",
        "getOperand",
        "DL",
        "tie",
        "SplitVector",
        "getNode",
        "HiOperands"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SplitVPOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  assert(ISD::isVPOpcode(Op.getOpcode()) && \"Not a VP op\");\n  auto [LoVT, HiVT] = DAG.GetSplitDestVTs(Op.getValueType());\n  SDLoc DL(Op);\n\n  SmallVector<SDValue, 4> LoOperands(Op.getNumOperands());\n  SmallVector<SDValue, 4> HiOperands(Op.getNumOperands());\n\n  for (unsigned j = 0; j != Op.getNumOperands(); ++j) {\n    if (ISD::getVPExplicitVectorLengthIdx(Op.getOpcode()) == j) {\n      std::tie(LoOperands[j], HiOperands[j]) =\n          DAG.SplitEVL(Op.getOperand(j), Op.getValueType(), DL);\n      continue;\n    }\n    if (!Op.getOperand(j).getValueType().isVector()) {\n      LoOperands[j] = Op.getOperand(j);\n      HiOperands[j] = Op.getOperand(j);\n      continue;\n    }\n    std::tie(LoOperands[j], HiOperands[j]) =\n        DAG.SplitVector(Op.getOperand(j), DL);\n  }\n\n  SDValue LoRes =\n      DAG.getNode(Op.getOpcode(), DL, LoVT, LoOperands, Op->getFlags());\n  SDValue HiRes =\n      DAG.getNode(Op.getOpcode(), DL, HiVT, HiOperands, Op->getFlags());\n\n  return DAG.getNode(ISD::CONCAT_VECTORS, DL, Op.getValueType(), LoRes, HiRes);\n}",
      "start_line": 5754,
      "end_line": 5783,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "LoOperands",
        "getFlags",
        "getValueType",
        "GetSplitDestVTs",
        "isVector",
        "SplitEVL",
        "getOperand",
        "tie",
        "DL",
        "SplitVector",
        "getNode",
        "HiOperands"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SplitVectorReductionOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n\n  auto [Lo, Hi] = DAG.SplitVector(Op.getOperand(1), DL);\n  auto [MaskLo, MaskHi] = DAG.SplitVector(Op.getOperand(2), DL);\n  auto [EVLLo, EVLHi] =\n      DAG.SplitEVL(Op.getOperand(3), Op.getOperand(1).getValueType(), DL);\n\n  SDValue ResLo =\n      DAG.getNode(Op.getOpcode(), DL, Op.getValueType(),\n                  {Op.getOperand(0), Lo, MaskLo, EVLLo}, Op->getFlags());\n  return DAG.getNode(Op.getOpcode(), DL, Op.getValueType(),\n                     {ResLo, Hi, MaskHi, EVLHi}, Op->getFlags());\n}",
      "start_line": 5785,
      "end_line": 5798,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getFlags",
        "getValueType",
        "getOperand",
        "SplitEVL",
        "DL",
        "SplitVector",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SplitStrictFPVectorOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n\n  assert(Op->isStrictFPOpcode());\n\n  auto [LoVT, HiVT] = DAG.GetSplitDestVTs(Op->getValueType(0));\n\n  SDVTList LoVTs = DAG.getVTList(LoVT, Op->getValueType(1));\n  SDVTList HiVTs = DAG.getVTList(HiVT, Op->getValueType(1));\n\n  SDLoc DL(Op);\n\n  SmallVector<SDValue, 4> LoOperands(Op.getNumOperands());\n  SmallVector<SDValue, 4> HiOperands(Op.getNumOperands());\n\n  for (unsigned j = 0; j != Op.getNumOperands(); ++j) {\n    if (!Op.getOperand(j).getValueType().isVector()) {\n      LoOperands[j] = Op.getOperand(j);\n      HiOperands[j] = Op.getOperand(j);\n      continue;\n    }\n    std::tie(LoOperands[j], HiOperands[j]) =\n        DAG.SplitVector(Op.getOperand(j), DL);\n  }\n\n  SDValue LoRes =\n      DAG.getNode(Op.getOpcode(), DL, LoVTs, LoOperands, Op->getFlags());\n  HiOperands[0] = LoRes.getValue(1);\n  SDValue HiRes =\n      DAG.getNode(Op.getOpcode(), DL, HiVTs, HiOperands, Op->getFlags());\n\n  SDValue V = DAG.getNode(ISD::CONCAT_VECTORS, DL, Op->getValueType(0),\n                          LoRes.getValue(0), HiRes.getValue(0));\n  return DAG.getMergeValues({V, HiRes.getValue(1)}, DL);\n}",
      "start_line": 5800,
      "end_line": 5833,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMergeValues",
        "getValue",
        "getVTList",
        "LoOperands",
        "getFlags",
        "getValueType",
        "GetSplitDestVTs",
        "isVector",
        "getOperand",
        "DL",
        "tie",
        "SplitVector",
        "getNode",
        "HiOperands"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerOperation",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  switch (Op.getOpcode()) {\n  default:\n    report_fatal_error(\"unimplemented operand\");\n  case ISD::ATOMIC_FENCE:\n    return LowerATOMIC_FENCE(Op, DAG, Subtarget);\n  case ISD::GlobalAddress:\n    return lowerGlobalAddress(Op, DAG);\n  case ISD::BlockAddress:\n    return lowerBlockAddress(Op, DAG);\n  case ISD::ConstantPool:\n    return lowerConstantPool(Op, DAG);\n  case ISD::JumpTable:\n    return lowerJumpTable(Op, DAG);\n  case ISD::GlobalTLSAddress:\n    return lowerGlobalTLSAddress(Op, DAG);\n  case ISD::Constant:\n    return lowerConstant(Op, DAG, Subtarget);\n  case ISD::SELECT:\n    return lowerSELECT(Op, DAG);\n  case ISD::BRCOND:\n    return lowerBRCOND(Op, DAG);\n  case ISD::VASTART:\n    return lowerVASTART(Op, DAG);\n  case ISD::FRAMEADDR:\n    return lowerFRAMEADDR(Op, DAG);\n  case ISD::RETURNADDR:\n    return lowerRETURNADDR(Op, DAG);\n  case ISD::SHL_PARTS:\n    return lowerShiftLeftParts(Op, DAG);\n  case ISD::SRA_PARTS:\n    return lowerShiftRightParts(Op, DAG, true);\n  case ISD::SRL_PARTS:\n    return lowerShiftRightParts(Op, DAG, false);\n  case ISD::ROTL:\n  case ISD::ROTR:\n    if (Op.getValueType().isFixedLengthVector()) {\n      assert(Subtarget.hasStdExtZvkb());\n      return lowerToScalableOp(Op, DAG);\n    }\n    assert(Subtarget.hasVendorXTHeadBb() &&\n           !(Subtarget.hasStdExtZbb() || Subtarget.hasStdExtZbkb()) &&\n           \"Unexpected custom legalization\");\n    // XTHeadBb only supports rotate by constant.\n    if (!isa<ConstantSDNode>(Op.getOperand(1)))\n      return SDValue();\n    return Op;\n  case ISD::BITCAST: {\n    SDLoc DL(Op);\n    EVT VT = Op.getValueType();\n    SDValue Op0 = Op.getOperand(0);\n    EVT Op0VT = Op0.getValueType();\n    MVT XLenVT = Subtarget.getXLenVT();\n    if (VT == MVT::f16 && Op0VT == MVT::i16 &&\n        Subtarget.hasStdExtZfhminOrZhinxmin()) {\n      SDValue NewOp0 = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Op0);\n      SDValue FPConv = DAG.getNode(RISCVISD::FMV_H_X, DL, MVT::f16, NewOp0);\n      return FPConv;\n    }\n    if (VT == MVT::bf16 && Op0VT == MVT::i16 &&\n        Subtarget.hasStdExtZfbfmin()) {\n      SDValue NewOp0 = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Op0);\n      SDValue FPConv = DAG.getNode(RISCVISD::FMV_H_X, DL, MVT::bf16, NewOp0);\n      return FPConv;\n    }\n    if (VT == MVT::f32 && Op0VT == MVT::i32 && Subtarget.is64Bit() &&\n        Subtarget.hasStdExtFOrZfinx()) {\n      SDValue NewOp0 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op0);\n      SDValue FPConv =\n          DAG.getNode(RISCVISD::FMV_W_X_RV64, DL, MVT::f32, NewOp0);\n      return FPConv;\n    }\n    if (VT == MVT::f64 && Op0VT == MVT::i64 && XLenVT == MVT::i32 &&\n        Subtarget.hasStdExtZfa()) {\n      SDValue Lo, Hi;\n      std::tie(Lo, Hi) = DAG.SplitScalar(Op0, DL, MVT::i32, MVT::i32);\n      SDValue RetReg =\n          DAG.getNode(RISCVISD::BuildPairF64, DL, MVT::f64, Lo, Hi);\n      return RetReg;\n    }\n\n    // Consider other scalar<->scalar casts as legal if the types are legal.\n    // Otherwise expand them.\n    if (!VT.isVector() && !Op0VT.isVector()) {\n      if (isTypeLegal(VT) && isTypeLegal(Op0VT))\n        return Op;\n      return SDValue();\n    }\n\n    assert(!VT.isScalableVector() && !Op0VT.isScalableVector() &&\n           \"Unexpected types\");\n\n    if (VT.isFixedLengthVector()) {\n      // We can handle fixed length vector bitcasts with a simple replacement\n      // in isel.\n      if (Op0VT.isFixedLengthVector())\n        return Op;\n      // When bitcasting from scalar to fixed-length vector, insert the scalar\n      // into a one-element vector of the result type, and perform a vector\n      // bitcast.\n      if (!Op0VT.isVector()) {\n        EVT BVT = EVT::getVectorVT(*DAG.getContext(), Op0VT, 1);\n        if (!isTypeLegal(BVT))\n          return SDValue();\n        return DAG.getBitcast(VT, DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, BVT,\n                                              DAG.getUNDEF(BVT), Op0,\n                                              DAG.getConstant(0, DL, XLenVT)));\n      }\n      return SDValue();\n    }\n    // Custom-legalize bitcasts from fixed-length vector types to scalar types\n    // thus: bitcast the vector to a one-element vector type whose element type\n    // is the same as the result type, and extract the first element.\n    if (!VT.isVector() && Op0VT.isFixedLengthVector()) {\n      EVT BVT = EVT::getVectorVT(*DAG.getContext(), VT, 1);\n      if (!isTypeLegal(BVT))\n        return SDValue();\n      SDValue BVec = DAG.getBitcast(BVT, Op0);\n      return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, BVec,\n                         DAG.getConstant(0, DL, XLenVT));\n    }\n    return SDValue();\n  }\n  case ISD::INTRINSIC_WO_CHAIN:\n    return LowerINTRINSIC_WO_CHAIN(Op, DAG);\n  case ISD::INTRINSIC_W_CHAIN:\n    return LowerINTRINSIC_W_CHAIN(Op, DAG);\n  case ISD::INTRINSIC_VOID:\n    return LowerINTRINSIC_VOID(Op, DAG);\n  case ISD::IS_FPCLASS:\n    return LowerIS_FPCLASS(Op, DAG);\n  case ISD::BITREVERSE: {\n    MVT VT = Op.getSimpleValueType();\n    if (VT.isFixedLengthVector()) {\n      assert(Subtarget.hasStdExtZvbb());\n      return lowerToScalableOp(Op, DAG);\n    }\n    SDLoc DL(Op);\n    assert(Subtarget.hasStdExtZbkb() && \"Unexpected custom legalization\");\n    assert(Op.getOpcode() == ISD::BITREVERSE && \"Unexpected opcode\");\n    // Expand bitreverse to a bswap(rev8) followed by brev8.\n    SDValue BSwap = DAG.getNode(ISD::BSWAP, DL, VT, Op.getOperand(0));\n    return DAG.getNode(RISCVISD::BREV8, DL, VT, BSwap);\n  }\n  case ISD::TRUNCATE:\n    // Only custom-lower vector truncates\n    if (!Op.getSimpleValueType().isVector())\n      return Op;\n    return lowerVectorTruncLike(Op, DAG);\n  case ISD::ANY_EXTEND:\n  case ISD::ZERO_EXTEND:\n    if (Op.getOperand(0).getValueType().isVector() &&\n        Op.getOperand(0).getValueType().getVectorElementType() == MVT::i1)\n      return lowerVectorMaskExt(Op, DAG, /*ExtVal*/ 1);\n    return lowerFixedLengthVectorExtendToRVV(Op, DAG, RISCVISD::VZEXT_VL);\n  case ISD::SIGN_EXTEND:\n    if (Op.getOperand(0).getValueType().isVector() &&\n        Op.getOperand(0).getValueType().getVectorElementType() == MVT::i1)\n      return lowerVectorMaskExt(Op, DAG, /*ExtVal*/ -1);\n    return lowerFixedLengthVectorExtendToRVV(Op, DAG, RISCVISD::VSEXT_VL);\n  case ISD::SPLAT_VECTOR_PARTS:\n    return lowerSPLAT_VECTOR_PARTS(Op, DAG);\n  case ISD::INSERT_VECTOR_ELT:\n    return lowerINSERT_VECTOR_ELT(Op, DAG);\n  case ISD::EXTRACT_VECTOR_ELT:\n    return lowerEXTRACT_VECTOR_ELT(Op, DAG);\n  case ISD::SCALAR_TO_VECTOR: {\n    MVT VT = Op.getSimpleValueType();\n    SDLoc DL(Op);\n    SDValue Scalar = Op.getOperand(0);\n    if (VT.getVectorElementType() == MVT::i1) {\n      MVT WideVT = VT.changeVectorElementType(MVT::i8);\n      SDValue V = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, WideVT, Scalar);\n      return DAG.getNode(ISD::TRUNCATE, DL, VT, V);\n    }\n    MVT ContainerVT = VT;\n    if (VT.isFixedLengthVector())\n      ContainerVT = getContainerForFixedLengthVector(VT);\n    SDValue VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n    Scalar = DAG.getNode(ISD::ANY_EXTEND, DL, Subtarget.getXLenVT(), Scalar);\n    SDValue V = DAG.getNode(RISCVISD::VMV_S_X_VL, DL, ContainerVT,\n                            DAG.getUNDEF(ContainerVT), Scalar, VL);\n    if (VT.isFixedLengthVector())\n      V = convertFromScalableVector(VT, V, DAG, Subtarget);\n    return V;\n  }\n  case ISD::VSCALE: {\n    MVT XLenVT = Subtarget.getXLenVT();\n    MVT VT = Op.getSimpleValueType();\n    SDLoc DL(Op);\n    SDValue Res = DAG.getNode(RISCVISD::READ_VLENB, DL, XLenVT);\n    // We define our scalable vector types for lmul=1 to use a 64 bit known\n    // minimum size. e.g. <vscale x 2 x i32>. VLENB is in bytes so we calculate\n    // vscale as VLENB / 8.\n    static_assert(RISCV::RVVBitsPerBlock == 64, \"Unexpected bits per block!\");\n    if (Subtarget.getRealMinVLen() < RISCV::RVVBitsPerBlock)\n      report_fatal_error(\"Support for VLEN==32 is incomplete.\");\n    // We assume VLENB is a multiple of 8. We manually choose the best shift\n    // here because SimplifyDemandedBits isn't always able to simplify it.\n    uint64_t Val = Op.getConstantOperandVal(0);\n    if (isPowerOf2_64(Val)) {\n      uint64_t Log2 = Log2_64(Val);\n      if (Log2 < 3)\n        Res = DAG.getNode(ISD::SRL, DL, XLenVT, Res,\n                          DAG.getConstant(3 - Log2, DL, VT));\n      else if (Log2 > 3)\n        Res = DAG.getNode(ISD::SHL, DL, XLenVT, Res,\n                          DAG.getConstant(Log2 - 3, DL, XLenVT));\n    } else if ((Val % 8) == 0) {\n      // If the multiplier is a multiple of 8, scale it down to avoid needing\n      // to shift the VLENB value.\n      Res = DAG.getNode(ISD::MUL, DL, XLenVT, Res,\n                        DAG.getConstant(Val / 8, DL, XLenVT));\n    } else {\n      SDValue VScale = DAG.getNode(ISD::SRL, DL, XLenVT, Res,\n                                   DAG.getConstant(3, DL, XLenVT));\n      Res = DAG.getNode(ISD::MUL, DL, XLenVT, VScale,\n                        DAG.getConstant(Val, DL, XLenVT));\n    }\n    return DAG.getNode(ISD::TRUNCATE, DL, VT, Res);\n  }\n  case ISD::FPOWI: {\n    // Custom promote f16 powi with illegal i32 integer type on RV64. Once\n    // promoted this will be legalized into a libcall by LegalizeIntegerTypes.\n    if (Op.getValueType() == MVT::f16 && Subtarget.is64Bit() &&\n        Op.getOperand(1).getValueType() == MVT::i32) {\n      SDLoc DL(Op);\n      SDValue Op0 = DAG.getNode(ISD::FP_EXTEND, DL, MVT::f32, Op.getOperand(0));\n      SDValue Powi =\n          DAG.getNode(ISD::FPOWI, DL, MVT::f32, Op0, Op.getOperand(1));\n      return DAG.getNode(ISD::FP_ROUND, DL, MVT::f16, Powi,\n                         DAG.getIntPtrConstant(0, DL, /*isTarget=*/true));\n    }\n    return SDValue();\n  }\n  case ISD::FMAXIMUM:\n  case ISD::FMINIMUM:\n    if (Op.getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVectorOp(Op, DAG);\n    return lowerFMAXIMUM_FMINIMUM(Op, DAG, Subtarget);\n  case ISD::FP_EXTEND: {\n    SDLoc DL(Op);\n    EVT VT = Op.getValueType();\n    SDValue Op0 = Op.getOperand(0);\n    EVT Op0VT = Op0.getValueType();\n    if (VT == MVT::f32 && Op0VT == MVT::bf16 && Subtarget.hasStdExtZfbfmin())\n      return DAG.getNode(RISCVISD::FP_EXTEND_BF16, DL, MVT::f32, Op0);\n    if (VT == MVT::f64 && Op0VT == MVT::bf16 && Subtarget.hasStdExtZfbfmin()) {\n      SDValue FloatVal =\n          DAG.getNode(RISCVISD::FP_EXTEND_BF16, DL, MVT::f32, Op0);\n      return DAG.getNode(ISD::FP_EXTEND, DL, MVT::f64, FloatVal);\n    }\n\n    if (!Op.getValueType().isVector())\n      return Op;\n    return lowerVectorFPExtendOrRoundLike(Op, DAG);\n  }\n  case ISD::FP_ROUND: {\n    SDLoc DL(Op);\n    EVT VT = Op.getValueType();\n    SDValue Op0 = Op.getOperand(0);\n    EVT Op0VT = Op0.getValueType();\n    if (VT == MVT::bf16 && Op0VT == MVT::f32 && Subtarget.hasStdExtZfbfmin())\n      return DAG.getNode(RISCVISD::FP_ROUND_BF16, DL, MVT::bf16, Op0);\n    if (VT == MVT::bf16 && Op0VT == MVT::f64 && Subtarget.hasStdExtZfbfmin() &&\n        Subtarget.hasStdExtDOrZdinx()) {\n      SDValue FloatVal =\n          DAG.getNode(ISD::FP_ROUND, DL, MVT::f32, Op0,\n                      DAG.getIntPtrConstant(0, DL, /*isTarget=*/true));\n      return DAG.getNode(RISCVISD::FP_ROUND_BF16, DL, MVT::bf16, FloatVal);\n    }\n\n    if (!Op.getValueType().isVector())\n      return Op;\n    return lowerVectorFPExtendOrRoundLike(Op, DAG);\n  }\n  case ISD::STRICT_FP_ROUND:\n  case ISD::STRICT_FP_EXTEND:\n    return lowerStrictFPExtendOrRoundLike(Op, DAG);\n  case ISD::SINT_TO_FP:\n  case ISD::UINT_TO_FP:\n    if (Op.getValueType().isVector() &&\n        Op.getValueType().getScalarType() == MVT::f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16())) {\n      if (Op.getValueType() == MVT::nxv32f16)\n        return SplitVectorOp(Op, DAG);\n      // int -> f32\n      SDLoc DL(Op);\n      MVT NVT =\n          MVT::getVectorVT(MVT::f32, Op.getValueType().getVectorElementCount());\n      SDValue NC = DAG.getNode(Op.getOpcode(), DL, NVT, Op->ops());\n      // f32 -> f16\n      return DAG.getNode(ISD::FP_ROUND, DL, Op.getValueType(), NC,\n                         DAG.getIntPtrConstant(0, DL, /*isTarget=*/true));\n    }\n    [[fallthrough]];\n  case ISD::FP_TO_SINT:\n  case ISD::FP_TO_UINT:\n    if (SDValue Op1 = Op.getOperand(0);\n        Op1.getValueType().isVector() &&\n        Op1.getValueType().getScalarType() == MVT::f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16())) {\n      if (Op1.getValueType() == MVT::nxv32f16)\n        return SplitVectorOp(Op, DAG);\n      // f16 -> f32\n      SDLoc DL(Op);\n      MVT NVT = MVT::getVectorVT(MVT::f32,\n                                 Op1.getValueType().getVectorElementCount());\n      SDValue WidenVec = DAG.getNode(ISD::FP_EXTEND, DL, NVT, Op1);\n      // f32 -> int\n      return DAG.getNode(Op.getOpcode(), DL, Op.getValueType(), WidenVec);\n    }\n    [[fallthrough]];\n  case ISD::STRICT_FP_TO_SINT:\n  case ISD::STRICT_FP_TO_UINT:\n  case ISD::STRICT_SINT_TO_FP:\n  case ISD::STRICT_UINT_TO_FP: {\n    // RVV can only do fp<->int conversions to types half/double the size as\n    // the source. We custom-lower any conversions that do two hops into\n    // sequences.\n    MVT VT = Op.getSimpleValueType();\n    if (!VT.isVector())\n      return Op;\n    SDLoc DL(Op);\n    bool IsStrict = Op->isStrictFPOpcode();\n    SDValue Src = Op.getOperand(0 + IsStrict);\n    MVT EltVT = VT.getVectorElementType();\n    MVT SrcVT = Src.getSimpleValueType();\n    MVT SrcEltVT = SrcVT.getVectorElementType();\n    unsigned EltSize = EltVT.getSizeInBits();\n    unsigned SrcEltSize = SrcEltVT.getSizeInBits();\n    assert(isPowerOf2_32(EltSize) && isPowerOf2_32(SrcEltSize) &&\n           \"Unexpected vector element types\");\n\n    bool IsInt2FP = SrcEltVT.isInteger();\n    // Widening conversions\n    if (EltSize > (2 * SrcEltSize)) {\n      if (IsInt2FP) {\n        // Do a regular integer sign/zero extension then convert to float.\n        MVT IVecVT = MVT::getVectorVT(MVT::getIntegerVT(EltSize / 2),\n                                      VT.getVectorElementCount());\n        unsigned ExtOpcode = (Op.getOpcode() == ISD::UINT_TO_FP ||\n                              Op.getOpcode() == ISD::STRICT_UINT_TO_FP)\n                                 ? ISD::ZERO_EXTEND\n                                 : ISD::SIGN_EXTEND;\n        SDValue Ext = DAG.getNode(ExtOpcode, DL, IVecVT, Src);\n        if (IsStrict)\n          return DAG.getNode(Op.getOpcode(), DL, Op->getVTList(),\n                             Op.getOperand(0), Ext);\n        return DAG.getNode(Op.getOpcode(), DL, VT, Ext);\n      }\n      // FP2Int\n      assert(SrcEltVT == MVT::f16 && \"Unexpected FP_TO_[US]INT lowering\");\n      // Do one doubling fp_extend then complete the operation by converting\n      // to int.\n      MVT InterimFVT = MVT::getVectorVT(MVT::f32, VT.getVectorElementCount());\n      if (IsStrict) {\n        auto [FExt, Chain] =\n            DAG.getStrictFPExtendOrRound(Src, Op.getOperand(0), DL, InterimFVT);\n        return DAG.getNode(Op.getOpcode(), DL, Op->getVTList(), Chain, FExt);\n      }\n      SDValue FExt = DAG.getFPExtendOrRound(Src, DL, InterimFVT);\n      return DAG.getNode(Op.getOpcode(), DL, VT, FExt);\n    }\n\n    // Narrowing conversions\n    if (SrcEltSize > (2 * EltSize)) {\n      if (IsInt2FP) {\n        // One narrowing int_to_fp, then an fp_round.\n        assert(EltVT == MVT::f16 && \"Unexpected [US]_TO_FP lowering\");\n        MVT InterimFVT = MVT::getVectorVT(MVT::f32, VT.getVectorElementCount());\n        if (IsStrict) {\n          SDValue Int2FP = DAG.getNode(Op.getOpcode(), DL,\n                                       DAG.getVTList(InterimFVT, MVT::Other),\n                                       Op.getOperand(0), Src);\n          SDValue Chain = Int2FP.getValue(1);\n          return DAG.getStrictFPExtendOrRound(Int2FP, Chain, DL, VT).first;\n        }\n        SDValue Int2FP = DAG.getNode(Op.getOpcode(), DL, InterimFVT, Src);\n        return DAG.getFPExtendOrRound(Int2FP, DL, VT);\n      }\n      // FP2Int\n      // One narrowing fp_to_int, then truncate the integer. If the float isn't\n      // representable by the integer, the result is poison.\n      MVT IVecVT = MVT::getVectorVT(MVT::getIntegerVT(SrcEltSize / 2),\n                                    VT.getVectorElementCount());\n      if (IsStrict) {\n        SDValue FP2Int =\n            DAG.getNode(Op.getOpcode(), DL, DAG.getVTList(IVecVT, MVT::Other),\n                        Op.getOperand(0), Src);\n        SDValue Res = DAG.getNode(ISD::TRUNCATE, DL, VT, FP2Int);\n        return DAG.getMergeValues({Res, FP2Int.getValue(1)}, DL);\n      }\n      SDValue FP2Int = DAG.getNode(Op.getOpcode(), DL, IVecVT, Src);\n      return DAG.getNode(ISD::TRUNCATE, DL, VT, FP2Int);\n    }\n\n    // Scalable vectors can exit here. Patterns will handle equally-sized\n    // conversions halving/doubling ones.\n    if (!VT.isFixedLengthVector())\n      return Op;\n\n    // For fixed-length vectors we lower to a custom \"VL\" node.\n    unsigned RVVOpc = 0;\n    switch (Op.getOpcode()) {\n    default:\n      llvm_unreachable(\"Impossible opcode\");\n    case ISD::FP_TO_SINT:\n      RVVOpc = RISCVISD::VFCVT_RTZ_X_F_VL;\n      break;\n    case ISD::FP_TO_UINT:\n      RVVOpc = RISCVISD::VFCVT_RTZ_XU_F_VL;\n      break;\n    case ISD::SINT_TO_FP:\n      RVVOpc = RISCVISD::SINT_TO_FP_VL;\n      break;\n    case ISD::UINT_TO_FP:\n      RVVOpc = RISCVISD::UINT_TO_FP_VL;\n      break;\n    case ISD::STRICT_FP_TO_SINT:\n      RVVOpc = RISCVISD::STRICT_VFCVT_RTZ_X_F_VL;\n      break;\n    case ISD::STRICT_FP_TO_UINT:\n      RVVOpc = RISCVISD::STRICT_VFCVT_RTZ_XU_F_VL;\n      break;\n    case ISD::STRICT_SINT_TO_FP:\n      RVVOpc = RISCVISD::STRICT_SINT_TO_FP_VL;\n      break;\n    case ISD::STRICT_UINT_TO_FP:\n      RVVOpc = RISCVISD::STRICT_UINT_TO_FP_VL;\n      break;\n    }\n\n    MVT ContainerVT = getContainerForFixedLengthVector(VT);\n    MVT SrcContainerVT = getContainerForFixedLengthVector(SrcVT);\n    assert(ContainerVT.getVectorElementCount() == SrcContainerVT.getVectorElementCount() &&\n           \"Expected same element count\");\n\n    auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n    Src = convertToScalableVector(SrcContainerVT, Src, DAG, Subtarget);\n    if (IsStrict) {\n      Src = DAG.getNode(RVVOpc, DL, DAG.getVTList(ContainerVT, MVT::Other),\n                        Op.getOperand(0), Src, Mask, VL);\n      SDValue SubVec = convertFromScalableVector(VT, Src, DAG, Subtarget);\n      return DAG.getMergeValues({SubVec, Src.getValue(1)}, DL);\n    }\n    Src = DAG.getNode(RVVOpc, DL, ContainerVT, Src, Mask, VL);\n    return convertFromScalableVector(VT, Src, DAG, Subtarget);\n  }\n  case ISD::FP_TO_SINT_SAT:\n  case ISD::FP_TO_UINT_SAT:\n    return lowerFP_TO_INT_SAT(Op, DAG, Subtarget);\n  case ISD::FP_TO_BF16: {\n    // Custom lower to ensure the libcall return is passed in an FPR on hard\n    // float ABIs.\n    assert(!Subtarget.isSoftFPABI() && \"Unexpected custom legalization\");\n    SDLoc DL(Op);\n    MakeLibCallOptions CallOptions;\n    RTLIB::Libcall LC =\n        RTLIB::getFPROUND(Op.getOperand(0).getValueType(), MVT::bf16);\n    SDValue Res =\n        makeLibCall(DAG, LC, MVT::f32, Op.getOperand(0), CallOptions, DL).first;\n    if (Subtarget.is64Bit() && !RV64LegalI32)\n      return DAG.getNode(RISCVISD::FMV_X_ANYEXTW_RV64, DL, MVT::i64, Res);\n    return DAG.getBitcast(MVT::i32, Res);\n  }\n  case ISD::BF16_TO_FP: {\n    assert(Subtarget.hasStdExtFOrZfinx() && \"Unexpected custom legalization\");\n    MVT VT = Op.getSimpleValueType();\n    SDLoc DL(Op);\n    Op = DAG.getNode(\n        ISD::SHL, DL, Op.getOperand(0).getValueType(), Op.getOperand(0),\n        DAG.getShiftAmountConstant(16, Op.getOperand(0).getValueType(), DL));\n    SDValue Res = Subtarget.is64Bit()\n                      ? DAG.getNode(RISCVISD::FMV_W_X_RV64, DL, MVT::f32, Op)\n                      : DAG.getBitcast(MVT::f32, Op);\n    // fp_extend if the target VT is bigger than f32.\n    if (VT != MVT::f32)\n      return DAG.getNode(ISD::FP_EXTEND, DL, VT, Res);\n    return Res;\n  }\n  case ISD::FP_TO_FP16: {\n    // Custom lower to ensure the libcall return is passed in an FPR on hard\n    // float ABIs.\n    assert(Subtarget.hasStdExtFOrZfinx() && \"Unexpected custom legalisation\");\n    SDLoc DL(Op);\n    MakeLibCallOptions CallOptions;\n    RTLIB::Libcall LC =\n        RTLIB::getFPROUND(Op.getOperand(0).getValueType(), MVT::f16);\n    SDValue Res =\n        makeLibCall(DAG, LC, MVT::f32, Op.getOperand(0), CallOptions, DL).first;\n    if (Subtarget.is64Bit() && !RV64LegalI32)\n      return DAG.getNode(RISCVISD::FMV_X_ANYEXTW_RV64, DL, MVT::i64, Res);\n    return DAG.getBitcast(MVT::i32, Res);\n  }\n  case ISD::FP16_TO_FP: {\n    // Custom lower to ensure the libcall argument is passed in an FPR on hard\n    // float ABIs.\n    assert(Subtarget.hasStdExtFOrZfinx() && \"Unexpected custom legalisation\");\n    SDLoc DL(Op);\n    MakeLibCallOptions CallOptions;\n    SDValue Arg = Subtarget.is64Bit()\n                      ? DAG.getNode(RISCVISD::FMV_W_X_RV64, DL, MVT::f32,\n                                    Op.getOperand(0))\n                      : DAG.getBitcast(MVT::f32, Op.getOperand(0));\n    SDValue Res =\n        makeLibCall(DAG, RTLIB::FPEXT_F16_F32, MVT::f32, Arg, CallOptions, DL)\n            .first;\n    return Res;\n  }\n  case ISD::FTRUNC:\n  case ISD::FCEIL:\n  case ISD::FFLOOR:\n  case ISD::FNEARBYINT:\n  case ISD::FRINT:\n  case ISD::FROUND:\n  case ISD::FROUNDEVEN:\n    return lowerFTRUNC_FCEIL_FFLOOR_FROUND(Op, DAG, Subtarget);\n  case ISD::LRINT:\n  case ISD::LLRINT:\n    return lowerVectorXRINT(Op, DAG, Subtarget);\n  case ISD::VECREDUCE_ADD:\n  case ISD::VECREDUCE_UMAX:\n  case ISD::VECREDUCE_SMAX:\n  case ISD::VECREDUCE_UMIN:\n  case ISD::VECREDUCE_SMIN:\n    return lowerVECREDUCE(Op, DAG);\n  case ISD::VECREDUCE_AND:\n  case ISD::VECREDUCE_OR:\n  case ISD::VECREDUCE_XOR:\n    if (Op.getOperand(0).getValueType().getVectorElementType() == MVT::i1)\n      return lowerVectorMaskVecReduction(Op, DAG, /*IsVP*/ false);\n    return lowerVECREDUCE(Op, DAG);\n  case ISD::VECREDUCE_FADD:\n  case ISD::VECREDUCE_SEQ_FADD:\n  case ISD::VECREDUCE_FMIN:\n  case ISD::VECREDUCE_FMAX:\n    return lowerFPVECREDUCE(Op, DAG);\n  case ISD::VP_REDUCE_ADD:\n  case ISD::VP_REDUCE_UMAX:\n  case ISD::VP_REDUCE_SMAX:\n  case ISD::VP_REDUCE_UMIN:\n  case ISD::VP_REDUCE_SMIN:\n  case ISD::VP_REDUCE_FADD:\n  case ISD::VP_REDUCE_SEQ_FADD:\n  case ISD::VP_REDUCE_FMIN:\n  case ISD::VP_REDUCE_FMAX:\n    if (Op.getOperand(1).getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVectorReductionOp(Op, DAG);\n    return lowerVPREDUCE(Op, DAG);\n  case ISD::VP_REDUCE_AND:\n  case ISD::VP_REDUCE_OR:\n  case ISD::VP_REDUCE_XOR:\n    if (Op.getOperand(1).getValueType().getVectorElementType() == MVT::i1)\n      return lowerVectorMaskVecReduction(Op, DAG, /*IsVP*/ true);\n    return lowerVPREDUCE(Op, DAG);\n  case ISD::UNDEF: {\n    MVT ContainerVT = getContainerForFixedLengthVector(Op.getSimpleValueType());\n    return convertFromScalableVector(Op.getSimpleValueType(),\n                                     DAG.getUNDEF(ContainerVT), DAG, Subtarget);\n  }\n  case ISD::INSERT_SUBVECTOR:\n    return lowerINSERT_SUBVECTOR(Op, DAG);\n  case ISD::EXTRACT_SUBVECTOR:\n    return lowerEXTRACT_SUBVECTOR(Op, DAG);\n  case ISD::VECTOR_DEINTERLEAVE:\n    return lowerVECTOR_DEINTERLEAVE(Op, DAG);\n  case ISD::VECTOR_INTERLEAVE:\n    return lowerVECTOR_INTERLEAVE(Op, DAG);\n  case ISD::STEP_VECTOR:\n    return lowerSTEP_VECTOR(Op, DAG);\n  case ISD::VECTOR_REVERSE:\n    return lowerVECTOR_REVERSE(Op, DAG);\n  case ISD::VECTOR_SPLICE:\n    return lowerVECTOR_SPLICE(Op, DAG);\n  case ISD::BUILD_VECTOR:\n    return lowerBUILD_VECTOR(Op, DAG, Subtarget);\n  case ISD::SPLAT_VECTOR:\n    if (Op.getValueType().getScalarType() == MVT::f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16())) {\n      if (Op.getValueType() == MVT::nxv32f16)\n        return SplitVectorOp(Op, DAG);\n      SDLoc DL(Op);\n      SDValue NewScalar =\n          DAG.getNode(ISD::FP_EXTEND, DL, MVT::f32, Op.getOperand(0));\n      SDValue NewSplat = DAG.getNode(\n          ISD::SPLAT_VECTOR, DL,\n          MVT::getVectorVT(MVT::f32, Op.getValueType().getVectorElementCount()),\n          NewScalar);\n      return DAG.getNode(ISD::FP_ROUND, DL, Op.getValueType(), NewSplat,\n                         DAG.getIntPtrConstant(0, DL, /*isTarget=*/true));\n    }\n    if (Op.getValueType().getVectorElementType() == MVT::i1)\n      return lowerVectorMaskSplat(Op, DAG);\n    return SDValue();\n  case ISD::VECTOR_SHUFFLE:\n    return lowerVECTOR_SHUFFLE(Op, DAG, Subtarget);\n  case ISD::CONCAT_VECTORS: {\n    // Split CONCAT_VECTORS into a series of INSERT_SUBVECTOR nodes. This is\n    // better than going through the stack, as the default expansion does.\n    SDLoc DL(Op);\n    MVT VT = Op.getSimpleValueType();\n    unsigned NumOpElts =\n        Op.getOperand(0).getSimpleValueType().getVectorMinNumElements();\n    SDValue Vec = DAG.getUNDEF(VT);\n    for (const auto &OpIdx : enumerate(Op->ops())) {\n      SDValue SubVec = OpIdx.value();\n      // Don't insert undef subvectors.\n      if (SubVec.isUndef())\n        continue;\n      Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, Vec, SubVec,\n                        DAG.getIntPtrConstant(OpIdx.index() * NumOpElts, DL));\n    }\n    return Vec;\n  }\n  case ISD::LOAD:\n    if (auto V = expandUnalignedRVVLoad(Op, DAG))\n      return V;\n    if (Op.getValueType().isFixedLengthVector())\n      return lowerFixedLengthVectorLoadToRVV(Op, DAG);\n    return Op;\n  case ISD::STORE:\n    if (auto V = expandUnalignedRVVStore(Op, DAG))\n      return V;\n    if (Op.getOperand(1).getValueType().isFixedLengthVector())\n      return lowerFixedLengthVectorStoreToRVV(Op, DAG);\n    return Op;\n  case ISD::MLOAD:\n  case ISD::VP_LOAD:\n    return lowerMaskedLoad(Op, DAG);\n  case ISD::MSTORE:\n  case ISD::VP_STORE:\n    return lowerMaskedStore(Op, DAG);\n  case ISD::SELECT_CC: {\n    // This occurs because we custom legalize SETGT and SETUGT for setcc. That\n    // causes LegalizeDAG to think we need to custom legalize select_cc. Expand\n    // into separate SETCC+SELECT just like LegalizeDAG.\n    SDValue Tmp1 = Op.getOperand(0);\n    SDValue Tmp2 = Op.getOperand(1);\n    SDValue True = Op.getOperand(2);\n    SDValue False = Op.getOperand(3);\n    EVT VT = Op.getValueType();\n    SDValue CC = Op.getOperand(4);\n    EVT CmpVT = Tmp1.getValueType();\n    EVT CCVT =\n        getSetCCResultType(DAG.getDataLayout(), *DAG.getContext(), CmpVT);\n    SDLoc DL(Op);\n    SDValue Cond =\n        DAG.getNode(ISD::SETCC, DL, CCVT, Tmp1, Tmp2, CC, Op->getFlags());\n    return DAG.getSelect(DL, VT, Cond, True, False);\n  }\n  case ISD::SETCC: {\n    MVT OpVT = Op.getOperand(0).getSimpleValueType();\n    if (OpVT.isScalarInteger()) {\n      MVT VT = Op.getSimpleValueType();\n      SDValue LHS = Op.getOperand(0);\n      SDValue RHS = Op.getOperand(1);\n      ISD::CondCode CCVal = cast<CondCodeSDNode>(Op.getOperand(2))->get();\n      assert((CCVal == ISD::SETGT || CCVal == ISD::SETUGT) &&\n             \"Unexpected CondCode\");\n\n      SDLoc DL(Op);\n\n      // If the RHS is a constant in the range [-2049, 0) or (0, 2046], we can\n      // convert this to the equivalent of (set(u)ge X, C+1) by using\n      // (xori (slti(u) X, C+1), 1). This avoids materializing a small constant\n      // in a register.\n      if (isa<ConstantSDNode>(RHS)) {\n        int64_t Imm = cast<ConstantSDNode>(RHS)->getSExtValue();\n        if (Imm != 0 && isInt<12>((uint64_t)Imm + 1)) {\n          // If this is an unsigned compare and the constant is -1, incrementing\n          // the constant would change behavior. The result should be false.\n          if (CCVal == ISD::SETUGT && Imm == -1)\n            return DAG.getConstant(0, DL, VT);\n          // Using getSetCCSwappedOperands will convert SET(U)GT->SET(U)LT.\n          CCVal = ISD::getSetCCSwappedOperands(CCVal);\n          SDValue SetCC = DAG.getSetCC(\n              DL, VT, LHS, DAG.getConstant(Imm + 1, DL, OpVT), CCVal);\n          return DAG.getLogicalNOT(DL, SetCC, VT);\n        }\n      }\n\n      // Not a constant we could handle, swap the operands and condition code to\n      // SETLT/SETULT.\n      CCVal = ISD::getSetCCSwappedOperands(CCVal);\n      return DAG.getSetCC(DL, VT, RHS, LHS, CCVal);\n    }\n\n    if (Op.getOperand(0).getSimpleValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVectorOp(Op, DAG);\n\n    return lowerFixedLengthVectorSetccToRVV(Op, DAG);\n  }\n  case ISD::ADD:\n  case ISD::SUB:\n  case ISD::MUL:\n  case ISD::MULHS:\n  case ISD::MULHU:\n  case ISD::AND:\n  case ISD::OR:\n  case ISD::XOR:\n  case ISD::SDIV:\n  case ISD::SREM:\n  case ISD::UDIV:\n  case ISD::UREM:\n  case ISD::BSWAP:\n  case ISD::CTPOP:\n    return lowerToScalableOp(Op, DAG);\n  case ISD::SHL:\n  case ISD::SRA:\n  case ISD::SRL:\n    if (Op.getSimpleValueType().isFixedLengthVector())\n      return lowerToScalableOp(Op, DAG);\n    // This can be called for an i32 shift amount that needs to be promoted.\n    assert(Op.getOperand(1).getValueType() == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n    return SDValue();\n  case ISD::FADD:\n  case ISD::FSUB:\n  case ISD::FMUL:\n  case ISD::FDIV:\n  case ISD::FNEG:\n  case ISD::FABS:\n  case ISD::FSQRT:\n  case ISD::FMA:\n  case ISD::FMINNUM:\n  case ISD::FMAXNUM:\n    if (Op.getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVectorOp(Op, DAG);\n    [[fallthrough]];\n  case ISD::AVGFLOORU:\n  case ISD::AVGCEILU:\n  case ISD::SADDSAT:\n  case ISD::UADDSAT:\n  case ISD::SSUBSAT:\n  case ISD::USUBSAT:\n  case ISD::SMIN:\n  case ISD::SMAX:\n  case ISD::UMIN:\n  case ISD::UMAX:\n    return lowerToScalableOp(Op, DAG);\n  case ISD::ABS:\n  case ISD::VP_ABS:\n    return lowerABS(Op, DAG);\n  case ISD::CTLZ:\n  case ISD::CTLZ_ZERO_UNDEF:\n  case ISD::CTTZ:\n  case ISD::CTTZ_ZERO_UNDEF:\n    if (Subtarget.hasStdExtZvbb())\n      return lowerToScalableOp(Op, DAG);\n    assert(Op.getOpcode() != ISD::CTTZ);\n    return lowerCTLZ_CTTZ_ZERO_UNDEF(Op, DAG);\n  case ISD::VSELECT:\n    return lowerFixedLengthVectorSelectToRVV(Op, DAG);\n  case ISD::FCOPYSIGN:\n    if (Op.getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVectorOp(Op, DAG);\n    return lowerFixedLengthVectorFCOPYSIGNToRVV(Op, DAG);\n  case ISD::STRICT_FADD:\n  case ISD::STRICT_FSUB:\n  case ISD::STRICT_FMUL:\n  case ISD::STRICT_FDIV:\n  case ISD::STRICT_FSQRT:\n  case ISD::STRICT_FMA:\n    if (Op.getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitStrictFPVectorOp(Op, DAG);\n    return lowerToScalableOp(Op, DAG);\n  case ISD::STRICT_FSETCC:\n  case ISD::STRICT_FSETCCS:\n    return lowerVectorStrictFSetcc(Op, DAG);\n  case ISD::STRICT_FCEIL:\n  case ISD::STRICT_FRINT:\n  case ISD::STRICT_FFLOOR:\n  case ISD::STRICT_FTRUNC:\n  case ISD::STRICT_FNEARBYINT:\n  case ISD::STRICT_FROUND:\n  case ISD::STRICT_FROUNDEVEN:\n    return lowerVectorStrictFTRUNC_FCEIL_FFLOOR_FROUND(Op, DAG, Subtarget);\n  case ISD::MGATHER:\n  case ISD::VP_GATHER:\n    return lowerMaskedGather(Op, DAG);\n  case ISD::MSCATTER:\n  case ISD::VP_SCATTER:\n    return lowerMaskedScatter(Op, DAG);\n  case ISD::GET_ROUNDING:\n    return lowerGET_ROUNDING(Op, DAG);\n  case ISD::SET_ROUNDING:\n    return lowerSET_ROUNDING(Op, DAG);\n  case ISD::EH_DWARF_CFA:\n    return lowerEH_DWARF_CFA(Op, DAG);\n  case ISD::VP_SELECT:\n  case ISD::VP_MERGE:\n  case ISD::VP_ADD:\n  case ISD::VP_SUB:\n  case ISD::VP_MUL:\n  case ISD::VP_SDIV:\n  case ISD::VP_UDIV:\n  case ISD::VP_SREM:\n  case ISD::VP_UREM:\n    return lowerVPOp(Op, DAG);\n  case ISD::VP_AND:\n  case ISD::VP_OR:\n  case ISD::VP_XOR:\n    return lowerLogicVPOp(Op, DAG);\n  case ISD::VP_FADD:\n  case ISD::VP_FSUB:\n  case ISD::VP_FMUL:\n  case ISD::VP_FDIV:\n  case ISD::VP_FNEG:\n  case ISD::VP_FABS:\n  case ISD::VP_SQRT:\n  case ISD::VP_FMA:\n  case ISD::VP_FMINNUM:\n  case ISD::VP_FMAXNUM:\n  case ISD::VP_FCOPYSIGN:\n    if (Op.getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVPOp(Op, DAG);\n    [[fallthrough]];\n  case ISD::VP_ASHR:\n  case ISD::VP_LSHR:\n  case ISD::VP_SHL:\n    return lowerVPOp(Op, DAG);\n  case ISD::VP_IS_FPCLASS:\n    return LowerIS_FPCLASS(Op, DAG);\n  case ISD::VP_SIGN_EXTEND:\n  case ISD::VP_ZERO_EXTEND:\n    if (Op.getOperand(0).getSimpleValueType().getVectorElementType() == MVT::i1)\n      return lowerVPExtMaskOp(Op, DAG);\n    return lowerVPOp(Op, DAG);\n  case ISD::VP_TRUNCATE:\n    return lowerVectorTruncLike(Op, DAG);\n  case ISD::VP_FP_EXTEND:\n  case ISD::VP_FP_ROUND:\n    return lowerVectorFPExtendOrRoundLike(Op, DAG);\n  case ISD::VP_SINT_TO_FP:\n  case ISD::VP_UINT_TO_FP:\n    if (Op.getValueType().isVector() &&\n        Op.getValueType().getScalarType() == MVT::f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16())) {\n      if (Op.getValueType() == MVT::nxv32f16)\n        return SplitVPOp(Op, DAG);\n      // int -> f32\n      SDLoc DL(Op);\n      MVT NVT =\n          MVT::getVectorVT(MVT::f32, Op.getValueType().getVectorElementCount());\n      auto NC = DAG.getNode(Op.getOpcode(), DL, NVT, Op->ops());\n      // f32 -> f16\n      return DAG.getNode(ISD::FP_ROUND, DL, Op.getValueType(), NC,\n                         DAG.getIntPtrConstant(0, DL, /*isTarget=*/true));\n    }\n    [[fallthrough]];\n  case ISD::VP_FP_TO_SINT:\n  case ISD::VP_FP_TO_UINT:\n    if (SDValue Op1 = Op.getOperand(0);\n        Op1.getValueType().isVector() &&\n        Op1.getValueType().getScalarType() == MVT::f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16())) {\n      if (Op1.getValueType() == MVT::nxv32f16)\n        return SplitVPOp(Op, DAG);\n      // f16 -> f32\n      SDLoc DL(Op);\n      MVT NVT = MVT::getVectorVT(MVT::f32,\n                                 Op1.getValueType().getVectorElementCount());\n      SDValue WidenVec = DAG.getNode(ISD::FP_EXTEND, DL, NVT, Op1);\n      // f32 -> int\n      return DAG.getNode(Op.getOpcode(), DL, Op.getValueType(),\n                         {WidenVec, Op.getOperand(1), Op.getOperand(2)});\n    }\n    return lowerVPFPIntConvOp(Op, DAG);\n  case ISD::VP_SETCC:\n    if (Op.getOperand(0).getSimpleValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVPOp(Op, DAG);\n    if (Op.getOperand(0).getSimpleValueType().getVectorElementType() == MVT::i1)\n      return lowerVPSetCCMaskOp(Op, DAG);\n    [[fallthrough]];\n  case ISD::VP_SMIN:\n  case ISD::VP_SMAX:\n  case ISD::VP_UMIN:\n  case ISD::VP_UMAX:\n  case ISD::VP_BITREVERSE:\n  case ISD::VP_BSWAP:\n    return lowerVPOp(Op, DAG);\n  case ISD::VP_CTLZ:\n  case ISD::VP_CTLZ_ZERO_UNDEF:\n    if (Subtarget.hasStdExtZvbb())\n      return lowerVPOp(Op, DAG);\n    return lowerCTLZ_CTTZ_ZERO_UNDEF(Op, DAG);\n  case ISD::VP_CTTZ:\n  case ISD::VP_CTTZ_ZERO_UNDEF:\n    if (Subtarget.hasStdExtZvbb())\n      return lowerVPOp(Op, DAG);\n    return lowerCTLZ_CTTZ_ZERO_UNDEF(Op, DAG);\n  case ISD::VP_CTPOP:\n    return lowerVPOp(Op, DAG);\n  case ISD::EXPERIMENTAL_VP_STRIDED_LOAD:\n    return lowerVPStridedLoad(Op, DAG);\n  case ISD::EXPERIMENTAL_VP_STRIDED_STORE:\n    return lowerVPStridedStore(Op, DAG);\n  case ISD::VP_FCEIL:\n  case ISD::VP_FFLOOR:\n  case ISD::VP_FRINT:\n  case ISD::VP_FNEARBYINT:\n  case ISD::VP_FROUND:\n  case ISD::VP_FROUNDEVEN:\n  case ISD::VP_FROUNDTOZERO:\n    if (Op.getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVPOp(Op, DAG);\n    return lowerVectorFTRUNC_FCEIL_FFLOOR_FROUND(Op, DAG, Subtarget);\n  case ISD::VP_FMAXIMUM:\n  case ISD::VP_FMINIMUM:\n    if (Op.getValueType() == MVT::nxv32f16 &&\n        (Subtarget.hasVInstructionsF16Minimal() &&\n         !Subtarget.hasVInstructionsF16()))\n      return SplitVPOp(Op, DAG);\n    return lowerFMAXIMUM_FMINIMUM(Op, DAG, Subtarget);\n  case ISD::EXPERIMENTAL_VP_SPLICE:\n    return lowerVPSpliceExperimental(Op, DAG);\n  case ISD::EXPERIMENTAL_VP_REVERSE:\n    return lowerVPReverseExperimental(Op, DAG);\n  }\n}",
      "start_line": 5835,
      "end_line": 6780,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "lowerVectorMaskVecReduction",
        "hasStdExtZbkb",
        "xori",
        "lowerJumpTable",
        "getConstant",
        "is64Bit",
        "getFPROUND",
        "getSizeInBits",
        "makeLibCall",
        "bswap",
        "SplitVectorOp",
        "lowerVectorStrictFTRUNC_FCEIL_FFLOOR_FROUND",
        "LowerINTRINSIC_WO_CHAIN",
        "lowerFMAXIMUM_FMINIMUM",
        "LowerINTRINSIC_W_CHAIN",
        "lowerFixedLengthVectorSelectToRVV",
        "isVector",
        "lowerVPREDUCE",
        "or",
        "getSetCCSwappedOperands",
        "value",
        "lowerMaskedScatter",
        "getValueType",
        "ops",
        "lowerBlockAddress",
        "hasVInstructionsF16",
        "lowerMaskedStore",
        "SplitScalar",
        "lowerFRAMEADDR",
        "lowerMaskedGather",
        "lowerFixedLengthVectorExtendToRVV",
        "hasStdExtZbb",
        "report_fatal_error",
        "lowerFixedLengthVectorStoreToRVV",
        "SET",
        "getFPExtendOrRound",
        "lowerVectorStrictFSetcc",
        "lowerEXTRACT_SUBVECTOR",
        "lowerMaskedLoad",
        "getVectorElementCount",
        "isFixedLengthVector",
        "lowerShiftRightParts",
        "lowerSPLAT_VECTOR_PARTS",
        "getValue",
        "lowerShiftLeftParts",
        "getOpcode",
        "getContainerForFixedLengthVector",
        "getOperand",
        "changeVectorElementType",
        "getStrictFPExtendOrRound",
        "lowerEH_DWARF_CFA",
        "lowerCTLZ_CTTZ_ZERO_UNDEF",
        "lowerGlobalAddress",
        "getNode",
        "getContext",
        "lowerSELECT",
        "lowerVECTOR_REVERSE",
        "LowerIS_FPCLASS",
        "convertFromScalableVector",
        "hasStdExtFOrZfinx",
        "lowerVPOp",
        "lowerLogicVPOp",
        "lowerVASTART",
        "isScalableVector",
        "lowerVECTOR_DEINTERLEAVE",
        "lowerVectorXRINT",
        "lowerSET_ROUNDING",
        "getShiftAmountConstant",
        "getVectorMinNumElements",
        "isStrictFPOpcode",
        "lowerVECTOR_INTERLEAVE",
        "getUNDEF",
        "getIntPtrConstant",
        "getVectorVT",
        "lowerFP_TO_INT_SAT",
        "SDValue",
        "getSetCC",
        "isTypeLegal",
        "lowerEXTRACT_VECTOR_ELT",
        "getVTList",
        "lowerVECREDUCE",
        "lowerVectorFTRUNC_FCEIL_FFLOOR_FROUND",
        "lowerGlobalTLSAddress",
        "lowerINSERT_SUBVECTOR",
        "lowerVPReverseExperimental",
        "hasVInstructionsF16Minimal",
        "getSetCCResultType",
        "lowerFixedLengthVectorSetccToRVV",
        "SplitVPOp",
        "lowerBUILD_VECTOR",
        "lowerVECTOR_SPLICE",
        "getMergeValues",
        "hasStdExtDOrZdinx",
        "lowerVPSpliceExperimental",
        "get",
        "LowerINTRINSIC_VOID",
        "DL",
        "getXLenVT",
        "getSExtValue",
        "lowerFixedLengthVectorLoadToRVV",
        "lowerGET_ROUNDING",
        "getDefaultVLOps",
        "lowerINSERT_VECTOR_ELT",
        "lowerFPVECREDUCE",
        "SplitVectorReductionOp",
        "convertToScalableVector",
        "getSimpleValueType",
        "getBitcast",
        "lowerVPStridedLoad",
        "lowerRETURNADDR",
        "getConstantOperandVal",
        "lowerVectorFPExtendOrRoundLike",
        "lowerFTRUNC_FCEIL_FFLOOR_FROUND",
        "getSelect",
        "lowerABS",
        "lowerVectorTruncLike",
        "lowerToScalableOp",
        "getVectorElementType",
        "isPowerOf2_32",
        "lowerSTEP_VECTOR",
        "SplitStrictFPVectorOp",
        "lowerFixedLengthVectorFCOPYSIGNToRVV",
        "lowerStrictFPExtendOrRoundLike",
        "lowerVPFPIntConvOp",
        "lowerConstant",
        "lowerVectorMaskExt",
        "lowerVPSetCCMaskOp",
        "lowerVPStridedStore",
        "tie",
        "isInteger",
        "lowerVPExtMaskOp",
        "getScalarType",
        "LowerATOMIC_FENCE",
        "lowerBRCOND",
        "static_assert",
        "lowerVECTOR_SHUFFLE",
        "Log2_64",
        "lowerConstantPool",
        "lowerVectorMaskSplat",
        "getLogicalNOT",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTargetNode",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "GlobalAddressSDNode",
          "name": "*N"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "EVT",
          "name": "Ty"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned",
          "name": "Flags"
        }
      ],
      "body": "{\n  return DAG.getTargetGlobalAddress(N->getGlobal(), DL, Ty, 0, Flags);\n}",
      "start_line": 6782,
      "end_line": 6785,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getTargetGlobalAddress"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTargetNode",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "BlockAddressSDNode",
          "name": "*N"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "EVT",
          "name": "Ty"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned",
          "name": "Flags"
        }
      ],
      "body": "{\n  return DAG.getTargetBlockAddress(N->getBlockAddress(), Ty, N->getOffset(),\n                                   Flags);\n}",
      "start_line": 6787,
      "end_line": 6791,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getTargetBlockAddress",
        "getOffset"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTargetNode",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "ConstantPoolSDNode",
          "name": "*N"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "EVT",
          "name": "Ty"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned",
          "name": "Flags"
        }
      ],
      "body": "{\n  return DAG.getTargetConstantPool(N->getConstVal(), Ty, N->getAlign(),\n                                   N->getOffset(), Flags);\n}",
      "start_line": 6793,
      "end_line": 6797,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getTargetConstantPool",
        "getOffset",
        "getAlign"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTargetNode",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "JumpTableSDNode",
          "name": "*N"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "EVT",
          "name": "Ty"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned",
          "name": "Flags"
        }
      ],
      "body": "{\n  return DAG.getTargetJumpTable(N->getIndex(), Ty, Flags);\n}",
      "start_line": 6799,
      "end_line": 6802,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getTargetJumpTable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getAddr",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "NodeTy",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "bool",
          "name": "IsLocal"
        },
        {
          "type": "bool",
          "name": "IsExternWeak"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  EVT Ty = getPointerTy(DAG.getDataLayout());\n\n  // When HWASAN is used and tagging of global variables is enabled\n  // they should be accessed via the GOT, since the tagged address of a global\n  // is incompatible with existing code models. This also applies to non-pic\n  // mode.\n  if (isPositionIndependent() || Subtarget.allowTaggedGlobals()) {\n    SDValue Addr = getTargetNode(N, DL, Ty, DAG, 0);\n    if (IsLocal && !Subtarget.allowTaggedGlobals())\n      // Use PC-relative addressing to access the symbol. This generates the\n      // pattern (PseudoLLA sym), which expands to (addi (auipc %pcrel_hi(sym))\n      // %pcrel_lo(auipc)).\n      return DAG.getNode(RISCVISD::LLA, DL, Ty, Addr);\n\n    // Use PC-relative addressing to access the GOT for this symbol, then load\n    // the address from the GOT. This generates the pattern (PseudoLGA sym),\n    // which expands to (ld (addi (auipc %got_pcrel_hi(sym)) %pcrel_lo(auipc))).\n    SDValue Load =\n        SDValue(DAG.getMachineNode(RISCV::PseudoLGA, DL, Ty, Addr), 0);\n    MachineFunction &MF = DAG.getMachineFunction();\n    MachineMemOperand *MemOp = MF.getMachineMemOperand(\n        MachinePointerInfo::getGOT(MF),\n        MachineMemOperand::MOLoad | MachineMemOperand::MODereferenceable |\n            MachineMemOperand::MOInvariant,\n        LLT(Ty.getSimpleVT()), Align(Ty.getFixedSizeInBits() / 8));\n    DAG.setNodeMemRefs(cast<MachineSDNode>(Load.getNode()), {MemOp});\n    return Load;\n  }\n\n  switch (getTargetMachine().getCodeModel()) {\n  default:\n    report_fatal_error(\"Unsupported code model for lowering\");\n  case CodeModel::Small: {\n    // Generate a sequence for accessing addresses within the first 2 GiB of\n    // address space. This generates the pattern (addi (lui %hi(sym)) %lo(sym)).\n    SDValue AddrHi = getTargetNode(N, DL, Ty, DAG, RISCVII::MO_HI);\n    SDValue AddrLo = getTargetNode(N, DL, Ty, DAG, RISCVII::MO_LO);\n    SDValue MNHi = DAG.getNode(RISCVISD::HI, DL, Ty, AddrHi);\n    return DAG.getNode(RISCVISD::ADD_LO, DL, Ty, MNHi, AddrLo);\n  }\n  case CodeModel::Medium: {\n    SDValue Addr = getTargetNode(N, DL, Ty, DAG, 0);\n    if (IsExternWeak) {\n      // An extern weak symbol may be undefined, i.e. have value 0, which may\n      // not be within 2GiB of PC, so use GOT-indirect addressing to access the\n      // symbol. This generates the pattern (PseudoLGA sym), which expands to\n      // (ld (addi (auipc %got_pcrel_hi(sym)) %pcrel_lo(auipc))).\n      SDValue Load =\n          SDValue(DAG.getMachineNode(RISCV::PseudoLGA, DL, Ty, Addr), 0);\n      MachineFunction &MF = DAG.getMachineFunction();\n      MachineMemOperand *MemOp = MF.getMachineMemOperand(\n          MachinePointerInfo::getGOT(MF),\n          MachineMemOperand::MOLoad | MachineMemOperand::MODereferenceable |\n              MachineMemOperand::MOInvariant,\n          LLT(Ty.getSimpleVT()), Align(Ty.getFixedSizeInBits() / 8));\n      DAG.setNodeMemRefs(cast<MachineSDNode>(Load.getNode()), {MemOp});\n      return Load;\n    }\n\n    // Generate a sequence for accessing addresses within any 2GiB range within\n    // the address space. This generates the pattern (PseudoLLA sym), which\n    // expands to (addi (auipc %pcrel_hi(sym)) %pcrel_lo(auipc)).\n    return DAG.getNode(RISCVISD::LLA, DL, Ty, Addr);\n  }\n  }\n}",
      "start_line": 6805,
      "end_line": 6873,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "setNodeMemRefs",
        "SDValue",
        "getCodeModel",
        "lo",
        "getMachineFunction",
        "getMachineMemOperand",
        "getNode",
        "allowTaggedGlobals",
        "to",
        "pattern",
        "Align",
        "ld",
        "getTargetNode",
        "DL",
        "pcrel_lo",
        "report_fatal_error",
        "LLT",
        "getPointerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerGlobalAddress",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  GlobalAddressSDNode *N = cast<GlobalAddressSDNode>(Op);\n  assert(N->getOffset() == 0 && \"unexpected offset in global node\");\n  const GlobalValue *GV = N->getGlobal();\n  return getAddr(N, DAG, GV->isDSOLocal(), GV->hasExternalWeakLinkage());\n}",
      "start_line": 6875,
      "end_line": 6881,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getGlobal",
        "hasExternalWeakLinkage",
        "getAddr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerBlockAddress",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  BlockAddressSDNode *N = cast<BlockAddressSDNode>(Op);\n\n  return getAddr(N, DAG);\n}",
      "start_line": 6883,
      "end_line": 6888,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getAddr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerConstantPool",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  ConstantPoolSDNode *N = cast<ConstantPoolSDNode>(Op);\n\n  return getAddr(N, DAG);\n}",
      "start_line": 6890,
      "end_line": 6895,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getAddr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerJumpTable",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  JumpTableSDNode *N = cast<JumpTableSDNode>(Op);\n\n  return getAddr(N, DAG);\n}",
      "start_line": 6897,
      "end_line": 6902,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getAddr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getStaticTLSAddr",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "GlobalAddressSDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "bool",
          "name": "UseGOT"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  EVT Ty = getPointerTy(DAG.getDataLayout());\n  const GlobalValue *GV = N->getGlobal();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  if (UseGOT) {\n    // Use PC-relative addressing to access the GOT for this TLS symbol, then\n    // load the address from the GOT and add the thread pointer. This generates\n    // the pattern (PseudoLA_TLS_IE sym), which expands to\n    // (ld (auipc %tls_ie_pcrel_hi(sym)) %pcrel_lo(auipc)).\n    SDValue Addr = DAG.getTargetGlobalAddress(GV, DL, Ty, 0, 0);\n    SDValue Load =\n        SDValue(DAG.getMachineNode(RISCV::PseudoLA_TLS_IE, DL, Ty, Addr), 0);\n    MachineFunction &MF = DAG.getMachineFunction();\n    MachineMemOperand *MemOp = MF.getMachineMemOperand(\n        MachinePointerInfo::getGOT(MF),\n        MachineMemOperand::MOLoad | MachineMemOperand::MODereferenceable |\n            MachineMemOperand::MOInvariant,\n        LLT(Ty.getSimpleVT()), Align(Ty.getFixedSizeInBits() / 8));\n    DAG.setNodeMemRefs(cast<MachineSDNode>(Load.getNode()), {MemOp});\n\n    // Add the thread pointer.\n    SDValue TPReg = DAG.getRegister(RISCV::X4, XLenVT);\n    return DAG.getNode(ISD::ADD, DL, Ty, Load, TPReg);\n  }\n\n  // Generate a sequence for accessing the address relative to the thread\n  // pointer, with the appropriate adjustment for the thread pointer offset.\n  // This generates the pattern\n  // (add (add_tprel (lui %tprel_hi(sym)) tp %tprel_add(sym)) %tprel_lo(sym))\n  SDValue AddrHi =\n      DAG.getTargetGlobalAddress(GV, DL, Ty, 0, RISCVII::MO_TPREL_HI);\n  SDValue AddrAdd =\n      DAG.getTargetGlobalAddress(GV, DL, Ty, 0, RISCVII::MO_TPREL_ADD);\n  SDValue AddrLo =\n      DAG.getTargetGlobalAddress(GV, DL, Ty, 0, RISCVII::MO_TPREL_LO);\n\n  SDValue MNHi = DAG.getNode(RISCVISD::HI, DL, Ty, AddrHi);\n  SDValue TPReg = DAG.getRegister(RISCV::X4, XLenVT);\n  SDValue MNAdd =\n      DAG.getNode(RISCVISD::ADD_TPREL, DL, Ty, MNHi, TPReg, AddrAdd);\n  return DAG.getNode(RISCVISD::ADD_LO, DL, Ty, MNAdd, AddrLo);\n}",
      "start_line": 6904,
      "end_line": 6949,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "add",
        "getTargetGlobalAddress",
        "ld",
        "pcrel_lo",
        "setNodeMemRefs",
        "getMachineFunction",
        "pattern",
        "LLT",
        "getPointerTy",
        "getGlobal",
        "SDValue",
        "tprel_lo",
        "getRegister",
        "tprel_add",
        "getNode",
        "Align",
        "DL",
        "getXLenVT",
        "getMachineMemOperand"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getDynamicTLSAddr",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "GlobalAddressSDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  EVT Ty = getPointerTy(DAG.getDataLayout());\n  IntegerType *CallTy = Type::getIntNTy(*DAG.getContext(), Ty.getSizeInBits());\n  const GlobalValue *GV = N->getGlobal();\n\n  // Use a PC-relative addressing mode to access the global dynamic GOT address.\n  // This generates the pattern (PseudoLA_TLS_GD sym), which expands to\n  // (addi (auipc %tls_gd_pcrel_hi(sym)) %pcrel_lo(auipc)).\n  SDValue Addr = DAG.getTargetGlobalAddress(GV, DL, Ty, 0, 0);\n  SDValue Load =\n      SDValue(DAG.getMachineNode(RISCV::PseudoLA_TLS_GD, DL, Ty, Addr), 0);\n\n  // Prepare argument list to generate call.\n  ArgListTy Args;\n  ArgListEntry Entry;\n  Entry.Node = Load;\n  Entry.Ty = CallTy;\n  Args.push_back(Entry);\n\n  // Setup call to __tls_get_addr.\n  TargetLowering::CallLoweringInfo CLI(DAG);\n  CLI.setDebugLoc(DL)\n      .setChain(DAG.getEntryNode())\n      .setLibCallee(CallingConv::C, CallTy,\n                    DAG.getExternalSymbol(\"__tls_get_addr\", Ty),\n                    std::move(Args));\n\n  return LowerCallTo(CLI).first;\n}",
      "start_line": 6951,
      "end_line": 6981,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getGlobal",
        "SDValue",
        "setDebugLoc",
        "move",
        "push_back",
        "getIntNTy",
        "pattern",
        "getTargetGlobalAddress",
        "setLibCallee",
        "getSizeInBits",
        "DL",
        "pcrel_lo",
        "addi",
        "setChain",
        "LowerCallTo",
        "CLI",
        "getPointerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTLSDescAddr",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "GlobalAddressSDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  EVT Ty = getPointerTy(DAG.getDataLayout());\n  const GlobalValue *GV = N->getGlobal();\n\n  // Use a PC-relative addressing mode to access the global dynamic GOT address.\n  // This generates the pattern (PseudoLA_TLSDESC sym), which expands to\n  //\n  // auipc tX, %tlsdesc_hi(symbol)         // R_RISCV_TLSDESC_HI20(symbol)\n  // lw    tY, tX, %tlsdesc_lo_load(label) // R_RISCV_TLSDESC_LOAD_LO12_I(label)\n  // addi  a0, tX, %tlsdesc_lo_add(label)  // R_RISCV_TLSDESC_ADD_LO12_I(label)\n  // jalr  t0, tY                          // R_RISCV_TLSDESC_CALL(label)\n  SDValue Addr = DAG.getTargetGlobalAddress(GV, DL, Ty, 0, 0);\n  return SDValue(DAG.getMachineNode(RISCV::PseudoLA_TLSDESC, DL, Ty, Addr), 0);\n}",
      "start_line": 6983,
      "end_line": 6998,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getGlobal",
        "SDValue",
        "R_RISCV_TLSDESC_ADD_LO12_I",
        "tlsdesc_lo_load",
        "tlsdesc_hi",
        "pattern",
        "R_RISCV_TLSDESC_LOAD_LO12_I",
        "getTargetGlobalAddress",
        "R_RISCV_TLSDESC_CALL",
        "DL",
        "R_RISCV_TLSDESC_HI20",
        "tlsdesc_lo_add",
        "getPointerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerGlobalTLSAddress",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  GlobalAddressSDNode *N = cast<GlobalAddressSDNode>(Op);\n  assert(N->getOffset() == 0 && \"unexpected offset in global node\");\n\n  if (DAG.getTarget().useEmulatedTLS())\n    return LowerToTLSEmulatedModel(N, DAG);\n\n  TLSModel::Model Model = getTargetMachine().getTLSModel(N->getGlobal());\n\n  if (DAG.getMachineFunction().getFunction().getCallingConv() ==\n      CallingConv::GHC)\n    report_fatal_error(\"In GHC calling convention TLS is not supported\");\n\n  SDValue Addr;\n  switch (Model) {\n  case TLSModel::LocalExec:\n    Addr = getStaticTLSAddr(N, DAG, /*UseGOT=*/false);\n    break;\n  case TLSModel::InitialExec:\n    Addr = getStaticTLSAddr(N, DAG, /*UseGOT=*/true);\n    break;\n  case TLSModel::LocalDynamic:\n  case TLSModel::GeneralDynamic:\n    Addr = DAG.getTarget().useTLSDESC() ? getTLSDescAddr(N, DAG)\n                                        : getDynamicTLSAddr(N, DAG);\n    break;\n  }\n\n  return Addr;\n}",
      "start_line": 7000,
      "end_line": 7030,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "lowerGlobalTLSAddress",
          "condition": "Model",
          "cases": [
            {
              "label": "TLSModel",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "TLSModel",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "TLSModel",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "TLSModel",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getTargetMachine",
        "useTLSDESC",
        "getTLSModel",
        "getCallingConv",
        "getDynamicTLSAddr",
        "getTarget",
        "LowerToTLSEmulatedModel",
        "getStaticTLSAddr",
        "report_fatal_error",
        "useEmulatedTLS",
        "getFunction",
        "getTLSDescAddr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "LHS == RHS2 && RHS ==",
          "name": "LHS2"
        }
      ],
      "body": "{\n    CC2 = ISD::getSetCCSwappedOperands(CC2);\n    if (CC == CC2)\n      return true;\n    if (CC == ISD::getSetCCInverse(CC2, LHS2.getValueType()))\n      return false;\n  }",
      "start_line": 7047,
      "end_line": 7053,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getSetCCSwappedOperands"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineSelectToBinOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue CondV = N->getOperand(0);\n  SDValue TrueV = N->getOperand(1);\n  SDValue FalseV = N->getOperand(2);\n  MVT VT = N->getSimpleValueType(0);\n  SDLoc DL(N);\n\n  if (!Subtarget.hasConditionalMoveFusion()) {\n    // (select c, -1, y) -> -c | y\n    if (isAllOnesConstant(TrueV)) {\n      SDValue Neg = DAG.getNegative(CondV, DL, VT);\n      return DAG.getNode(ISD::OR, DL, VT, Neg, FalseV);\n    }\n    // (select c, y, -1) -> (c-1) | y\n    if (isAllOnesConstant(FalseV)) {\n      SDValue Neg = DAG.getNode(ISD::ADD, DL, VT, CondV,\n                                DAG.getAllOnesConstant(DL, VT));\n      return DAG.getNode(ISD::OR, DL, VT, Neg, TrueV);\n    }\n\n    // (select c, 0, y) -> (c-1) & y\n    if (isNullConstant(TrueV)) {\n      SDValue Neg = DAG.getNode(ISD::ADD, DL, VT, CondV,\n                                DAG.getAllOnesConstant(DL, VT));\n      return DAG.getNode(ISD::AND, DL, VT, Neg, FalseV);\n    }\n    // (select c, y, 0) -> -c & y\n    if (isNullConstant(FalseV)) {\n      SDValue Neg = DAG.getNegative(CondV, DL, VT);\n      return DAG.getNode(ISD::AND, DL, VT, Neg, TrueV);\n    }\n  }\n\n  // Try to fold (select (setcc lhs, rhs, cc), truev, falsev) into bitwise ops\n  // when both truev and falsev are also setcc.\n  if (CondV.getOpcode() == ISD::SETCC && TrueV.getOpcode() == ISD::SETCC &&\n      FalseV.getOpcode() == ISD::SETCC) {\n    SDValue LHS = CondV.getOperand(0);\n    SDValue RHS = CondV.getOperand(1);\n    ISD::CondCode CC = cast<CondCodeSDNode>(CondV.getOperand(2))->get();\n\n    // (select x, x, y) -> x | y\n    // (select !x, x, y) -> x & y\n    if (std::optional<bool> MatchResult = matchSetCC(LHS, RHS, CC, TrueV)) {\n      return DAG.getNode(*MatchResult ? ISD::OR : ISD::AND, DL, VT, TrueV,\n                         FalseV);\n    }\n    // (select x, y, x) -> x & y\n    // (select !x, y, x) -> x | y\n    if (std::optional<bool> MatchResult = matchSetCC(LHS, RHS, CC, FalseV)) {\n      return DAG.getNode(*MatchResult ? ISD::AND : ISD::OR, DL, VT, TrueV,\n                         FalseV);\n    }\n  }\n\n  return SDValue();\n}",
      "start_line": 7058,
      "end_line": 7115,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getNegative",
        "getOpcode",
        "getSimpleValueType",
        "get",
        "getOperand",
        "DL",
        "getNode",
        "fold"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "foldBinOpIntoSelectIfProfitable",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*BO"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (Subtarget.hasShortForwardBranchOpt())\n    return SDValue();\n\n  unsigned SelOpNo = 0;\n  SDValue Sel = BO->getOperand(0);\n  if (Sel.getOpcode() != ISD::SELECT || !Sel.hasOneUse()) {\n    SelOpNo = 1;\n    Sel = BO->getOperand(1);\n  }\n\n  if (Sel.getOpcode() != ISD::SELECT || !Sel.hasOneUse())\n    return SDValue();\n\n  unsigned ConstSelOpNo = 1;\n  unsigned OtherSelOpNo = 2;\n  if (!dyn_cast<ConstantSDNode>(Sel->getOperand(ConstSelOpNo))) {\n    ConstSelOpNo = 2;\n    OtherSelOpNo = 1;\n  }\n  SDValue ConstSelOp = Sel->getOperand(ConstSelOpNo);\n  ConstantSDNode *ConstSelOpNode = dyn_cast<ConstantSDNode>(ConstSelOp);\n  if (!ConstSelOpNode || ConstSelOpNode->isOpaque())\n    return SDValue();\n\n  SDValue ConstBinOp = BO->getOperand(SelOpNo ^ 1);\n  ConstantSDNode *ConstBinOpNode = dyn_cast<ConstantSDNode>(ConstBinOp);\n  if (!ConstBinOpNode || ConstBinOpNode->isOpaque())\n    return SDValue();\n\n  SDLoc DL(Sel);\n  EVT VT = BO->getValueType(0);\n\n  SDValue NewConstOps[2] = {ConstSelOp, ConstBinOp};\n  if (SelOpNo == 1)\n    std::swap(NewConstOps[0], NewConstOps[1]);\n\n  SDValue NewConstOp =\n      DAG.FoldConstantArithmetic(BO->getOpcode(), DL, VT, NewConstOps);\n  if (!NewConstOp)\n    return SDValue();\n\n  const APInt &NewConstAPInt = NewConstOp->getAsAPIntVal();\n  if (!NewConstAPInt.isZero() && !NewConstAPInt.isAllOnes())\n    return SDValue();\n\n  SDValue OtherSelOp = Sel->getOperand(OtherSelOpNo);\n  SDValue NewNonConstOps[2] = {OtherSelOp, ConstBinOp};\n  if (SelOpNo == 1)\n    std::swap(NewNonConstOps[0], NewNonConstOps[1]);\n  SDValue NewNonConstOp = DAG.getNode(BO->getOpcode(), DL, VT, NewNonConstOps);\n\n  SDValue NewT = (ConstSelOpNo == 1) ? NewConstOp : NewNonConstOp;\n  SDValue NewF = (ConstSelOpNo == 1) ? NewNonConstOp : NewConstOp;\n  return DAG.getSelect(DL, VT, Sel.getOperand(0), NewT, NewF);\n}",
      "start_line": 7123,
      "end_line": 7180,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "getAsAPIntVal",
        "swap",
        "getSelect",
        "getValueType",
        "getOperand",
        "isAllOnes",
        "DL",
        "getNode",
        "FoldConstantArithmetic"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerSELECT",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue CondV = Op.getOperand(0);\n  SDValue TrueV = Op.getOperand(1);\n  SDValue FalseV = Op.getOperand(2);\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  // Lower vector SELECTs to VSELECTs by splatting the condition.\n  if (VT.isVector()) {\n    MVT SplatCondVT = VT.changeVectorElementType(MVT::i1);\n    SDValue CondSplat = DAG.getSplat(SplatCondVT, DL, CondV);\n    return DAG.getNode(ISD::VSELECT, DL, VT, CondSplat, TrueV, FalseV);\n  }\n\n  // When Zicond or XVentanaCondOps is present, emit CZERO_EQZ and CZERO_NEZ\n  // nodes to implement the SELECT. Performing the lowering here allows for\n  // greater control over when CZERO_{EQZ/NEZ} are used vs another branchless\n  // sequence or RISCVISD::SELECT_CC node (branch-based select).\n  if ((Subtarget.hasStdExtZicond() || Subtarget.hasVendorXVentanaCondOps()) &&\n      VT.isScalarInteger()) {\n    // (select c, t, 0) -> (czero_eqz t, c)\n    if (isNullConstant(FalseV))\n      return DAG.getNode(RISCVISD::CZERO_EQZ, DL, VT, TrueV, CondV);\n    // (select c, 0, f) -> (czero_nez f, c)\n    if (isNullConstant(TrueV))\n      return DAG.getNode(RISCVISD::CZERO_NEZ, DL, VT, FalseV, CondV);\n\n    // (select c, (and f, x), f) -> (or (and f, x), (czero_nez f, c))\n    if (TrueV.getOpcode() == ISD::AND &&\n        (TrueV.getOperand(0) == FalseV || TrueV.getOperand(1) == FalseV))\n      return DAG.getNode(\n          ISD::OR, DL, VT, TrueV,\n          DAG.getNode(RISCVISD::CZERO_NEZ, DL, VT, FalseV, CondV));\n    // (select c, t, (and t, x)) -> (or (czero_eqz t, c), (and t, x))\n    if (FalseV.getOpcode() == ISD::AND &&\n        (FalseV.getOperand(0) == TrueV || FalseV.getOperand(1) == TrueV))\n      return DAG.getNode(\n          ISD::OR, DL, VT, FalseV,\n          DAG.getNode(RISCVISD::CZERO_EQZ, DL, VT, TrueV, CondV));\n\n    // Try some other optimizations before falling back to generic lowering.\n    if (SDValue V = combineSelectToBinOp(Op.getNode(), DAG, Subtarget))\n      return V;\n\n    // (select c, t, f) -> (or (czero_eqz t, c), (czero_nez f, c))\n    // Unless we have the short forward branch optimization.\n    if (!Subtarget.hasConditionalMoveFusion())\n      return DAG.getNode(\n          ISD::OR, DL, VT,\n          DAG.getNode(RISCVISD::CZERO_EQZ, DL, VT, TrueV, CondV),\n          DAG.getNode(RISCVISD::CZERO_NEZ, DL, VT, FalseV, CondV));\n  }\n\n  if (SDValue V = combineSelectToBinOp(Op.getNode(), DAG, Subtarget))\n    return V;\n\n  if (Op.hasOneUse()) {\n    unsigned UseOpc = Op->use_begin()->getOpcode();\n    if (isBinOp(UseOpc) && DAG.isSafeToSpeculativelyExecute(UseOpc)) {\n      SDNode *BinOp = *Op->use_begin();\n      if (SDValue NewSel = foldBinOpIntoSelectIfProfitable(*Op->use_begin(),\n                                                           DAG, Subtarget)) {\n        DAG.ReplaceAllUsesWith(BinOp, &NewSel);\n        return lowerSELECT(NewSel, DAG);\n      }\n    }\n  }\n\n  // (select cc, 1.0, 0.0) -> (sint_to_fp (zext cc))\n  // (select cc, 0.0, 1.0) -> (sint_to_fp (zext (xor cc, 1)))\n  const ConstantFPSDNode *FPTV = dyn_cast<ConstantFPSDNode>(TrueV);\n  const ConstantFPSDNode *FPFV = dyn_cast<ConstantFPSDNode>(FalseV);\n  if (FPTV && FPFV) {\n    if (FPTV->isExactlyValue(1.0) && FPFV->isExactlyValue(0.0))\n      return DAG.getNode(ISD::SINT_TO_FP, DL, VT, CondV);\n    if (FPTV->isExactlyValue(0.0) && FPFV->isExactlyValue(1.0)) {\n      SDValue XOR = DAG.getNode(ISD::XOR, DL, XLenVT, CondV,\n                                DAG.getConstant(1, DL, XLenVT));\n      return DAG.getNode(ISD::SINT_TO_FP, DL, VT, XOR);\n    }\n  }\n\n  // If the condition is not an integer SETCC which operates on XLenVT, we need\n  // to emit a RISCVISD::SELECT_CC comparing the condition to zero. i.e.:\n  // (select condv, truev, falsev)\n  // -> (riscvisd::select_cc condv, zero, setne, truev, falsev)\n  if (CondV.getOpcode() != ISD::SETCC ||\n      CondV.getOperand(0).getSimpleValueType() != XLenVT) {\n    SDValue Zero = DAG.getConstant(0, DL, XLenVT);\n    SDValue SetNE = DAG.getCondCode(ISD::SETNE);\n\n    SDValue Ops[] = {CondV, Zero, SetNE, TrueV, FalseV};\n\n    return DAG.getNode(RISCVISD::SELECT_CC, DL, VT, Ops);\n  }\n\n  // If the CondV is the output of a SETCC node which operates on XLenVT inputs,\n  // then merge the SETCC node into the lowered RISCVISD::SELECT_CC to take\n  // advantage of the integer compare+branch instructions. i.e.:\n  // (select (setcc lhs, rhs, cc), truev, falsev)\n  // -> (riscvisd::select_cc lhs, rhs, cc, truev, falsev)\n  SDValue LHS = CondV.getOperand(0);\n  SDValue RHS = CondV.getOperand(1);\n  ISD::CondCode CCVal = cast<CondCodeSDNode>(CondV.getOperand(2))->get();\n\n  // Special case for a select of 2 constants that have a diffence of 1.\n  // Normally this is done by DAGCombine, but if the select is introduced by\n  // type legalization or op legalization, we miss it. Restricting to SETLT\n  // case for now because that is what signed saturating add/sub need.\n  // FIXME: We don't need the condition to be SETLT or even a SETCC,\n  // but we would probably want to swap the true/false values if the condition\n  // is SETGE/SETLE to avoid an XORI.\n  if (isa<ConstantSDNode>(TrueV) && isa<ConstantSDNode>(FalseV) &&\n      CCVal == ISD::SETLT) {\n    const APInt &TrueVal = TrueV->getAsAPIntVal();\n    const APInt &FalseVal = FalseV->getAsAPIntVal();\n    if (TrueVal - 1 == FalseVal)\n      return DAG.getNode(ISD::ADD, DL, VT, CondV, FalseV);\n    if (TrueVal + 1 == FalseVal)\n      return DAG.getNode(ISD::SUB, DL, VT, FalseV, CondV);\n  }\n\n  translateSetCCForBranch(DL, LHS, RHS, CCVal, DAG);\n  // 1 < x ? x : 1 -> 0 < x ? x : 1\n  if (isOneConstant(LHS) && (CCVal == ISD::SETLT || CCVal == ISD::SETULT) &&\n      RHS == TrueV && LHS == FalseV) {\n    LHS = DAG.getConstant(0, DL, VT);\n    // 0 <u x is the same as x != 0.\n    if (CCVal == ISD::SETULT) {\n      std::swap(LHS, RHS);\n      CCVal = ISD::SETNE;\n    }\n  }\n\n  // x <s -1 ? x : -1 -> x <s 0 ? x : -1\n  if (isAllOnesConstant(RHS) && CCVal == ISD::SETLT && LHS == TrueV &&\n      RHS == FalseV) {\n    RHS = DAG.getConstant(0, DL, VT);\n  }\n\n  SDValue TargetCC = DAG.getCondCode(CCVal);\n\n  if (isa<ConstantSDNode>(TrueV) && !isa<ConstantSDNode>(FalseV)) {\n    // (select (setcc lhs, rhs, CC), constant, falsev)\n    // -> (select (setcc lhs, rhs, InverseCC), falsev, constant)\n    std::swap(TrueV, FalseV);\n    TargetCC = DAG.getCondCode(ISD::getSetCCInverse(CCVal, LHS.getValueType()));\n  }\n\n  SDValue Ops[] = {LHS, RHS, TargetCC, TrueV, FalseV};\n  return DAG.getNode(RISCVISD::SELECT_CC, DL, VT, Ops);\n}",
      "start_line": 7182,
      "end_line": 7334,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getAsAPIntVal",
        "lowerSELECT",
        "isExactlyValue",
        "isScalarInteger",
        "getSimpleValueType",
        "getConstant",
        "use_begin",
        "select",
        "sint_to_fp",
        "hasVendorXVentanaCondOps",
        "or",
        "translateSetCCForBranch",
        "getSplat",
        "swap",
        "getOpcode",
        "getOperand",
        "node",
        "getCondCode",
        "changeVectorElementType",
        "isSafeToSpeculativelyExecute",
        "get",
        "DL",
        "getXLenVT",
        "ReplaceAllUsesWith",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerBRCOND",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue CondV = Op.getOperand(1);\n  SDLoc DL(Op);\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  if (CondV.getOpcode() == ISD::SETCC &&\n      CondV.getOperand(0).getValueType() == XLenVT) {\n    SDValue LHS = CondV.getOperand(0);\n    SDValue RHS = CondV.getOperand(1);\n    ISD::CondCode CCVal = cast<CondCodeSDNode>(CondV.getOperand(2))->get();\n\n    translateSetCCForBranch(DL, LHS, RHS, CCVal, DAG);\n\n    SDValue TargetCC = DAG.getCondCode(CCVal);\n    return DAG.getNode(RISCVISD::BR_CC, DL, Op.getValueType(), Op.getOperand(0),\n                       LHS, RHS, TargetCC, Op.getOperand(2));\n  }\n\n  return DAG.getNode(RISCVISD::BR_CC, DL, Op.getValueType(), Op.getOperand(0),\n                     CondV, DAG.getConstant(0, DL, XLenVT),\n                     DAG.getCondCode(ISD::SETNE), Op.getOperand(2));\n}",
      "start_line": 7336,
      "end_line": 7357,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getConstant",
        "get",
        "getValueType",
        "getOperand",
        "DL",
        "getCondCode",
        "getXLenVT",
        "translateSetCCForBranch",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVASTART",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MachineFunction &MF = DAG.getMachineFunction();\n  RISCVMachineFunctionInfo *FuncInfo = MF.getInfo<RISCVMachineFunctionInfo>();\n\n  SDLoc DL(Op);\n  SDValue FI = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),\n                                 getPointerTy(MF.getDataLayout()));\n\n  // vastart just stores the address of the VarArgsFrameIndex slot into the\n  // memory location argument.\n  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();\n  return DAG.getStore(Op.getOperand(0), DL, FI, Op.getOperand(1),\n                      MachinePointerInfo(SV));\n}",
      "start_line": 7359,
      "end_line": 7372,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getFrameIndex",
        "getValue",
        "getMachineFunction",
        "getStore",
        "getOperand",
        "DL",
        "MachinePointerInfo",
        "getPointerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFRAMEADDR",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  const RISCVRegisterInfo &RI = *Subtarget.getRegisterInfo();\n  MachineFunction &MF = DAG.getMachineFunction();\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  MFI.setFrameAddressIsTaken(true);\n  Register FrameReg = RI.getFrameRegister(MF);\n  int XLenInBytes = Subtarget.getXLen() / 8;\n\n  EVT VT = Op.getValueType();\n  SDLoc DL(Op);\n  SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), DL, FrameReg, VT);\n  unsigned Depth = Op.getConstantOperandVal(0);\n  while (Depth--) {\n    int Offset = -(XLenInBytes * 2);\n    SDValue Ptr = DAG.getNode(ISD::ADD, DL, VT, FrameAddr,\n                              DAG.getIntPtrConstant(Offset, DL));\n    FrameAddr =\n        DAG.getLoad(VT, DL, DAG.getEntryNode(), Ptr, MachinePointerInfo());\n  }\n  return FrameAddr;\n}",
      "start_line": 7374,
      "end_line": 7395,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCopyFromReg",
        "getConstantOperandVal",
        "getMachineFunction",
        "getLoad",
        "getFrameInfo",
        "getValueType",
        "getRegisterInfo",
        "getXLen",
        "DL",
        "getFrameRegister",
        "MachinePointerInfo",
        "getNode",
        "setFrameAddressIsTaken"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerRETURNADDR",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  const RISCVRegisterInfo &RI = *Subtarget.getRegisterInfo();\n  MachineFunction &MF = DAG.getMachineFunction();\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  MFI.setReturnAddressIsTaken(true);\n  MVT XLenVT = Subtarget.getXLenVT();\n  int XLenInBytes = Subtarget.getXLen() / 8;\n\n  if (verifyReturnAddressArgumentIsConstant(Op, DAG))\n    return SDValue();\n\n  EVT VT = Op.getValueType();\n  SDLoc DL(Op);\n  unsigned Depth = Op.getConstantOperandVal(0);\n  if (Depth) {\n    int Off = -XLenInBytes;\n    SDValue FrameAddr = lowerFRAMEADDR(Op, DAG);\n    SDValue Offset = DAG.getConstant(Off, DL, VT);\n    return DAG.getLoad(VT, DL, DAG.getEntryNode(),\n                       DAG.getNode(ISD::ADD, DL, VT, FrameAddr, Offset),\n                       MachinePointerInfo());\n  }\n\n  // Return the value of the return address register, marking it an implicit\n  // live-in.\n  Register Reg = MF.addLiveIn(RI.getRARegister(), getRegClassFor(XLenVT));\n  return DAG.getCopyFromReg(DAG.getEntryNode(), DL, Reg, XLenVT);\n}",
      "start_line": 7397,
      "end_line": 7425,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCopyFromReg",
        "SDValue",
        "getConstantOperandVal",
        "getMachineFunction",
        "getLoad",
        "getConstant",
        "getFrameInfo",
        "getValueType",
        "lowerFRAMEADDR",
        "addLiveIn",
        "setReturnAddressIsTaken",
        "getRegisterInfo",
        "getXLen",
        "DL",
        "getXLenVT",
        "MachinePointerInfo",
        "getNode",
        "getRegClassFor"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerShiftLeftParts",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue Lo = Op.getOperand(0);\n  SDValue Hi = Op.getOperand(1);\n  SDValue Shamt = Op.getOperand(2);\n  EVT VT = Lo.getValueType();\n\n  // if Shamt-XLEN < 0: // Shamt < XLEN\n  //   Lo = Lo << Shamt\n  //   Hi = (Hi << Shamt) | ((Lo >>u 1) >>u (XLEN-1 - Shamt))\n  // else:\n  //   Lo = 0\n  //   Hi = Lo << (Shamt-XLEN)\n\n  SDValue Zero = DAG.getConstant(0, DL, VT);\n  SDValue One = DAG.getConstant(1, DL, VT);\n  SDValue MinusXLen = DAG.getConstant(-(int)Subtarget.getXLen(), DL, VT);\n  SDValue XLenMinus1 = DAG.getConstant(Subtarget.getXLen() - 1, DL, VT);\n  SDValue ShamtMinusXLen = DAG.getNode(ISD::ADD, DL, VT, Shamt, MinusXLen);\n  SDValue XLenMinus1Shamt = DAG.getNode(ISD::SUB, DL, VT, XLenMinus1, Shamt);\n\n  SDValue LoTrue = DAG.getNode(ISD::SHL, DL, VT, Lo, Shamt);\n  SDValue ShiftRight1Lo = DAG.getNode(ISD::SRL, DL, VT, Lo, One);\n  SDValue ShiftRightLo =\n      DAG.getNode(ISD::SRL, DL, VT, ShiftRight1Lo, XLenMinus1Shamt);\n  SDValue ShiftLeftHi = DAG.getNode(ISD::SHL, DL, VT, Hi, Shamt);\n  SDValue HiTrue = DAG.getNode(ISD::OR, DL, VT, ShiftLeftHi, ShiftRightLo);\n  SDValue HiFalse = DAG.getNode(ISD::SHL, DL, VT, Lo, ShamtMinusXLen);\n\n  SDValue CC = DAG.getSetCC(DL, VT, ShamtMinusXLen, Zero, ISD::SETLT);\n\n  Lo = DAG.getNode(ISD::SELECT, DL, VT, CC, LoTrue, Zero);\n  Hi = DAG.getNode(ISD::SELECT, DL, VT, CC, HiTrue, HiFalse);\n\n  SDValue Parts[2] = {Lo, Hi};\n  return DAG.getMergeValues(Parts, DL);\n}",
      "start_line": 7427,
      "end_line": 7464,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMergeValues",
        "getSetCC",
        "getConstant",
        "u",
        "getValueType",
        "getOperand",
        "getXLen",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerShiftRightParts",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "bool",
          "name": "IsSRA"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue Lo = Op.getOperand(0);\n  SDValue Hi = Op.getOperand(1);\n  SDValue Shamt = Op.getOperand(2);\n  EVT VT = Lo.getValueType();\n\n  // SRA expansion:\n  //   if Shamt-XLEN < 0: // Shamt < XLEN\n  //     Lo = (Lo >>u Shamt) | ((Hi << 1) << (XLEN-1 - ShAmt))\n  //     Hi = Hi >>s Shamt\n  //   else:\n  //     Lo = Hi >>s (Shamt-XLEN);\n  //     Hi = Hi >>s (XLEN-1)\n  //\n  // SRL expansion:\n  //   if Shamt-XLEN < 0: // Shamt < XLEN\n  //     Lo = (Lo >>u Shamt) | ((Hi << 1) << (XLEN-1 - ShAmt))\n  //     Hi = Hi >>u Shamt\n  //   else:\n  //     Lo = Hi >>u (Shamt-XLEN);\n  //     Hi = 0;\n\n  unsigned ShiftRightOp = IsSRA ? ISD::SRA : ISD::SRL;\n\n  SDValue Zero = DAG.getConstant(0, DL, VT);\n  SDValue One = DAG.getConstant(1, DL, VT);\n  SDValue MinusXLen = DAG.getConstant(-(int)Subtarget.getXLen(), DL, VT);\n  SDValue XLenMinus1 = DAG.getConstant(Subtarget.getXLen() - 1, DL, VT);\n  SDValue ShamtMinusXLen = DAG.getNode(ISD::ADD, DL, VT, Shamt, MinusXLen);\n  SDValue XLenMinus1Shamt = DAG.getNode(ISD::SUB, DL, VT, XLenMinus1, Shamt);\n\n  SDValue ShiftRightLo = DAG.getNode(ISD::SRL, DL, VT, Lo, Shamt);\n  SDValue ShiftLeftHi1 = DAG.getNode(ISD::SHL, DL, VT, Hi, One);\n  SDValue ShiftLeftHi =\n      DAG.getNode(ISD::SHL, DL, VT, ShiftLeftHi1, XLenMinus1Shamt);\n  SDValue LoTrue = DAG.getNode(ISD::OR, DL, VT, ShiftRightLo, ShiftLeftHi);\n  SDValue HiTrue = DAG.getNode(ShiftRightOp, DL, VT, Hi, Shamt);\n  SDValue LoFalse = DAG.getNode(ShiftRightOp, DL, VT, Hi, ShamtMinusXLen);\n  SDValue HiFalse =\n      IsSRA ? DAG.getNode(ISD::SRA, DL, VT, Hi, XLenMinus1) : Zero;\n\n  SDValue CC = DAG.getSetCC(DL, VT, ShamtMinusXLen, Zero, ISD::SETLT);\n\n  Lo = DAG.getNode(ISD::SELECT, DL, VT, CC, LoTrue, LoFalse);\n  Hi = DAG.getNode(ISD::SELECT, DL, VT, CC, HiTrue, HiFalse);\n\n  SDValue Parts[2] = {Lo, Hi};\n  return DAG.getMergeValues(Parts, DL);\n}",
      "start_line": 7466,
      "end_line": 7516,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMergeValues",
        "getSetCC",
        "getConstant",
        "u",
        "getValueType",
        "s",
        "getOperand",
        "getXLen",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorMaskSplat",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue SplatVal = Op.getOperand(0);\n  // All-zeros or all-ones splats are handled specially.\n  if (ISD::isConstantSplatVectorAllOnes(Op.getNode())) {\n    SDValue VL = getDefaultScalableVLOps(VT, DL, DAG, Subtarget).second;\n    return DAG.getNode(RISCVISD::VMSET_VL, DL, VT, VL);\n  }\n  if (ISD::isConstantSplatVectorAllZeros(Op.getNode())) {\n    SDValue VL = getDefaultScalableVLOps(VT, DL, DAG, Subtarget).second;\n    return DAG.getNode(RISCVISD::VMCLR_VL, DL, VT, VL);\n  }\n  MVT InterVT = VT.changeVectorElementType(MVT::i8);\n  SplatVal = DAG.getNode(ISD::AND, DL, SplatVal.getValueType(), SplatVal,\n                         DAG.getConstant(1, DL, SplatVal.getValueType()));\n  SDValue LHS = DAG.getSplatVector(InterVT, DL, SplatVal);\n  SDValue Zero = DAG.getConstant(0, DL, InterVT);\n  return DAG.getSetCC(DL, VT, LHS, Zero, ISD::SETNE);\n}",
      "start_line": 7520,
      "end_line": 7540,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getSetCC",
        "getSplatVector",
        "getConstant",
        "getSimpleValueType",
        "getOperand",
        "DL",
        "changeVectorElementType",
        "getDefaultScalableVLOps",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerSPLAT_VECTOR_PARTS",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VecVT = Op.getSimpleValueType();\n  assert(!Subtarget.is64Bit() && VecVT.getVectorElementType() == MVT::i64 &&\n         \"Unexpected SPLAT_VECTOR_PARTS lowering\");\n\n  assert(Op.getNumOperands() == 2 && \"Unexpected number of operands!\");\n  SDValue Lo = Op.getOperand(0);\n  SDValue Hi = Op.getOperand(1);\n\n  MVT ContainerVT = VecVT;\n  if (VecVT.isFixedLengthVector())\n    ContainerVT = getContainerForFixedLengthVector(VecVT);\n\n  auto VL = getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget).second;\n\n  SDValue Res =\n      splatPartsI64WithVL(DL, ContainerVT, SDValue(), Lo, Hi, VL, DAG);\n\n  if (VecVT.isFixedLengthVector())\n    Res = convertFromScalableVector(VecVT, Res, DAG, Subtarget);\n\n  return Res;\n}",
      "start_line": 7546,
      "end_line": 7570,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "getVectorElementType",
        "DL",
        "splatPartsI64WithVL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorMaskExt",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "int64_t",
          "name": "ExtTrueVal"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VecVT = Op.getSimpleValueType();\n  SDValue Src = Op.getOperand(0);\n  // Only custom-lower extensions from mask types\n  assert(Src.getValueType().isVector() &&\n         Src.getValueType().getVectorElementType() == MVT::i1);\n\n  if (VecVT.isScalableVector()) {\n    SDValue SplatZero = DAG.getConstant(0, DL, VecVT);\n    SDValue SplatTrueVal = DAG.getConstant(ExtTrueVal, DL, VecVT);\n    return DAG.getNode(ISD::VSELECT, DL, VecVT, Src, SplatTrueVal, SplatZero);\n  }\n\n  MVT ContainerVT = getContainerForFixedLengthVector(VecVT);\n  MVT I1ContainerVT =\n      MVT::getVectorVT(MVT::i1, ContainerVT.getVectorElementCount());\n\n  SDValue CC = convertToScalableVector(I1ContainerVT, Src, DAG, Subtarget);\n\n  SDValue VL = getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget).second;\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  SDValue SplatZero = DAG.getConstant(0, DL, XLenVT);\n  SDValue SplatTrueVal = DAG.getConstant(ExtTrueVal, DL, XLenVT);\n\n  SplatZero = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                          DAG.getUNDEF(ContainerVT), SplatZero, VL);\n  SplatTrueVal = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                             DAG.getUNDEF(ContainerVT), SplatTrueVal, VL);\n  SDValue Select =\n      DAG.getNode(RISCVISD::VMERGE_VL, DL, ContainerVT, CC, SplatTrueVal,\n                  SplatZero, DAG.getUNDEF(ContainerVT), VL);\n\n  return convertFromScalableVector(VecVT, Select, DAG, Subtarget);\n}",
      "start_line": 7576,
      "end_line": 7612,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getValueType",
        "isVector",
        "getContainerForFixedLengthVector",
        "getOperand",
        "convertFromScalableVector",
        "getVectorElementType",
        "DL",
        "getVectorVT",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFixedLengthVectorExtendToRVV",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned",
          "name": "ExtendOpc"
        }
      ],
      "body": "{\n  MVT ExtVT = Op.getSimpleValueType();\n  // Only custom-lower extensions from fixed-length vector types.\n  if (!ExtVT.isFixedLengthVector())\n    return Op;\n  MVT VT = Op.getOperand(0).getSimpleValueType();\n  // Grab the canonical container type for the extended type. Infer the smaller\n  // type from that to ensure the same number of vector elements, as we know\n  // the LMUL will be sufficient to hold the smaller type.\n  MVT ContainerExtVT = getContainerForFixedLengthVector(ExtVT);\n  // Get the extended container type manually to ensure the same number of\n  // vector elements between source and dest.\n  MVT ContainerVT = MVT::getVectorVT(VT.getVectorElementType(),\n                                     ContainerExtVT.getVectorElementCount());\n\n  SDValue Op1 =\n      convertToScalableVector(ContainerVT, Op.getOperand(0), DAG, Subtarget);\n\n  SDLoc DL(Op);\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  SDValue Ext = DAG.getNode(ExtendOpc, DL, ContainerExtVT, Op1, Mask, VL);\n\n  return convertFromScalableVector(ExtVT, Ext, DAG, Subtarget);\n}",
      "start_line": 7614,
      "end_line": 7639,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getOperand",
        "getVectorVT",
        "DL",
        "getVectorElementCount",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorMaskTruncLike",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  bool IsVPTrunc = Op.getOpcode() == ISD::VP_TRUNCATE;\n  SDLoc DL(Op);\n  EVT MaskVT = Op.getValueType();\n  // Only expect to custom-lower truncations to mask types\n  assert(MaskVT.isVector() && MaskVT.getVectorElementType() == MVT::i1 &&\n         \"Unexpected type for vector mask lowering\");\n  SDValue Src = Op.getOperand(0);\n  MVT VecVT = Src.getSimpleValueType();\n  SDValue Mask, VL;\n  if (IsVPTrunc) {\n    Mask = Op.getOperand(1);\n    VL = Op.getOperand(2);\n  }\n  // If this is a fixed vector, we need to convert it to a scalable vector.\n  MVT ContainerVT = VecVT;\n\n  if (VecVT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VecVT);\n    Src = convertToScalableVector(ContainerVT, Src, DAG, Subtarget);\n    if (IsVPTrunc) {\n      MVT MaskContainerVT =\n          getContainerForFixedLengthVector(Mask.getSimpleValueType());\n      Mask = convertToScalableVector(MaskContainerVT, Mask, DAG, Subtarget);\n    }\n  }\n\n  if (!IsVPTrunc) {\n    std::tie(Mask, VL) =\n        getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget);\n  }\n\n  SDValue SplatOne = DAG.getConstant(1, DL, Subtarget.getXLenVT());\n  SDValue SplatZero = DAG.getConstant(0, DL, Subtarget.getXLenVT());\n\n  SplatOne = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                         DAG.getUNDEF(ContainerVT), SplatOne, VL);\n  SplatZero = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                          DAG.getUNDEF(ContainerVT), SplatZero, VL);\n\n  MVT MaskContainerVT = ContainerVT.changeVectorElementType(MVT::i1);\n  SDValue Trunc = DAG.getNode(RISCVISD::AND_VL, DL, ContainerVT, Src, SplatOne,\n                              DAG.getUNDEF(ContainerVT), Mask, VL);\n  Trunc = DAG.getNode(RISCVISD::SETCC_VL, DL, MaskContainerVT,\n                      {Trunc, SplatZero, DAG.getCondCode(ISD::SETNE),\n                       DAG.getUNDEF(MaskContainerVT), Mask, VL});\n  if (MaskVT.isFixedLengthVector())\n    Trunc = convertFromScalableVector(MaskVT, Trunc, DAG, Subtarget);\n  return Trunc;\n}",
      "start_line": 7644,
      "end_line": 7694,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getOpcode",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getValueType",
        "getContainerForFixedLengthVector",
        "getUNDEF",
        "getOperand",
        "convertFromScalableVector",
        "getVectorElementType",
        "DL",
        "tie",
        "changeVectorElementType",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorTruncLike",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  bool IsVPTrunc = Op.getOpcode() == ISD::VP_TRUNCATE;\n  SDLoc DL(Op);\n\n  MVT VT = Op.getSimpleValueType();\n  // Only custom-lower vector truncates\n  assert(VT.isVector() && \"Unexpected type for vector truncate lowering\");\n\n  // Truncates to mask types are handled differently\n  if (VT.getVectorElementType() == MVT::i1)\n    return lowerVectorMaskTruncLike(Op, DAG);\n\n  // RVV only has truncates which operate from SEW*2->SEW, so lower arbitrary\n  // truncates as a series of \"RISCVISD::TRUNCATE_VECTOR_VL\" nodes which\n  // truncate by one power of two at a time.\n  MVT DstEltVT = VT.getVectorElementType();\n\n  SDValue Src = Op.getOperand(0);\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT SrcEltVT = SrcVT.getVectorElementType();\n\n  assert(DstEltVT.bitsLT(SrcEltVT) && isPowerOf2_64(DstEltVT.getSizeInBits()) &&\n         isPowerOf2_64(SrcEltVT.getSizeInBits()) &&\n         \"Unexpected vector truncate lowering\");\n\n  MVT ContainerVT = SrcVT;\n  SDValue Mask, VL;\n  if (IsVPTrunc) {\n    Mask = Op.getOperand(1);\n    VL = Op.getOperand(2);\n  }\n  if (SrcVT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(SrcVT);\n    Src = convertToScalableVector(ContainerVT, Src, DAG, Subtarget);\n    if (IsVPTrunc) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n  }\n\n  SDValue Result = Src;\n  if (!IsVPTrunc) {\n    std::tie(Mask, VL) =\n        getDefaultVLOps(SrcVT, ContainerVT, DL, DAG, Subtarget);\n  }\n\n  LLVMContext &Context = *DAG.getContext();\n  const ElementCount Count = ContainerVT.getVectorElementCount();\n  do {\n    SrcEltVT = MVT::getIntegerVT(SrcEltVT.getSizeInBits() / 2);\n    EVT ResultVT = EVT::getVectorVT(Context, SrcEltVT, Count);\n    Result = DAG.getNode(RISCVISD::TRUNCATE_VECTOR_VL, DL, ResultVT, Result,\n                         Mask, VL);\n  } while (SrcEltVT != DstEltVT);\n\n  if (SrcVT.isFixedLengthVector())\n    Result = convertFromScalableVector(VT, Result, DAG, Subtarget);\n\n  return Result;\n}",
      "start_line": 7696,
      "end_line": 7756,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isPowerOf2_64",
        "getContext",
        "getMaskTypeFor",
        "getDefaultVLOps",
        "getOpcode",
        "convertToScalableVector",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "getVectorElementType",
        "DL",
        "tie",
        "getVectorElementCount",
        "getIntegerVT",
        "lowerVectorMaskTruncLike",
        "getVectorVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerStrictFPExtendOrRoundLike",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue Chain = Op.getOperand(0);\n  SDValue Src = Op.getOperand(1);\n  MVT VT = Op.getSimpleValueType();\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    MVT SrcContainerVT = getContainerForFixedLengthVector(SrcVT);\n    ContainerVT =\n        SrcContainerVT.changeVectorElementType(VT.getVectorElementType());\n    Src = convertToScalableVector(SrcContainerVT, Src, DAG, Subtarget);\n  }\n\n  auto [Mask, VL] = getDefaultVLOps(SrcVT, ContainerVT, DL, DAG, Subtarget);\n\n  // RVV can only widen/truncate fp to types double/half the size as the source.\n  if ((VT.getVectorElementType() == MVT::f64 &&\n       SrcVT.getVectorElementType() == MVT::f16) ||\n      (VT.getVectorElementType() == MVT::f16 &&\n       SrcVT.getVectorElementType() == MVT::f64)) {\n    // For double rounding, the intermediate rounding should be round-to-odd.\n    unsigned InterConvOpc = Op.getOpcode() == ISD::STRICT_FP_EXTEND\n                                ? RISCVISD::STRICT_FP_EXTEND_VL\n                                : RISCVISD::STRICT_VFNCVT_ROD_VL;\n    MVT InterVT = ContainerVT.changeVectorElementType(MVT::f32);\n    Src = DAG.getNode(InterConvOpc, DL, DAG.getVTList(InterVT, MVT::Other),\n                      Chain, Src, Mask, VL);\n    Chain = Src.getValue(1);\n  }\n\n  unsigned ConvOpc = Op.getOpcode() == ISD::STRICT_FP_EXTEND\n                         ? RISCVISD::STRICT_FP_EXTEND_VL\n                         : RISCVISD::STRICT_FP_ROUND_VL;\n  SDValue Res = DAG.getNode(ConvOpc, DL, DAG.getVTList(ContainerVT, MVT::Other),\n                            Chain, Src, Mask, VL);\n  if (VT.isFixedLengthVector()) {\n    // StrictFP operations have two result values. Their lowered result should\n    // have same result count.\n    SDValue SubVec = convertFromScalableVector(VT, Res, DAG, Subtarget);\n    Res = DAG.getMergeValues({SubVec, Res.getValue(1)}, DL);\n  }\n  return Res;\n}",
      "start_line": 7758,
      "end_line": 7803,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getMergeValues",
        "getValue",
        "getNode",
        "getOpcode",
        "convertToScalableVector",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "getVectorElementType",
        "DL",
        "changeVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorFPExtendOrRoundLike",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  bool IsVP =\n      Op.getOpcode() == ISD::VP_FP_ROUND || Op.getOpcode() == ISD::VP_FP_EXTEND;\n  bool IsExtend =\n      Op.getOpcode() == ISD::VP_FP_EXTEND || Op.getOpcode() == ISD::FP_EXTEND;\n  // RVV can only do truncate fp to types half the size as the source. We\n  // custom-lower f64->f16 rounds via RVV's round-to-odd float\n  // conversion instruction.\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  assert(VT.isVector() && \"Unexpected type for vector truncate lowering\");\n\n  SDValue Src = Op.getOperand(0);\n  MVT SrcVT = Src.getSimpleValueType();\n\n  bool IsDirectExtend = IsExtend && (VT.getVectorElementType() != MVT::f64 ||\n                                     SrcVT.getVectorElementType() != MVT::f16);\n  bool IsDirectTrunc = !IsExtend && (VT.getVectorElementType() != MVT::f16 ||\n                                     SrcVT.getVectorElementType() != MVT::f64);\n\n  bool IsDirectConv = IsDirectExtend || IsDirectTrunc;\n\n  // Prepare any fixed-length vector operands.\n  MVT ContainerVT = VT;\n  SDValue Mask, VL;\n  if (IsVP) {\n    Mask = Op.getOperand(1);\n    VL = Op.getOperand(2);\n  }\n  if (VT.isFixedLengthVector()) {\n    MVT SrcContainerVT = getContainerForFixedLengthVector(SrcVT);\n    ContainerVT =\n        SrcContainerVT.changeVectorElementType(VT.getVectorElementType());\n    Src = convertToScalableVector(SrcContainerVT, Src, DAG, Subtarget);\n    if (IsVP) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n  }\n\n  if (!IsVP)\n    std::tie(Mask, VL) =\n        getDefaultVLOps(SrcVT, ContainerVT, DL, DAG, Subtarget);\n\n  unsigned ConvOpc = IsExtend ? RISCVISD::FP_EXTEND_VL : RISCVISD::FP_ROUND_VL;\n\n  if (IsDirectConv) {\n    Src = DAG.getNode(ConvOpc, DL, ContainerVT, Src, Mask, VL);\n    if (VT.isFixedLengthVector())\n      Src = convertFromScalableVector(VT, Src, DAG, Subtarget);\n    return Src;\n  }\n\n  unsigned InterConvOpc =\n      IsExtend ? RISCVISD::FP_EXTEND_VL : RISCVISD::VFNCVT_ROD_VL;\n\n  MVT InterVT = ContainerVT.changeVectorElementType(MVT::f32);\n  SDValue IntermediateConv =\n      DAG.getNode(InterConvOpc, DL, InterVT, Src, Mask, VL);\n  SDValue Result =\n      DAG.getNode(ConvOpc, DL, ContainerVT, IntermediateConv, Mask, VL);\n  if (VT.isFixedLengthVector())\n    return convertFromScalableVector(VT, Result, DAG, Subtarget);\n  return Result;\n}",
      "start_line": 7805,
      "end_line": 7872,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMaskTypeFor",
        "getDefaultVLOps",
        "getNode",
        "getOpcode",
        "convertToScalableVector",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "getVectorElementType",
        "DL",
        "tie",
        "changeVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerINSERT_VECTOR_ELT",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VecVT = Op.getSimpleValueType();\n  SDValue Vec = Op.getOperand(0);\n  SDValue Val = Op.getOperand(1);\n  SDValue Idx = Op.getOperand(2);\n\n  if (VecVT.getVectorElementType() == MVT::i1) {\n    // FIXME: For now we just promote to an i8 vector and insert into that,\n    // but this is probably not optimal.\n    MVT WideVT = MVT::getVectorVT(MVT::i8, VecVT.getVectorElementCount());\n    Vec = DAG.getNode(ISD::ZERO_EXTEND, DL, WideVT, Vec);\n    Vec = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, WideVT, Vec, Val, Idx);\n    return DAG.getNode(ISD::TRUNCATE, DL, VecVT, Vec);\n  }\n\n  MVT ContainerVT = VecVT;\n  // If the operand is a fixed-length vector, convert to a scalable one.\n  if (VecVT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VecVT);\n    Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n  }\n\n  // If we know the index we're going to insert at, we can shrink Vec so that\n  // we're performing the scalar inserts and slideup on a smaller LMUL.\n  MVT OrigContainerVT = ContainerVT;\n  SDValue OrigVec = Vec;\n  SDValue AlignedIdx;\n  if (auto *IdxC = dyn_cast<ConstantSDNode>(Idx)) {\n    const unsigned OrigIdx = IdxC->getZExtValue();\n    // Do we know an upper bound on LMUL?\n    if (auto ShrunkVT = getSmallestVTForIndex(ContainerVT, OrigIdx,\n                                              DL, DAG, Subtarget)) {\n      ContainerVT = *ShrunkVT;\n      AlignedIdx = DAG.getVectorIdxConstant(0, DL);\n    }\n\n    // If we're compiling for an exact VLEN value, we can always perform\n    // the insert in m1 as we can determine the register corresponding to\n    // the index in the register group.\n    const unsigned MinVLen = Subtarget.getRealMinVLen();\n    const unsigned MaxVLen = Subtarget.getRealMaxVLen();\n    const MVT M1VT = getLMUL1VT(ContainerVT);\n    if (MinVLen == MaxVLen && ContainerVT.bitsGT(M1VT)) {\n      EVT ElemVT = VecVT.getVectorElementType();\n      unsigned ElemsPerVReg = MinVLen / ElemVT.getFixedSizeInBits();\n      unsigned RemIdx = OrigIdx % ElemsPerVReg;\n      unsigned SubRegIdx = OrigIdx / ElemsPerVReg;\n      unsigned ExtractIdx =\n          SubRegIdx * M1VT.getVectorElementCount().getKnownMinValue();\n      AlignedIdx = DAG.getVectorIdxConstant(ExtractIdx, DL);\n      Idx = DAG.getVectorIdxConstant(RemIdx, DL);\n      ContainerVT = M1VT;\n    }\n\n    if (AlignedIdx)\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ContainerVT, Vec,\n                        AlignedIdx);\n  }\n\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  bool IsLegalInsert = Subtarget.is64Bit() || Val.getValueType() != MVT::i64;\n  // Even i64-element vectors on RV32 can be lowered without scalar\n  // legalization if the most-significant 32 bits of the value are not affected\n  // by the sign-extension of the lower 32 bits.\n  // TODO: We could also catch sign extensions of a 32-bit value.\n  if (!IsLegalInsert && isa<ConstantSDNode>(Val)) {\n    const auto *CVal = cast<ConstantSDNode>(Val);\n    if (isInt<32>(CVal->getSExtValue())) {\n      IsLegalInsert = true;\n      Val = DAG.getConstant(CVal->getSExtValue(), DL, MVT::i32);\n    }\n  }\n\n  auto [Mask, VL] = getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget);\n\n  SDValue ValInVec;\n\n  if (IsLegalInsert) {\n    unsigned Opc =\n        VecVT.isFloatingPoint() ? RISCVISD::VFMV_S_F_VL : RISCVISD::VMV_S_X_VL;\n    if (isNullConstant(Idx)) {\n      if (!VecVT.isFloatingPoint())\n        Val = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Val);\n      Vec = DAG.getNode(Opc, DL, ContainerVT, Vec, Val, VL);\n\n      if (AlignedIdx)\n        Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, OrigContainerVT, OrigVec,\n                          Vec, AlignedIdx);\n      if (!VecVT.isFixedLengthVector())\n        return Vec;\n      return convertFromScalableVector(VecVT, Vec, DAG, Subtarget);\n    }\n    ValInVec = lowerScalarInsert(Val, VL, ContainerVT, DL, DAG, Subtarget);\n  } else {\n    // On RV32, i64-element vectors must be specially handled to place the\n    // value at element 0, by using two vslide1down instructions in sequence on\n    // the i32 split lo/hi value. Use an equivalently-sized i32 vector for\n    // this.\n    SDValue ValLo, ValHi;\n    std::tie(ValLo, ValHi) = DAG.SplitScalar(Val, DL, MVT::i32, MVT::i32);\n    MVT I32ContainerVT =\n        MVT::getVectorVT(MVT::i32, ContainerVT.getVectorElementCount() * 2);\n    SDValue I32Mask =\n        getDefaultScalableVLOps(I32ContainerVT, DL, DAG, Subtarget).first;\n    // Limit the active VL to two.\n    SDValue InsertI64VL = DAG.getConstant(2, DL, XLenVT);\n    // If the Idx is 0 we can insert directly into the vector.\n    if (isNullConstant(Idx)) {\n      // First slide in the lo value, then the hi in above it. We use slide1down\n      // to avoid the register group overlap constraint of vslide1up.\n      ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,\n                             Vec, Vec, ValLo, I32Mask, InsertI64VL);\n      // If the source vector is undef don't pass along the tail elements from\n      // the previous slide1down.\n      SDValue Tail = Vec.isUndef() ? Vec : ValInVec;\n      ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,\n                             Tail, ValInVec, ValHi, I32Mask, InsertI64VL);\n      // Bitcast back to the right container type.\n      ValInVec = DAG.getBitcast(ContainerVT, ValInVec);\n\n      if (AlignedIdx)\n        ValInVec =\n            DAG.getNode(ISD::INSERT_SUBVECTOR, DL, OrigContainerVT, OrigVec,\n                        ValInVec, AlignedIdx);\n      if (!VecVT.isFixedLengthVector())\n        return ValInVec;\n      return convertFromScalableVector(VecVT, ValInVec, DAG, Subtarget);\n    }\n\n    // First slide in the lo value, then the hi in above it. We use slide1down\n    // to avoid the register group overlap constraint of vslide1up.\n    ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,\n                           DAG.getUNDEF(I32ContainerVT),\n                           DAG.getUNDEF(I32ContainerVT), ValLo,\n                           I32Mask, InsertI64VL);\n    ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,\n                           DAG.getUNDEF(I32ContainerVT), ValInVec, ValHi,\n                           I32Mask, InsertI64VL);\n    // Bitcast back to the right container type.\n    ValInVec = DAG.getBitcast(ContainerVT, ValInVec);\n  }\n\n  // Now that the value is in a vector, slide it into position.\n  SDValue InsertVL =\n      DAG.getNode(ISD::ADD, DL, XLenVT, Idx, DAG.getConstant(1, DL, XLenVT));\n\n  // Use tail agnostic policy if Idx is the last index of Vec.\n  unsigned Policy = RISCVII::TAIL_UNDISTURBED_MASK_UNDISTURBED;\n  if (VecVT.isFixedLengthVector() && isa<ConstantSDNode>(Idx) &&\n      Idx->getAsZExtVal() + 1 == VecVT.getVectorNumElements())\n    Policy = RISCVII::TAIL_AGNOSTIC;\n  SDValue Slideup = getVSlideup(DAG, Subtarget, DL, ContainerVT, Vec, ValInVec,\n                                Idx, Mask, InsertVL, Policy);\n\n  if (AlignedIdx)\n    Slideup = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, OrigContainerVT, OrigVec,\n                          Slideup, AlignedIdx);\n  if (!VecVT.isFixedLengthVector())\n    return Slideup;\n  return convertFromScalableVector(VecVT, Slideup, DAG, Subtarget);\n}",
      "start_line": 7906,
      "end_line": 8069,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getRealMaxVLen",
        "getRealMinVLen",
        "getAsZExtVal",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "is64Bit",
        "getBitcast",
        "getKnownMinValue",
        "isFloatingPoint",
        "isUndef",
        "getUNDEF",
        "getVectorIdxConstant",
        "getVectorNumElements",
        "getVectorElementType",
        "getVectorVT",
        "getVectorElementCount",
        "getFixedSizeInBits",
        "getZExtValue",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getOperand",
        "tie",
        "getDefaultScalableVLOps",
        "lowerScalarInsert",
        "getLMUL1VT",
        "SplitScalar",
        "DL",
        "getXLenVT",
        "getVSlideup",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerEXTRACT_VECTOR_ELT",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue Idx = Op.getOperand(1);\n  SDValue Vec = Op.getOperand(0);\n  EVT EltVT = Op.getValueType();\n  MVT VecVT = Vec.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  if (VecVT.getVectorElementType() == MVT::i1) {\n    // Use vfirst.m to extract the first bit.\n    if (isNullConstant(Idx)) {\n      MVT ContainerVT = VecVT;\n      if (VecVT.isFixedLengthVector()) {\n        ContainerVT = getContainerForFixedLengthVector(VecVT);\n        Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n      }\n      auto [Mask, VL] = getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget);\n      SDValue Vfirst =\n          DAG.getNode(RISCVISD::VFIRST_VL, DL, XLenVT, Vec, Mask, VL);\n      SDValue Res = DAG.getSetCC(DL, XLenVT, Vfirst,\n                                 DAG.getConstant(0, DL, XLenVT), ISD::SETEQ);\n      return DAG.getNode(ISD::TRUNCATE, DL, EltVT, Res);\n    }\n    if (VecVT.isFixedLengthVector()) {\n      unsigned NumElts = VecVT.getVectorNumElements();\n      if (NumElts >= 8) {\n        MVT WideEltVT;\n        unsigned WidenVecLen;\n        SDValue ExtractElementIdx;\n        SDValue ExtractBitIdx;\n        unsigned MaxEEW = Subtarget.getELen();\n        MVT LargestEltVT = MVT::getIntegerVT(\n            std::min(MaxEEW, unsigned(XLenVT.getSizeInBits())));\n        if (NumElts <= LargestEltVT.getSizeInBits()) {\n          assert(isPowerOf2_32(NumElts) &&\n                 \"the number of elements should be power of 2\");\n          WideEltVT = MVT::getIntegerVT(NumElts);\n          WidenVecLen = 1;\n          ExtractElementIdx = DAG.getConstant(0, DL, XLenVT);\n          ExtractBitIdx = Idx;\n        } else {\n          WideEltVT = LargestEltVT;\n          WidenVecLen = NumElts / WideEltVT.getSizeInBits();\n          // extract element index = index / element width\n          ExtractElementIdx = DAG.getNode(\n              ISD::SRL, DL, XLenVT, Idx,\n              DAG.getConstant(Log2_64(WideEltVT.getSizeInBits()), DL, XLenVT));\n          // mask bit index = index % element width\n          ExtractBitIdx = DAG.getNode(\n              ISD::AND, DL, XLenVT, Idx,\n              DAG.getConstant(WideEltVT.getSizeInBits() - 1, DL, XLenVT));\n        }\n        MVT WideVT = MVT::getVectorVT(WideEltVT, WidenVecLen);\n        Vec = DAG.getNode(ISD::BITCAST, DL, WideVT, Vec);\n        SDValue ExtractElt = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, XLenVT,\n                                         Vec, ExtractElementIdx);\n        // Extract the bit from GPR.\n        SDValue ShiftRight =\n            DAG.getNode(ISD::SRL, DL, XLenVT, ExtractElt, ExtractBitIdx);\n        SDValue Res = DAG.getNode(ISD::AND, DL, XLenVT, ShiftRight,\n                                  DAG.getConstant(1, DL, XLenVT));\n        return DAG.getNode(ISD::TRUNCATE, DL, EltVT, Res);\n      }\n    }\n    // Otherwise, promote to an i8 vector and extract from that.\n    MVT WideVT = MVT::getVectorVT(MVT::i8, VecVT.getVectorElementCount());\n    Vec = DAG.getNode(ISD::ZERO_EXTEND, DL, WideVT, Vec);\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, EltVT, Vec, Idx);\n  }\n\n  // If this is a fixed vector, we need to convert it to a scalable vector.\n  MVT ContainerVT = VecVT;\n  if (VecVT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VecVT);\n    Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n  }\n\n  // If we're compiling for an exact VLEN value and we have a known\n  // constant index, we can always perform the extract in m1 (or\n  // smaller) as we can determine the register corresponding to\n  // the index in the register group.\n  const unsigned MinVLen = Subtarget.getRealMinVLen();\n  const unsigned MaxVLen = Subtarget.getRealMaxVLen();\n  if (auto *IdxC = dyn_cast<ConstantSDNode>(Idx);\n      IdxC && MinVLen == MaxVLen &&\n      VecVT.getSizeInBits().getKnownMinValue() > MinVLen) {\n    MVT M1VT = getLMUL1VT(ContainerVT);\n    unsigned OrigIdx = IdxC->getZExtValue();\n    EVT ElemVT = VecVT.getVectorElementType();\n    unsigned ElemsPerVReg = MinVLen / ElemVT.getFixedSizeInBits();\n    unsigned RemIdx = OrigIdx % ElemsPerVReg;\n    unsigned SubRegIdx = OrigIdx / ElemsPerVReg;\n    unsigned ExtractIdx =\n      SubRegIdx * M1VT.getVectorElementCount().getKnownMinValue();\n    Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, M1VT, Vec,\n                      DAG.getVectorIdxConstant(ExtractIdx, DL));\n    Idx = DAG.getVectorIdxConstant(RemIdx, DL);\n    ContainerVT = M1VT;\n  }\n\n  // Reduce the LMUL of our slidedown and vmv.x.s to the smallest LMUL which\n  // contains our index.\n  std::optional<uint64_t> MaxIdx;\n  if (VecVT.isFixedLengthVector())\n    MaxIdx = VecVT.getVectorNumElements() - 1;\n  if (auto *IdxC = dyn_cast<ConstantSDNode>(Idx))\n    MaxIdx = IdxC->getZExtValue();\n  if (MaxIdx) {\n    if (auto SmallerVT =\n            getSmallestVTForIndex(ContainerVT, *MaxIdx, DL, DAG, Subtarget)) {\n      ContainerVT = *SmallerVT;\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ContainerVT, Vec,\n                        DAG.getConstant(0, DL, XLenVT));\n    }\n  }\n\n  // If after narrowing, the required slide is still greater than LMUL2,\n  // fallback to generic expansion and go through the stack.  This is done\n  // for a subtle reason: extracting *all* elements out of a vector is\n  // widely expected to be linear in vector size, but because vslidedown\n  // is linear in LMUL, performing N extracts using vslidedown becomes\n  // O(n^2) / (VLEN/ETYPE) work.  On the surface, going through the stack\n  // seems to have the same problem (the store is linear in LMUL), but the\n  // generic expansion *memoizes* the store, and thus for many extracts of\n  // the same vector we end up with one store and a bunch of loads.\n  // TODO: We don't have the same code for insert_vector_elt because we\n  // have BUILD_VECTOR and handle the degenerate case there.  Should we\n  // consider adding an inverse BUILD_VECTOR node?\n  MVT LMUL2VT = getLMUL1VT(ContainerVT).getDoubleNumVectorElementsVT();\n  if (ContainerVT.bitsGT(LMUL2VT) && VecVT.isFixedLengthVector())\n    return SDValue();\n\n  // If the index is 0, the vector is already in the right position.\n  if (!isNullConstant(Idx)) {\n    // Use a VL of 1 to avoid processing more elements than we need.\n    auto [Mask, VL] = getDefaultVLOps(1, ContainerVT, DL, DAG, Subtarget);\n    Vec = getVSlidedown(DAG, Subtarget, DL, ContainerVT,\n                        DAG.getUNDEF(ContainerVT), Vec, Idx, Mask, VL);\n  }\n\n  if (!EltVT.isInteger()) {\n    // Floating-point extracts are handled in TableGen.\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, EltVT, Vec,\n                       DAG.getConstant(0, DL, XLenVT));\n  }\n\n  SDValue Elt0 = DAG.getNode(RISCVISD::VMV_X_S, DL, XLenVT, Vec);\n  return DAG.getNode(ISD::TRUNCATE, DL, EltVT, Elt0);\n}",
      "start_line": 8075,
      "end_line": 8224,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getDoubleNumVectorElementsVT",
        "getRealMaxVLen",
        "getRealMinVLen",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getSizeInBits",
        "getKnownMinValue",
        "getVectorIdxConstant",
        "getVectorNumElements",
        "getVectorElementType",
        "getVectorVT",
        "getVectorElementCount",
        "getFixedSizeInBits",
        "isFixedLengthVector",
        "getZExtValue",
        "SDValue",
        "getSetCC",
        "O",
        "m1",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getVSlidedown",
        "getOperand",
        "getELen",
        "getLMUL1VT",
        "problem",
        "getIntegerVT",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorIntrinsicScalars",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert((Op.getOpcode() == ISD::INTRINSIC_VOID ||\n          Op.getOpcode() == ISD::INTRINSIC_WO_CHAIN ||\n          Op.getOpcode() == ISD::INTRINSIC_W_CHAIN) &&\n         \"Unexpected opcode\");\n\n  if (!Subtarget.hasVInstructions())\n    return SDValue();\n\n  bool HasChain = Op.getOpcode() == ISD::INTRINSIC_VOID ||\n                  Op.getOpcode() == ISD::INTRINSIC_W_CHAIN;\n  unsigned IntNo = Op.getConstantOperandVal(HasChain ? 1 : 0);\n\n  SDLoc DL(Op);\n\n  const RISCVVIntrinsicsTable::RISCVVIntrinsicInfo *II =\n      RISCVVIntrinsicsTable::getRISCVVIntrinsicInfo(IntNo);\n  if (!II || !II->hasScalarOperand())\n    return SDValue();\n\n  unsigned SplatOp = II->ScalarOperand + 1 + HasChain;\n  assert(SplatOp < Op.getNumOperands());\n\n  SmallVector<SDValue, 8> Operands(Op->op_begin(), Op->op_end());\n  SDValue &ScalarOp = Operands[SplatOp];\n  MVT OpVT = ScalarOp.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  // If this isn't a scalar, or its type is XLenVT we're done.\n  if (!OpVT.isScalarInteger() || OpVT == XLenVT)\n    return SDValue();\n\n  // Simplest case is that the operand needs to be promoted to XLenVT.\n  if (OpVT.bitsLT(XLenVT)) {\n    // If the operand is a constant, sign extend to increase our chances\n    // of being able to use a .vi instruction. ANY_EXTEND would become a\n    // a zero extend and the simm5 check in isel would fail.\n    // FIXME: Should we ignore the upper bits in isel instead?\n    unsigned ExtOpc =\n        isa<ConstantSDNode>(ScalarOp) ? ISD::SIGN_EXTEND : ISD::ANY_EXTEND;\n    ScalarOp = DAG.getNode(ExtOpc, DL, XLenVT, ScalarOp);\n    return DAG.getNode(Op->getOpcode(), DL, Op->getVTList(), Operands);\n  }\n\n  // Use the previous operand to get the vXi64 VT. The result might be a mask\n  // VT for compares. Using the previous operand assumes that the previous\n  // operand will never have a smaller element size than a scalar operand and\n  // that a widening operation never uses SEW=64.\n  // NOTE: If this fails the below assert, we can probably just find the\n  // element count from any operand or result and use it to construct the VT.\n  assert(II->ScalarOperand > 0 && \"Unexpected splat operand!\");\n  MVT VT = Op.getOperand(SplatOp - 1).getSimpleValueType();\n\n  // The more complex case is when the scalar is larger than XLenVT.\n  assert(XLenVT == MVT::i32 && OpVT == MVT::i64 &&\n         VT.getVectorElementType() == MVT::i64 && \"Unexpected VTs!\");\n\n  // If this is a sign-extended 32-bit value, we can truncate it and rely on the\n  // instruction to sign-extend since SEW>XLEN.\n  if (DAG.ComputeNumSignBits(ScalarOp) > 32) {\n    ScalarOp = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, ScalarOp);\n    return DAG.getNode(Op->getOpcode(), DL, Op->getVTList(), Operands);\n  }\n\n  switch (IntNo) {\n  case Intrinsic::riscv_vslide1up:\n  case Intrinsic::riscv_vslide1down:\n  case Intrinsic::riscv_vslide1up_mask:\n  case Intrinsic::riscv_vslide1down_mask: {\n    // We need to special case these when the scalar is larger than XLen.\n    unsigned NumOps = Op.getNumOperands();\n    bool IsMasked = NumOps == 7;\n\n    // Convert the vector source to the equivalent nxvXi32 vector.\n    MVT I32VT = MVT::getVectorVT(MVT::i32, VT.getVectorElementCount() * 2);\n    SDValue Vec = DAG.getBitcast(I32VT, Operands[2]);\n    SDValue ScalarLo, ScalarHi;\n    std::tie(ScalarLo, ScalarHi) =\n        DAG.SplitScalar(ScalarOp, DL, MVT::i32, MVT::i32);\n\n    // Double the VL since we halved SEW.\n    SDValue AVL = getVLOperand(Op);\n    SDValue I32VL;\n\n    // Optimize for constant AVL\n    if (isa<ConstantSDNode>(AVL)) {\n      const auto [MinVLMAX, MaxVLMAX] =\n          RISCVTargetLowering::computeVLMAXBounds(VT, Subtarget);\n\n      uint64_t AVLInt = AVL->getAsZExtVal();\n      if (AVLInt <= MinVLMAX) {\n        I32VL = DAG.getConstant(2 * AVLInt, DL, XLenVT);\n      } else if (AVLInt >= 2 * MaxVLMAX) {\n        // Just set vl to VLMAX in this situation\n        RISCVII::VLMUL Lmul = RISCVTargetLowering::getLMUL(I32VT);\n        SDValue LMUL = DAG.getConstant(Lmul, DL, XLenVT);\n        unsigned Sew = RISCVVType::encodeSEW(I32VT.getScalarSizeInBits());\n        SDValue SEW = DAG.getConstant(Sew, DL, XLenVT);\n        SDValue SETVLMAX = DAG.getTargetConstant(\n            Intrinsic::riscv_vsetvlimax, DL, MVT::i32);\n        I32VL = DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, XLenVT, SETVLMAX, SEW,\n                            LMUL);\n      } else {\n        // For AVL between (MinVLMAX, 2 * MaxVLMAX), the actual working vl\n        // is related to the hardware implementation.\n        // So let the following code handle\n      }\n    }\n    if (!I32VL) {\n      RISCVII::VLMUL Lmul = RISCVTargetLowering::getLMUL(VT);\n      SDValue LMUL = DAG.getConstant(Lmul, DL, XLenVT);\n      unsigned Sew = RISCVVType::encodeSEW(VT.getScalarSizeInBits());\n      SDValue SEW = DAG.getConstant(Sew, DL, XLenVT);\n      SDValue SETVL =\n          DAG.getTargetConstant(Intrinsic::riscv_vsetvli, DL, MVT::i32);\n      // Using vsetvli instruction to get actually used length which related to\n      // the hardware implementation\n      SDValue VL = DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, XLenVT, SETVL, AVL,\n                               SEW, LMUL);\n      I32VL =\n          DAG.getNode(ISD::SHL, DL, XLenVT, VL, DAG.getConstant(1, DL, XLenVT));\n    }\n\n    SDValue I32Mask = getAllOnesMask(I32VT, I32VL, DL, DAG);\n\n    // Shift the two scalar parts in using SEW=32 slide1up/slide1down\n    // instructions.\n    SDValue Passthru;\n    if (IsMasked)\n      Passthru = DAG.getUNDEF(I32VT);\n    else\n      Passthru = DAG.getBitcast(I32VT, Operands[1]);\n\n    if (IntNo == Intrinsic::riscv_vslide1up ||\n        IntNo == Intrinsic::riscv_vslide1up_mask) {\n      Vec = DAG.getNode(RISCVISD::VSLIDE1UP_VL, DL, I32VT, Passthru, Vec,\n                        ScalarHi, I32Mask, I32VL);\n      Vec = DAG.getNode(RISCVISD::VSLIDE1UP_VL, DL, I32VT, Passthru, Vec,\n                        ScalarLo, I32Mask, I32VL);\n    } else {\n      Vec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32VT, Passthru, Vec,\n                        ScalarLo, I32Mask, I32VL);\n      Vec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32VT, Passthru, Vec,\n                        ScalarHi, I32Mask, I32VL);\n    }\n\n    // Convert back to nxvXi64.\n    Vec = DAG.getBitcast(VT, Vec);\n\n    if (!IsMasked)\n      return Vec;\n    // Apply mask after the operation.\n    SDValue Mask = Operands[NumOps - 3];\n    SDValue MaskedOff = Operands[1];\n    // Assume Policy operand is the last operand.\n    uint64_t Policy = Operands[NumOps - 1]->getAsZExtVal();\n    // We don't need to select maskedoff if it's undef.\n    if (MaskedOff.isUndef())\n      return Vec;\n    // TAMU\n    if (Policy == RISCVII::TAIL_AGNOSTIC)\n      return DAG.getNode(RISCVISD::VMERGE_VL, DL, VT, Mask, Vec, MaskedOff,\n                         DAG.getUNDEF(VT), AVL);\n    // TUMA or TUMU: Currently we always emit tumu policy regardless of tuma.\n    // It's fine because vmerge does not care mask policy.\n    return DAG.getNode(RISCVISD::VMERGE_VL, DL, VT, Mask, Vec, MaskedOff,\n                       MaskedOff, AVL);\n  }\n  }\n\n  // We need to convert the scalar to a splat vector.\n  SDValue VL = getVLOperand(Op);\n  assert(VL.getValueType() == XLenVT);\n  ScalarOp = splatSplitI64WithVL(DL, VT, SDValue(), ScalarOp, VL, DAG);\n  return DAG.getNode(Op->getOpcode(), DL, Op->getVTList(), Operands);\n}",
      "start_line": 8228,
      "end_line": 8404,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "lowerVectorIntrinsicScalars",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "these when the scalar is larger than XLen.\n    unsigned NumOps = Op.getNumOperands();\n    bool IsMasked = NumOps == 7;\n\n    // Convert the vector source to the equivalent nxvXi32 vector.\n    MVT I32VT = MVT",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getAsZExtVal",
        "getLMUL",
        "getConstant",
        "getSimpleValueType",
        "getBitcast",
        "between",
        "Operands",
        "computeVLMAXBounds",
        "op_end",
        "getConstantOperandVal",
        "getNumOperands",
        "getRISCVVIntrinsicInfo",
        "getUNDEF",
        "getVectorVT",
        "getVLOperand",
        "splatSplitI64WithVL",
        "encodeSEW",
        "SDValue",
        "getVTList",
        "getOpcode",
        "getOperand",
        "tie",
        "getAllOnesMask",
        "SplitScalar",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "AVLInt >= 2 *",
          "name": "MaxVLMAX"
        }
      ],
      "body": "{\n        // Just set vl to VLMAX in this situation\n        RISCVII::VLMUL Lmul = RISCVTargetLowering::getLMUL(I32VT);\n        SDValue LMUL = DAG.getConstant(Lmul, DL, XLenVT);\n        unsigned Sew = RISCVVType::encodeSEW(I32VT.getScalarSizeInBits());\n        SDValue SEW = DAG.getConstant(Sew, DL, XLenVT);\n        SDValue SETVLMAX = DAG.getTargetConstant(\n            Intrinsic::riscv_vsetvlimax, DL, MVT::i32);\n        I32VL = DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, XLenVT, SETVLMAX, SEW,\n                            LMUL);\n      }",
      "start_line": 8321,
      "end_line": 8331,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getLMUL",
        "getConstant",
        "encodeSEW",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerGetVectorLength",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  // The smallest LMUL is only valid for the smallest element width.\n  const unsigned ElementWidth = 8;\n\n  // Determine the VF that corresponds to LMUL 1 for ElementWidth.\n  unsigned LMul1VF = RISCV::RVVBitsPerBlock / ElementWidth;\n  // We don't support VF==1 with ELEN==32.\n  unsigned MinVF = RISCV::RVVBitsPerBlock / Subtarget.getELen();\n\n  unsigned VF = N->getConstantOperandVal(2);\n  assert(VF >= MinVF && VF <= (LMul1VF * 8) && isPowerOf2_32(VF) &&\n         \"Unexpected VF\");\n  (void)MinVF;\n\n  bool Fractional = VF < LMul1VF;\n  unsigned LMulVal = Fractional ? LMul1VF / VF : VF / LMul1VF;\n  unsigned VLMUL = (unsigned)RISCVVType::encodeLMUL(LMulVal, Fractional);\n  unsigned VSEW = RISCVVType::encodeSEW(ElementWidth);\n\n  SDLoc DL(N);\n\n  SDValue LMul = DAG.getTargetConstant(VLMUL, DL, XLenVT);\n  SDValue Sew = DAG.getTargetConstant(VSEW, DL, XLenVT);\n\n  SDValue AVL = DAG.getNode(ISD::ZERO_EXTEND, DL, XLenVT, N->getOperand(1));\n\n  SDValue ID = DAG.getTargetConstant(Intrinsic::riscv_vsetvli, DL, XLenVT);\n  SDValue Res =\n      DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, XLenVT, ID, AVL, Sew, LMul);\n  return DAG.getNode(ISD::TRUNCATE, DL, N->getValueType(0), Res);\n}",
      "start_line": 8416,
      "end_line": 8449,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "encodeLMUL",
        "getConstantOperandVal",
        "getELen",
        "isPowerOf2_32",
        "DL",
        "getXLenVT",
        "encodeSEW",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVCIXOperands",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDValue",
          "name": "&Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SmallVector<SDValue>",
          "name": "&Ops"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n\n  const RISCVSubtarget &Subtarget =\n      DAG.getMachineFunction().getSubtarget<RISCVSubtarget>();\n  for (const SDValue &V : Op->op_values()) {\n    EVT ValType = V.getValueType();\n    if (ValType.isScalableVector() && ValType.isFloatingPoint()) {\n      MVT InterimIVT =\n          MVT::getVectorVT(MVT::getIntegerVT(ValType.getScalarSizeInBits()),\n                           ValType.getVectorElementCount());\n      Ops.push_back(DAG.getBitcast(InterimIVT, V));\n    } else if (ValType.isFixedLengthVector()) {\n      MVT OpContainerVT = getContainerForFixedLengthVector(\n          DAG, V.getSimpleValueType(), Subtarget);\n      Ops.push_back(convertToScalableVector(OpContainerVT, V, DAG, Subtarget));\n    } else\n      Ops.push_back(V);\n  }\n}",
      "start_line": 8451,
      "end_line": 8471,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "isFloatingPoint",
        "getMachineFunction",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getVectorVT",
        "DL",
        "getVectorElementCount",
        "push_back"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isValidEGW",
      "return_type": "bool",
      "parameters": [
        {
          "type": "int",
          "name": "EGS"
        },
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  return (Subtarget.getRealMinVLen() *\n             VT.getSizeInBits().getKnownMinValue()) / RISCV::RVVBitsPerBlock >=\n         EGS * VT.getScalarSizeInBits();\n}",
      "start_line": 8474,
      "end_line": 8479,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getSizeInBits",
        "getKnownMinValue",
        "getScalarSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerINTRINSIC_WO_CHAIN",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  unsigned IntNo = Op.getConstantOperandVal(0);\n  SDLoc DL(Op);\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  switch (IntNo) {\n  default:\n    break; // Don't custom lower most intrinsics.\n  case Intrinsic::thread_pointer: {\n    EVT PtrVT = getPointerTy(DAG.getDataLayout());\n    return DAG.getRegister(RISCV::X4, PtrVT);\n  }\n  case Intrinsic::riscv_orc_b:\n  case Intrinsic::riscv_brev8:\n  case Intrinsic::riscv_sha256sig0:\n  case Intrinsic::riscv_sha256sig1:\n  case Intrinsic::riscv_sha256sum0:\n  case Intrinsic::riscv_sha256sum1:\n  case Intrinsic::riscv_sm3p0:\n  case Intrinsic::riscv_sm3p1: {\n    unsigned Opc;\n    switch (IntNo) {\n    case Intrinsic::riscv_orc_b:      Opc = RISCVISD::ORC_B;      break;\n    case Intrinsic::riscv_brev8:      Opc = RISCVISD::BREV8;      break;\n    case Intrinsic::riscv_sha256sig0: Opc = RISCVISD::SHA256SIG0; break;\n    case Intrinsic::riscv_sha256sig1: Opc = RISCVISD::SHA256SIG1; break;\n    case Intrinsic::riscv_sha256sum0: Opc = RISCVISD::SHA256SUM0; break;\n    case Intrinsic::riscv_sha256sum1: Opc = RISCVISD::SHA256SUM1; break;\n    case Intrinsic::riscv_sm3p0:      Opc = RISCVISD::SM3P0;      break;\n    case Intrinsic::riscv_sm3p1:      Opc = RISCVISD::SM3P1;      break;\n    }\n\n    if (RV64LegalI32 && Subtarget.is64Bit() && Op.getValueType() == MVT::i32) {\n      SDValue NewOp =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op.getOperand(1));\n      SDValue Res = DAG.getNode(Opc, DL, MVT::i64, NewOp);\n      return DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res);\n    }\n\n    return DAG.getNode(Opc, DL, XLenVT, Op.getOperand(1));\n  }\n  case Intrinsic::riscv_sm4ks:\n  case Intrinsic::riscv_sm4ed: {\n    unsigned Opc =\n        IntNo == Intrinsic::riscv_sm4ks ? RISCVISD::SM4KS : RISCVISD::SM4ED;\n\n    if (RV64LegalI32 && Subtarget.is64Bit() && Op.getValueType() == MVT::i32) {\n      SDValue NewOp0 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op.getOperand(1));\n      SDValue NewOp1 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op.getOperand(2));\n      SDValue Res =\n          DAG.getNode(Opc, DL, MVT::i64, NewOp0, NewOp1, Op.getOperand(3));\n      return DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res);\n    }\n\n    return DAG.getNode(Opc, DL, XLenVT, Op.getOperand(1), Op.getOperand(2),\n                       Op.getOperand(3));\n  }\n  case Intrinsic::riscv_zip:\n  case Intrinsic::riscv_unzip: {\n    unsigned Opc =\n        IntNo == Intrinsic::riscv_zip ? RISCVISD::ZIP : RISCVISD::UNZIP;\n    return DAG.getNode(Opc, DL, XLenVT, Op.getOperand(1));\n  }\n  case Intrinsic::riscv_clmul:\n    if (RV64LegalI32 && Subtarget.is64Bit() && Op.getValueType() == MVT::i32) {\n      SDValue NewOp0 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op.getOperand(1));\n      SDValue NewOp1 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op.getOperand(2));\n      SDValue Res = DAG.getNode(RISCVISD::CLMUL, DL, MVT::i64, NewOp0, NewOp1);\n      return DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res);\n    }\n    return DAG.getNode(RISCVISD::CLMUL, DL, XLenVT, Op.getOperand(1),\n                       Op.getOperand(2));\n  case Intrinsic::riscv_clmulh:\n  case Intrinsic::riscv_clmulr: {\n    unsigned Opc =\n        IntNo == Intrinsic::riscv_clmulh ? RISCVISD::CLMULH : RISCVISD::CLMULR;\n    if (RV64LegalI32 && Subtarget.is64Bit() && Op.getValueType() == MVT::i32) {\n      SDValue NewOp0 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op.getOperand(1));\n      SDValue NewOp1 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Op.getOperand(2));\n      NewOp0 = DAG.getNode(ISD::SHL, DL, MVT::i64, NewOp0,\n                           DAG.getConstant(32, DL, MVT::i64));\n      NewOp1 = DAG.getNode(ISD::SHL, DL, MVT::i64, NewOp1,\n                           DAG.getConstant(32, DL, MVT::i64));\n      SDValue Res = DAG.getNode(Opc, DL, MVT::i64, NewOp0, NewOp1);\n      Res = DAG.getNode(ISD::SRL, DL, MVT::i64, Res,\n                        DAG.getConstant(32, DL, MVT::i64));\n      return DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res);\n    }\n\n    return DAG.getNode(Opc, DL, XLenVT, Op.getOperand(1), Op.getOperand(2));\n  }\n  case Intrinsic::experimental_get_vector_length:\n    return lowerGetVectorLength(Op.getNode(), DAG, Subtarget);\n  case Intrinsic::riscv_vmv_x_s: {\n    SDValue Res = DAG.getNode(RISCVISD::VMV_X_S, DL, XLenVT, Op.getOperand(1));\n    return DAG.getNode(ISD::TRUNCATE, DL, Op.getValueType(), Res);\n  }\n  case Intrinsic::riscv_vfmv_f_s:\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, Op.getValueType(),\n                       Op.getOperand(1), DAG.getConstant(0, DL, XLenVT));\n  case Intrinsic::riscv_vmv_v_x:\n    return lowerScalarSplat(Op.getOperand(1), Op.getOperand(2),\n                            Op.getOperand(3), Op.getSimpleValueType(), DL, DAG,\n                            Subtarget);\n  case Intrinsic::riscv_vfmv_v_f:\n    return DAG.getNode(RISCVISD::VFMV_V_F_VL, DL, Op.getValueType(),\n                       Op.getOperand(1), Op.getOperand(2), Op.getOperand(3));\n  case Intrinsic::riscv_vmv_s_x: {\n    SDValue Scalar = Op.getOperand(2);\n\n    if (Scalar.getValueType().bitsLE(XLenVT)) {\n      Scalar = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Scalar);\n      return DAG.getNode(RISCVISD::VMV_S_X_VL, DL, Op.getValueType(),\n                         Op.getOperand(1), Scalar, Op.getOperand(3));\n    }\n\n    assert(Scalar.getValueType() == MVT::i64 && \"Unexpected scalar VT!\");\n\n    // This is an i64 value that lives in two scalar registers. We have to\n    // insert this in a convoluted way. First we build vXi64 splat containing\n    // the two values that we assemble using some bit math. Next we'll use\n    // vid.v and vmseq to build a mask with bit 0 set. Then we'll use that mask\n    // to merge element 0 from our splat into the source vector.\n    // FIXME: This is probably not the best way to do this, but it is\n    // consistent with INSERT_VECTOR_ELT lowering so it is a good starting\n    // point.\n    //   sw lo, (a0)\n    //   sw hi, 4(a0)\n    //   vlse vX, (a0)\n    //\n    //   vid.v      vVid\n    //   vmseq.vx   mMask, vVid, 0\n    //   vmerge.vvm vDest, vSrc, vVal, mMask\n    MVT VT = Op.getSimpleValueType();\n    SDValue Vec = Op.getOperand(1);\n    SDValue VL = getVLOperand(Op);\n\n    SDValue SplattedVal = splatSplitI64WithVL(DL, VT, SDValue(), Scalar, VL, DAG);\n    if (Op.getOperand(1).isUndef())\n      return SplattedVal;\n    SDValue SplattedIdx =\n        DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, DAG.getUNDEF(VT),\n                    DAG.getConstant(0, DL, MVT::i32), VL);\n\n    MVT MaskVT = getMaskTypeFor(VT);\n    SDValue Mask = getAllOnesMask(VT, VL, DL, DAG);\n    SDValue VID = DAG.getNode(RISCVISD::VID_VL, DL, VT, Mask, VL);\n    SDValue SelectCond =\n        DAG.getNode(RISCVISD::SETCC_VL, DL, MaskVT,\n                    {VID, SplattedIdx, DAG.getCondCode(ISD::SETEQ),\n                     DAG.getUNDEF(MaskVT), Mask, VL});\n    return DAG.getNode(RISCVISD::VMERGE_VL, DL, VT, SelectCond, SplattedVal,\n                       Vec, DAG.getUNDEF(VT), VL);\n  }\n  case Intrinsic::riscv_vfmv_s_f:\n    return DAG.getNode(RISCVISD::VFMV_S_F_VL, DL, Op.getSimpleValueType(),\n                       Op.getOperand(1), Op.getOperand(2), Op.getOperand(3));\n  // EGS * EEW >= 128 bits\n  case Intrinsic::riscv_vaesdf_vv:\n  case Intrinsic::riscv_vaesdf_vs:\n  case Intrinsic::riscv_vaesdm_vv:\n  case Intrinsic::riscv_vaesdm_vs:\n  case Intrinsic::riscv_vaesef_vv:\n  case Intrinsic::riscv_vaesef_vs:\n  case Intrinsic::riscv_vaesem_vv:\n  case Intrinsic::riscv_vaesem_vs:\n  case Intrinsic::riscv_vaeskf1:\n  case Intrinsic::riscv_vaeskf2:\n  case Intrinsic::riscv_vaesz_vs:\n  case Intrinsic::riscv_vsm4k:\n  case Intrinsic::riscv_vsm4r_vv:\n  case Intrinsic::riscv_vsm4r_vs: {\n    if (!isValidEGW(4, Op.getSimpleValueType(), Subtarget) ||\n        !isValidEGW(4, Op->getOperand(1).getSimpleValueType(), Subtarget) ||\n        !isValidEGW(4, Op->getOperand(2).getSimpleValueType(), Subtarget))\n      report_fatal_error(\"EGW should be greater than or equal to 4 * SEW.\");\n    return Op;\n  }\n  // EGS * EEW >= 256 bits\n  case Intrinsic::riscv_vsm3c:\n  case Intrinsic::riscv_vsm3me: {\n    if (!isValidEGW(8, Op.getSimpleValueType(), Subtarget) ||\n        !isValidEGW(8, Op->getOperand(1).getSimpleValueType(), Subtarget))\n      report_fatal_error(\"EGW should be greater than or equal to 8 * SEW.\");\n    return Op;\n  }\n  // zvknha(SEW=32)/zvknhb(SEW=[32|64])\n  case Intrinsic::riscv_vsha2ch:\n  case Intrinsic::riscv_vsha2cl:\n  case Intrinsic::riscv_vsha2ms: {\n    if (Op->getSimpleValueType(0).getScalarSizeInBits() == 64 &&\n        !Subtarget.hasStdExtZvknhb())\n      report_fatal_error(\"SEW=64 needs Zvknhb to be enabled.\");\n    if (!isValidEGW(4, Op.getSimpleValueType(), Subtarget) ||\n        !isValidEGW(4, Op->getOperand(1).getSimpleValueType(), Subtarget) ||\n        !isValidEGW(4, Op->getOperand(2).getSimpleValueType(), Subtarget))\n      report_fatal_error(\"EGW should be greater than or equal to 4 * SEW.\");\n    return Op;\n  }\n  case Intrinsic::riscv_sf_vc_v_x:\n  case Intrinsic::riscv_sf_vc_v_i:\n  case Intrinsic::riscv_sf_vc_v_xv:\n  case Intrinsic::riscv_sf_vc_v_iv:\n  case Intrinsic::riscv_sf_vc_v_vv:\n  case Intrinsic::riscv_sf_vc_v_fv:\n  case Intrinsic::riscv_sf_vc_v_xvv:\n  case Intrinsic::riscv_sf_vc_v_ivv:\n  case Intrinsic::riscv_sf_vc_v_vvv:\n  case Intrinsic::riscv_sf_vc_v_fvv:\n  case Intrinsic::riscv_sf_vc_v_xvw:\n  case Intrinsic::riscv_sf_vc_v_ivw:\n  case Intrinsic::riscv_sf_vc_v_vvw:\n  case Intrinsic::riscv_sf_vc_v_fvw: {\n    MVT VT = Op.getSimpleValueType();\n\n    SmallVector<SDValue> Ops;\n    getVCIXOperands(Op, DAG, Ops);\n\n    MVT RetVT = VT;\n    if (VT.isFixedLengthVector())\n      RetVT = getContainerForFixedLengthVector(VT);\n    else if (VT.isFloatingPoint())\n      RetVT = MVT::getVectorVT(MVT::getIntegerVT(VT.getScalarSizeInBits()),\n                               VT.getVectorElementCount());\n\n    SDValue NewNode = DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, RetVT, Ops);\n\n    if (VT.isFixedLengthVector())\n      NewNode = convertFromScalableVector(VT, NewNode, DAG, Subtarget);\n    else if (VT.isFloatingPoint())\n      NewNode = DAG.getBitcast(VT, NewNode);\n\n    if (Op == NewNode)\n      break;\n\n    return NewNode;\n  }\n  }\n\n  return lowerVectorIntrinsicScalars(Op, DAG, Subtarget);\n}",
      "start_line": 8481,
      "end_line": 8728,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "LowerINTRINSIC_WO_CHAIN",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "LowerINTRINSIC_WO_CHAIN",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "lowerVectorIntrinsicScalars",
        "bitsLE",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getVCIXOperands",
        "getBitcast",
        "zvknhb",
        "getScalarSizeInBits",
        "getConstantOperandVal",
        "4",
        "isValidEGW",
        "isUndef",
        "getUNDEF",
        "getVectorVT",
        "lowerScalarSplat",
        "getVectorElementCount",
        "getVLOperand",
        "splatSplitI64WithVL",
        "getPointerTy",
        "getRegister",
        "getValueType",
        "hasStdExtZvknhb",
        "getContainerForFixedLengthVector",
        "getOperand",
        "lowerGetVectorLength",
        "getAllOnesMask",
        "getMaskTypeFor",
        "DL",
        "zvknha",
        "getXLenVT",
        "report_fatal_error",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerINTRINSIC_W_CHAIN",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  unsigned IntNo = Op.getConstantOperandVal(1);\n  switch (IntNo) {\n  default:\n    break;\n  case Intrinsic::riscv_masked_strided_load: {\n    SDLoc DL(Op);\n    MVT XLenVT = Subtarget.getXLenVT();\n\n    // If the mask is known to be all ones, optimize to an unmasked intrinsic;\n    // the selection of the masked intrinsics doesn't do this for us.\n    SDValue Mask = Op.getOperand(5);\n    bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n    MVT VT = Op->getSimpleValueType(0);\n    MVT ContainerVT = VT;\n    if (VT.isFixedLengthVector())\n      ContainerVT = getContainerForFixedLengthVector(VT);\n\n    SDValue PassThru = Op.getOperand(2);\n    if (!IsUnmasked) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      if (VT.isFixedLengthVector()) {\n        Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n        PassThru = convertToScalableVector(ContainerVT, PassThru, DAG, Subtarget);\n      }\n    }\n\n    auto *Load = cast<MemIntrinsicSDNode>(Op);\n    SDValue VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n    SDValue Ptr = Op.getOperand(3);\n    SDValue Stride = Op.getOperand(4);\n    SDValue Result, Chain;\n\n    // TODO: We restrict this to unmasked loads currently in consideration of\n    // the complexity of hanlding all falses masks.\n    if (IsUnmasked && isNullConstant(Stride)) {\n      MVT ScalarVT = ContainerVT.getVectorElementType();\n      SDValue ScalarLoad =\n          DAG.getExtLoad(ISD::ZEXTLOAD, DL, XLenVT, Load->getChain(), Ptr,\n                         ScalarVT, Load->getMemOperand());\n      Chain = ScalarLoad.getValue(1);\n      Result = lowerScalarSplat(SDValue(), ScalarLoad, VL, ContainerVT, DL, DAG,\n                                Subtarget);\n    } else {\n      SDValue IntID = DAG.getTargetConstant(\n          IsUnmasked ? Intrinsic::riscv_vlse : Intrinsic::riscv_vlse_mask, DL,\n          XLenVT);\n\n      SmallVector<SDValue, 8> Ops{Load->getChain(), IntID};\n      if (IsUnmasked)\n        Ops.push_back(DAG.getUNDEF(ContainerVT));\n      else\n        Ops.push_back(PassThru);\n      Ops.push_back(Ptr);\n      Ops.push_back(Stride);\n      if (!IsUnmasked)\n        Ops.push_back(Mask);\n      Ops.push_back(VL);\n      if (!IsUnmasked) {\n        SDValue Policy =\n            DAG.getTargetConstant(RISCVII::TAIL_AGNOSTIC, DL, XLenVT);\n        Ops.push_back(Policy);\n      }\n\n      SDVTList VTs = DAG.getVTList({ContainerVT, MVT::Other});\n      Result =\n          DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops,\n                                  Load->getMemoryVT(), Load->getMemOperand());\n      Chain = Result.getValue(1);\n    }\n    if (VT.isFixedLengthVector())\n      Result = convertFromScalableVector(VT, Result, DAG, Subtarget);\n    return DAG.getMergeValues({Result, Chain}, DL);\n  }\n  case Intrinsic::riscv_seg2_load:\n  case Intrinsic::riscv_seg3_load:\n  case Intrinsic::riscv_seg4_load:\n  case Intrinsic::riscv_seg5_load:\n  case Intrinsic::riscv_seg6_load:\n  case Intrinsic::riscv_seg7_load:\n  case Intrinsic::riscv_seg8_load: {\n    SDLoc DL(Op);\n    static const Intrinsic::ID VlsegInts[7] = {\n        Intrinsic::riscv_vlseg2, Intrinsic::riscv_vlseg3,\n        Intrinsic::riscv_vlseg4, Intrinsic::riscv_vlseg5,\n        Intrinsic::riscv_vlseg6, Intrinsic::riscv_vlseg7,\n        Intrinsic::riscv_vlseg8};\n    unsigned NF = Op->getNumValues() - 1;\n    assert(NF >= 2 && NF <= 8 && \"Unexpected seg number\");\n    MVT XLenVT = Subtarget.getXLenVT();\n    MVT VT = Op->getSimpleValueType(0);\n    MVT ContainerVT = getContainerForFixedLengthVector(VT);\n\n    SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG,\n                         Subtarget);\n    SDValue IntID = DAG.getTargetConstant(VlsegInts[NF - 2], DL, XLenVT);\n    auto *Load = cast<MemIntrinsicSDNode>(Op);\n    SmallVector<EVT, 9> ContainerVTs(NF, ContainerVT);\n    ContainerVTs.push_back(MVT::Other);\n    SDVTList VTs = DAG.getVTList(ContainerVTs);\n    SmallVector<SDValue, 12> Ops = {Load->getChain(), IntID};\n    Ops.insert(Ops.end(), NF, DAG.getUNDEF(ContainerVT));\n    Ops.push_back(Op.getOperand(2));\n    Ops.push_back(VL);\n    SDValue Result =\n        DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops,\n                                Load->getMemoryVT(), Load->getMemOperand());\n    SmallVector<SDValue, 9> Results;\n    for (unsigned int RetIdx = 0; RetIdx < NF; RetIdx++)\n      Results.push_back(convertFromScalableVector(VT, Result.getValue(RetIdx),\n                                                  DAG, Subtarget));\n    Results.push_back(Result.getValue(NF));\n    return DAG.getMergeValues(Results, DL);\n  }\n  case Intrinsic::riscv_sf_vc_v_x_se:\n  case Intrinsic::riscv_sf_vc_v_i_se:\n  case Intrinsic::riscv_sf_vc_v_xv_se:\n  case Intrinsic::riscv_sf_vc_v_iv_se:\n  case Intrinsic::riscv_sf_vc_v_vv_se:\n  case Intrinsic::riscv_sf_vc_v_fv_se:\n  case Intrinsic::riscv_sf_vc_v_xvv_se:\n  case Intrinsic::riscv_sf_vc_v_ivv_se:\n  case Intrinsic::riscv_sf_vc_v_vvv_se:\n  case Intrinsic::riscv_sf_vc_v_fvv_se:\n  case Intrinsic::riscv_sf_vc_v_xvw_se:\n  case Intrinsic::riscv_sf_vc_v_ivw_se:\n  case Intrinsic::riscv_sf_vc_v_vvw_se:\n  case Intrinsic::riscv_sf_vc_v_fvw_se: {\n    MVT VT = Op.getSimpleValueType();\n    SDLoc DL(Op);\n    SmallVector<SDValue> Ops;\n    getVCIXOperands(Op, DAG, Ops);\n\n    MVT RetVT = VT;\n    if (VT.isFixedLengthVector())\n      RetVT = getContainerForFixedLengthVector(VT);\n    else if (VT.isFloatingPoint())\n      RetVT = MVT::getVectorVT(MVT::getIntegerVT(RetVT.getScalarSizeInBits()),\n                               RetVT.getVectorElementCount());\n\n    SDVTList VTs = DAG.getVTList({RetVT, MVT::Other});\n    SDValue NewNode = DAG.getNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops);\n\n    if (VT.isFixedLengthVector()) {\n      SDValue FixedVector =\n          convertFromScalableVector(VT, NewNode, DAG, Subtarget);\n      NewNode = DAG.getMergeValues({FixedVector, NewNode.getValue(1)}, DL);\n    } else if (VT.isFloatingPoint()) {\n      SDValue BitCast = DAG.getBitcast(VT, NewNode.getValue(0));\n      NewNode = DAG.getMergeValues({BitCast, NewNode.getValue(1)}, DL);\n    }\n\n    if (Op == NewNode)\n      break;\n\n    return NewNode;\n  }\n  }\n\n  return lowerVectorIntrinsicScalars(Op, DAG, Subtarget);\n}",
      "start_line": 8730,
      "end_line": 8892,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "LowerINTRINSIC_W_CHAIN",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getDefaultVLOps",
        "lowerVectorIntrinsicScalars",
        "getChain",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getVCIXOperands",
        "getBitcast",
        "getConstantOperandVal",
        "getUNDEF",
        "getMemIntrinsicNode",
        "getVectorElementType",
        "getVectorVT",
        "lowerScalarSplat",
        "getVectorElementCount",
        "push_back",
        "getNumValues",
        "getMemOperand",
        "getExtLoad",
        "getValue",
        "getVTList",
        "getContainerForFixedLengthVector",
        "getOperand",
        "getVLOp",
        "insert",
        "isConstantSplatVectorAllOnes",
        "getMaskTypeFor",
        "getMergeValues",
        "ContainerVTs",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerINTRINSIC_VOID",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  unsigned IntNo = Op.getConstantOperandVal(1);\n  switch (IntNo) {\n  default:\n    break;\n  case Intrinsic::riscv_masked_strided_store: {\n    SDLoc DL(Op);\n    MVT XLenVT = Subtarget.getXLenVT();\n\n    // If the mask is known to be all ones, optimize to an unmasked intrinsic;\n    // the selection of the masked intrinsics doesn't do this for us.\n    SDValue Mask = Op.getOperand(5);\n    bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n    SDValue Val = Op.getOperand(2);\n    MVT VT = Val.getSimpleValueType();\n    MVT ContainerVT = VT;\n    if (VT.isFixedLengthVector()) {\n      ContainerVT = getContainerForFixedLengthVector(VT);\n      Val = convertToScalableVector(ContainerVT, Val, DAG, Subtarget);\n    }\n    if (!IsUnmasked) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      if (VT.isFixedLengthVector())\n        Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n\n    SDValue VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n\n    SDValue IntID = DAG.getTargetConstant(\n        IsUnmasked ? Intrinsic::riscv_vsse : Intrinsic::riscv_vsse_mask, DL,\n        XLenVT);\n\n    auto *Store = cast<MemIntrinsicSDNode>(Op);\n    SmallVector<SDValue, 8> Ops{Store->getChain(), IntID};\n    Ops.push_back(Val);\n    Ops.push_back(Op.getOperand(3)); // Ptr\n    Ops.push_back(Op.getOperand(4)); // Stride\n    if (!IsUnmasked)\n      Ops.push_back(Mask);\n    Ops.push_back(VL);\n\n    return DAG.getMemIntrinsicNode(ISD::INTRINSIC_VOID, DL, Store->getVTList(),\n                                   Ops, Store->getMemoryVT(),\n                                   Store->getMemOperand());\n  }\n  case Intrinsic::riscv_seg2_store:\n  case Intrinsic::riscv_seg3_store:\n  case Intrinsic::riscv_seg4_store:\n  case Intrinsic::riscv_seg5_store:\n  case Intrinsic::riscv_seg6_store:\n  case Intrinsic::riscv_seg7_store:\n  case Intrinsic::riscv_seg8_store: {\n    SDLoc DL(Op);\n    static const Intrinsic::ID VssegInts[] = {\n        Intrinsic::riscv_vsseg2, Intrinsic::riscv_vsseg3,\n        Intrinsic::riscv_vsseg4, Intrinsic::riscv_vsseg5,\n        Intrinsic::riscv_vsseg6, Intrinsic::riscv_vsseg7,\n        Intrinsic::riscv_vsseg8};\n    // Operands are (chain, int_id, vec*, ptr, vl)\n    unsigned NF = Op->getNumOperands() - 4;\n    assert(NF >= 2 && NF <= 8 && \"Unexpected seg number\");\n    MVT XLenVT = Subtarget.getXLenVT();\n    MVT VT = Op->getOperand(2).getSimpleValueType();\n    MVT ContainerVT = getContainerForFixedLengthVector(VT);\n\n    SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG,\n                         Subtarget);\n    SDValue IntID = DAG.getTargetConstant(VssegInts[NF - 2], DL, XLenVT);\n    SDValue Ptr = Op->getOperand(NF + 2);\n\n    auto *FixedIntrinsic = cast<MemIntrinsicSDNode>(Op);\n    SmallVector<SDValue, 12> Ops = {FixedIntrinsic->getChain(), IntID};\n    for (unsigned i = 0; i < NF; i++)\n      Ops.push_back(convertToScalableVector(\n          ContainerVT, FixedIntrinsic->getOperand(2 + i), DAG, Subtarget));\n    Ops.append({Ptr, VL});\n\n    return DAG.getMemIntrinsicNode(\n        ISD::INTRINSIC_VOID, DL, DAG.getVTList(MVT::Other), Ops,\n        FixedIntrinsic->getMemoryVT(), FixedIntrinsic->getMemOperand());\n  }\n  case Intrinsic::riscv_sf_vc_x_se_e8mf8:\n  case Intrinsic::riscv_sf_vc_x_se_e8mf4:\n  case Intrinsic::riscv_sf_vc_x_se_e8mf2:\n  case Intrinsic::riscv_sf_vc_x_se_e8m1:\n  case Intrinsic::riscv_sf_vc_x_se_e8m2:\n  case Intrinsic::riscv_sf_vc_x_se_e8m4:\n  case Intrinsic::riscv_sf_vc_x_se_e8m8:\n  case Intrinsic::riscv_sf_vc_x_se_e16mf4:\n  case Intrinsic::riscv_sf_vc_x_se_e16mf2:\n  case Intrinsic::riscv_sf_vc_x_se_e16m1:\n  case Intrinsic::riscv_sf_vc_x_se_e16m2:\n  case Intrinsic::riscv_sf_vc_x_se_e16m4:\n  case Intrinsic::riscv_sf_vc_x_se_e16m8:\n  case Intrinsic::riscv_sf_vc_x_se_e32mf2:\n  case Intrinsic::riscv_sf_vc_x_se_e32m1:\n  case Intrinsic::riscv_sf_vc_x_se_e32m2:\n  case Intrinsic::riscv_sf_vc_x_se_e32m4:\n  case Intrinsic::riscv_sf_vc_x_se_e32m8:\n  case Intrinsic::riscv_sf_vc_x_se_e64m1:\n  case Intrinsic::riscv_sf_vc_x_se_e64m2:\n  case Intrinsic::riscv_sf_vc_x_se_e64m4:\n  case Intrinsic::riscv_sf_vc_x_se_e64m8:\n  case Intrinsic::riscv_sf_vc_i_se_e8mf8:\n  case Intrinsic::riscv_sf_vc_i_se_e8mf4:\n  case Intrinsic::riscv_sf_vc_i_se_e8mf2:\n  case Intrinsic::riscv_sf_vc_i_se_e8m1:\n  case Intrinsic::riscv_sf_vc_i_se_e8m2:\n  case Intrinsic::riscv_sf_vc_i_se_e8m4:\n  case Intrinsic::riscv_sf_vc_i_se_e8m8:\n  case Intrinsic::riscv_sf_vc_i_se_e16mf4:\n  case Intrinsic::riscv_sf_vc_i_se_e16mf2:\n  case Intrinsic::riscv_sf_vc_i_se_e16m1:\n  case Intrinsic::riscv_sf_vc_i_se_e16m2:\n  case Intrinsic::riscv_sf_vc_i_se_e16m4:\n  case Intrinsic::riscv_sf_vc_i_se_e16m8:\n  case Intrinsic::riscv_sf_vc_i_se_e32mf2:\n  case Intrinsic::riscv_sf_vc_i_se_e32m1:\n  case Intrinsic::riscv_sf_vc_i_se_e32m2:\n  case Intrinsic::riscv_sf_vc_i_se_e32m4:\n  case Intrinsic::riscv_sf_vc_i_se_e32m8:\n  case Intrinsic::riscv_sf_vc_i_se_e64m1:\n  case Intrinsic::riscv_sf_vc_i_se_e64m2:\n  case Intrinsic::riscv_sf_vc_i_se_e64m4:\n  case Intrinsic::riscv_sf_vc_i_se_e64m8:\n  case Intrinsic::riscv_sf_vc_xv_se:\n  case Intrinsic::riscv_sf_vc_iv_se:\n  case Intrinsic::riscv_sf_vc_vv_se:\n  case Intrinsic::riscv_sf_vc_fv_se:\n  case Intrinsic::riscv_sf_vc_xvv_se:\n  case Intrinsic::riscv_sf_vc_ivv_se:\n  case Intrinsic::riscv_sf_vc_vvv_se:\n  case Intrinsic::riscv_sf_vc_fvv_se:\n  case Intrinsic::riscv_sf_vc_xvw_se:\n  case Intrinsic::riscv_sf_vc_ivw_se:\n  case Intrinsic::riscv_sf_vc_vvw_se:\n  case Intrinsic::riscv_sf_vc_fvw_se: {\n    SmallVector<SDValue> Ops;\n    getVCIXOperands(Op, DAG, Ops);\n\n    SDValue NewNode =\n        DAG.getNode(ISD::INTRINSIC_VOID, SDLoc(Op), Op->getVTList(), Ops);\n\n    if (Op == NewNode)\n      break;\n\n    return NewNode;\n  }\n  }\n\n  return lowerVectorIntrinsicScalars(Op, DAG, Subtarget);\n}",
      "start_line": 8894,
      "end_line": 9047,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "LowerINTRINSIC_VOID",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getDefaultVLOps",
        "lowerVectorIntrinsicScalars",
        "getChain",
        "getMemoryVT",
        "convertToScalableVector",
        "getSimpleValueType",
        "getVCIXOperands",
        "getConstantOperandVal",
        "getNumOperands",
        "getMemIntrinsicNode",
        "are",
        "push_back",
        "getMemOperand",
        "getVTList",
        "getContainerForFixedLengthVector",
        "append",
        "getOperand",
        "getVLOp",
        "isConstantSplatVectorAllOnes",
        "getMaskTypeFor",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRVVReductionOp",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "ISDOpcode"
        }
      ],
      "body": "{\n  switch (ISDOpcode) {\n  default:\n    llvm_unreachable(\"Unhandled reduction\");\n  case ISD::VP_REDUCE_ADD:\n  case ISD::VECREDUCE_ADD:\n    return RISCVISD::VECREDUCE_ADD_VL;\n  case ISD::VP_REDUCE_UMAX:\n  case ISD::VECREDUCE_UMAX:\n    return RISCVISD::VECREDUCE_UMAX_VL;\n  case ISD::VP_REDUCE_SMAX:\n  case ISD::VECREDUCE_SMAX:\n    return RISCVISD::VECREDUCE_SMAX_VL;\n  case ISD::VP_REDUCE_UMIN:\n  case ISD::VECREDUCE_UMIN:\n    return RISCVISD::VECREDUCE_UMIN_VL;\n  case ISD::VP_REDUCE_SMIN:\n  case ISD::VECREDUCE_SMIN:\n    return RISCVISD::VECREDUCE_SMIN_VL;\n  case ISD::VP_REDUCE_AND:\n  case ISD::VECREDUCE_AND:\n    return RISCVISD::VECREDUCE_AND_VL;\n  case ISD::VP_REDUCE_OR:\n  case ISD::VECREDUCE_OR:\n    return RISCVISD::VECREDUCE_OR_VL;\n  case ISD::VP_REDUCE_XOR:\n  case ISD::VECREDUCE_XOR:\n    return RISCVISD::VECREDUCE_XOR_VL;\n  case ISD::VP_REDUCE_FADD:\n    return RISCVISD::VECREDUCE_FADD_VL;\n  case ISD::VP_REDUCE_SEQ_FADD:\n    return RISCVISD::VECREDUCE_SEQ_FADD_VL;\n  case ISD::VP_REDUCE_FMAX:\n    return RISCVISD::VECREDUCE_FMAX_VL;\n  case ISD::VP_REDUCE_FMIN:\n    return RISCVISD::VECREDUCE_FMIN_VL;\n  }\n\n}",
      "start_line": 9049,
      "end_line": 9087,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getRVVReductionOp",
          "condition": "ISDOpcode",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorMaskVecReduction",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "bool",
          "name": "IsVP"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue Vec = Op.getOperand(IsVP ? 1 : 0);\n  MVT VecVT = Vec.getSimpleValueType();\n  assert((Op.getOpcode() == ISD::VECREDUCE_AND ||\n          Op.getOpcode() == ISD::VECREDUCE_OR ||\n          Op.getOpcode() == ISD::VECREDUCE_XOR ||\n          Op.getOpcode() == ISD::VP_REDUCE_AND ||\n          Op.getOpcode() == ISD::VP_REDUCE_OR ||\n          Op.getOpcode() == ISD::VP_REDUCE_XOR) &&\n         \"Unexpected reduction lowering\");\n\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  MVT ContainerVT = VecVT;\n  if (VecVT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VecVT);\n    Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n  }\n\n  SDValue Mask, VL;\n  if (IsVP) {\n    Mask = Op.getOperand(2);\n    VL = Op.getOperand(3);\n  } else {\n    std::tie(Mask, VL) =\n        getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget);\n  }\n\n  unsigned BaseOpc;\n  ISD::CondCode CC;\n  SDValue Zero = DAG.getConstant(0, DL, XLenVT);\n\n  switch (Op.getOpcode()) {\n  default:\n    llvm_unreachable(\"Unhandled reduction\");\n  case ISD::VECREDUCE_AND:\n  case ISD::VP_REDUCE_AND: {\n    // vcpop ~x == 0\n    SDValue TrueMask = DAG.getNode(RISCVISD::VMSET_VL, DL, ContainerVT, VL);\n    Vec = DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Vec, TrueMask, VL);\n    Vec = DAG.getNode(RISCVISD::VCPOP_VL, DL, XLenVT, Vec, Mask, VL);\n    CC = ISD::SETEQ;\n    BaseOpc = ISD::AND;\n    break;\n  }\n  case ISD::VECREDUCE_OR:\n  case ISD::VP_REDUCE_OR:\n    // vcpop x != 0\n    Vec = DAG.getNode(RISCVISD::VCPOP_VL, DL, XLenVT, Vec, Mask, VL);\n    CC = ISD::SETNE;\n    BaseOpc = ISD::OR;\n    break;\n  case ISD::VECREDUCE_XOR:\n  case ISD::VP_REDUCE_XOR: {\n    // ((vcpop x) & 1) != 0\n    SDValue One = DAG.getConstant(1, DL, XLenVT);\n    Vec = DAG.getNode(RISCVISD::VCPOP_VL, DL, XLenVT, Vec, Mask, VL);\n    Vec = DAG.getNode(ISD::AND, DL, XLenVT, Vec, One);\n    CC = ISD::SETNE;\n    BaseOpc = ISD::XOR;\n    break;\n  }\n  }\n\n  SDValue SetCC = DAG.getSetCC(DL, XLenVT, Vec, Zero, CC);\n  SetCC = DAG.getNode(ISD::TRUNCATE, DL, Op.getValueType(), SetCC);\n\n  if (!IsVP)\n    return SetCC;\n\n  // Now include the start value in the operation.\n  // Note that we must return the start value when no elements are operated\n  // upon. The vcpop instructions we've emitted in each case above will return\n  // 0 for an inactive vector, and so we've already received the neutral value:\n  // AND gives us (0 == 0) -> 1 and OR/XOR give us (0 != 0) -> 0. Therefore we\n  // can simply include the start value.\n  return DAG.getNode(BaseOpc, DL, Op.getValueType(), SetCC, Op.getOperand(0));\n}",
      "start_line": 9089,
      "end_line": 9169,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getSetCC",
        "us",
        "getOpcode",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "getOperand",
        "DL",
        "tie",
        "getXLenVT",
        "llvm_unreachable",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isNonZeroAVL",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "AVL"
        }
      ],
      "body": "{\n  auto *RegisterAVL = dyn_cast<RegisterSDNode>(AVL);\n  auto *ImmAVL = dyn_cast<ConstantSDNode>(AVL);\n  return (RegisterAVL && RegisterAVL->getReg() == RISCV::X0) ||\n         (ImmAVL && ImmAVL->getZExtValue() >= 1);\n}",
      "start_line": 9171,
      "end_line": 9176,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getZExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerReductionSeq",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "unsigned",
          "name": "RVVOpcode"
        },
        {
          "type": "MVT",
          "name": "ResVT"
        },
        {
          "type": "SDValue",
          "name": "StartValue"
        },
        {
          "type": "SDValue",
          "name": "Vec"
        },
        {
          "type": "SDValue",
          "name": "Mask"
        },
        {
          "type": "SDValue",
          "name": "VL"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  const MVT VecVT = Vec.getSimpleValueType();\n  const MVT M1VT = getLMUL1VT(VecVT);\n  const MVT XLenVT = Subtarget.getXLenVT();\n  const bool NonZeroAVL = isNonZeroAVL(VL);\n\n  // The reduction needs an LMUL1 input; do the splat at either LMUL1\n  // or the original VT if fractional.\n  auto InnerVT = VecVT.bitsLE(M1VT) ? VecVT : M1VT;\n  // We reuse the VL of the reduction to reduce vsetvli toggles if we can\n  // prove it is non-zero.  For the AVL=0 case, we need the scalar to\n  // be the result of the reduction operation.\n  auto InnerVL = NonZeroAVL ? VL : DAG.getConstant(1, DL, XLenVT);\n  SDValue InitialValue = lowerScalarInsert(StartValue, InnerVL, InnerVT, DL,\n                                           DAG, Subtarget);\n  if (M1VT != InnerVT)\n    InitialValue = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, M1VT,\n                               DAG.getUNDEF(M1VT),\n                               InitialValue, DAG.getConstant(0, DL, XLenVT));\n  SDValue PassThru = NonZeroAVL ? DAG.getUNDEF(M1VT) : InitialValue;\n  SDValue Policy = DAG.getTargetConstant(RISCVII::TAIL_AGNOSTIC, DL, XLenVT);\n  SDValue Ops[] = {PassThru, Vec, InitialValue, Mask, VL, Policy};\n  SDValue Reduction = DAG.getNode(RVVOpcode, DL, M1VT, Ops);\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, ResVT, Reduction,\n                     DAG.getConstant(0, DL, XLenVT));\n}",
      "start_line": 9180,
      "end_line": 9208,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "bitsLE",
        "isNonZeroAVL",
        "getConstant",
        "getSimpleValueType",
        "getUNDEF",
        "getXLenVT",
        "lowerScalarInsert",
        "getLMUL1VT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECREDUCE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue Vec = Op.getOperand(0);\n  EVT VecEVT = Vec.getValueType();\n\n  unsigned BaseOpc = ISD::getVecReduceBaseOpcode(Op.getOpcode());\n\n  // Due to ordering in legalize types we may have a vector type that needs to\n  // be split. Do that manually so we can get down to a legal type.\n  while (getTypeAction(*DAG.getContext(), VecEVT) ==\n         TargetLowering::TypeSplitVector) {\n    auto [Lo, Hi] = DAG.SplitVector(Vec, DL);\n    VecEVT = Lo.getValueType();\n    Vec = DAG.getNode(BaseOpc, DL, VecEVT, Lo, Hi);\n  }\n\n  // TODO: The type may need to be widened rather than split. Or widened before\n  // it can be split.\n  if (!isTypeLegal(VecEVT))\n    return SDValue();\n\n  MVT VecVT = VecEVT.getSimpleVT();\n  MVT VecEltVT = VecVT.getVectorElementType();\n  unsigned RVVOpcode = getRVVReductionOp(Op.getOpcode());\n\n  MVT ContainerVT = VecVT;\n  if (VecVT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VecVT);\n    Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n  }\n\n  auto [Mask, VL] = getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget);\n\n  SDValue StartV = DAG.getNeutralElement(BaseOpc, DL, VecEltVT, SDNodeFlags());\n  switch (BaseOpc) {\n  case ISD::AND:\n  case ISD::OR:\n  case ISD::UMAX:\n  case ISD::UMIN:\n  case ISD::SMAX:\n  case ISD::SMIN:\n    MVT XLenVT = Subtarget.getXLenVT();\n    StartV = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VecEltVT, Vec,\n                         DAG.getConstant(0, DL, XLenVT));\n  }\n  return lowerReductionSeq(RVVOpcode, Op.getSimpleValueType(), StartV, Vec,\n                           Mask, VL, DL, DAG, Subtarget);\n}",
      "start_line": 9210,
      "end_line": 9258,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "lowerVECREDUCE",
          "condition": "BaseOpc",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "SDValue",
        "getDefaultVLOps",
        "lowerReductionSeq",
        "convertToScalableVector",
        "getVecReduceBaseOpcode",
        "getValueType",
        "getSimpleVT",
        "getContainerForFixedLengthVector",
        "getOperand",
        "getVectorElementType",
        "DL",
        "getNeutralElement",
        "getXLenVT",
        "getRVVReductionOp",
        "SplitVector",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFPVECREDUCE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VecEltVT = Op.getSimpleValueType();\n\n  unsigned RVVOpcode;\n  SDValue VectorVal, ScalarVal;\n  std::tie(RVVOpcode, VectorVal, ScalarVal) =\n      getRVVFPReductionOpAndOperands(Op, DAG, VecEltVT, Subtarget);\n  MVT VecVT = VectorVal.getSimpleValueType();\n\n  MVT ContainerVT = VecVT;\n  if (VecVT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VecVT);\n    VectorVal = convertToScalableVector(ContainerVT, VectorVal, DAG, Subtarget);\n  }\n\n  auto [Mask, VL] = getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget);\n  return lowerReductionSeq(RVVOpcode, Op.getSimpleValueType(), ScalarVal,\n                           VectorVal, Mask, VL, DL, DAG, Subtarget);\n}",
      "start_line": 9295,
      "end_line": 9315,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getRVVFPReductionOpAndOperands",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "tie",
        "DL",
        "lowerReductionSeq"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPREDUCE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue Vec = Op.getOperand(1);\n  EVT VecEVT = Vec.getValueType();\n\n  // TODO: The type may need to be widened rather than split. Or widened before\n  // it can be split.\n  if (!isTypeLegal(VecEVT))\n    return SDValue();\n\n  MVT VecVT = VecEVT.getSimpleVT();\n  unsigned RVVOpcode = getRVVReductionOp(Op.getOpcode());\n\n  if (VecVT.isFixedLengthVector()) {\n    auto ContainerVT = getContainerForFixedLengthVector(VecVT);\n    Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n  }\n\n  SDValue VL = Op.getOperand(3);\n  SDValue Mask = Op.getOperand(2);\n  return lowerReductionSeq(RVVOpcode, Op.getSimpleValueType(), Op.getOperand(0),\n                           Vec, Mask, VL, DL, DAG, Subtarget);\n}",
      "start_line": 9317,
      "end_line": 9340,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "lowerReductionSeq",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getSimpleVT",
        "getOperand",
        "DL",
        "getRVVReductionOp"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerINSERT_SUBVECTOR",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue Vec = Op.getOperand(0);\n  SDValue SubVec = Op.getOperand(1);\n  MVT VecVT = Vec.getSimpleValueType();\n  MVT SubVecVT = SubVec.getSimpleValueType();\n\n  SDLoc DL(Op);\n  MVT XLenVT = Subtarget.getXLenVT();\n  unsigned OrigIdx = Op.getConstantOperandVal(2);\n  const RISCVRegisterInfo *TRI = Subtarget.getRegisterInfo();\n\n  // We don't have the ability to slide mask vectors up indexed by their i1\n  // elements; the smallest we can do is i8. Often we are able to bitcast to\n  // equivalent i8 vectors. Note that when inserting a fixed-length vector\n  // into a scalable one, we might not necessarily have enough scalable\n  // elements to safely divide by 8: nxv1i1 = insert nxv1i1, v4i1 is valid.\n  if (SubVecVT.getVectorElementType() == MVT::i1 &&\n      (OrigIdx != 0 || !Vec.isUndef())) {\n    if (VecVT.getVectorMinNumElements() >= 8 &&\n        SubVecVT.getVectorMinNumElements() >= 8) {\n      assert(OrigIdx % 8 == 0 && \"Invalid index\");\n      assert(VecVT.getVectorMinNumElements() % 8 == 0 &&\n             SubVecVT.getVectorMinNumElements() % 8 == 0 &&\n             \"Unexpected mask vector lowering\");\n      OrigIdx /= 8;\n      SubVecVT =\n          MVT::getVectorVT(MVT::i8, SubVecVT.getVectorMinNumElements() / 8,\n                           SubVecVT.isScalableVector());\n      VecVT = MVT::getVectorVT(MVT::i8, VecVT.getVectorMinNumElements() / 8,\n                               VecVT.isScalableVector());\n      Vec = DAG.getBitcast(VecVT, Vec);\n      SubVec = DAG.getBitcast(SubVecVT, SubVec);\n    } else {\n      // We can't slide this mask vector up indexed by its i1 elements.\n      // This poses a problem when we wish to insert a scalable vector which\n      // can't be re-expressed as a larger type. Just choose the slow path and\n      // extend to a larger type, then truncate back down.\n      MVT ExtVecVT = VecVT.changeVectorElementType(MVT::i8);\n      MVT ExtSubVecVT = SubVecVT.changeVectorElementType(MVT::i8);\n      Vec = DAG.getNode(ISD::ZERO_EXTEND, DL, ExtVecVT, Vec);\n      SubVec = DAG.getNode(ISD::ZERO_EXTEND, DL, ExtSubVecVT, SubVec);\n      Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, ExtVecVT, Vec, SubVec,\n                        Op.getOperand(2));\n      SDValue SplatZero = DAG.getConstant(0, DL, ExtVecVT);\n      return DAG.getSetCC(DL, VecVT, Vec, SplatZero, ISD::SETNE);\n    }\n  }\n\n  // If the subvector vector is a fixed-length type, we cannot use subregister\n  // manipulation to simplify the codegen; we don't know which register of a\n  // LMUL group contains the specific subvector as we only know the minimum\n  // register size. Therefore we must slide the vector group up the full\n  // amount.\n  if (SubVecVT.isFixedLengthVector()) {\n    if (OrigIdx == 0 && Vec.isUndef() && !VecVT.isFixedLengthVector())\n      return Op;\n    MVT ContainerVT = VecVT;\n    if (VecVT.isFixedLengthVector()) {\n      ContainerVT = getContainerForFixedLengthVector(VecVT);\n      Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n    }\n\n    if (OrigIdx == 0 && Vec.isUndef() && VecVT.isFixedLengthVector()) {\n      SubVec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, ContainerVT,\n                           DAG.getUNDEF(ContainerVT), SubVec,\n                           DAG.getConstant(0, DL, XLenVT));\n      SubVec = convertFromScalableVector(VecVT, SubVec, DAG, Subtarget);\n      return DAG.getBitcast(Op.getValueType(), SubVec);\n    }\n\n    SubVec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, ContainerVT,\n                         DAG.getUNDEF(ContainerVT), SubVec,\n                         DAG.getConstant(0, DL, XLenVT));\n    SDValue Mask =\n        getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget).first;\n    // Set the vector length to only the number of elements we care about. Note\n    // that for slideup this includes the offset.\n    unsigned EndIndex = OrigIdx + SubVecVT.getVectorNumElements();\n    SDValue VL = getVLOp(EndIndex, ContainerVT, DL, DAG, Subtarget);\n\n    // Use tail agnostic policy if we're inserting over Vec's tail.\n    unsigned Policy = RISCVII::TAIL_UNDISTURBED_MASK_UNDISTURBED;\n    if (VecVT.isFixedLengthVector() && EndIndex == VecVT.getVectorNumElements())\n      Policy = RISCVII::TAIL_AGNOSTIC;\n\n    // If we're inserting into the lowest elements, use a tail undisturbed\n    // vmv.v.v.\n    if (OrigIdx == 0) {\n      SubVec =\n          DAG.getNode(RISCVISD::VMV_V_V_VL, DL, ContainerVT, Vec, SubVec, VL);\n    } else {\n      SDValue SlideupAmt = DAG.getConstant(OrigIdx, DL, XLenVT);\n      SubVec = getVSlideup(DAG, Subtarget, DL, ContainerVT, Vec, SubVec,\n                           SlideupAmt, Mask, VL, Policy);\n    }\n\n    if (VecVT.isFixedLengthVector())\n      SubVec = convertFromScalableVector(VecVT, SubVec, DAG, Subtarget);\n    return DAG.getBitcast(Op.getValueType(), SubVec);\n  }\n\n  unsigned SubRegIdx, RemIdx;\n  std::tie(SubRegIdx, RemIdx) =\n      RISCVTargetLowering::decomposeSubvectorInsertExtractToSubRegs(\n          VecVT, SubVecVT, OrigIdx, TRI);\n\n  RISCVII::VLMUL SubVecLMUL = RISCVTargetLowering::getLMUL(SubVecVT);\n  bool IsSubVecPartReg = SubVecLMUL == RISCVII::VLMUL::LMUL_F2 ||\n                         SubVecLMUL == RISCVII::VLMUL::LMUL_F4 ||\n                         SubVecLMUL == RISCVII::VLMUL::LMUL_F8;\n\n  // 1. If the Idx has been completely eliminated and this subvector's size is\n  // a vector register or a multiple thereof, or the surrounding elements are\n  // undef, then this is a subvector insert which naturally aligns to a vector\n  // register. These can easily be handled using subregister manipulation.\n  // 2. If the subvector is smaller than a vector register, then the insertion\n  // must preserve the undisturbed elements of the register. We do this by\n  // lowering to an EXTRACT_SUBVECTOR grabbing the nearest LMUL=1 vector type\n  // (which resolves to a subregister copy), performing a VSLIDEUP to place the\n  // subvector within the vector register, and an INSERT_SUBVECTOR of that\n  // LMUL=1 type back into the larger vector (resolving to another subregister\n  // operation). See below for how our VSLIDEUP works. We go via a LMUL=1 type\n  // to avoid allocating a large register group to hold our subvector.\n  if (RemIdx == 0 && (!IsSubVecPartReg || Vec.isUndef()))\n    return Op;\n\n  // VSLIDEUP works by leaving elements 0<i<OFFSET undisturbed, elements\n  // OFFSET<=i<VL set to the \"subvector\" and vl<=i<VLMAX set to the tail policy\n  // (in our case undisturbed). This means we can set up a subvector insertion\n  // where OFFSET is the insertion offset, and the VL is the OFFSET plus the\n  // size of the subvector.\n  MVT InterSubVT = VecVT;\n  SDValue AlignedExtract = Vec;\n  unsigned AlignedIdx = OrigIdx - RemIdx;\n  if (VecVT.bitsGT(getLMUL1VT(VecVT))) {\n    InterSubVT = getLMUL1VT(VecVT);\n    // Extract a subvector equal to the nearest full vector register type. This\n    // should resolve to a EXTRACT_SUBREG instruction.\n    AlignedExtract = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, InterSubVT, Vec,\n                                 DAG.getConstant(AlignedIdx, DL, XLenVT));\n  }\n\n  SubVec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, InterSubVT,\n                       DAG.getUNDEF(InterSubVT), SubVec,\n                       DAG.getConstant(0, DL, XLenVT));\n\n  auto [Mask, VL] = getDefaultScalableVLOps(VecVT, DL, DAG, Subtarget);\n\n  VL = computeVLMax(SubVecVT, DL, DAG);\n\n  // If we're inserting into the lowest elements, use a tail undisturbed\n  // vmv.v.v.\n  if (RemIdx == 0) {\n    SubVec = DAG.getNode(RISCVISD::VMV_V_V_VL, DL, InterSubVT, AlignedExtract,\n                         SubVec, VL);\n  } else {\n    SDValue SlideupAmt =\n        DAG.getVScale(DL, XLenVT, APInt(XLenVT.getSizeInBits(), RemIdx));\n\n    // Construct the vector length corresponding to RemIdx + length(SubVecVT).\n    VL = DAG.getNode(ISD::ADD, DL, XLenVT, SlideupAmt, VL);\n\n    SubVec = getVSlideup(DAG, Subtarget, DL, InterSubVT, AlignedExtract, SubVec,\n                         SlideupAmt, Mask, VL);\n  }\n\n  // If required, insert this subvector back into the correct vector register.\n  // This should resolve to an INSERT_SUBREG instruction.\n  if (VecVT.bitsGT(InterSubVT))\n    SubVec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VecVT, Vec, SubVec,\n                         DAG.getConstant(AlignedIdx, DL, XLenVT));\n\n  // We might have bitcast from a mask type: cast back to the original type if\n  // required.\n  return DAG.getBitcast(Op.getSimpleValueType(), SubVec);\n}",
      "start_line": 9342,
      "end_line": 9518,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getVScale",
        "getLMUL",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "length",
        "getBitcast",
        "isScalableVector",
        "getConstantOperandVal",
        "getVectorMinNumElements",
        "isUndef",
        "getVectorNumElements",
        "getVectorVT",
        "isFixedLengthVector",
        "getSetCC",
        "getContainerForFixedLengthVector",
        "getOperand",
        "getRegisterInfo",
        "tie",
        "getVLOp",
        "getDefaultScalableVLOps",
        "getLMUL1VT",
        "changeVectorElementType",
        "decomposeSubvectorInsertExtractToSubRegs",
        "vector",
        "DL",
        "getXLenVT",
        "getVSlideup",
        "computeVLMax",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerEXTRACT_SUBVECTOR",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue Vec = Op.getOperand(0);\n  MVT SubVecVT = Op.getSimpleValueType();\n  MVT VecVT = Vec.getSimpleValueType();\n\n  SDLoc DL(Op);\n  MVT XLenVT = Subtarget.getXLenVT();\n  unsigned OrigIdx = Op.getConstantOperandVal(1);\n  const RISCVRegisterInfo *TRI = Subtarget.getRegisterInfo();\n\n  // We don't have the ability to slide mask vectors down indexed by their i1\n  // elements; the smallest we can do is i8. Often we are able to bitcast to\n  // equivalent i8 vectors. Note that when extracting a fixed-length vector\n  // from a scalable one, we might not necessarily have enough scalable\n  // elements to safely divide by 8: v8i1 = extract nxv1i1 is valid.\n  if (SubVecVT.getVectorElementType() == MVT::i1 && OrigIdx != 0) {\n    if (VecVT.getVectorMinNumElements() >= 8 &&\n        SubVecVT.getVectorMinNumElements() >= 8) {\n      assert(OrigIdx % 8 == 0 && \"Invalid index\");\n      assert(VecVT.getVectorMinNumElements() % 8 == 0 &&\n             SubVecVT.getVectorMinNumElements() % 8 == 0 &&\n             \"Unexpected mask vector lowering\");\n      OrigIdx /= 8;\n      SubVecVT =\n          MVT::getVectorVT(MVT::i8, SubVecVT.getVectorMinNumElements() / 8,\n                           SubVecVT.isScalableVector());\n      VecVT = MVT::getVectorVT(MVT::i8, VecVT.getVectorMinNumElements() / 8,\n                               VecVT.isScalableVector());\n      Vec = DAG.getBitcast(VecVT, Vec);\n    } else {\n      // We can't slide this mask vector down, indexed by its i1 elements.\n      // This poses a problem when we wish to extract a scalable vector which\n      // can't be re-expressed as a larger type. Just choose the slow path and\n      // extend to a larger type, then truncate back down.\n      // TODO: We could probably improve this when extracting certain fixed\n      // from fixed, where we can extract as i8 and shift the correct element\n      // right to reach the desired subvector?\n      MVT ExtVecVT = VecVT.changeVectorElementType(MVT::i8);\n      MVT ExtSubVecVT = SubVecVT.changeVectorElementType(MVT::i8);\n      Vec = DAG.getNode(ISD::ZERO_EXTEND, DL, ExtVecVT, Vec);\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ExtSubVecVT, Vec,\n                        Op.getOperand(1));\n      SDValue SplatZero = DAG.getConstant(0, DL, ExtSubVecVT);\n      return DAG.getSetCC(DL, SubVecVT, Vec, SplatZero, ISD::SETNE);\n    }\n  }\n\n  // With an index of 0 this is a cast-like subvector, which can be performed\n  // with subregister operations.\n  if (OrigIdx == 0)\n    return Op;\n\n  // If the subvector vector is a fixed-length type, we cannot use subregister\n  // manipulation to simplify the codegen; we don't know which register of a\n  // LMUL group contains the specific subvector as we only know the minimum\n  // register size. Therefore we must slide the vector group down the full\n  // amount.\n  if (SubVecVT.isFixedLengthVector()) {\n    MVT ContainerVT = VecVT;\n    if (VecVT.isFixedLengthVector()) {\n      ContainerVT = getContainerForFixedLengthVector(VecVT);\n      Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n    }\n\n    // Shrink down Vec so we're performing the slidedown on a smaller LMUL.\n    unsigned LastIdx = OrigIdx + SubVecVT.getVectorNumElements() - 1;\n    if (auto ShrunkVT =\n            getSmallestVTForIndex(ContainerVT, LastIdx, DL, DAG, Subtarget)) {\n      ContainerVT = *ShrunkVT;\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ContainerVT, Vec,\n                        DAG.getVectorIdxConstant(0, DL));\n    }\n\n    SDValue Mask =\n        getDefaultVLOps(VecVT, ContainerVT, DL, DAG, Subtarget).first;\n    // Set the vector length to only the number of elements we care about. This\n    // avoids sliding down elements we're going to discard straight away.\n    SDValue VL = getVLOp(SubVecVT.getVectorNumElements(), ContainerVT, DL, DAG,\n                         Subtarget);\n    SDValue SlidedownAmt = DAG.getConstant(OrigIdx, DL, XLenVT);\n    SDValue Slidedown =\n        getVSlidedown(DAG, Subtarget, DL, ContainerVT,\n                      DAG.getUNDEF(ContainerVT), Vec, SlidedownAmt, Mask, VL);\n    // Now we can use a cast-like subvector extract to get the result.\n    Slidedown = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, SubVecVT, Slidedown,\n                            DAG.getConstant(0, DL, XLenVT));\n    return DAG.getBitcast(Op.getValueType(), Slidedown);\n  }\n\n  unsigned SubRegIdx, RemIdx;\n  std::tie(SubRegIdx, RemIdx) =\n      RISCVTargetLowering::decomposeSubvectorInsertExtractToSubRegs(\n          VecVT, SubVecVT, OrigIdx, TRI);\n\n  // If the Idx has been completely eliminated then this is a subvector extract\n  // which naturally aligns to a vector register. These can easily be handled\n  // using subregister manipulation.\n  if (RemIdx == 0)\n    return Op;\n\n  // Else SubVecVT is a fractional LMUL and may need to be slid down.\n  assert(RISCVVType::decodeVLMUL(getLMUL(SubVecVT)).second);\n\n  // If the vector type is an LMUL-group type, extract a subvector equal to the\n  // nearest full vector register type.\n  MVT InterSubVT = VecVT;\n  if (VecVT.bitsGT(getLMUL1VT(VecVT))) {\n    // If VecVT has an LMUL > 1, then SubVecVT should have a smaller LMUL, and\n    // we should have successfully decomposed the extract into a subregister.\n    assert(SubRegIdx != RISCV::NoSubRegister);\n    InterSubVT = getLMUL1VT(VecVT);\n    Vec = DAG.getTargetExtractSubreg(SubRegIdx, DL, InterSubVT, Vec);\n  }\n\n  // Slide this vector register down by the desired number of elements in order\n  // to place the desired subvector starting at element 0.\n  SDValue SlidedownAmt =\n      DAG.getVScale(DL, XLenVT, APInt(XLenVT.getSizeInBits(), RemIdx));\n\n  auto [Mask, VL] = getDefaultScalableVLOps(InterSubVT, DL, DAG, Subtarget);\n  SDValue Slidedown =\n      getVSlidedown(DAG, Subtarget, DL, InterSubVT, DAG.getUNDEF(InterSubVT),\n                    Vec, SlidedownAmt, Mask, VL);\n\n  // Now the vector is in the right position, extract our final subvector. This\n  // should resolve to a COPY.\n  Slidedown = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, SubVecVT, Slidedown,\n                          DAG.getConstant(0, DL, XLenVT));\n\n  // We might have bitcast from a mask type: cast back to the original type if\n  // required.\n  return DAG.getBitcast(Op.getSimpleValueType(), Slidedown);\n}",
      "start_line": 9520,
      "end_line": 9653,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getVScale",
        "getTargetExtractSubreg",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getBitcast",
        "isScalableVector",
        "getConstantOperandVal",
        "getVectorMinNumElements",
        "getVectorNumElements",
        "getVectorVT",
        "getSetCC",
        "getContainerForFixedLengthVector",
        "getVSlidedown",
        "getOperand",
        "getRegisterInfo",
        "tie",
        "getVLOp",
        "getDefaultScalableVLOps",
        "getLMUL1VT",
        "changeVectorElementType",
        "decomposeSubvectorInsertExtractToSubRegs",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "widenVectorOpsToi8",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MVT VT = N.getSimpleValueType();\n  MVT WideVT = VT.changeVectorElementType(MVT::i8);\n  SmallVector<SDValue, 4> WideOps;\n  for (SDValue Op : N->ops()) {\n    assert(Op.getSimpleValueType() == VT &&\n           \"Operands and result must be same type\");\n    WideOps.push_back(DAG.getNode(ISD::ZERO_EXTEND, DL, WideVT, Op));\n  }\n\n  unsigned NumVals = N->getNumValues();\n\n  SDVTList VTs = DAG.getVTList(SmallVector<EVT, 4>(\n      NumVals, N.getValueType().changeVectorElementType(MVT::i8)));\n  SDValue WideN = DAG.getNode(N.getOpcode(), DL, VTs, WideOps);\n  SmallVector<SDValue, 4> TruncVals;\n  for (unsigned I = 0; I < NumVals; I++) {\n    TruncVals.push_back(\n        DAG.getSetCC(DL, N->getSimpleValueType(I), WideN.getValue(I),\n                     DAG.getConstant(0, DL, WideVT), ISD::SETNE));\n  }\n\n  if (TruncVals.size() > 1)\n    return DAG.getMergeValues(TruncVals, DL);\n  return TruncVals.front();\n}",
      "start_line": 9657,
      "end_line": 9683,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNumValues",
        "getMergeValues",
        "getValue",
        "getVTList",
        "getNode",
        "getConstant",
        "getSimpleValueType",
        "push_back",
        "front",
        "changeVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_DEINTERLEAVE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VecVT = Op.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  assert(VecVT.isScalableVector() &&\n         \"vector_interleave on non-scalable vector!\");\n\n  // 1 bit element vectors need to be widened to e8\n  if (VecVT.getVectorElementType() == MVT::i1)\n    return widenVectorOpsToi8(Op, DL, DAG);\n\n  // If the VT is LMUL=8, we need to split and reassemble.\n  if (VecVT.getSizeInBits().getKnownMinValue() ==\n      (8 * RISCV::RVVBitsPerBlock)) {\n    auto [Op0Lo, Op0Hi] = DAG.SplitVectorOperand(Op.getNode(), 0);\n    auto [Op1Lo, Op1Hi] = DAG.SplitVectorOperand(Op.getNode(), 1);\n    EVT SplitVT = Op0Lo.getValueType();\n\n    SDValue ResLo = DAG.getNode(ISD::VECTOR_DEINTERLEAVE, DL,\n                                DAG.getVTList(SplitVT, SplitVT), Op0Lo, Op0Hi);\n    SDValue ResHi = DAG.getNode(ISD::VECTOR_DEINTERLEAVE, DL,\n                                DAG.getVTList(SplitVT, SplitVT), Op1Lo, Op1Hi);\n\n    SDValue Even = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT,\n                               ResLo.getValue(0), ResHi.getValue(0));\n    SDValue Odd = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT, ResLo.getValue(1),\n                              ResHi.getValue(1));\n    return DAG.getMergeValues({Even, Odd}, DL);\n  }\n\n  // Concatenate the two vectors as one vector to deinterleave\n  MVT ConcatVT =\n      MVT::getVectorVT(VecVT.getVectorElementType(),\n                       VecVT.getVectorElementCount().multiplyCoefficientBy(2));\n  SDValue Concat = DAG.getNode(ISD::CONCAT_VECTORS, DL, ConcatVT,\n                               Op.getOperand(0), Op.getOperand(1));\n\n  // We want to operate on all lanes, so get the mask and VL and mask for it\n  auto [Mask, VL] = getDefaultScalableVLOps(ConcatVT, DL, DAG, Subtarget);\n  SDValue Passthru = DAG.getUNDEF(ConcatVT);\n\n  // We can deinterleave through vnsrl.wi if the element type is smaller than\n  // ELEN\n  if (VecVT.getScalarSizeInBits() < Subtarget.getELen()) {\n    SDValue Even =\n        getDeinterleaveViaVNSRL(DL, VecVT, Concat, true, Subtarget, DAG);\n    SDValue Odd =\n        getDeinterleaveViaVNSRL(DL, VecVT, Concat, false, Subtarget, DAG);\n    return DAG.getMergeValues({Even, Odd}, DL);\n  }\n\n  // For the indices, use the same SEW to avoid an extra vsetvli\n  MVT IdxVT = ConcatVT.changeVectorElementTypeToInteger();\n  // Create a vector of even indices {0, 2, 4, ...}\n  SDValue EvenIdx =\n      DAG.getStepVector(DL, IdxVT, APInt(IdxVT.getScalarSizeInBits(), 2));\n  // Create a vector of odd indices {1, 3, 5, ... }\n  SDValue OddIdx =\n      DAG.getNode(ISD::ADD, DL, IdxVT, EvenIdx, DAG.getConstant(1, DL, IdxVT));\n\n  // Gather the even and odd elements into two separate vectors\n  SDValue EvenWide = DAG.getNode(RISCVISD::VRGATHER_VV_VL, DL, ConcatVT,\n                                 Concat, EvenIdx, Passthru, Mask, VL);\n  SDValue OddWide = DAG.getNode(RISCVISD::VRGATHER_VV_VL, DL, ConcatVT,\n                                Concat, OddIdx, Passthru, Mask, VL);\n\n  // Extract the result half of the gather for even and odd\n  SDValue Even = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VecVT, EvenWide,\n                             DAG.getConstant(0, DL, XLenVT));\n  SDValue Odd = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VecVT, OddWide,\n                            DAG.getConstant(0, DL, XLenVT));\n\n  return DAG.getMergeValues({Even, Odd}, DL);\n}",
      "start_line": 9685,
      "end_line": 9760,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "widenVectorOpsToi8",
        "changeVectorElementTypeToInteger",
        "getSimpleValueType",
        "getKnownMinValue",
        "getStepVector",
        "getUNDEF",
        "getVectorVT",
        "getVectorElementCount",
        "SplitVectorOperand",
        "getValue",
        "getValueType",
        "getOperand",
        "getELen",
        "getDeinterleaveViaVNSRL",
        "getDefaultScalableVLOps",
        "getMergeValues",
        "multiplyCoefficientBy",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_INTERLEAVE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VecVT = Op.getSimpleValueType();\n\n  assert(VecVT.isScalableVector() &&\n         \"vector_interleave on non-scalable vector!\");\n\n  // i1 vectors need to be widened to i8\n  if (VecVT.getVectorElementType() == MVT::i1)\n    return widenVectorOpsToi8(Op, DL, DAG);\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  SDValue VL = DAG.getRegister(RISCV::X0, XLenVT);\n\n  // If the VT is LMUL=8, we need to split and reassemble.\n  if (VecVT.getSizeInBits().getKnownMinValue() == (8 * RISCV::RVVBitsPerBlock)) {\n    auto [Op0Lo, Op0Hi] = DAG.SplitVectorOperand(Op.getNode(), 0);\n    auto [Op1Lo, Op1Hi] = DAG.SplitVectorOperand(Op.getNode(), 1);\n    EVT SplitVT = Op0Lo.getValueType();\n\n    SDValue ResLo = DAG.getNode(ISD::VECTOR_INTERLEAVE, DL,\n                                DAG.getVTList(SplitVT, SplitVT), Op0Lo, Op1Lo);\n    SDValue ResHi = DAG.getNode(ISD::VECTOR_INTERLEAVE, DL,\n                                DAG.getVTList(SplitVT, SplitVT), Op0Hi, Op1Hi);\n\n    SDValue Lo = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT,\n                             ResLo.getValue(0), ResLo.getValue(1));\n    SDValue Hi = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT,\n                             ResHi.getValue(0), ResHi.getValue(1));\n    return DAG.getMergeValues({Lo, Hi}, DL);\n  }\n\n  SDValue Interleaved;\n\n  // If the element type is smaller than ELEN, then we can interleave with\n  // vwaddu.vv and vwmaccu.vx\n  if (VecVT.getScalarSizeInBits() < Subtarget.getELen()) {\n    Interleaved = getWideningInterleave(Op.getOperand(0), Op.getOperand(1), DL,\n                                        DAG, Subtarget);\n  } else {\n    // Otherwise, fallback to using vrgathere16.vv\n    MVT ConcatVT =\n      MVT::getVectorVT(VecVT.getVectorElementType(),\n                       VecVT.getVectorElementCount().multiplyCoefficientBy(2));\n    SDValue Concat = DAG.getNode(ISD::CONCAT_VECTORS, DL, ConcatVT,\n                                 Op.getOperand(0), Op.getOperand(1));\n\n    MVT IdxVT = ConcatVT.changeVectorElementType(MVT::i16);\n\n    // 0 1 2 3 4 5 6 7 ...\n    SDValue StepVec = DAG.getStepVector(DL, IdxVT);\n\n    // 1 1 1 1 1 1 1 1 ...\n    SDValue Ones = DAG.getSplatVector(IdxVT, DL, DAG.getConstant(1, DL, XLenVT));\n\n    // 1 0 1 0 1 0 1 0 ...\n    SDValue OddMask = DAG.getNode(ISD::AND, DL, IdxVT, StepVec, Ones);\n    OddMask = DAG.getSetCC(\n        DL, IdxVT.changeVectorElementType(MVT::i1), OddMask,\n        DAG.getSplatVector(IdxVT, DL, DAG.getConstant(0, DL, XLenVT)),\n        ISD::CondCode::SETNE);\n\n    SDValue VLMax = DAG.getSplatVector(IdxVT, DL, computeVLMax(VecVT, DL, DAG));\n\n    // Build up the index vector for interleaving the concatenated vector\n    //      0      0      1      1      2      2      3      3 ...\n    SDValue Idx = DAG.getNode(ISD::SRL, DL, IdxVT, StepVec, Ones);\n    //      0      n      1    n+1      2    n+2      3    n+3 ...\n    Idx =\n        DAG.getNode(RISCVISD::ADD_VL, DL, IdxVT, Idx, VLMax, Idx, OddMask, VL);\n\n    // Then perform the interleave\n    //   v[0]   v[n]   v[1] v[n+1]   v[2] v[n+2]   v[3] v[n+3] ...\n    SDValue TrueMask = getAllOnesMask(IdxVT, VL, DL, DAG);\n    Interleaved = DAG.getNode(RISCVISD::VRGATHEREI16_VV_VL, DL, ConcatVT,\n                              Concat, Idx, DAG.getUNDEF(ConcatVT), TrueMask, VL);\n  }\n\n  // Extract the two halves from the interleaved result\n  SDValue Lo = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VecVT, Interleaved,\n                           DAG.getVectorIdxConstant(0, DL));\n  SDValue Hi = DAG.getNode(\n      ISD::EXTRACT_SUBVECTOR, DL, VecVT, Interleaved,\n      DAG.getVectorIdxConstant(VecVT.getVectorMinNumElements(), DL));\n\n  return DAG.getMergeValues({Lo, Hi}, DL);\n}",
      "start_line": 9762,
      "end_line": 9849,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "widenVectorOpsToi8",
        "getSplatVector",
        "getSimpleValueType",
        "getKnownMinValue",
        "getStepVector",
        "getWideningInterleave",
        "getVectorVT",
        "getVectorElementCount",
        "SplitVectorOperand",
        "getSetCC",
        "getValue",
        "getRegister",
        "getValueType",
        "getOperand",
        "getELen",
        "changeVectorElementType",
        "getAllOnesMask",
        "getMergeValues",
        "multiplyCoefficientBy",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerSTEP_VECTOR",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  assert(VT.isScalableVector() && \"Expected scalable vector\");\n  MVT XLenVT = Subtarget.getXLenVT();\n  auto [Mask, VL] = getDefaultScalableVLOps(VT, DL, DAG, Subtarget);\n  SDValue StepVec = DAG.getNode(RISCVISD::VID_VL, DL, VT, Mask, VL);\n  uint64_t StepValImm = Op.getConstantOperandVal(0);\n  if (StepValImm != 1) {\n    if (isPowerOf2_64(StepValImm)) {\n      SDValue StepVal =\n          DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, DAG.getUNDEF(VT),\n                      DAG.getConstant(Log2_64(StepValImm), DL, XLenVT), VL);\n      StepVec = DAG.getNode(ISD::SHL, DL, VT, StepVec, StepVal);\n    } else {\n      SDValue StepVal = lowerScalarSplat(\n          SDValue(), DAG.getConstant(StepValImm, DL, VT.getVectorElementType()),\n          VL, VT, DL, DAG, Subtarget);\n      StepVec = DAG.getNode(ISD::MUL, DL, VT, StepVec, StepVal);\n    }\n  }\n  return StepVec;\n}",
      "start_line": 9853,
      "end_line": 9876,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getConstantOperandVal",
        "getConstant",
        "getSimpleValueType",
        "DL",
        "lowerScalarSplat",
        "getXLenVT",
        "getDefaultScalableVLOps",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_REVERSE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VecVT = Op.getSimpleValueType();\n  if (VecVT.getVectorElementType() == MVT::i1) {\n    MVT WidenVT = MVT::getVectorVT(MVT::i8, VecVT.getVectorElementCount());\n    SDValue Op1 = DAG.getNode(ISD::ZERO_EXTEND, DL, WidenVT, Op.getOperand(0));\n    SDValue Op2 = DAG.getNode(ISD::VECTOR_REVERSE, DL, WidenVT, Op1);\n    return DAG.getNode(ISD::TRUNCATE, DL, VecVT, Op2);\n  }\n  unsigned EltSize = VecVT.getScalarSizeInBits();\n  unsigned MinSize = VecVT.getSizeInBits().getKnownMinValue();\n  unsigned VectorBitsMax = Subtarget.getRealMaxVLen();\n  unsigned MaxVLMAX =\n    RISCVTargetLowering::computeVLMAX(VectorBitsMax, EltSize, MinSize);\n\n  unsigned GatherOpc = RISCVISD::VRGATHER_VV_VL;\n  MVT IntVT = VecVT.changeVectorElementTypeToInteger();\n\n  // If this is SEW=8 and VLMAX is potentially more than 256, we need\n  // to use vrgatherei16.vv.\n  // TODO: It's also possible to use vrgatherei16.vv for other types to\n  // decrease register width for the index calculation.\n  if (MaxVLMAX > 256 && EltSize == 8) {\n    // If this is LMUL=8, we have to split before can use vrgatherei16.vv.\n    // Reverse each half, then reassemble them in reverse order.\n    // NOTE: It's also possible that after splitting that VLMAX no longer\n    // requires vrgatherei16.vv.\n    if (MinSize == (8 * RISCV::RVVBitsPerBlock)) {\n      auto [Lo, Hi] = DAG.SplitVectorOperand(Op.getNode(), 0);\n      auto [LoVT, HiVT] = DAG.GetSplitDestVTs(VecVT);\n      Lo = DAG.getNode(ISD::VECTOR_REVERSE, DL, LoVT, Lo);\n      Hi = DAG.getNode(ISD::VECTOR_REVERSE, DL, HiVT, Hi);\n      // Reassemble the low and high pieces reversed.\n      // FIXME: This is a CONCAT_VECTORS.\n      SDValue Res =\n          DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VecVT, DAG.getUNDEF(VecVT), Hi,\n                      DAG.getIntPtrConstant(0, DL));\n      return DAG.getNode(\n          ISD::INSERT_SUBVECTOR, DL, VecVT, Res, Lo,\n          DAG.getIntPtrConstant(LoVT.getVectorMinNumElements(), DL));\n    }\n\n    // Just promote the int type to i16 which will double the LMUL.\n    IntVT = MVT::getVectorVT(MVT::i16, VecVT.getVectorElementCount());\n    GatherOpc = RISCVISD::VRGATHEREI16_VV_VL;\n  }\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  auto [Mask, VL] = getDefaultScalableVLOps(VecVT, DL, DAG, Subtarget);\n\n  // Calculate VLMAX-1 for the desired SEW.\n  SDValue VLMinus1 = DAG.getNode(ISD::SUB, DL, XLenVT,\n                                 computeVLMax(VecVT, DL, DAG),\n                                 DAG.getConstant(1, DL, XLenVT));\n\n  // Splat VLMAX-1 taking care to handle SEW==64 on RV32.\n  bool IsRV32E64 =\n      !Subtarget.is64Bit() && IntVT.getVectorElementType() == MVT::i64;\n  SDValue SplatVL;\n  if (!IsRV32E64)\n    SplatVL = DAG.getSplatVector(IntVT, DL, VLMinus1);\n  else\n    SplatVL = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IntVT, DAG.getUNDEF(IntVT),\n                          VLMinus1, DAG.getRegister(RISCV::X0, XLenVT));\n\n  SDValue VID = DAG.getNode(RISCVISD::VID_VL, DL, IntVT, Mask, VL);\n  SDValue Indices = DAG.getNode(RISCVISD::SUB_VL, DL, IntVT, SplatVL, VID,\n                                DAG.getUNDEF(IntVT), Mask, VL);\n\n  return DAG.getNode(GatherOpc, DL, VecVT, Op.getOperand(0), Indices,\n                     DAG.getUNDEF(VecVT), Mask, VL);\n}",
      "start_line": 9883,
      "end_line": 9955,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "computeVLMAX",
        "getRealMaxVLen",
        "changeVectorElementTypeToInteger",
        "getSplatVector",
        "getConstant",
        "getSimpleValueType",
        "is64Bit",
        "getSizeInBits",
        "getKnownMinValue",
        "getScalarSizeInBits",
        "getUNDEF",
        "getIntPtrConstant",
        "getVectorVT",
        "getVectorElementType",
        "SplitVectorOperand",
        "getRegister",
        "getDefaultScalableVLOps",
        "GetSplitDestVTs",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVECTOR_SPLICE",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  SDValue V1 = Op.getOperand(0);\n  SDValue V2 = Op.getOperand(1);\n  MVT XLenVT = Subtarget.getXLenVT();\n  MVT VecVT = Op.getSimpleValueType();\n\n  SDValue VLMax = computeVLMax(VecVT, DL, DAG);\n\n  int64_t ImmValue = cast<ConstantSDNode>(Op.getOperand(2))->getSExtValue();\n  SDValue DownOffset, UpOffset;\n  if (ImmValue >= 0) {\n    // The operand is a TargetConstant, we need to rebuild it as a regular\n    // constant.\n    DownOffset = DAG.getConstant(ImmValue, DL, XLenVT);\n    UpOffset = DAG.getNode(ISD::SUB, DL, XLenVT, VLMax, DownOffset);\n  } else {\n    // The operand is a TargetConstant, we need to rebuild it as a regular\n    // constant rather than negating the original operand.\n    UpOffset = DAG.getConstant(-ImmValue, DL, XLenVT);\n    DownOffset = DAG.getNode(ISD::SUB, DL, XLenVT, VLMax, UpOffset);\n  }\n\n  SDValue TrueMask = getAllOnesMask(VecVT, VLMax, DL, DAG);\n\n  SDValue SlideDown =\n      getVSlidedown(DAG, Subtarget, DL, VecVT, DAG.getUNDEF(VecVT), V1,\n                    DownOffset, TrueMask, UpOffset);\n  return getVSlideup(DAG, Subtarget, DL, VecVT, SlideDown, V2, UpOffset,\n                     TrueMask, DAG.getRegister(RISCV::X0, XLenVT),\n                     RISCVII::TAIL_AGNOSTIC);\n}",
      "start_line": 9957,
      "end_line": 9989,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNode",
        "getConstant",
        "getSimpleValueType",
        "getVSlidedown",
        "getOperand",
        "DL",
        "getXLenVT",
        "getVSlideup",
        "computeVLMax",
        "getAllOnesMask",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFixedLengthVectorLoadToRVV",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  auto *Load = cast<LoadSDNode>(Op);\n\n  assert(allowsMemoryAccessForAlignment(*DAG.getContext(), DAG.getDataLayout(),\n                                        Load->getMemoryVT(),\n                                        *Load->getMemOperand()) &&\n         \"Expecting a correctly-aligned load\");\n\n  MVT VT = Op.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n  MVT ContainerVT = getContainerForFixedLengthVector(VT);\n\n  // If we know the exact VLEN and our fixed length vector completely fills\n  // the container, use a whole register load instead.\n  const auto [MinVLMAX, MaxVLMAX] =\n      RISCVTargetLowering::computeVLMAXBounds(ContainerVT, Subtarget);\n  if (MinVLMAX == MaxVLMAX && MinVLMAX == VT.getVectorNumElements() &&\n      getLMUL1VT(ContainerVT).bitsLE(ContainerVT)) {\n    SDValue NewLoad =\n        DAG.getLoad(ContainerVT, DL, Load->getChain(), Load->getBasePtr(),\n                    Load->getMemOperand());\n    SDValue Result = convertFromScalableVector(VT, NewLoad, DAG, Subtarget);\n    return DAG.getMergeValues({Result, NewLoad.getValue(1)}, DL);\n  }\n\n  SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG, Subtarget);\n\n  bool IsMaskOp = VT.getVectorElementType() == MVT::i1;\n  SDValue IntID = DAG.getTargetConstant(\n      IsMaskOp ? Intrinsic::riscv_vlm : Intrinsic::riscv_vle, DL, XLenVT);\n  SmallVector<SDValue, 4> Ops{Load->getChain(), IntID};\n  if (!IsMaskOp)\n    Ops.push_back(DAG.getUNDEF(ContainerVT));\n  Ops.push_back(Load->getBasePtr());\n  Ops.push_back(VL);\n  SDVTList VTs = DAG.getVTList({ContainerVT, MVT::Other});\n  SDValue NewLoad =\n      DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops,\n                              Load->getMemoryVT(), Load->getMemOperand());\n\n  SDValue Result = convertFromScalableVector(VT, NewLoad, DAG, Subtarget);\n  return DAG.getMergeValues({Result, NewLoad.getValue(1)}, DL);\n}",
      "start_line": 9991,
      "end_line": 10036,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getChain",
        "getMemoryVT",
        "bitsLE",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getDataLayout",
        "computeVLMAXBounds",
        "getMemIntrinsicNode",
        "getVectorElementType",
        "push_back",
        "getMemOperand",
        "getVTList",
        "getLoad",
        "getContainerForFixedLengthVector",
        "getVLOp",
        "getLMUL1VT",
        "getMergeValues",
        "getBasePtr",
        "DL",
        "getXLenVT",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFixedLengthVectorStoreToRVV",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  auto *Store = cast<StoreSDNode>(Op);\n\n  assert(allowsMemoryAccessForAlignment(*DAG.getContext(), DAG.getDataLayout(),\n                                        Store->getMemoryVT(),\n                                        *Store->getMemOperand()) &&\n         \"Expecting a correctly-aligned store\");\n\n  SDValue StoreVal = Store->getValue();\n  MVT VT = StoreVal.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  // If the size less than a byte, we need to pad with zeros to make a byte.\n  if (VT.getVectorElementType() == MVT::i1 && VT.getVectorNumElements() < 8) {\n    VT = MVT::v8i1;\n    StoreVal = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT,\n                           DAG.getConstant(0, DL, VT), StoreVal,\n                           DAG.getIntPtrConstant(0, DL));\n  }\n\n  MVT ContainerVT = getContainerForFixedLengthVector(VT);\n\n  SDValue NewValue =\n      convertToScalableVector(ContainerVT, StoreVal, DAG, Subtarget);\n\n\n  // If we know the exact VLEN and our fixed length vector completely fills\n  // the container, use a whole register store instead.\n  const auto [MinVLMAX, MaxVLMAX] =\n      RISCVTargetLowering::computeVLMAXBounds(ContainerVT, Subtarget);\n  if (MinVLMAX == MaxVLMAX && MinVLMAX == VT.getVectorNumElements() &&\n      getLMUL1VT(ContainerVT).bitsLE(ContainerVT))\n    return DAG.getStore(Store->getChain(), DL, NewValue, Store->getBasePtr(),\n                        Store->getMemOperand());\n\n  SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG,\n                       Subtarget);\n\n  bool IsMaskOp = VT.getVectorElementType() == MVT::i1;\n  SDValue IntID = DAG.getTargetConstant(\n      IsMaskOp ? Intrinsic::riscv_vsm : Intrinsic::riscv_vse, DL, XLenVT);\n  return DAG.getMemIntrinsicNode(\n      ISD::INTRINSIC_VOID, DL, DAG.getVTList(MVT::Other),\n      {Store->getChain(), IntID, NewValue, Store->getBasePtr(), VL},\n      Store->getMemoryVT(), Store->getMemOperand());\n}",
      "start_line": 10038,
      "end_line": 10086,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getChain",
        "getMemoryVT",
        "bitsLE",
        "convertToScalableVector",
        "getStore",
        "getSimpleValueType",
        "getDataLayout",
        "computeVLMAXBounds",
        "getVectorNumElements",
        "getMemIntrinsicNode",
        "getIntPtrConstant",
        "getVectorElementType",
        "getMemOperand",
        "getValue",
        "getContainerForFixedLengthVector",
        "getVLOp",
        "getLMUL1VT",
        "getBasePtr",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerMaskedLoad",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  const auto *MemSD = cast<MemSDNode>(Op);\n  EVT MemVT = MemSD->getMemoryVT();\n  MachineMemOperand *MMO = MemSD->getMemOperand();\n  SDValue Chain = MemSD->getChain();\n  SDValue BasePtr = MemSD->getBasePtr();\n\n  SDValue Mask, PassThru, VL;\n  if (const auto *VPLoad = dyn_cast<VPLoadSDNode>(Op)) {\n    Mask = VPLoad->getMask();\n    PassThru = DAG.getUNDEF(VT);\n    VL = VPLoad->getVectorLength();\n  } else {\n    const auto *MLoad = cast<MaskedLoadSDNode>(Op);\n    Mask = MLoad->getMask();\n    PassThru = MLoad->getPassThru();\n  }\n\n  bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    PassThru = convertToScalableVector(ContainerVT, PassThru, DAG, Subtarget);\n    if (!IsUnmasked) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n  }\n\n  if (!VL)\n    VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n\n  unsigned IntID =\n      IsUnmasked ? Intrinsic::riscv_vle : Intrinsic::riscv_vle_mask;\n  SmallVector<SDValue, 8> Ops{Chain, DAG.getTargetConstant(IntID, DL, XLenVT)};\n  if (IsUnmasked)\n    Ops.push_back(DAG.getUNDEF(ContainerVT));\n  else\n    Ops.push_back(PassThru);\n  Ops.push_back(BasePtr);\n  if (!IsUnmasked)\n    Ops.push_back(Mask);\n  Ops.push_back(VL);\n  if (!IsUnmasked)\n    Ops.push_back(DAG.getTargetConstant(RISCVII::TAIL_AGNOSTIC, DL, XLenVT));\n\n  SDVTList VTs = DAG.getVTList({ContainerVT, MVT::Other});\n\n  SDValue Result =\n      DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops, MemVT, MMO);\n  Chain = Result.getValue(1);\n\n  if (VT.isFixedLengthVector())\n    Result = convertFromScalableVector(VT, Result, DAG, Subtarget);\n\n  return DAG.getMergeValues({Result, Chain}, DL);\n}",
      "start_line": 10088,
      "end_line": 10151,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getChain",
        "getMemoryVT",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getMask",
        "getVectorLength",
        "getUNDEF",
        "getPassThru",
        "getMemIntrinsicNode",
        "push_back",
        "getMemOperand",
        "getValue",
        "getVTList",
        "getContainerForFixedLengthVector",
        "isConstantSplatVectorAllOnes",
        "getMaskTypeFor",
        "getMergeValues",
        "getBasePtr",
        "DL",
        "getXLenVT",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerMaskedStore",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n\n  const auto *MemSD = cast<MemSDNode>(Op);\n  EVT MemVT = MemSD->getMemoryVT();\n  MachineMemOperand *MMO = MemSD->getMemOperand();\n  SDValue Chain = MemSD->getChain();\n  SDValue BasePtr = MemSD->getBasePtr();\n  SDValue Val, Mask, VL;\n\n  if (const auto *VPStore = dyn_cast<VPStoreSDNode>(Op)) {\n    Val = VPStore->getValue();\n    Mask = VPStore->getMask();\n    VL = VPStore->getVectorLength();\n  } else {\n    const auto *MStore = cast<MaskedStoreSDNode>(Op);\n    Val = MStore->getValue();\n    Mask = MStore->getMask();\n  }\n\n  bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n  MVT VT = Val.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n\n    Val = convertToScalableVector(ContainerVT, Val, DAG, Subtarget);\n    if (!IsUnmasked) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n  }\n\n  if (!VL)\n    VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n\n  unsigned IntID =\n      IsUnmasked ? Intrinsic::riscv_vse : Intrinsic::riscv_vse_mask;\n  SmallVector<SDValue, 8> Ops{Chain, DAG.getTargetConstant(IntID, DL, XLenVT)};\n  Ops.push_back(Val);\n  Ops.push_back(BasePtr);\n  if (!IsUnmasked)\n    Ops.push_back(Mask);\n  Ops.push_back(VL);\n\n  return DAG.getMemIntrinsicNode(ISD::INTRINSIC_VOID, DL,\n                                 DAG.getVTList(MVT::Other), Ops, MemVT, MMO);\n}",
      "start_line": 10153,
      "end_line": 10204,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMemOperand",
        "getMaskTypeFor",
        "getDefaultVLOps",
        "getBasePtr",
        "getValue",
        "getChain",
        "getMemoryVT",
        "getMask",
        "getVectorLength",
        "convertToScalableVector",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "getMemIntrinsicNode",
        "DL",
        "getXLenVT",
        "push_back",
        "isConstantSplatVectorAllOnes",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFixedLengthVectorSetccToRVV",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MVT InVT = Op.getOperand(0).getSimpleValueType();\n  MVT ContainerVT = getContainerForFixedLengthVector(InVT);\n\n  MVT VT = Op.getSimpleValueType();\n\n  SDValue Op1 =\n      convertToScalableVector(ContainerVT, Op.getOperand(0), DAG, Subtarget);\n  SDValue Op2 =\n      convertToScalableVector(ContainerVT, Op.getOperand(1), DAG, Subtarget);\n\n  SDLoc DL(Op);\n  auto [Mask, VL] = getDefaultVLOps(VT.getVectorNumElements(), ContainerVT, DL,\n                                    DAG, Subtarget);\n  MVT MaskVT = getMaskTypeFor(ContainerVT);\n\n  SDValue Cmp =\n      DAG.getNode(RISCVISD::SETCC_VL, DL, MaskVT,\n                  {Op1, Op2, Op.getOperand(2), DAG.getUNDEF(MaskVT), Mask, VL});\n\n  return convertFromScalableVector(VT, Cmp, DAG, Subtarget);\n}",
      "start_line": 10206,
      "end_line": 10229,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getMaskTypeFor",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getUNDEF",
        "getOperand",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVectorStrictFSetcc",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  unsigned Opc = Op.getOpcode();\n  SDLoc DL(Op);\n  SDValue Chain = Op.getOperand(0);\n  SDValue Op1 = Op.getOperand(1);\n  SDValue Op2 = Op.getOperand(2);\n  SDValue CC = Op.getOperand(3);\n  ISD::CondCode CCVal = cast<CondCodeSDNode>(CC)->get();\n  MVT VT = Op.getSimpleValueType();\n  MVT InVT = Op1.getSimpleValueType();\n\n  // RVV VMFEQ/VMFNE ignores qNan, so we expand strict_fsetccs with OEQ/UNE\n  // condition code.\n  if (Opc == ISD::STRICT_FSETCCS) {\n    // Expand strict_fsetccs(x, oeq) to\n    // (and strict_fsetccs(x, y, oge), strict_fsetccs(x, y, ole))\n    SDVTList VTList = Op->getVTList();\n    if (CCVal == ISD::SETEQ || CCVal == ISD::SETOEQ) {\n      SDValue OLECCVal = DAG.getCondCode(ISD::SETOLE);\n      SDValue Tmp1 = DAG.getNode(ISD::STRICT_FSETCCS, DL, VTList, Chain, Op1,\n                                 Op2, OLECCVal);\n      SDValue Tmp2 = DAG.getNode(ISD::STRICT_FSETCCS, DL, VTList, Chain, Op2,\n                                 Op1, OLECCVal);\n      SDValue OutChain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other,\n                                     Tmp1.getValue(1), Tmp2.getValue(1));\n      // Tmp1 and Tmp2 might be the same node.\n      if (Tmp1 != Tmp2)\n        Tmp1 = DAG.getNode(ISD::AND, DL, VT, Tmp1, Tmp2);\n      return DAG.getMergeValues({Tmp1, OutChain}, DL);\n    }\n\n    // Expand (strict_fsetccs x, y, une) to (not (strict_fsetccs x, y, oeq))\n    if (CCVal == ISD::SETNE || CCVal == ISD::SETUNE) {\n      SDValue OEQCCVal = DAG.getCondCode(ISD::SETOEQ);\n      SDValue OEQ = DAG.getNode(ISD::STRICT_FSETCCS, DL, VTList, Chain, Op1,\n                                Op2, OEQCCVal);\n      SDValue Res = DAG.getNOT(DL, OEQ, VT);\n      return DAG.getMergeValues({Res, OEQ.getValue(1)}, DL);\n    }\n  }\n\n  MVT ContainerInVT = InVT;\n  if (InVT.isFixedLengthVector()) {\n    ContainerInVT = getContainerForFixedLengthVector(InVT);\n    Op1 = convertToScalableVector(ContainerInVT, Op1, DAG, Subtarget);\n    Op2 = convertToScalableVector(ContainerInVT, Op2, DAG, Subtarget);\n  }\n  MVT MaskVT = getMaskTypeFor(ContainerInVT);\n\n  auto [Mask, VL] = getDefaultVLOps(InVT, ContainerInVT, DL, DAG, Subtarget);\n\n  SDValue Res;\n  if (Opc == ISD::STRICT_FSETCC &&\n      (CCVal == ISD::SETLT || CCVal == ISD::SETOLT || CCVal == ISD::SETLE ||\n       CCVal == ISD::SETOLE)) {\n    // VMFLT/VMFLE/VMFGT/VMFGE raise exception for qNan. Generate a mask to only\n    // active when both input elements are ordered.\n    SDValue True = getAllOnesMask(ContainerInVT, VL, DL, DAG);\n    SDValue OrderMask1 = DAG.getNode(\n        RISCVISD::STRICT_FSETCC_VL, DL, DAG.getVTList(MaskVT, MVT::Other),\n        {Chain, Op1, Op1, DAG.getCondCode(ISD::SETOEQ), DAG.getUNDEF(MaskVT),\n         True, VL});\n    SDValue OrderMask2 = DAG.getNode(\n        RISCVISD::STRICT_FSETCC_VL, DL, DAG.getVTList(MaskVT, MVT::Other),\n        {Chain, Op2, Op2, DAG.getCondCode(ISD::SETOEQ), DAG.getUNDEF(MaskVT),\n         True, VL});\n    Mask =\n        DAG.getNode(RISCVISD::VMAND_VL, DL, MaskVT, OrderMask1, OrderMask2, VL);\n    // Use Mask as the merge operand to let the result be 0 if either of the\n    // inputs is unordered.\n    Res = DAG.getNode(RISCVISD::STRICT_FSETCCS_VL, DL,\n                      DAG.getVTList(MaskVT, MVT::Other),\n                      {Chain, Op1, Op2, CC, Mask, Mask, VL});\n  } else {\n    unsigned RVVOpc = Opc == ISD::STRICT_FSETCC ? RISCVISD::STRICT_FSETCC_VL\n                                                : RISCVISD::STRICT_FSETCCS_VL;\n    Res = DAG.getNode(RVVOpc, DL, DAG.getVTList(MaskVT, MVT::Other),\n                      {Chain, Op1, Op2, CC, DAG.getUNDEF(MaskVT), Mask, VL});\n  }\n\n  if (VT.isFixedLengthVector()) {\n    SDValue SubVec = convertFromScalableVector(VT, Res, DAG, Subtarget);\n    return DAG.getMergeValues({SubVec, Res.getValue(1)}, DL);\n  }\n  return Res;\n}",
      "start_line": 10231,
      "end_line": 10317,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNOT",
        "getDefaultVLOps",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getUNDEF",
        "strict_fsetccs",
        "Expand",
        "getValue",
        "getVTList",
        "getOpcode",
        "getContainerForFixedLengthVector",
        "getOperand",
        "getCondCode",
        "getAllOnesMask",
        "getMergeValues",
        "getMaskTypeFor",
        "to",
        "get",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerABS",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue X = Op.getOperand(0);\n\n  assert((Op.getOpcode() == ISD::VP_ABS || VT.isFixedLengthVector()) &&\n         \"Unexpected type for ISD::ABS\");\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    X = convertToScalableVector(ContainerVT, X, DAG, Subtarget);\n  }\n\n  SDValue Mask, VL;\n  if (Op->getOpcode() == ISD::VP_ABS) {\n    Mask = Op->getOperand(1);\n    if (VT.isFixedLengthVector())\n      Mask = convertToScalableVector(getMaskTypeFor(ContainerVT), Mask, DAG,\n                                     Subtarget);\n    VL = Op->getOperand(2);\n  } else\n    std::tie(Mask, VL) = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  SDValue SplatZero = DAG.getNode(\n      RISCVISD::VMV_V_X_VL, DL, ContainerVT, DAG.getUNDEF(ContainerVT),\n      DAG.getConstant(0, DL, Subtarget.getXLenVT()), VL);\n  SDValue NegX = DAG.getNode(RISCVISD::SUB_VL, DL, ContainerVT, SplatZero, X,\n                             DAG.getUNDEF(ContainerVT), Mask, VL);\n  SDValue Max = DAG.getNode(RISCVISD::SMAX_VL, DL, ContainerVT, X, NegX,\n                            DAG.getUNDEF(ContainerVT), Mask, VL);\n\n  if (VT.isFixedLengthVector())\n    Max = convertFromScalableVector(VT, Max, DAG, Subtarget);\n  return Max;\n}",
      "start_line": 10320,
      "end_line": 10355,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getNode",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "DL",
        "tie",
        "isFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFixedLengthVectorFCOPYSIGNToRVV",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue Mag = Op.getOperand(0);\n  SDValue Sign = Op.getOperand(1);\n  assert(Mag.getValueType() == Sign.getValueType() &&\n         \"Can only handle COPYSIGN with matching types.\");\n\n  MVT ContainerVT = getContainerForFixedLengthVector(VT);\n  Mag = convertToScalableVector(ContainerVT, Mag, DAG, Subtarget);\n  Sign = convertToScalableVector(ContainerVT, Sign, DAG, Subtarget);\n\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n\n  SDValue CopySign = DAG.getNode(RISCVISD::FCOPYSIGN_VL, DL, ContainerVT, Mag,\n                                 Sign, DAG.getUNDEF(ContainerVT), Mask, VL);\n\n  return convertFromScalableVector(VT, CopySign, DAG, Subtarget);\n}",
      "start_line": 10357,
      "end_line": 10376,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "getValueType",
        "convertFromScalableVector",
        "getOperand",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerFixedLengthVectorSelectToRVV",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  MVT ContainerVT = getContainerForFixedLengthVector(VT);\n\n  MVT I1ContainerVT =\n      MVT::getVectorVT(MVT::i1, ContainerVT.getVectorElementCount());\n\n  SDValue CC =\n      convertToScalableVector(I1ContainerVT, Op.getOperand(0), DAG, Subtarget);\n  SDValue Op1 =\n      convertToScalableVector(ContainerVT, Op.getOperand(1), DAG, Subtarget);\n  SDValue Op2 =\n      convertToScalableVector(ContainerVT, Op.getOperand(2), DAG, Subtarget);\n\n  SDLoc DL(Op);\n  SDValue VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n\n  SDValue Select = DAG.getNode(RISCVISD::VMERGE_VL, DL, ContainerVT, CC, Op1,\n                               Op2, DAG.getUNDEF(ContainerVT), VL);\n\n  return convertFromScalableVector(VT, Select, DAG, Subtarget);\n}",
      "start_line": 10378,
      "end_line": 10400,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getVectorVT",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerToScalableOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  unsigned NewOpc = getRISCVVLOp(Op);\n  bool HasMergeOp = hasMergeOp(NewOpc);\n  bool HasMask = hasMaskOp(NewOpc);\n\n  MVT VT = Op.getSimpleValueType();\n  MVT ContainerVT = getContainerForFixedLengthVector(VT);\n\n  // Create list of operands by converting existing ones to scalable types.\n  SmallVector<SDValue, 6> Ops;\n  for (const SDValue &V : Op->op_values()) {\n    assert(!isa<VTSDNode>(V) && \"Unexpected VTSDNode node!\");\n\n    // Pass through non-vector operands.\n    if (!V.getValueType().isVector()) {\n      Ops.push_back(V);\n      continue;\n    }\n\n    // \"cast\" fixed length vector to a scalable vector.\n    assert(useRVVForFixedLengthVectorVT(V.getSimpleValueType()) &&\n           \"Only fixed length vectors are supported!\");\n    Ops.push_back(convertToScalableVector(ContainerVT, V, DAG, Subtarget));\n  }\n\n  SDLoc DL(Op);\n  auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);\n  if (HasMergeOp)\n    Ops.push_back(DAG.getUNDEF(ContainerVT));\n  if (HasMask)\n    Ops.push_back(Mask);\n  Ops.push_back(VL);\n\n  // StrictFP operations have two result values. Their lowered result should\n  // have same result count.\n  if (Op->isStrictFPOpcode()) {\n    SDValue ScalableRes =\n        DAG.getNode(NewOpc, DL, DAG.getVTList(ContainerVT, MVT::Other), Ops,\n                    Op->getFlags());\n    SDValue SubVec = convertFromScalableVector(VT, ScalableRes, DAG, Subtarget);\n    return DAG.getMergeValues({SubVec, ScalableRes.getValue(1)}, DL);\n  }\n\n  SDValue ScalableRes =\n      DAG.getNode(NewOpc, DL, ContainerVT, Ops, Op->getFlags());\n  return convertFromScalableVector(VT, ScalableRes, DAG, Subtarget);\n}",
      "start_line": 10402,
      "end_line": 10449,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasMergeOp",
        "getDefaultVLOps",
        "getMergeValues",
        "getRISCVVLOp",
        "hasMaskOp",
        "getNode",
        "getFlags",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "isVector",
        "convertFromScalableVector",
        "DL",
        "push_back"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  unsigned RISCVISDOpc = getRISCVVLOp(Op);\n  bool HasMergeOp = hasMergeOp(RISCVISDOpc);\n\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SmallVector<SDValue, 4> Ops;\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector())\n    ContainerVT = getContainerForFixedLengthVector(VT);\n\n  for (const auto &OpIdx : enumerate(Op->ops())) {\n    SDValue V = OpIdx.value();\n    assert(!isa<VTSDNode>(V) && \"Unexpected VTSDNode node!\");\n    // Add dummy merge value before the mask. Or if there isn't a mask, before\n    // EVL.\n    if (HasMergeOp) {\n      auto MaskIdx = ISD::getVPMaskIdx(Op.getOpcode());\n      if (MaskIdx) {\n        if (*MaskIdx == OpIdx.index())\n          Ops.push_back(DAG.getUNDEF(ContainerVT));\n      } else if (ISD::getVPExplicitVectorLengthIdx(Op.getOpcode()) ==\n                 OpIdx.index()) {\n        if (Op.getOpcode() == ISD::VP_MERGE) {\n          // For VP_MERGE, copy the false operand instead of an undef value.\n          Ops.push_back(Ops.back());\n        } else {\n          assert(Op.getOpcode() == ISD::VP_SELECT);\n          // For VP_SELECT, add an undef value.\n          Ops.push_back(DAG.getUNDEF(ContainerVT));\n        }\n      }\n    }\n    // Pass through operands which aren't fixed-length vectors.\n    if (!V.getValueType().isFixedLengthVector()) {\n      Ops.push_back(V);\n      continue;\n    }\n    // \"cast\" fixed length vector to a scalable vector.\n    MVT OpVT = V.getSimpleValueType();\n    MVT ContainerVT = getContainerForFixedLengthVector(OpVT);\n    assert(useRVVForFixedLengthVectorVT(OpVT) &&\n           \"Only fixed length vectors are supported!\");\n    Ops.push_back(convertToScalableVector(ContainerVT, V, DAG, Subtarget));\n  }\n\n  if (!VT.isFixedLengthVector())\n    return DAG.getNode(RISCVISDOpc, DL, VT, Ops, Op->getFlags());\n\n  SDValue VPOp = DAG.getNode(RISCVISDOpc, DL, ContainerVT, Ops, Op->getFlags());\n\n  return convertFromScalableVector(VT, VPOp, DAG, Subtarget);\n}",
      "start_line": 10456,
      "end_line": 10509,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasMergeOp",
        "value",
        "getRISCVVLOp",
        "isFixedLengthVector",
        "getNode",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "index",
        "convertFromScalableVector",
        "DL",
        "getVPMaskIdx",
        "push_back"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPExtMaskOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  SDValue Src = Op.getOperand(0);\n  // NOTE: Mask is dropped.\n  SDValue VL = Op.getOperand(2);\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    MVT SrcVT = MVT::getVectorVT(MVT::i1, ContainerVT.getVectorElementCount());\n    Src = convertToScalableVector(SrcVT, Src, DAG, Subtarget);\n  }\n\n  MVT XLenVT = Subtarget.getXLenVT();\n  SDValue Zero = DAG.getConstant(0, DL, XLenVT);\n  SDValue ZeroSplat = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                                  DAG.getUNDEF(ContainerVT), Zero, VL);\n\n  SDValue SplatValue = DAG.getConstant(\n      Op.getOpcode() == ISD::VP_ZERO_EXTEND ? 1 : -1, DL, XLenVT);\n  SDValue Splat = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                              DAG.getUNDEF(ContainerVT), SplatValue, VL);\n\n  SDValue Result = DAG.getNode(RISCVISD::VMERGE_VL, DL, ContainerVT, Src, Splat,\n                               ZeroSplat, DAG.getUNDEF(ContainerVT), VL);\n  if (!VT.isFixedLengthVector())\n    return Result;\n  return convertFromScalableVector(VT, Result, DAG, Subtarget);\n}",
      "start_line": 10511,
      "end_line": 10542,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "getVectorVT",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPSetCCMaskOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  SDValue Op1 = Op.getOperand(0);\n  SDValue Op2 = Op.getOperand(1);\n  ISD::CondCode Condition = cast<CondCodeSDNode>(Op.getOperand(2))->get();\n  // NOTE: Mask is dropped.\n  SDValue VL = Op.getOperand(4);\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    Op1 = convertToScalableVector(ContainerVT, Op1, DAG, Subtarget);\n    Op2 = convertToScalableVector(ContainerVT, Op2, DAG, Subtarget);\n  }\n\n  SDValue Result;\n  SDValue AllOneMask = DAG.getNode(RISCVISD::VMSET_VL, DL, ContainerVT, VL);\n\n  switch (Condition) {\n  default:\n    break;\n  // X != Y  --> (X^Y)\n  case ISD::SETNE:\n    Result = DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Op1, Op2, VL);\n    break;\n  // X == Y  --> ~(X^Y)\n  case ISD::SETEQ: {\n    SDValue Temp =\n        DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Op1, Op2, VL);\n    Result =\n        DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Temp, AllOneMask, VL);\n    break;\n  }\n  // X >s Y   -->  X == 0 & Y == 1  -->  ~X & Y\n  // X <u Y   -->  X == 0 & Y == 1  -->  ~X & Y\n  case ISD::SETGT:\n  case ISD::SETULT: {\n    SDValue Temp =\n        DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Op1, AllOneMask, VL);\n    Result = DAG.getNode(RISCVISD::VMAND_VL, DL, ContainerVT, Temp, Op2, VL);\n    break;\n  }\n  // X <s Y   --> X == 1 & Y == 0  -->  ~Y & X\n  // X >u Y   --> X == 1 & Y == 0  -->  ~Y & X\n  case ISD::SETLT:\n  case ISD::SETUGT: {\n    SDValue Temp =\n        DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Op2, AllOneMask, VL);\n    Result = DAG.getNode(RISCVISD::VMAND_VL, DL, ContainerVT, Op1, Temp, VL);\n    break;\n  }\n  // X >=s Y  --> X == 0 | Y == 1  -->  ~X | Y\n  // X <=u Y  --> X == 0 | Y == 1  -->  ~X | Y\n  case ISD::SETGE:\n  case ISD::SETULE: {\n    SDValue Temp =\n        DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Op1, AllOneMask, VL);\n    Result = DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Temp, Op2, VL);\n    break;\n  }\n  // X <=s Y  --> X == 1 | Y == 0  -->  ~Y | X\n  // X >=u Y  --> X == 1 | Y == 0  -->  ~Y | X\n  case ISD::SETLE:\n  case ISD::SETUGE: {\n    SDValue Temp =\n        DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Op2, AllOneMask, VL);\n    Result = DAG.getNode(RISCVISD::VMXOR_VL, DL, ContainerVT, Temp, Op1, VL);\n    break;\n  }\n  }\n\n  if (!VT.isFixedLengthVector())\n    return Result;\n  return convertFromScalableVector(VT, Result, DAG, Subtarget);\n}",
      "start_line": 10544,
      "end_line": 10621,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "lowerVPSetCCMaskOp",
          "condition": "Condition",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "convertToScalableVector",
        "getSimpleValueType",
        "get",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getOperand",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPFPIntConvOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n\n  SDValue Src = Op.getOperand(0);\n  SDValue Mask = Op.getOperand(1);\n  SDValue VL = Op.getOperand(2);\n  unsigned RISCVISDOpc = getRISCVVLOp(Op);\n\n  MVT DstVT = Op.getSimpleValueType();\n  MVT SrcVT = Src.getSimpleValueType();\n  if (DstVT.isFixedLengthVector()) {\n    DstVT = getContainerForFixedLengthVector(DstVT);\n    SrcVT = getContainerForFixedLengthVector(SrcVT);\n    Src = convertToScalableVector(SrcVT, Src, DAG, Subtarget);\n    MVT MaskVT = getMaskTypeFor(DstVT);\n    Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n  }\n\n  unsigned DstEltSize = DstVT.getScalarSizeInBits();\n  unsigned SrcEltSize = SrcVT.getScalarSizeInBits();\n\n  SDValue Result;\n  if (DstEltSize >= SrcEltSize) { // Single-width and widening conversion.\n    if (SrcVT.isInteger()) {\n      assert(DstVT.isFloatingPoint() && \"Wrong input/output vector types\");\n\n      unsigned RISCVISDExtOpc = RISCVISDOpc == RISCVISD::SINT_TO_FP_VL\n                                    ? RISCVISD::VSEXT_VL\n                                    : RISCVISD::VZEXT_VL;\n\n      // Do we need to do any pre-widening before converting?\n      if (SrcEltSize == 1) {\n        MVT IntVT = DstVT.changeVectorElementTypeToInteger();\n        MVT XLenVT = Subtarget.getXLenVT();\n        SDValue Zero = DAG.getConstant(0, DL, XLenVT);\n        SDValue ZeroSplat = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IntVT,\n                                        DAG.getUNDEF(IntVT), Zero, VL);\n        SDValue One = DAG.getConstant(\n            RISCVISDExtOpc == RISCVISD::VZEXT_VL ? 1 : -1, DL, XLenVT);\n        SDValue OneSplat = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IntVT,\n                                       DAG.getUNDEF(IntVT), One, VL);\n        Src = DAG.getNode(RISCVISD::VMERGE_VL, DL, IntVT, Src, OneSplat,\n                          ZeroSplat, DAG.getUNDEF(IntVT), VL);\n      } else if (DstEltSize > (2 * SrcEltSize)) {\n        // Widen before converting.\n        MVT IntVT = MVT::getVectorVT(MVT::getIntegerVT(DstEltSize / 2),\n                                     DstVT.getVectorElementCount());\n        Src = DAG.getNode(RISCVISDExtOpc, DL, IntVT, Src, Mask, VL);\n      }\n\n      Result = DAG.getNode(RISCVISDOpc, DL, DstVT, Src, Mask, VL);\n    } else {\n      assert(SrcVT.isFloatingPoint() && DstVT.isInteger() &&\n             \"Wrong input/output vector types\");\n\n      // Convert f16 to f32 then convert f32 to i64.\n      if (DstEltSize > (2 * SrcEltSize)) {\n        assert(SrcVT.getVectorElementType() == MVT::f16 && \"Unexpected type!\");\n        MVT InterimFVT =\n            MVT::getVectorVT(MVT::f32, DstVT.getVectorElementCount());\n        Src =\n            DAG.getNode(RISCVISD::FP_EXTEND_VL, DL, InterimFVT, Src, Mask, VL);\n      }\n\n      Result = DAG.getNode(RISCVISDOpc, DL, DstVT, Src, Mask, VL);\n    }\n  } else { // Narrowing + Conversion\n    if (SrcVT.isInteger()) {\n      assert(DstVT.isFloatingPoint() && \"Wrong input/output vector types\");\n      // First do a narrowing convert to an FP type half the size, then round\n      // the FP type to a small FP type if needed.\n\n      MVT InterimFVT = DstVT;\n      if (SrcEltSize > (2 * DstEltSize)) {\n        assert(SrcEltSize == (4 * DstEltSize) && \"Unexpected types!\");\n        assert(DstVT.getVectorElementType() == MVT::f16 && \"Unexpected type!\");\n        InterimFVT = MVT::getVectorVT(MVT::f32, DstVT.getVectorElementCount());\n      }\n\n      Result = DAG.getNode(RISCVISDOpc, DL, InterimFVT, Src, Mask, VL);\n\n      if (InterimFVT != DstVT) {\n        Src = Result;\n        Result = DAG.getNode(RISCVISD::FP_ROUND_VL, DL, DstVT, Src, Mask, VL);\n      }\n    } else {\n      assert(SrcVT.isFloatingPoint() && DstVT.isInteger() &&\n             \"Wrong input/output vector types\");\n      // First do a narrowing conversion to an integer half the size, then\n      // truncate if needed.\n\n      if (DstEltSize == 1) {\n        // First convert to the same size integer, then convert to mask using\n        // setcc.\n        assert(SrcEltSize >= 16 && \"Unexpected FP type!\");\n        MVT InterimIVT = MVT::getVectorVT(MVT::getIntegerVT(SrcEltSize),\n                                          DstVT.getVectorElementCount());\n        Result = DAG.getNode(RISCVISDOpc, DL, InterimIVT, Src, Mask, VL);\n\n        // Compare the integer result to 0. The integer should be 0 or 1/-1,\n        // otherwise the conversion was undefined.\n        MVT XLenVT = Subtarget.getXLenVT();\n        SDValue SplatZero = DAG.getConstant(0, DL, XLenVT);\n        SplatZero = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, InterimIVT,\n                                DAG.getUNDEF(InterimIVT), SplatZero, VL);\n        Result = DAG.getNode(RISCVISD::SETCC_VL, DL, DstVT,\n                             {Result, SplatZero, DAG.getCondCode(ISD::SETNE),\n                              DAG.getUNDEF(DstVT), Mask, VL});\n      } else {\n        MVT InterimIVT = MVT::getVectorVT(MVT::getIntegerVT(SrcEltSize / 2),\n                                          DstVT.getVectorElementCount());\n\n        Result = DAG.getNode(RISCVISDOpc, DL, InterimIVT, Src, Mask, VL);\n\n        while (InterimIVT != DstVT) {\n          SrcEltSize /= 2;\n          Src = Result;\n          InterimIVT = MVT::getVectorVT(MVT::getIntegerVT(SrcEltSize / 2),\n                                        DstVT.getVectorElementCount());\n          Result = DAG.getNode(RISCVISD::TRUNCATE_VECTOR_VL, DL, InterimIVT,\n                               Src, Mask, VL);\n        }\n      }\n    }\n  }\n\n  MVT VT = Op.getSimpleValueType();\n  if (!VT.isFixedLengthVector())\n    return Result;\n  return convertFromScalableVector(VT, Result, DAG, Subtarget);\n}",
      "start_line": 10624,
      "end_line": 10755,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMaskTypeFor",
        "getRISCVVLOp",
        "changeVectorElementTypeToInteger",
        "getNode",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "convertFromScalableVector",
        "getUNDEF",
        "getOperand",
        "DL",
        "getVectorVT",
        "getXLenVT",
        "isInteger",
        "getScalarSizeInBits",
        "getVectorElementCount"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPSpliceExperimental",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n\n  SDValue Op1 = Op.getOperand(0);\n  SDValue Op2 = Op.getOperand(1);\n  SDValue Offset = Op.getOperand(2);\n  SDValue Mask = Op.getOperand(3);\n  SDValue EVL1 = Op.getOperand(4);\n  SDValue EVL2 = Op.getOperand(5);\n\n  const MVT XLenVT = Subtarget.getXLenVT();\n  MVT VT = Op.getSimpleValueType();\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    Op1 = convertToScalableVector(ContainerVT, Op1, DAG, Subtarget);\n    Op2 = convertToScalableVector(ContainerVT, Op2, DAG, Subtarget);\n    MVT MaskVT = getMaskTypeFor(ContainerVT);\n    Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n  }\n\n  bool IsMaskVector = VT.getVectorElementType() == MVT::i1;\n  if (IsMaskVector) {\n    ContainerVT = ContainerVT.changeVectorElementType(MVT::i8);\n\n    // Expand input operands\n    SDValue SplatOneOp1 = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                                      DAG.getUNDEF(ContainerVT),\n                                      DAG.getConstant(1, DL, XLenVT), EVL1);\n    SDValue SplatZeroOp1 = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                                       DAG.getUNDEF(ContainerVT),\n                                       DAG.getConstant(0, DL, XLenVT), EVL1);\n    Op1 = DAG.getNode(RISCVISD::VMERGE_VL, DL, ContainerVT, Op1, SplatOneOp1,\n                      SplatZeroOp1, DAG.getUNDEF(ContainerVT), EVL1);\n\n    SDValue SplatOneOp2 = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                                      DAG.getUNDEF(ContainerVT),\n                                      DAG.getConstant(1, DL, XLenVT), EVL2);\n    SDValue SplatZeroOp2 = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                                       DAG.getUNDEF(ContainerVT),\n                                       DAG.getConstant(0, DL, XLenVT), EVL2);\n    Op2 = DAG.getNode(RISCVISD::VMERGE_VL, DL, ContainerVT, Op2, SplatOneOp2,\n                      SplatZeroOp2, DAG.getUNDEF(ContainerVT), EVL2);\n  }\n\n  int64_t ImmValue = cast<ConstantSDNode>(Offset)->getSExtValue();\n  SDValue DownOffset, UpOffset;\n  if (ImmValue >= 0) {\n    // The operand is a TargetConstant, we need to rebuild it as a regular\n    // constant.\n    DownOffset = DAG.getConstant(ImmValue, DL, XLenVT);\n    UpOffset = DAG.getNode(ISD::SUB, DL, XLenVT, EVL1, DownOffset);\n  } else {\n    // The operand is a TargetConstant, we need to rebuild it as a regular\n    // constant rather than negating the original operand.\n    UpOffset = DAG.getConstant(-ImmValue, DL, XLenVT);\n    DownOffset = DAG.getNode(ISD::SUB, DL, XLenVT, EVL1, UpOffset);\n  }\n\n  SDValue SlideDown =\n      getVSlidedown(DAG, Subtarget, DL, ContainerVT, DAG.getUNDEF(ContainerVT),\n                    Op1, DownOffset, Mask, UpOffset);\n  SDValue Result = getVSlideup(DAG, Subtarget, DL, ContainerVT, SlideDown, Op2,\n                               UpOffset, Mask, EVL2, RISCVII::TAIL_AGNOSTIC);\n\n  if (IsMaskVector) {\n    // Truncate Result back to a mask vector (Result has same EVL as Op2)\n    Result = DAG.getNode(\n        RISCVISD::SETCC_VL, DL, ContainerVT.changeVectorElementType(MVT::i1),\n        {Result, DAG.getConstant(0, DL, ContainerVT),\n         DAG.getCondCode(ISD::SETNE), DAG.getUNDEF(getMaskTypeFor(ContainerVT)),\n         Mask, EVL2});\n  }\n\n  if (!VT.isFixedLengthVector())\n    return Result;\n  return convertFromScalableVector(VT, Result, DAG, Subtarget);\n}",
      "start_line": 10757,
      "end_line": 10836,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMaskTypeFor",
        "getNode",
        "getVSlideup",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "getVSlidedown",
        "vector",
        "getOperand",
        "getUNDEF",
        "convertFromScalableVector",
        "getVectorElementType",
        "DL",
        "getCondCode",
        "getXLenVT",
        "getSExtValue",
        "changeVectorElementType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPReverseExperimental",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  SDValue Op1 = Op.getOperand(0);\n  SDValue Mask = Op.getOperand(1);\n  SDValue EVL = Op.getOperand(2);\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    Op1 = convertToScalableVector(ContainerVT, Op1, DAG, Subtarget);\n    MVT MaskVT = getMaskTypeFor(ContainerVT);\n    Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n  }\n\n  MVT GatherVT = ContainerVT;\n  MVT IndicesVT = ContainerVT.changeVectorElementTypeToInteger();\n  // Check if we are working with mask vectors\n  bool IsMaskVector = ContainerVT.getVectorElementType() == MVT::i1;\n  if (IsMaskVector) {\n    GatherVT = IndicesVT = ContainerVT.changeVectorElementType(MVT::i8);\n\n    // Expand input operand\n    SDValue SplatOne = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,\n                                   DAG.getUNDEF(IndicesVT),\n                                   DAG.getConstant(1, DL, XLenVT), EVL);\n    SDValue SplatZero = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,\n                                    DAG.getUNDEF(IndicesVT),\n                                    DAG.getConstant(0, DL, XLenVT), EVL);\n    Op1 = DAG.getNode(RISCVISD::VMERGE_VL, DL, IndicesVT, Op1, SplatOne,\n                      SplatZero, DAG.getUNDEF(IndicesVT), EVL);\n  }\n\n  unsigned EltSize = GatherVT.getScalarSizeInBits();\n  unsigned MinSize = GatherVT.getSizeInBits().getKnownMinValue();\n  unsigned VectorBitsMax = Subtarget.getRealMaxVLen();\n  unsigned MaxVLMAX =\n      RISCVTargetLowering::computeVLMAX(VectorBitsMax, EltSize, MinSize);\n\n  unsigned GatherOpc = RISCVISD::VRGATHER_VV_VL;\n  // If this is SEW=8 and VLMAX is unknown or more than 256, we need\n  // to use vrgatherei16.vv.\n  // TODO: It's also possible to use vrgatherei16.vv for other types to\n  // decrease register width for the index calculation.\n  // NOTE: This code assumes VLMAX <= 65536 for LMUL=8 SEW=16.\n  if (MaxVLMAX > 256 && EltSize == 8) {\n    // If this is LMUL=8, we have to split before using vrgatherei16.vv.\n    // Split the vector in half and reverse each half using a full register\n    // reverse.\n    // Swap the halves and concatenate them.\n    // Slide the concatenated result by (VLMax - VL).\n    if (MinSize == (8 * RISCV::RVVBitsPerBlock)) {\n      auto [LoVT, HiVT] = DAG.GetSplitDestVTs(GatherVT);\n      auto [Lo, Hi] = DAG.SplitVector(Op1, DL);\n\n      SDValue LoRev = DAG.getNode(ISD::VECTOR_REVERSE, DL, LoVT, Lo);\n      SDValue HiRev = DAG.getNode(ISD::VECTOR_REVERSE, DL, HiVT, Hi);\n\n      // Reassemble the low and high pieces reversed.\n      // NOTE: this Result is unmasked (because we do not need masks for\n      // shuffles). If in the future this has to change, we can use a SELECT_VL\n      // between Result and UNDEF using the mask originally passed to VP_REVERSE\n      SDValue Result =\n          DAG.getNode(ISD::CONCAT_VECTORS, DL, GatherVT, HiRev, LoRev);\n\n      // Slide off any elements from past EVL that were reversed into the low\n      // elements.\n      unsigned MinElts = GatherVT.getVectorMinNumElements();\n      SDValue VLMax = DAG.getNode(ISD::VSCALE, DL, XLenVT,\n                                  DAG.getConstant(MinElts, DL, XLenVT));\n      SDValue Diff = DAG.getNode(ISD::SUB, DL, XLenVT, VLMax, EVL);\n\n      Result = getVSlidedown(DAG, Subtarget, DL, GatherVT,\n                             DAG.getUNDEF(GatherVT), Result, Diff, Mask, EVL);\n\n      if (IsMaskVector) {\n        // Truncate Result back to a mask vector\n        Result =\n            DAG.getNode(RISCVISD::SETCC_VL, DL, ContainerVT,\n                        {Result, DAG.getConstant(0, DL, GatherVT),\n                         DAG.getCondCode(ISD::SETNE),\n                         DAG.getUNDEF(getMaskTypeFor(ContainerVT)), Mask, EVL});\n      }\n\n      if (!VT.isFixedLengthVector())\n        return Result;\n      return convertFromScalableVector(VT, Result, DAG, Subtarget);\n    }\n\n    // Just promote the int type to i16 which will double the LMUL.\n    IndicesVT = MVT::getVectorVT(MVT::i16, IndicesVT.getVectorElementCount());\n    GatherOpc = RISCVISD::VRGATHEREI16_VV_VL;\n  }\n\n  SDValue VID = DAG.getNode(RISCVISD::VID_VL, DL, IndicesVT, Mask, EVL);\n  SDValue VecLen =\n      DAG.getNode(ISD::SUB, DL, XLenVT, EVL, DAG.getConstant(1, DL, XLenVT));\n  SDValue VecLenSplat = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,\n                                    DAG.getUNDEF(IndicesVT), VecLen, EVL);\n  SDValue VRSUB = DAG.getNode(RISCVISD::SUB_VL, DL, IndicesVT, VecLenSplat, VID,\n                              DAG.getUNDEF(IndicesVT), Mask, EVL);\n  SDValue Result = DAG.getNode(GatherOpc, DL, GatherVT, Op1, VRSUB,\n                               DAG.getUNDEF(GatherVT), Mask, EVL);\n\n  if (IsMaskVector) {\n    // Truncate Result back to a mask vector\n    Result = DAG.getNode(\n        RISCVISD::SETCC_VL, DL, ContainerVT,\n        {Result, DAG.getConstant(0, DL, GatherVT), DAG.getCondCode(ISD::SETNE),\n         DAG.getUNDEF(getMaskTypeFor(ContainerVT)), Mask, EVL});\n  }\n\n  if (!VT.isFixedLengthVector())\n    return Result;\n  return convertFromScalableVector(VT, Result, DAG, Subtarget);\n}",
      "start_line": 10838,
      "end_line": 10957,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "computeVLMAX",
        "getRealMaxVLen",
        "changeVectorElementTypeToInteger",
        "unmasked",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getSizeInBits",
        "getKnownMinValue",
        "getScalarSizeInBits",
        "getVectorMinNumElements",
        "getUNDEF",
        "getVectorElementType",
        "getVectorVT",
        "getContainerForFixedLengthVector",
        "getVSlidedown",
        "getOperand",
        "by",
        "getCondCode",
        "SplitVector",
        "changeVectorElementType",
        "getMaskTypeFor",
        "GetSplitDestVTs",
        "DL",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerLogicVPOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MVT VT = Op.getSimpleValueType();\n  if (VT.getVectorElementType() != MVT::i1)\n    return lowerVPOp(Op, DAG);\n\n  // It is safe to drop mask parameter as masked-off elements are undef.\n  SDValue Op1 = Op->getOperand(0);\n  SDValue Op2 = Op->getOperand(1);\n  SDValue VL = Op->getOperand(3);\n\n  MVT ContainerVT = VT;\n  const bool IsFixed = VT.isFixedLengthVector();\n  if (IsFixed) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    Op1 = convertToScalableVector(ContainerVT, Op1, DAG, Subtarget);\n    Op2 = convertToScalableVector(ContainerVT, Op2, DAG, Subtarget);\n  }\n\n  SDLoc DL(Op);\n  SDValue Val = DAG.getNode(getRISCVVLOp(Op), DL, ContainerVT, Op1, Op2, VL);\n  if (!IsFixed)\n    return Val;\n  return convertFromScalableVector(VT, Val, DAG, Subtarget);\n}",
      "start_line": 10959,
      "end_line": 10983,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNode",
        "convertToScalableVector",
        "getContainerForFixedLengthVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getOperand",
        "DL",
        "lowerVPOp",
        "isFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPStridedLoad",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT XLenVT = Subtarget.getXLenVT();\n  MVT VT = Op.getSimpleValueType();\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector())\n    ContainerVT = getContainerForFixedLengthVector(VT);\n\n  SDVTList VTs = DAG.getVTList({ContainerVT, MVT::Other});\n\n  auto *VPNode = cast<VPStridedLoadSDNode>(Op);\n  // Check if the mask is known to be all ones\n  SDValue Mask = VPNode->getMask();\n  bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n  SDValue IntID = DAG.getTargetConstant(IsUnmasked ? Intrinsic::riscv_vlse\n                                                   : Intrinsic::riscv_vlse_mask,\n                                        DL, XLenVT);\n  SmallVector<SDValue, 8> Ops{VPNode->getChain(), IntID,\n                              DAG.getUNDEF(ContainerVT), VPNode->getBasePtr(),\n                              VPNode->getStride()};\n  if (!IsUnmasked) {\n    if (VT.isFixedLengthVector()) {\n      MVT MaskVT = ContainerVT.changeVectorElementType(MVT::i1);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n    Ops.push_back(Mask);\n  }\n  Ops.push_back(VPNode->getVectorLength());\n  if (!IsUnmasked) {\n    SDValue Policy = DAG.getTargetConstant(RISCVII::TAIL_AGNOSTIC, DL, XLenVT);\n    Ops.push_back(Policy);\n  }\n\n  SDValue Result =\n      DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops,\n                              VPNode->getMemoryVT(), VPNode->getMemOperand());\n  SDValue Chain = Result.getValue(1);\n\n  if (VT.isFixedLengthVector())\n    Result = convertFromScalableVector(VT, Result, DAG, Subtarget);\n\n  return DAG.getMergeValues({Result, Chain}, DL);\n}",
      "start_line": 10985,
      "end_line": 11029,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getChain",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getMask",
        "getUNDEF",
        "getMemIntrinsicNode",
        "push_back",
        "getMemOperand",
        "getValue",
        "getVTList",
        "getContainerForFixedLengthVector",
        "changeVectorElementType",
        "isConstantSplatVectorAllOnes",
        "getMergeValues",
        "getBasePtr",
        "getStride",
        "DL",
        "getXLenVT",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVPStridedStore",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  auto *VPNode = cast<VPStridedStoreSDNode>(Op);\n  SDValue StoreVal = VPNode->getValue();\n  MVT VT = StoreVal.getSimpleValueType();\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    StoreVal = convertToScalableVector(ContainerVT, StoreVal, DAG, Subtarget);\n  }\n\n  // Check if the mask is known to be all ones\n  SDValue Mask = VPNode->getMask();\n  bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n  SDValue IntID = DAG.getTargetConstant(IsUnmasked ? Intrinsic::riscv_vsse\n                                                   : Intrinsic::riscv_vsse_mask,\n                                        DL, XLenVT);\n  SmallVector<SDValue, 8> Ops{VPNode->getChain(), IntID, StoreVal,\n                              VPNode->getBasePtr(), VPNode->getStride()};\n  if (!IsUnmasked) {\n    if (VT.isFixedLengthVector()) {\n      MVT MaskVT = ContainerVT.changeVectorElementType(MVT::i1);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n    Ops.push_back(Mask);\n  }\n  Ops.push_back(VPNode->getVectorLength());\n\n  return DAG.getMemIntrinsicNode(ISD::INTRINSIC_VOID, DL, VPNode->getVTList(),\n                                 Ops, VPNode->getMemoryVT(),\n                                 VPNode->getMemOperand());\n}",
      "start_line": 11031,
      "end_line": 11066,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMemOperand",
        "getValue",
        "getBasePtr",
        "getChain",
        "getMask",
        "getMemoryVT",
        "getStride",
        "convertToScalableVector",
        "getSimpleValueType",
        "getContainerForFixedLengthVector",
        "getMemIntrinsicNode",
        "DL",
        "push_back",
        "getXLenVT",
        "changeVectorElementType",
        "isConstantSplatVectorAllOnes",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerMaskedGather",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  const auto *MemSD = cast<MemSDNode>(Op.getNode());\n  EVT MemVT = MemSD->getMemoryVT();\n  MachineMemOperand *MMO = MemSD->getMemOperand();\n  SDValue Chain = MemSD->getChain();\n  SDValue BasePtr = MemSD->getBasePtr();\n\n  ISD::LoadExtType LoadExtType;\n  SDValue Index, Mask, PassThru, VL;\n\n  if (auto *VPGN = dyn_cast<VPGatherSDNode>(Op.getNode())) {\n    Index = VPGN->getIndex();\n    Mask = VPGN->getMask();\n    PassThru = DAG.getUNDEF(VT);\n    VL = VPGN->getVectorLength();\n    // VP doesn't support extending loads.\n    LoadExtType = ISD::NON_EXTLOAD;\n  } else {\n    // Else it must be a MGATHER.\n    auto *MGN = cast<MaskedGatherSDNode>(Op.getNode());\n    Index = MGN->getIndex();\n    Mask = MGN->getMask();\n    PassThru = MGN->getPassThru();\n    LoadExtType = MGN->getExtensionType();\n  }\n\n  MVT IndexVT = Index.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  assert(VT.getVectorElementCount() == IndexVT.getVectorElementCount() &&\n         \"Unexpected VTs!\");\n  assert(BasePtr.getSimpleValueType() == XLenVT && \"Unexpected pointer type\");\n  // Targets have to explicitly opt-in for extending vector loads.\n  assert(LoadExtType == ISD::NON_EXTLOAD &&\n         \"Unexpected extending MGATHER/VP_GATHER\");\n  (void)LoadExtType;\n\n  // If the mask is known to be all ones, optimize to an unmasked intrinsic;\n  // the selection of the masked intrinsics doesn't do this for us.\n  bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    IndexVT = MVT::getVectorVT(IndexVT.getVectorElementType(),\n                               ContainerVT.getVectorElementCount());\n\n    Index = convertToScalableVector(IndexVT, Index, DAG, Subtarget);\n\n    if (!IsUnmasked) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n      PassThru = convertToScalableVector(ContainerVT, PassThru, DAG, Subtarget);\n    }\n  }\n\n  if (!VL)\n    VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n\n  if (XLenVT == MVT::i32 && IndexVT.getVectorElementType().bitsGT(XLenVT)) {\n    IndexVT = IndexVT.changeVectorElementType(XLenVT);\n    Index = DAG.getNode(ISD::TRUNCATE, DL, IndexVT, Index);\n  }\n\n  unsigned IntID =\n      IsUnmasked ? Intrinsic::riscv_vluxei : Intrinsic::riscv_vluxei_mask;\n  SmallVector<SDValue, 8> Ops{Chain, DAG.getTargetConstant(IntID, DL, XLenVT)};\n  if (IsUnmasked)\n    Ops.push_back(DAG.getUNDEF(ContainerVT));\n  else\n    Ops.push_back(PassThru);\n  Ops.push_back(BasePtr);\n  Ops.push_back(Index);\n  if (!IsUnmasked)\n    Ops.push_back(Mask);\n  Ops.push_back(VL);\n  if (!IsUnmasked)\n    Ops.push_back(DAG.getTargetConstant(RISCVII::TAIL_AGNOSTIC, DL, XLenVT));\n\n  SDVTList VTs = DAG.getVTList({ContainerVT, MVT::Other});\n  SDValue Result =\n      DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops, MemVT, MMO);\n  Chain = Result.getValue(1);\n\n  if (VT.isFixedLengthVector())\n    Result = convertFromScalableVector(VT, Result, DAG, Subtarget);\n\n  return DAG.getMergeValues({Result, Chain}, DL);\n}",
      "start_line": 11074,
      "end_line": 11166,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getChain",
        "getMemoryVT",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "bitsGT",
        "getIndex",
        "getMask",
        "getVectorLength",
        "getUNDEF",
        "getPassThru",
        "getMemIntrinsicNode",
        "getVectorVT",
        "getVectorElementCount",
        "push_back",
        "getMemOperand",
        "getValue",
        "getVTList",
        "getContainerForFixedLengthVector",
        "getExtensionType",
        "changeVectorElementType",
        "isConstantSplatVectorAllOnes",
        "getMaskTypeFor",
        "getMergeValues",
        "getBasePtr",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerMaskedScatter",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(Op);\n  const auto *MemSD = cast<MemSDNode>(Op.getNode());\n  EVT MemVT = MemSD->getMemoryVT();\n  MachineMemOperand *MMO = MemSD->getMemOperand();\n  SDValue Chain = MemSD->getChain();\n  SDValue BasePtr = MemSD->getBasePtr();\n\n  bool IsTruncatingStore = false;\n  SDValue Index, Mask, Val, VL;\n\n  if (auto *VPSN = dyn_cast<VPScatterSDNode>(Op.getNode())) {\n    Index = VPSN->getIndex();\n    Mask = VPSN->getMask();\n    Val = VPSN->getValue();\n    VL = VPSN->getVectorLength();\n    // VP doesn't support truncating stores.\n    IsTruncatingStore = false;\n  } else {\n    // Else it must be a MSCATTER.\n    auto *MSN = cast<MaskedScatterSDNode>(Op.getNode());\n    Index = MSN->getIndex();\n    Mask = MSN->getMask();\n    Val = MSN->getValue();\n    IsTruncatingStore = MSN->isTruncatingStore();\n  }\n\n  MVT VT = Val.getSimpleValueType();\n  MVT IndexVT = Index.getSimpleValueType();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  assert(VT.getVectorElementCount() == IndexVT.getVectorElementCount() &&\n         \"Unexpected VTs!\");\n  assert(BasePtr.getSimpleValueType() == XLenVT && \"Unexpected pointer type\");\n  // Targets have to explicitly opt-in for extending vector loads and\n  // truncating vector stores.\n  assert(!IsTruncatingStore && \"Unexpected truncating MSCATTER/VP_SCATTER\");\n  (void)IsTruncatingStore;\n\n  // If the mask is known to be all ones, optimize to an unmasked intrinsic;\n  // the selection of the masked intrinsics doesn't do this for us.\n  bool IsUnmasked = ISD::isConstantSplatVectorAllOnes(Mask.getNode());\n\n  MVT ContainerVT = VT;\n  if (VT.isFixedLengthVector()) {\n    ContainerVT = getContainerForFixedLengthVector(VT);\n    IndexVT = MVT::getVectorVT(IndexVT.getVectorElementType(),\n                               ContainerVT.getVectorElementCount());\n\n    Index = convertToScalableVector(IndexVT, Index, DAG, Subtarget);\n    Val = convertToScalableVector(ContainerVT, Val, DAG, Subtarget);\n\n    if (!IsUnmasked) {\n      MVT MaskVT = getMaskTypeFor(ContainerVT);\n      Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);\n    }\n  }\n\n  if (!VL)\n    VL = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).second;\n\n  if (XLenVT == MVT::i32 && IndexVT.getVectorElementType().bitsGT(XLenVT)) {\n    IndexVT = IndexVT.changeVectorElementType(XLenVT);\n    Index = DAG.getNode(ISD::TRUNCATE, DL, IndexVT, Index);\n  }\n\n  unsigned IntID =\n      IsUnmasked ? Intrinsic::riscv_vsoxei : Intrinsic::riscv_vsoxei_mask;\n  SmallVector<SDValue, 8> Ops{Chain, DAG.getTargetConstant(IntID, DL, XLenVT)};\n  Ops.push_back(Val);\n  Ops.push_back(BasePtr);\n  Ops.push_back(Index);\n  if (!IsUnmasked)\n    Ops.push_back(Mask);\n  Ops.push_back(VL);\n\n  return DAG.getMemIntrinsicNode(ISD::INTRINSIC_VOID, DL,\n                                 DAG.getVTList(MVT::Other), Ops, MemVT, MMO);\n}",
      "start_line": 11174,
      "end_line": 11253,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "getChain",
        "getMemoryVT",
        "convertToScalableVector",
        "getSimpleValueType",
        "bitsGT",
        "getIndex",
        "getMask",
        "getVectorLength",
        "getMemIntrinsicNode",
        "getVectorVT",
        "getVectorElementCount",
        "push_back",
        "getMemOperand",
        "getValue",
        "getContainerForFixedLengthVector",
        "isTruncatingStore",
        "changeVectorElementType",
        "isConstantSplatVectorAllOnes",
        "getMaskTypeFor",
        "getBasePtr",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerGET_ROUNDING",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  const MVT XLenVT = Subtarget.getXLenVT();\n  SDLoc DL(Op);\n  SDValue Chain = Op->getOperand(0);\n  SDValue SysRegNo = DAG.getTargetConstant(\n      RISCVSysReg::lookupSysRegByName(\"FRM\")->Encoding, DL, XLenVT);\n  SDVTList VTs = DAG.getVTList(XLenVT, MVT::Other);\n  SDValue RM = DAG.getNode(RISCVISD::READ_CSR, DL, VTs, Chain, SysRegNo);\n\n  // Encoding used for rounding mode in RISC-V differs from that used in\n  // FLT_ROUNDS. To convert it the RISC-V rounding mode is used as an index in a\n  // table, which consists of a sequence of 4-bit fields, each representing\n  // corresponding FLT_ROUNDS mode.\n  static const int Table =\n      (int(RoundingMode::NearestTiesToEven) << 4 * RISCVFPRndMode::RNE) |\n      (int(RoundingMode::TowardZero) << 4 * RISCVFPRndMode::RTZ) |\n      (int(RoundingMode::TowardNegative) << 4 * RISCVFPRndMode::RDN) |\n      (int(RoundingMode::TowardPositive) << 4 * RISCVFPRndMode::RUP) |\n      (int(RoundingMode::NearestTiesToAway) << 4 * RISCVFPRndMode::RMM);\n\n  SDValue Shift =\n      DAG.getNode(ISD::SHL, DL, XLenVT, RM, DAG.getConstant(2, DL, XLenVT));\n  SDValue Shifted = DAG.getNode(ISD::SRL, DL, XLenVT,\n                                DAG.getConstant(Table, DL, XLenVT), Shift);\n  SDValue Masked = DAG.getNode(ISD::AND, DL, XLenVT, Shifted,\n                               DAG.getConstant(7, DL, XLenVT));\n\n  return DAG.getMergeValues({Masked, Chain}, DL);\n}",
      "start_line": 11255,
      "end_line": 11284,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMergeValues",
        "getVTList",
        "int",
        "getOperand",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerSET_ROUNDING",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  const MVT XLenVT = Subtarget.getXLenVT();\n  SDLoc DL(Op);\n  SDValue Chain = Op->getOperand(0);\n  SDValue RMValue = Op->getOperand(1);\n  SDValue SysRegNo = DAG.getTargetConstant(\n      RISCVSysReg::lookupSysRegByName(\"FRM\")->Encoding, DL, XLenVT);\n\n  // Encoding used for rounding mode in RISC-V differs from that used in\n  // FLT_ROUNDS. To convert it the C rounding mode is used as an index in\n  // a table, which consists of a sequence of 4-bit fields, each representing\n  // corresponding RISC-V mode.\n  static const unsigned Table =\n      (RISCVFPRndMode::RNE << 4 * int(RoundingMode::NearestTiesToEven)) |\n      (RISCVFPRndMode::RTZ << 4 * int(RoundingMode::TowardZero)) |\n      (RISCVFPRndMode::RDN << 4 * int(RoundingMode::TowardNegative)) |\n      (RISCVFPRndMode::RUP << 4 * int(RoundingMode::TowardPositive)) |\n      (RISCVFPRndMode::RMM << 4 * int(RoundingMode::NearestTiesToAway));\n\n  RMValue = DAG.getNode(ISD::ZERO_EXTEND, DL, XLenVT, RMValue);\n\n  SDValue Shift = DAG.getNode(ISD::SHL, DL, XLenVT, RMValue,\n                              DAG.getConstant(2, DL, XLenVT));\n  SDValue Shifted = DAG.getNode(ISD::SRL, DL, XLenVT,\n                                DAG.getConstant(Table, DL, XLenVT), Shift);\n  RMValue = DAG.getNode(ISD::AND, DL, XLenVT, Shifted,\n                        DAG.getConstant(0x7, DL, XLenVT));\n  return DAG.getNode(RISCVISD::WRITE_CSR, DL, MVT::Other, Chain, SysRegNo,\n                     RMValue);\n}",
      "start_line": 11286,
      "end_line": 11316,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "int",
        "getOperand",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerEH_DWARF_CFA",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MachineFunction &MF = DAG.getMachineFunction();\n\n  bool isRISCV64 = Subtarget.is64Bit();\n  EVT PtrVT = getPointerTy(DAG.getDataLayout());\n\n  int FI = MF.getFrameInfo().CreateFixedObject(isRISCV64 ? 8 : 4, 0, false);\n  return DAG.getFrameIndex(FI, PtrVT);\n}",
      "start_line": 11318,
      "end_line": 11327,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "CreateFixedObject",
        "getFrameIndex",
        "getMachineFunction",
        "getFrameInfo",
        "is64Bit",
        "getPointerTy"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRISCVWOpcode",
      "return_type": "NodeType",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        }
      ],
      "body": "{\n  switch (Opcode) {\n  default:\n    llvm_unreachable(\"Unexpected opcode\");\n  case ISD::SHL:\n    return RISCVISD::SLLW;\n  case ISD::SRA:\n    return RISCVISD::SRAW;\n  case ISD::SRL:\n    return RISCVISD::SRLW;\n  case ISD::SDIV:\n    return RISCVISD::DIVW;\n  case ISD::UDIV:\n    return RISCVISD::DIVUW;\n  case ISD::UREM:\n    return RISCVISD::REMUW;\n  case ISD::ROTL:\n    return RISCVISD::ROLW;\n  case ISD::ROTR:\n    return RISCVISD::RORW;\n  }\n}",
      "start_line": 11331,
      "end_line": 11352,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getRISCVWOpcode",
          "condition": "Opcode",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "customLegalizeToWOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned ExtOpc =",
          "name": "ISD::ANY_EXTEND"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  RISCVISD::NodeType WOpcode = getRISCVWOpcode(N->getOpcode());\n  SDValue NewOp0 = DAG.getNode(ExtOpc, DL, MVT::i64, N->getOperand(0));\n  SDValue NewOp1 = DAG.getNode(ExtOpc, DL, MVT::i64, N->getOperand(1));\n  SDValue NewRes = DAG.getNode(WOpcode, DL, MVT::i64, NewOp0, NewOp1);\n  // ReplaceNodeResults requires we maintain the same type for the return value.\n  return DAG.getNode(ISD::TRUNCATE, DL, N->getValueType(0), NewRes);\n}",
      "start_line": 11359,
      "end_line": 11368,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getNode",
        "getRISCVWOpcode",
        "DL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "customLegalizeToWOpWithSExt",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  SDValue NewOp0 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(0));\n  SDValue NewOp1 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(1));\n  SDValue NewWOp = DAG.getNode(N->getOpcode(), DL, MVT::i64, NewOp0, NewOp1);\n  SDValue NewRes = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, NewWOp,\n                               DAG.getValueType(MVT::i32));\n  return DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, NewRes);\n}",
      "start_line": 11372,
      "end_line": 11380,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getNode",
        "DL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "ReplaceNodeResults",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SmallVectorImpl<SDValue>",
          "name": "&Results"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  switch (N->getOpcode()) {\n  default:\n    llvm_unreachable(\"Don't know how to custom type legalize this operation!\");\n  case ISD::STRICT_FP_TO_SINT:\n  case ISD::STRICT_FP_TO_UINT:\n  case ISD::FP_TO_SINT:\n  case ISD::FP_TO_UINT: {\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n    bool IsStrict = N->isStrictFPOpcode();\n    bool IsSigned = N->getOpcode() == ISD::FP_TO_SINT ||\n                    N->getOpcode() == ISD::STRICT_FP_TO_SINT;\n    SDValue Op0 = IsStrict ? N->getOperand(1) : N->getOperand(0);\n    if (getTypeAction(*DAG.getContext(), Op0.getValueType()) !=\n        TargetLowering::TypeSoftenFloat) {\n      if (!isTypeLegal(Op0.getValueType()))\n        return;\n      if (IsStrict) {\n        SDValue Chain = N->getOperand(0);\n        // In absense of Zfh, promote f16 to f32, then convert.\n        if (Op0.getValueType() == MVT::f16 &&\n            !Subtarget.hasStdExtZfhOrZhinx()) {\n          Op0 = DAG.getNode(ISD::STRICT_FP_EXTEND, DL, {MVT::f32, MVT::Other},\n                            {Chain, Op0});\n          Chain = Op0.getValue(1);\n        }\n        unsigned Opc = IsSigned ? RISCVISD::STRICT_FCVT_W_RV64\n                                : RISCVISD::STRICT_FCVT_WU_RV64;\n        SDVTList VTs = DAG.getVTList(MVT::i64, MVT::Other);\n        SDValue Res = DAG.getNode(\n            Opc, DL, VTs, Chain, Op0,\n            DAG.getTargetConstant(RISCVFPRndMode::RTZ, DL, MVT::i64));\n        Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n        Results.push_back(Res.getValue(1));\n        return;\n      }\n      // For bf16, or f16 in absense of Zfh, promote [b]f16 to f32 and then\n      // convert.\n      if ((Op0.getValueType() == MVT::f16 &&\n           !Subtarget.hasStdExtZfhOrZhinx()) ||\n          Op0.getValueType() == MVT::bf16)\n        Op0 = DAG.getNode(ISD::FP_EXTEND, DL, MVT::f32, Op0);\n\n      unsigned Opc = IsSigned ? RISCVISD::FCVT_W_RV64 : RISCVISD::FCVT_WU_RV64;\n      SDValue Res =\n          DAG.getNode(Opc, DL, MVT::i64, Op0,\n                      DAG.getTargetConstant(RISCVFPRndMode::RTZ, DL, MVT::i64));\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n    // If the FP type needs to be softened, emit a library call using the 'si'\n    // version. If we left it to default legalization we'd end up with 'di'. If\n    // the FP type doesn't need to be softened just let generic type\n    // legalization promote the result type.\n    RTLIB::Libcall LC;\n    if (IsSigned)\n      LC = RTLIB::getFPTOSINT(Op0.getValueType(), N->getValueType(0));\n    else\n      LC = RTLIB::getFPTOUINT(Op0.getValueType(), N->getValueType(0));\n    MakeLibCallOptions CallOptions;\n    EVT OpVT = Op0.getValueType();\n    CallOptions.setTypeListBeforeSoften(OpVT, N->getValueType(0), true);\n    SDValue Chain = IsStrict ? N->getOperand(0) : SDValue();\n    SDValue Result;\n    std::tie(Result, Chain) =\n        makeLibCall(DAG, LC, N->getValueType(0), Op0, CallOptions, DL, Chain);\n    Results.push_back(Result);\n    if (IsStrict)\n      Results.push_back(Chain);\n    break;\n  }\n  case ISD::LROUND: {\n    SDValue Op0 = N->getOperand(0);\n    EVT Op0VT = Op0.getValueType();\n    if (getTypeAction(*DAG.getContext(), Op0.getValueType()) !=\n        TargetLowering::TypeSoftenFloat) {\n      if (!isTypeLegal(Op0VT))\n        return;\n\n      // In absense of Zfh, promote f16 to f32, then convert.\n      if (Op0.getValueType() == MVT::f16 && !Subtarget.hasStdExtZfhOrZhinx())\n        Op0 = DAG.getNode(ISD::FP_EXTEND, DL, MVT::f32, Op0);\n\n      SDValue Res =\n          DAG.getNode(RISCVISD::FCVT_W_RV64, DL, MVT::i64, Op0,\n                      DAG.getTargetConstant(RISCVFPRndMode::RMM, DL, MVT::i64));\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n    // If the FP type needs to be softened, emit a library call to lround. We'll\n    // need to truncate the result. We assume any value that doesn't fit in i32\n    // is allowed to return an unspecified value.\n    RTLIB::Libcall LC =\n        Op0.getValueType() == MVT::f64 ? RTLIB::LROUND_F64 : RTLIB::LROUND_F32;\n    MakeLibCallOptions CallOptions;\n    EVT OpVT = Op0.getValueType();\n    CallOptions.setTypeListBeforeSoften(OpVT, MVT::i64, true);\n    SDValue Result = makeLibCall(DAG, LC, MVT::i64, Op0, CallOptions, DL).first;\n    Result = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Result);\n    Results.push_back(Result);\n    break;\n  }\n  case ISD::READCYCLECOUNTER: {\n    assert(!Subtarget.is64Bit() &&\n           \"READCYCLECOUNTER only has custom type legalization on riscv32\");\n\n    SDVTList VTs = DAG.getVTList(MVT::i32, MVT::i32, MVT::Other);\n    SDValue RCW =\n        DAG.getNode(RISCVISD::READ_CYCLE_WIDE, DL, VTs, N->getOperand(0));\n\n    Results.push_back(\n        DAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, RCW, RCW.getValue(1)));\n    Results.push_back(RCW.getValue(2));\n    break;\n  }\n  case ISD::LOAD: {\n    if (!ISD::isNON_EXTLoad(N))\n      return;\n\n    // Use a SEXTLOAD instead of the default EXTLOAD. Similar to the\n    // sext_inreg we emit for ADD/SUB/MUL/SLLI.\n    LoadSDNode *Ld = cast<LoadSDNode>(N);\n\n    SDLoc dl(N);\n    SDValue Res = DAG.getExtLoad(ISD::SEXTLOAD, dl, MVT::i64, Ld->getChain(),\n                                 Ld->getBasePtr(), Ld->getMemoryVT(),\n                                 Ld->getMemOperand());\n    Results.push_back(DAG.getNode(ISD::TRUNCATE, dl, MVT::i32, Res));\n    Results.push_back(Res.getValue(1));\n    return;\n  }\n  case ISD::MUL: {\n    unsigned Size = N->getSimpleValueType(0).getSizeInBits();\n    unsigned XLen = Subtarget.getXLen();\n    // This multiply needs to be expanded, try to use MULHSU+MUL if possible.\n    if (Size > XLen) {\n      assert(Size == (XLen * 2) && \"Unexpected custom legalisation\");\n      SDValue LHS = N->getOperand(0);\n      SDValue RHS = N->getOperand(1);\n      APInt HighMask = APInt::getHighBitsSet(Size, XLen);\n\n      bool LHSIsU = DAG.MaskedValueIsZero(LHS, HighMask);\n      bool RHSIsU = DAG.MaskedValueIsZero(RHS, HighMask);\n      // We need exactly one side to be unsigned.\n      if (LHSIsU == RHSIsU)\n        return;\n\n      auto MakeMULPair = [&](SDValue S, SDValue U) {\n        MVT XLenVT = Subtarget.getXLenVT();\n        S = DAG.getNode(ISD::TRUNCATE, DL, XLenVT, S);\n        U = DAG.getNode(ISD::TRUNCATE, DL, XLenVT, U);\n        SDValue Lo = DAG.getNode(ISD::MUL, DL, XLenVT, S, U);\n        SDValue Hi = DAG.getNode(RISCVISD::MULHSU, DL, XLenVT, S, U);\n        return DAG.getNode(ISD::BUILD_PAIR, DL, N->getValueType(0), Lo, Hi);\n      };\n\n      bool LHSIsS = DAG.ComputeNumSignBits(LHS) > XLen;\n      bool RHSIsS = DAG.ComputeNumSignBits(RHS) > XLen;\n\n      // The other operand should be signed, but still prefer MULH when\n      // possible.\n      if (RHSIsU && LHSIsS && !RHSIsS)\n        Results.push_back(MakeMULPair(LHS, RHS));\n      else if (LHSIsU && RHSIsS && !LHSIsS)\n        Results.push_back(MakeMULPair(RHS, LHS));\n\n      return;\n    }\n    [[fallthrough]];\n  }\n  case ISD::ADD:\n  case ISD::SUB:\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n    Results.push_back(customLegalizeToWOpWithSExt(N, DAG));\n    break;\n  case ISD::SHL:\n  case ISD::SRA:\n  case ISD::SRL:\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n    if (N->getOperand(1).getOpcode() != ISD::Constant) {\n      // If we can use a BSET instruction, allow default promotion to apply.\n      if (N->getOpcode() == ISD::SHL && Subtarget.hasStdExtZbs() &&\n          isOneConstant(N->getOperand(0)))\n        break;\n      Results.push_back(customLegalizeToWOp(N, DAG));\n      break;\n    }\n\n    // Custom legalize ISD::SHL by placing a SIGN_EXTEND_INREG after. This is\n    // similar to customLegalizeToWOpWithSExt, but we must zero_extend the\n    // shift amount.\n    if (N->getOpcode() == ISD::SHL) {\n      SDLoc DL(N);\n      SDValue NewOp0 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(0));\n      SDValue NewOp1 =\n          DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i64, N->getOperand(1));\n      SDValue NewWOp = DAG.getNode(ISD::SHL, DL, MVT::i64, NewOp0, NewOp1);\n      SDValue NewRes = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, NewWOp,\n                                   DAG.getValueType(MVT::i32));\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, NewRes));\n    }\n\n    break;\n  case ISD::ROTL:\n  case ISD::ROTR:\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n    assert((Subtarget.hasStdExtZbb() || Subtarget.hasStdExtZbkb() ||\n            Subtarget.hasVendorXTHeadBb()) &&\n           \"Unexpected custom legalization\");\n    if (!isa<ConstantSDNode>(N->getOperand(1)) &&\n        !(Subtarget.hasStdExtZbb() || Subtarget.hasStdExtZbkb()))\n      return;\n    Results.push_back(customLegalizeToWOp(N, DAG));\n    break;\n  case ISD::CTTZ:\n  case ISD::CTTZ_ZERO_UNDEF:\n  case ISD::CTLZ:\n  case ISD::CTLZ_ZERO_UNDEF: {\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n\n    SDValue NewOp0 =\n        DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(0));\n    bool IsCTZ =\n        N->getOpcode() == ISD::CTTZ || N->getOpcode() == ISD::CTTZ_ZERO_UNDEF;\n    unsigned Opc = IsCTZ ? RISCVISD::CTZW : RISCVISD::CLZW;\n    SDValue Res = DAG.getNode(Opc, DL, MVT::i64, NewOp0);\n    Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n    return;\n  }\n  case ISD::SDIV:\n  case ISD::UDIV:\n  case ISD::UREM: {\n    MVT VT = N->getSimpleValueType(0);\n    assert((VT == MVT::i8 || VT == MVT::i16 || VT == MVT::i32) &&\n           Subtarget.is64Bit() && Subtarget.hasStdExtM() &&\n           \"Unexpected custom legalisation\");\n    // Don't promote division/remainder by constant since we should expand those\n    // to multiply by magic constant.\n    AttributeList Attr = DAG.getMachineFunction().getFunction().getAttributes();\n    if (N->getOperand(1).getOpcode() == ISD::Constant &&\n        !isIntDivCheap(N->getValueType(0), Attr))\n      return;\n\n    // If the input is i32, use ANY_EXTEND since the W instructions don't read\n    // the upper 32 bits. For other types we need to sign or zero extend\n    // based on the opcode.\n    unsigned ExtOpc = ISD::ANY_EXTEND;\n    if (VT != MVT::i32)\n      ExtOpc = N->getOpcode() == ISD::SDIV ? ISD::SIGN_EXTEND\n                                           : ISD::ZERO_EXTEND;\n\n    Results.push_back(customLegalizeToWOp(N, DAG, ExtOpc));\n    break;\n  }\n  case ISD::SADDO: {\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n\n    // If the RHS is a constant, we can simplify ConditionRHS below. Otherwise\n    // use the default legalization.\n    if (!isa<ConstantSDNode>(N->getOperand(1)))\n      return;\n\n    SDValue LHS = DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64, N->getOperand(0));\n    SDValue RHS = DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64, N->getOperand(1));\n    SDValue Res = DAG.getNode(ISD::ADD, DL, MVT::i64, LHS, RHS);\n    Res = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, Res,\n                      DAG.getValueType(MVT::i32));\n\n    SDValue Zero = DAG.getConstant(0, DL, MVT::i64);\n\n    // For an addition, the result should be less than one of the operands (LHS)\n    // if and only if the other operand (RHS) is negative, otherwise there will\n    // be overflow.\n    // For a subtraction, the result should be less than one of the operands\n    // (LHS) if and only if the other operand (RHS) is (non-zero) positive,\n    // otherwise there will be overflow.\n    EVT OType = N->getValueType(1);\n    SDValue ResultLowerThanLHS = DAG.getSetCC(DL, OType, Res, LHS, ISD::SETLT);\n    SDValue ConditionRHS = DAG.getSetCC(DL, OType, RHS, Zero, ISD::SETLT);\n\n    SDValue Overflow =\n        DAG.getNode(ISD::XOR, DL, OType, ConditionRHS, ResultLowerThanLHS);\n    Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n    Results.push_back(Overflow);\n    return;\n  }\n  case ISD::UADDO:\n  case ISD::USUBO: {\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n    bool IsAdd = N->getOpcode() == ISD::UADDO;\n    // Create an ADDW or SUBW.\n    SDValue LHS = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(0));\n    SDValue RHS = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(1));\n    SDValue Res =\n        DAG.getNode(IsAdd ? ISD::ADD : ISD::SUB, DL, MVT::i64, LHS, RHS);\n    Res = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, Res,\n                      DAG.getValueType(MVT::i32));\n\n    SDValue Overflow;\n    if (IsAdd && isOneConstant(RHS)) {\n      // Special case uaddo X, 1 overflowed if the addition result is 0.\n      // The general case (X + C) < C is not necessarily beneficial. Although we\n      // reduce the live range of X, we may introduce the materialization of\n      // constant C, especially when the setcc result is used by branch. We have\n      // no compare with constant and branch instructions.\n      Overflow = DAG.getSetCC(DL, N->getValueType(1), Res,\n                              DAG.getConstant(0, DL, MVT::i64), ISD::SETEQ);\n    } else if (IsAdd && isAllOnesConstant(RHS)) {\n      // Special case uaddo X, -1 overflowed if X != 0.\n      Overflow = DAG.getSetCC(DL, N->getValueType(1), N->getOperand(0),\n                              DAG.getConstant(0, DL, MVT::i32), ISD::SETNE);\n    } else {\n      // Sign extend the LHS and perform an unsigned compare with the ADDW\n      // result. Since the inputs are sign extended from i32, this is equivalent\n      // to comparing the lower 32 bits.\n      LHS = DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64, N->getOperand(0));\n      Overflow = DAG.getSetCC(DL, N->getValueType(1), Res, LHS,\n                              IsAdd ? ISD::SETULT : ISD::SETUGT);\n    }\n\n    Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n    Results.push_back(Overflow);\n    return;\n  }\n  case ISD::UADDSAT:\n  case ISD::USUBSAT: {\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n    if (Subtarget.hasStdExtZbb()) {\n      // With Zbb we can sign extend and let LegalizeDAG use minu/maxu. Using\n      // sign extend allows overflow of the lower 32 bits to be detected on\n      // the promoted size.\n      SDValue LHS =\n          DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64, N->getOperand(0));\n      SDValue RHS =\n          DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64, N->getOperand(1));\n      SDValue Res = DAG.getNode(N->getOpcode(), DL, MVT::i64, LHS, RHS);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n\n    // Without Zbb, expand to UADDO/USUBO+select which will trigger our custom\n    // promotion for UADDO/USUBO.\n    Results.push_back(expandAddSubSat(N, DAG));\n    return;\n  }\n  case ISD::ABS: {\n    assert(N->getValueType(0) == MVT::i32 && Subtarget.is64Bit() &&\n           \"Unexpected custom legalisation\");\n\n    if (Subtarget.hasStdExtZbb()) {\n      // Emit a special ABSW node that will be expanded to NEGW+MAX at isel.\n      // This allows us to remember that the result is sign extended. Expanding\n      // to NEGW+MAX here requires a Freeze which breaks ComputeNumSignBits.\n      SDValue Src = DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64,\n                                N->getOperand(0));\n      SDValue Abs = DAG.getNode(RISCVISD::ABSW, DL, MVT::i64, Src);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Abs));\n      return;\n    }\n\n    // Expand abs to Y = (sraiw X, 31); subw(xor(X, Y), Y)\n    SDValue Src = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(0));\n\n    // Freeze the source so we can increase it's use count.\n    Src = DAG.getFreeze(Src);\n\n    // Copy sign bit to all bits using the sraiw pattern.\n    SDValue SignFill = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, Src,\n                                   DAG.getValueType(MVT::i32));\n    SignFill = DAG.getNode(ISD::SRA, DL, MVT::i64, SignFill,\n                           DAG.getConstant(31, DL, MVT::i64));\n\n    SDValue NewRes = DAG.getNode(ISD::XOR, DL, MVT::i64, Src, SignFill);\n    NewRes = DAG.getNode(ISD::SUB, DL, MVT::i64, NewRes, SignFill);\n\n    // NOTE: The result is only required to be anyextended, but sext is\n    // consistent with type legalization of sub.\n    NewRes = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, NewRes,\n                         DAG.getValueType(MVT::i32));\n    Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, NewRes));\n    return;\n  }\n  case ISD::BITCAST: {\n    EVT VT = N->getValueType(0);\n    assert(VT.isInteger() && !VT.isVector() && \"Unexpected VT!\");\n    SDValue Op0 = N->getOperand(0);\n    EVT Op0VT = Op0.getValueType();\n    MVT XLenVT = Subtarget.getXLenVT();\n    if (VT == MVT::i16 && Op0VT == MVT::f16 &&\n        Subtarget.hasStdExtZfhminOrZhinxmin()) {\n      SDValue FPConv = DAG.getNode(RISCVISD::FMV_X_ANYEXTH, DL, XLenVT, Op0);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i16, FPConv));\n    } else if (VT == MVT::i16 && Op0VT == MVT::bf16 &&\n               Subtarget.hasStdExtZfbfmin()) {\n      SDValue FPConv = DAG.getNode(RISCVISD::FMV_X_ANYEXTH, DL, XLenVT, Op0);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i16, FPConv));\n    } else if (VT == MVT::i32 && Op0VT == MVT::f32 && Subtarget.is64Bit() &&\n               Subtarget.hasStdExtFOrZfinx()) {\n      SDValue FPConv =\n          DAG.getNode(RISCVISD::FMV_X_ANYEXTW_RV64, DL, MVT::i64, Op0);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, FPConv));\n    } else if (VT == MVT::i64 && Op0VT == MVT::f64 && XLenVT == MVT::i32 &&\n               Subtarget.hasStdExtZfa()) {\n      SDValue NewReg = DAG.getNode(RISCVISD::SplitF64, DL,\n                                   DAG.getVTList(MVT::i32, MVT::i32), Op0);\n      SDValue RetReg = DAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64,\n                                   NewReg.getValue(0), NewReg.getValue(1));\n      Results.push_back(RetReg);\n    } else if (!VT.isVector() && Op0VT.isFixedLengthVector() &&\n               isTypeLegal(Op0VT)) {\n      // Custom-legalize bitcasts from fixed-length vector types to illegal\n      // scalar types in order to improve codegen. Bitcast the vector to a\n      // one-element vector type whose element type is the same as the result\n      // type, and extract the first element.\n      EVT BVT = EVT::getVectorVT(*DAG.getContext(), VT, 1);\n      if (isTypeLegal(BVT)) {\n        SDValue BVec = DAG.getBitcast(BVT, Op0);\n        Results.push_back(DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, BVec,\n                                      DAG.getConstant(0, DL, XLenVT)));\n      }\n    }\n    break;\n  }\n  case RISCVISD::BREV8: {\n    MVT VT = N->getSimpleValueType(0);\n    MVT XLenVT = Subtarget.getXLenVT();\n    assert((VT == MVT::i16 || (VT == MVT::i32 && Subtarget.is64Bit())) &&\n           \"Unexpected custom legalisation\");\n    assert(Subtarget.hasStdExtZbkb() && \"Unexpected extension\");\n    SDValue NewOp = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, N->getOperand(0));\n    SDValue NewRes = DAG.getNode(N->getOpcode(), DL, XLenVT, NewOp);\n    // ReplaceNodeResults requires we maintain the same type for the return\n    // value.\n    Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, VT, NewRes));\n    break;\n  }\n  case ISD::EXTRACT_VECTOR_ELT: {\n    // Custom-legalize an EXTRACT_VECTOR_ELT where XLEN<SEW, as the SEW element\n    // type is illegal (currently only vXi64 RV32).\n    // With vmv.x.s, when SEW > XLEN, only the least-significant XLEN bits are\n    // transferred to the destination register. We issue two of these from the\n    // upper- and lower- halves of the SEW-bit vector element, slid down to the\n    // first element.\n    SDValue Vec = N->getOperand(0);\n    SDValue Idx = N->getOperand(1);\n\n    // The vector type hasn't been legalized yet so we can't issue target\n    // specific nodes if it needs legalization.\n    // FIXME: We would manually legalize if it's important.\n    if (!isTypeLegal(Vec.getValueType()))\n      return;\n\n    MVT VecVT = Vec.getSimpleValueType();\n\n    assert(!Subtarget.is64Bit() && N->getValueType(0) == MVT::i64 &&\n           VecVT.getVectorElementType() == MVT::i64 &&\n           \"Unexpected EXTRACT_VECTOR_ELT legalization\");\n\n    // If this is a fixed vector, we need to convert it to a scalable vector.\n    MVT ContainerVT = VecVT;\n    if (VecVT.isFixedLengthVector()) {\n      ContainerVT = getContainerForFixedLengthVector(VecVT);\n      Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);\n    }\n\n    MVT XLenVT = Subtarget.getXLenVT();\n\n    // Use a VL of 1 to avoid processing more elements than we need.\n    auto [Mask, VL] = getDefaultVLOps(1, ContainerVT, DL, DAG, Subtarget);\n\n    // Unless the index is known to be 0, we must slide the vector down to get\n    // the desired element into index 0.\n    if (!isNullConstant(Idx)) {\n      Vec = getVSlidedown(DAG, Subtarget, DL, ContainerVT,\n                          DAG.getUNDEF(ContainerVT), Vec, Idx, Mask, VL);\n    }\n\n    // Extract the lower XLEN bits of the correct vector element.\n    SDValue EltLo = DAG.getNode(RISCVISD::VMV_X_S, DL, XLenVT, Vec);\n\n    // To extract the upper XLEN bits of the vector element, shift the first\n    // element right by 32 bits and re-extract the lower XLEN bits.\n    SDValue ThirtyTwoV = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, ContainerVT,\n                                     DAG.getUNDEF(ContainerVT),\n                                     DAG.getConstant(32, DL, XLenVT), VL);\n    SDValue LShr32 =\n        DAG.getNode(RISCVISD::SRL_VL, DL, ContainerVT, Vec, ThirtyTwoV,\n                    DAG.getUNDEF(ContainerVT), Mask, VL);\n\n    SDValue EltHi = DAG.getNode(RISCVISD::VMV_X_S, DL, XLenVT, LShr32);\n\n    Results.push_back(DAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, EltLo, EltHi));\n    break;\n  }\n  case ISD::INTRINSIC_WO_CHAIN: {\n    unsigned IntNo = N->getConstantOperandVal(0);\n    switch (IntNo) {\n    default:\n      llvm_unreachable(\n          \"Don't know how to custom type legalize this intrinsic!\");\n    case Intrinsic::experimental_get_vector_length: {\n      SDValue Res = lowerGetVectorLength(N, DAG, Subtarget);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n    case Intrinsic::riscv_orc_b:\n    case Intrinsic::riscv_brev8:\n    case Intrinsic::riscv_sha256sig0:\n    case Intrinsic::riscv_sha256sig1:\n    case Intrinsic::riscv_sha256sum0:\n    case Intrinsic::riscv_sha256sum1:\n    case Intrinsic::riscv_sm3p0:\n    case Intrinsic::riscv_sm3p1: {\n      if (!Subtarget.is64Bit() || N->getValueType(0) != MVT::i32)\n        return;\n      unsigned Opc;\n      switch (IntNo) {\n      case Intrinsic::riscv_orc_b:      Opc = RISCVISD::ORC_B;      break;\n      case Intrinsic::riscv_brev8:      Opc = RISCVISD::BREV8;      break;\n      case Intrinsic::riscv_sha256sig0: Opc = RISCVISD::SHA256SIG0; break;\n      case Intrinsic::riscv_sha256sig1: Opc = RISCVISD::SHA256SIG1; break;\n      case Intrinsic::riscv_sha256sum0: Opc = RISCVISD::SHA256SUM0; break;\n      case Intrinsic::riscv_sha256sum1: Opc = RISCVISD::SHA256SUM1; break;\n      case Intrinsic::riscv_sm3p0:      Opc = RISCVISD::SM3P0;      break;\n      case Intrinsic::riscv_sm3p1:      Opc = RISCVISD::SM3P1;      break;\n      }\n\n      SDValue NewOp =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(1));\n      SDValue Res = DAG.getNode(Opc, DL, MVT::i64, NewOp);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n    case Intrinsic::riscv_sm4ks:\n    case Intrinsic::riscv_sm4ed: {\n      unsigned Opc =\n          IntNo == Intrinsic::riscv_sm4ks ? RISCVISD::SM4KS : RISCVISD::SM4ED;\n      SDValue NewOp0 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(1));\n      SDValue NewOp1 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(2));\n      SDValue Res =\n          DAG.getNode(Opc, DL, MVT::i64, NewOp0, NewOp1, N->getOperand(3));\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n    case Intrinsic::riscv_clmul: {\n      if (!Subtarget.is64Bit() || N->getValueType(0) != MVT::i32)\n        return;\n\n      SDValue NewOp0 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(1));\n      SDValue NewOp1 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(2));\n      SDValue Res = DAG.getNode(RISCVISD::CLMUL, DL, MVT::i64, NewOp0, NewOp1);\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n    case Intrinsic::riscv_clmulh:\n    case Intrinsic::riscv_clmulr: {\n      if (!Subtarget.is64Bit() || N->getValueType(0) != MVT::i32)\n        return;\n\n      // Extend inputs to XLen, and shift by 32. This will add 64 trailing zeros\n      // to the full 128-bit clmul result of multiplying two xlen values.\n      // Perform clmulr or clmulh on the shifted values. Finally, extract the\n      // upper 32 bits.\n      //\n      // The alternative is to mask the inputs to 32 bits and use clmul, but\n      // that requires two shifts to mask each input without zext.w.\n      // FIXME: If the inputs are known zero extended or could be freely\n      // zero extended, the mask form would be better.\n      SDValue NewOp0 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(1));\n      SDValue NewOp1 =\n          DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N->getOperand(2));\n      NewOp0 = DAG.getNode(ISD::SHL, DL, MVT::i64, NewOp0,\n                           DAG.getConstant(32, DL, MVT::i64));\n      NewOp1 = DAG.getNode(ISD::SHL, DL, MVT::i64, NewOp1,\n                           DAG.getConstant(32, DL, MVT::i64));\n      unsigned Opc = IntNo == Intrinsic::riscv_clmulh ? RISCVISD::CLMULH\n                                                      : RISCVISD::CLMULR;\n      SDValue Res = DAG.getNode(Opc, DL, MVT::i64, NewOp0, NewOp1);\n      Res = DAG.getNode(ISD::SRL, DL, MVT::i64, Res,\n                        DAG.getConstant(32, DL, MVT::i64));\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Res));\n      return;\n    }\n    case Intrinsic::riscv_vmv_x_s: {\n      EVT VT = N->getValueType(0);\n      MVT XLenVT = Subtarget.getXLenVT();\n      if (VT.bitsLT(XLenVT)) {\n        // Simple case just extract using vmv.x.s and truncate.\n        SDValue Extract = DAG.getNode(RISCVISD::VMV_X_S, DL,\n                                      Subtarget.getXLenVT(), N->getOperand(1));\n        Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, VT, Extract));\n        return;\n      }\n\n      assert(VT == MVT::i64 && !Subtarget.is64Bit() &&\n             \"Unexpected custom legalization\");\n\n      // We need to do the move in two steps.\n      SDValue Vec = N->getOperand(1);\n      MVT VecVT = Vec.getSimpleValueType();\n\n      // First extract the lower XLEN bits of the element.\n      SDValue EltLo = DAG.getNode(RISCVISD::VMV_X_S, DL, XLenVT, Vec);\n\n      // To extract the upper XLEN bits of the vector element, shift the first\n      // element right by 32 bits and re-extract the lower XLEN bits.\n      auto [Mask, VL] = getDefaultVLOps(1, VecVT, DL, DAG, Subtarget);\n\n      SDValue ThirtyTwoV =\n          DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VecVT, DAG.getUNDEF(VecVT),\n                      DAG.getConstant(32, DL, XLenVT), VL);\n      SDValue LShr32 = DAG.getNode(RISCVISD::SRL_VL, DL, VecVT, Vec, ThirtyTwoV,\n                                   DAG.getUNDEF(VecVT), Mask, VL);\n      SDValue EltHi = DAG.getNode(RISCVISD::VMV_X_S, DL, XLenVT, LShr32);\n\n      Results.push_back(\n          DAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, EltLo, EltHi));\n      break;\n    }\n    }\n    break;\n  }\n  case ISD::VECREDUCE_ADD:\n  case ISD::VECREDUCE_AND:\n  case ISD::VECREDUCE_OR:\n  case ISD::VECREDUCE_XOR:\n  case ISD::VECREDUCE_SMAX:\n  case ISD::VECREDUCE_UMAX:\n  case ISD::VECREDUCE_SMIN:\n  case ISD::VECREDUCE_UMIN:\n    if (SDValue V = lowerVECREDUCE(SDValue(N, 0), DAG))\n      Results.push_back(V);\n    break;\n  case ISD::VP_REDUCE_ADD:\n  case ISD::VP_REDUCE_AND:\n  case ISD::VP_REDUCE_OR:\n  case ISD::VP_REDUCE_XOR:\n  case ISD::VP_REDUCE_SMAX:\n  case ISD::VP_REDUCE_UMAX:\n  case ISD::VP_REDUCE_SMIN:\n  case ISD::VP_REDUCE_UMIN:\n    if (SDValue V = lowerVPREDUCE(SDValue(N, 0), DAG))\n      Results.push_back(V);\n    break;\n  case ISD::GET_ROUNDING: {\n    SDVTList VTs = DAG.getVTList(Subtarget.getXLenVT(), MVT::Other);\n    SDValue Res = DAG.getNode(ISD::GET_ROUNDING, DL, VTs, N->getOperand(0));\n    Results.push_back(Res.getValue(0));\n    Results.push_back(Res.getValue(1));\n    break;\n  }\n  }\n}",
      "start_line": 11382,
      "end_line": 12051,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "ReplaceNodeResults",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "just extract using vmv.x.s and truncate.\n        SDValue Extract = DAG.getNode(RISCVISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "ReplaceNodeResults",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "MaskedValueIsZero",
        "getDefaultVLOps",
        "hasStdExtZbkb",
        "isIntDivCheap",
        "getMemoryVT",
        "illegal",
        "convertToScalableVector",
        "getConstant",
        "getSimpleValueType",
        "operand",
        "is64Bit",
        "operands",
        "hasStdExtFOrZfinx",
        "makeLibCall",
        "getSizeInBits",
        "ComputeNumSignBits",
        "hasStdExtZbs",
        "getFreeze",
        "getBitcast",
        "hasStdExtM",
        "getConstantOperandVal",
        "getMachineFunction",
        "getFPTOSINT",
        "hasStdExtZbb",
        "isOneConstant",
        "isStrictFPOpcode",
        "isVector",
        "getVectorElementType",
        "is",
        "getVectorVT",
        "push_back",
        "isFixedLengthVector",
        "SDValue",
        "getMemOperand",
        "getSetCC",
        "getValue",
        "getVTList",
        "getExtLoad",
        "isTypeLegal",
        "getOpcode",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getVSlidedown",
        "getOperand",
        "getXLen",
        "tie",
        "hasVendorXTHeadBb",
        "getHighBitsSet",
        "lowerGetVectorLength",
        "subw",
        "getBasePtr",
        "dl",
        "setTypeListBeforeSoften",
        "hasStdExtZfhOrZhinx",
        "getAttributes",
        "DL",
        "getFPTOUINT",
        "getXLenVT",
        "llvm_unreachable",
        "getNode",
        "getFunction"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getVecReduceOpcode",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opc"
        }
      ],
      "body": "{\n  switch (Opc) {\n  default:\n    llvm_unreachable(\"Unhandled binary to transfrom reduction\");\n  case ISD::ADD:\n    return ISD::VECREDUCE_ADD;\n  case ISD::UMAX:\n    return ISD::VECREDUCE_UMAX;\n  case ISD::SMAX:\n    return ISD::VECREDUCE_SMAX;\n  case ISD::UMIN:\n    return ISD::VECREDUCE_UMIN;\n  case ISD::SMIN:\n    return ISD::VECREDUCE_SMIN;\n  case ISD::AND:\n    return ISD::VECREDUCE_AND;\n  case ISD::OR:\n    return ISD::VECREDUCE_OR;\n  case ISD::XOR:\n    return ISD::VECREDUCE_XOR;\n  case ISD::FADD:\n    // Note: This is the associative form of the generic reduction opcode.\n    return ISD::VECREDUCE_FADD;\n  }\n}",
      "start_line": 12055,
      "end_line": 12079,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getVecReduceOpcode",
          "condition": "Opc",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineBinOpOfExtractToReduceTree",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n\n  // This transforms need to run before all integer types have been legalized\n  // to i64 (so that the vector element type matches the add type), and while\n  // it's safe to introduce odd sized vector types.\n  if (DAG.NewNodesMustHaveLegalTypes)\n    return SDValue();\n\n  // Without V, this transform isn't useful.  We could form the (illegal)\n  // operations and let them be scalarized again, but there's really no point.\n  if (!Subtarget.hasVInstructions())\n    return SDValue();\n\n  const SDLoc DL(N);\n  const EVT VT = N->getValueType(0);\n  const unsigned Opc = N->getOpcode();\n\n  // For FADD, we only handle the case with reassociation allowed.  We\n  // could handle strict reduction order, but at the moment, there's no\n  // known reason to, and the complexity isn't worth it.\n  // TODO: Handle fminnum and fmaxnum here\n  if (!VT.isInteger() &&\n      (Opc != ISD::FADD || !N->getFlags().hasAllowReassociation()))\n    return SDValue();\n\n  const unsigned ReduceOpc = getVecReduceOpcode(Opc);\n  assert(Opc == ISD::getVecReduceBaseOpcode(ReduceOpc) &&\n         \"Inconsistent mappings\");\n  SDValue LHS = N->getOperand(0);\n  SDValue RHS = N->getOperand(1);\n\n  if (!LHS.hasOneUse() || !RHS.hasOneUse())\n    return SDValue();\n\n  if (RHS.getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n    std::swap(LHS, RHS);\n\n  if (RHS.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n      !isa<ConstantSDNode>(RHS.getOperand(1)))\n    return SDValue();\n\n  uint64_t RHSIdx = cast<ConstantSDNode>(RHS.getOperand(1))->getLimitedValue();\n  SDValue SrcVec = RHS.getOperand(0);\n  EVT SrcVecVT = SrcVec.getValueType();\n  assert(SrcVecVT.getVectorElementType() == VT);\n  if (SrcVecVT.isScalableVector())\n    return SDValue();\n\n  if (SrcVecVT.getScalarSizeInBits() > Subtarget.getELen())\n    return SDValue();\n\n  // match binop (extract_vector_elt V, 0), (extract_vector_elt V, 1) to\n  // reduce_op (extract_subvector [2 x VT] from V).  This will form the\n  // root of our reduction tree. TODO: We could extend this to any two\n  // adjacent aligned constant indices if desired.\n  if (LHS.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n      LHS.getOperand(0) == SrcVec && isa<ConstantSDNode>(LHS.getOperand(1))) {\n    uint64_t LHSIdx =\n      cast<ConstantSDNode>(LHS.getOperand(1))->getLimitedValue();\n    if (0 == std::min(LHSIdx, RHSIdx) && 1 == std::max(LHSIdx, RHSIdx)) {\n      EVT ReduceVT = EVT::getVectorVT(*DAG.getContext(), VT, 2);\n      SDValue Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ReduceVT, SrcVec,\n                                DAG.getVectorIdxConstant(0, DL));\n      return DAG.getNode(ReduceOpc, DL, VT, Vec, N->getFlags());\n    }\n  }\n\n  // Match (binop (reduce (extract_subvector V, 0),\n  //                      (extract_vector_elt V, sizeof(SubVec))))\n  // into a reduction of one more element from the original vector V.\n  if (LHS.getOpcode() != ReduceOpc)\n    return SDValue();\n\n  SDValue ReduceVec = LHS.getOperand(0);\n  if (ReduceVec.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n      ReduceVec.hasOneUse() && ReduceVec.getOperand(0) == RHS.getOperand(0) &&\n      isNullConstant(ReduceVec.getOperand(1)) &&\n      ReduceVec.getValueType().getVectorNumElements() == RHSIdx) {\n    // For illegal types (e.g. 3xi32), most will be combined again into a\n    // wider (hopefully legal) type.  If this is a terminal state, we are\n    // relying on type legalization here to produce something reasonable\n    // and this lowering quality could probably be improved. (TODO)\n    EVT ReduceVT = EVT::getVectorVT(*DAG.getContext(), VT, RHSIdx + 1);\n    SDValue Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ReduceVT, SrcVec,\n                              DAG.getVectorIdxConstant(0, DL));\n    auto Flags = ReduceVec->getFlags();\n    Flags.intersectWith(N->getFlags());\n    return DAG.getNode(ReduceOpc, DL, VT, Vec, Flags);\n  }\n\n  return SDValue();\n}",
      "start_line": 12087,
      "end_line": 12180,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLimitedValue",
        "binop",
        "reduce_op",
        "hasAllowReassociation",
        "getFlags",
        "Match",
        "getVectorNumElements",
        "getVecReduceOpcode",
        "getVectorVT",
        "max",
        "wider",
        "SDValue",
        "the",
        "swap",
        "i64",
        "getOpcode",
        "getValueType",
        "getOperand",
        "getELen",
        "intersectWith",
        "hasOneUse",
        "types",
        "DL",
        "isNullConstant",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineBinOpToReduce",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  auto BinOpToRVVReduce = [](unsigned Opc) {\n    switch (Opc) {\n    default:\n      llvm_unreachable(\"Unhandled binary to transfrom reduction\");\n    case ISD::ADD:\n      return RISCVISD::VECREDUCE_ADD_VL;\n    case ISD::UMAX:\n      return RISCVISD::VECREDUCE_UMAX_VL;\n    case ISD::SMAX:\n      return RISCVISD::VECREDUCE_SMAX_VL;\n    case ISD::UMIN:\n      return RISCVISD::VECREDUCE_UMIN_VL;\n    case ISD::SMIN:\n      return RISCVISD::VECREDUCE_SMIN_VL;\n    case ISD::AND:\n      return RISCVISD::VECREDUCE_AND_VL;\n    case ISD::OR:\n      return RISCVISD::VECREDUCE_OR_VL;\n    case ISD::XOR:\n      return RISCVISD::VECREDUCE_XOR_VL;\n    case ISD::FADD:\n      return RISCVISD::VECREDUCE_FADD_VL;\n    case ISD::FMAXNUM:\n      return RISCVISD::VECREDUCE_FMAX_VL;\n    case ISD::FMINNUM:\n      return RISCVISD::VECREDUCE_FMIN_VL;\n    }\n  };\n\n  auto IsReduction = [&BinOpToRVVReduce](SDValue V, unsigned Opc) {\n    return V.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n           isNullConstant(V.getOperand(1)) &&\n           V.getOperand(0).getOpcode() == BinOpToRVVReduce(Opc);\n  };\n\n  unsigned Opc = N->getOpcode();\n  unsigned ReduceIdx;\n  if (IsReduction(N->getOperand(0), Opc))\n    ReduceIdx = 0;\n  else if (IsReduction(N->getOperand(1), Opc))\n    ReduceIdx = 1;\n  else\n    return SDValue();\n\n  // Skip if FADD disallows reassociation but the combiner needs.\n  if (Opc == ISD::FADD && !N->getFlags().hasAllowReassociation())\n    return SDValue();\n\n  SDValue Extract = N->getOperand(ReduceIdx);\n  SDValue Reduce = Extract.getOperand(0);\n  if (!Extract.hasOneUse() || !Reduce.hasOneUse())\n    return SDValue();\n\n  SDValue ScalarV = Reduce.getOperand(2);\n  EVT ScalarVT = ScalarV.getValueType();\n  if (ScalarV.getOpcode() == ISD::INSERT_SUBVECTOR &&\n      ScalarV.getOperand(0)->isUndef() &&\n      isNullConstant(ScalarV.getOperand(2)))\n    ScalarV = ScalarV.getOperand(1);\n\n  // Make sure that ScalarV is a splat with VL=1.\n  if (ScalarV.getOpcode() != RISCVISD::VFMV_S_F_VL &&\n      ScalarV.getOpcode() != RISCVISD::VMV_S_X_VL &&\n      ScalarV.getOpcode() != RISCVISD::VMV_V_X_VL)\n    return SDValue();\n\n  if (!isNonZeroAVL(ScalarV.getOperand(2)))\n    return SDValue();\n\n  // Check the scalar of ScalarV is neutral element\n  // TODO: Deal with value other than neutral element.\n  if (!isNeutralConstant(N->getOpcode(), N->getFlags(), ScalarV.getOperand(1),\n                         0))\n    return SDValue();\n\n  // If the AVL is zero, operand 0 will be returned. So it's not safe to fold.\n  // FIXME: We might be able to improve this if operand 0 is undef.\n  if (!isNonZeroAVL(Reduce.getOperand(5)))\n    return SDValue();\n\n  SDValue NewStart = N->getOperand(1 - ReduceIdx);\n\n  SDLoc DL(N);\n  SDValue NewScalarV =\n      lowerScalarInsert(NewStart, ScalarV.getOperand(2),\n                        ScalarV.getSimpleValueType(), DL, DAG, Subtarget);\n\n  // If we looked through an INSERT_SUBVECTOR we need to restore it.\n  if (ScalarVT != ScalarV.getValueType())\n    NewScalarV =\n        DAG.getNode(ISD::INSERT_SUBVECTOR, DL, ScalarVT, DAG.getUNDEF(ScalarVT),\n                    NewScalarV, DAG.getConstant(0, DL, Subtarget.getXLenVT()));\n\n  SDValue Ops[] = {Reduce.getOperand(0), Reduce.getOperand(1),\n                   NewScalarV,           Reduce.getOperand(3),\n                   Reduce.getOperand(4), Reduce.getOperand(5)};\n  SDValue NewReduce =\n      DAG.getNode(Reduce.getOpcode(), DL, Reduce.getValueType(), Ops);\n  return DAG.getNode(Extract.getOpcode(), DL, Extract.getValueType(), NewReduce,\n                     Extract.getOperand(1));\n}",
      "start_line": 12184,
      "end_line": 12286,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "combineBinOpToReduce",
          "condition": "Opc",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "SDValue",
        "hasOneUse",
        "BinOpToRVVReduce",
        "getOpcode",
        "getFlags",
        "getConstant",
        "getSimpleValueType",
        "getValueType",
        "isUndef",
        "hasAllowReassociation",
        "getOperand",
        "DL",
        "isNullConstant",
        "lowerScalarInsert",
        "llvm_unreachable",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "transformAddShlImm",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // Perform this optimization only in the zba extension.\n  if (!Subtarget.hasStdExtZba())\n    return SDValue();\n\n  // Skip for vector types and larger types.\n  EVT VT = N->getValueType(0);\n  if (VT.isVector() || VT.getSizeInBits() > Subtarget.getXLen())\n    return SDValue();\n\n  // The two operand nodes must be SHL and have no other use.\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  if (N0->getOpcode() != ISD::SHL || N1->getOpcode() != ISD::SHL ||\n      !N0->hasOneUse() || !N1->hasOneUse())\n    return SDValue();\n\n  // Check c0 and c1.\n  auto *N0C = dyn_cast<ConstantSDNode>(N0->getOperand(1));\n  auto *N1C = dyn_cast<ConstantSDNode>(N1->getOperand(1));\n  if (!N0C || !N1C)\n    return SDValue();\n  int64_t C0 = N0C->getSExtValue();\n  int64_t C1 = N1C->getSExtValue();\n  if (C0 <= 0 || C1 <= 0)\n    return SDValue();\n\n  // Skip if SH1ADD/SH2ADD/SH3ADD are not applicable.\n  int64_t Bits = std::min(C0, C1);\n  int64_t Diff = std::abs(C0 - C1);\n  if (Diff != 1 && Diff != 2 && Diff != 3)\n    return SDValue();\n\n  // Build nodes.\n  SDLoc DL(N);\n  SDValue NS = (C0 < C1) ? N0->getOperand(0) : N1->getOperand(0);\n  SDValue NL = (C0 > C1) ? N0->getOperand(0) : N1->getOperand(0);\n  SDValue NA0 =\n      DAG.getNode(ISD::SHL, DL, VT, NL, DAG.getConstant(Diff, DL, VT));\n  SDValue NA1 = DAG.getNode(ISD::ADD, DL, VT, NA0, NS);\n  return DAG.getNode(ISD::SHL, DL, VT, NA1, DAG.getConstant(Bits, DL, VT));\n}",
      "start_line": 12290,
      "end_line": 12332,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "abs",
        "min",
        "getNode",
        "getOpcode",
        "getValueType",
        "getOperand",
        "getSizeInBits",
        "getXLen",
        "DL",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineSelectAndUse",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SDValue",
          "name": "Slct"
        },
        {
          "type": "SDValue",
          "name": "OtherOp"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "bool",
          "name": "AllOnes"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  EVT VT = N->getValueType(0);\n\n  // Skip vectors.\n  if (VT.isVector())\n    return SDValue();\n\n  if (!Subtarget.hasConditionalMoveFusion()) {\n    // (select cond, x, (and x, c)) has custom lowering with Zicond.\n    if ((!Subtarget.hasStdExtZicond() &&\n         !Subtarget.hasVendorXVentanaCondOps()) ||\n        N->getOpcode() != ISD::AND)\n      return SDValue();\n\n    // Maybe harmful when condition code has multiple use.\n    if (Slct.getOpcode() == ISD::SELECT && !Slct.getOperand(0).hasOneUse())\n      return SDValue();\n\n    // Maybe harmful when VT is wider than XLen.\n    if (VT.getSizeInBits() > Subtarget.getXLen())\n      return SDValue();\n  }\n\n  if ((Slct.getOpcode() != ISD::SELECT &&\n       Slct.getOpcode() != RISCVISD::SELECT_CC) ||\n      !Slct.hasOneUse())\n    return SDValue();\n\n  auto isZeroOrAllOnes = [](SDValue N, bool AllOnes) {\n    return AllOnes ? isAllOnesConstant(N) : isNullConstant(N);\n  };\n\n  bool SwapSelectOps;\n  unsigned OpOffset = Slct.getOpcode() == RISCVISD::SELECT_CC ? 2 : 0;\n  SDValue TrueVal = Slct.getOperand(1 + OpOffset);\n  SDValue FalseVal = Slct.getOperand(2 + OpOffset);\n  SDValue NonConstantVal;\n  if (isZeroOrAllOnes(TrueVal, AllOnes)) {\n    SwapSelectOps = false;\n    NonConstantVal = FalseVal;\n  } else if (isZeroOrAllOnes(FalseVal, AllOnes)) {\n    SwapSelectOps = true;\n    NonConstantVal = TrueVal;\n  } else\n    return SDValue();\n\n  // Slct is now know to be the desired identity constant when CC is true.\n  TrueVal = OtherOp;\n  FalseVal = DAG.getNode(N->getOpcode(), SDLoc(N), VT, OtherOp, NonConstantVal);\n  // Unless SwapSelectOps says the condition should be false.\n  if (SwapSelectOps)\n    std::swap(TrueVal, FalseVal);\n\n  if (Slct.getOpcode() == RISCVISD::SELECT_CC)\n    return DAG.getNode(RISCVISD::SELECT_CC, SDLoc(N), VT,\n                       {Slct.getOperand(0), Slct.getOperand(1),\n                        Slct.getOperand(2), TrueVal, FalseVal});\n\n  return DAG.getNode(ISD::SELECT, SDLoc(N), VT,\n                     {Slct.getOperand(0), TrueVal, FalseVal});\n}",
      "start_line": 12346,
      "end_line": 12408,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "swap",
        "getOpcode",
        "isAllOnesConstant",
        "getValueType",
        "getOperand",
        "isNullConstant",
        "getXLen",
        "SDLoc",
        "hasVendorXVentanaCondOps",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineSelectAndUseCommutative",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "bool",
          "name": "AllOnes"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  if (SDValue Result = combineSelectAndUse(N, N0, N1, DAG, AllOnes, Subtarget))\n    return Result;\n  if (SDValue Result = combineSelectAndUse(N, N1, N0, DAG, AllOnes, Subtarget))\n    return Result;\n  return SDValue();\n}",
      "start_line": 12411,
      "end_line": 12421,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getOperand"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "transformAddImmMulImm",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // Skip for vector types and larger types.\n  EVT VT = N->getValueType(0);\n  if (VT.isVector() || VT.getSizeInBits() > Subtarget.getXLen())\n    return SDValue();\n  // The first operand node must be a MUL and has no other use.\n  SDValue N0 = N->getOperand(0);\n  if (!N0->hasOneUse() || N0->getOpcode() != ISD::MUL)\n    return SDValue();\n  // Check if c0 and c1 match above conditions.\n  auto *N0C = dyn_cast<ConstantSDNode>(N0->getOperand(1));\n  auto *N1C = dyn_cast<ConstantSDNode>(N->getOperand(1));\n  if (!N0C || !N1C)\n    return SDValue();\n  // If N0C has multiple uses it's possible one of the cases in\n  // DAGCombiner::isMulAddWithConstProfitable will be true, which would result\n  // in an infinite loop.\n  if (!N0C->hasOneUse())\n    return SDValue();\n  int64_t C0 = N0C->getSExtValue();\n  int64_t C1 = N1C->getSExtValue();\n  int64_t CA, CB;\n  if (C0 == -1 || C0 == 0 || C0 == 1 || isInt<12>(C1))\n    return SDValue();\n  // Search for proper CA (non-zero) and CB that both are simm12.\n  if ((C1 / C0) != 0 && isInt<12>(C1 / C0) && isInt<12>(C1 % C0) &&\n      !isInt<12>(C0 * (C1 / C0))) {\n    CA = C1 / C0;\n    CB = C1 % C0;\n  } else if ((C1 / C0 + 1) != 0 && isInt<12>(C1 / C0 + 1) &&\n             isInt<12>(C1 % C0 - C0) && !isInt<12>(C0 * (C1 / C0 + 1))) {\n    CA = C1 / C0 + 1;\n    CB = C1 % C0 - C0;\n  } else if ((C1 / C0 - 1) != 0 && isInt<12>(C1 / C0 - 1) &&\n             isInt<12>(C1 % C0 + C0) && !isInt<12>(C0 * (C1 / C0 - 1))) {\n    CA = C1 / C0 - 1;\n    CB = C1 % C0 + C0;\n  } else\n    return SDValue();\n  // Build new nodes (add (mul (add x, c1/c0), c0), c1%c0).\n  SDLoc DL(N);\n  SDValue New0 = DAG.getNode(ISD::ADD, DL, VT, N0->getOperand(0),\n                             DAG.getConstant(CA, DL, VT));\n  SDValue New1 =\n      DAG.getNode(ISD::MUL, DL, VT, New0, DAG.getConstant(C0, DL, VT));\n  return DAG.getNode(ISD::ADD, DL, VT, New1, DAG.getConstant(CB, DL, VT));\n}",
      "start_line": 12441,
      "end_line": 12488,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "CA",
        "nodes",
        "getNode",
        "getOpcode",
        "getConstant",
        "getValueType",
        "getOperand",
        "getSizeInBits",
        "getXLen",
        "DL",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineAddOfBooleanXor",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N->getValueType(0);\n  SDLoc DL(N);\n\n  // RHS should be -1.\n  if (!isAllOnesConstant(N1))\n    return SDValue();\n\n  // Look for (xor X, 1).\n  if (N0.getOpcode() != ISD::XOR || !isOneConstant(N0.getOperand(1)))\n    return SDValue();\n\n  // First xor input should be 0 or 1.\n  APInt Mask = APInt::getBitsSetFrom(VT.getSizeInBits(), 1);\n  if (!DAG.MaskedValueIsZero(N0.getOperand(0), Mask))\n    return SDValue();\n\n  // Emit a negate of the setcc.\n  return DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(0, DL, VT),\n                     N0.getOperand(0));\n}",
      "start_line": 12491,
      "end_line": 12513,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "isOneConstant",
        "getValueType",
        "getOperand",
        "DL",
        "getBitsSetFrom",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performADDCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (SDValue V = combineAddOfBooleanXor(N, DAG))\n    return V;\n  if (SDValue V = transformAddImmMulImm(N, DAG, Subtarget))\n    return V;\n  if (SDValue V = transformAddShlImm(N, DAG, Subtarget))\n    return V;\n  if (SDValue V = combineBinOpToReduce(N, DAG, Subtarget))\n    return V;\n  if (SDValue V = combineBinOpOfExtractToReduceTree(N, DAG, Subtarget))\n    return V;\n\n  // fold (add (select lhs, rhs, cc, 0, y), x) ->\n  //      (select lhs, rhs, cc, x, (add x, y))\n  return combineSelectAndUseCommutative(N, DAG, /*AllOnes*/ false, Subtarget);\n}",
      "start_line": 12515,
      "end_line": 12531,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "combineSelectAndUseCommutative",
        "fold"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineSubOfBoolean",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N->getValueType(0);\n  SDLoc DL(N);\n\n  // Require a constant LHS.\n  auto *N0C = dyn_cast<ConstantSDNode>(N0);\n  if (!N0C)\n    return SDValue();\n\n  // All our optimizations involve subtracting 1 from the immediate and forming\n  // an ADDI. Make sure the new immediate is valid for an ADDI.\n  APInt ImmValMinus1 = N0C->getAPIntValue() - 1;\n  if (!ImmValMinus1.isSignedIntN(12))\n    return SDValue();\n\n  SDValue NewLHS;\n  if (N1.getOpcode() == ISD::SETCC && N1.hasOneUse()) {\n    // (sub constant, (setcc x, y, eq/neq)) ->\n    // (add (setcc x, y, neq/eq), constant - 1)\n    ISD::CondCode CCVal = cast<CondCodeSDNode>(N1.getOperand(2))->get();\n    EVT SetCCOpVT = N1.getOperand(0).getValueType();\n    if (!isIntEqualitySetCC(CCVal) || !SetCCOpVT.isInteger())\n      return SDValue();\n    CCVal = ISD::getSetCCInverse(CCVal, SetCCOpVT);\n    NewLHS =\n        DAG.getSetCC(SDLoc(N1), VT, N1.getOperand(0), N1.getOperand(1), CCVal);\n  } else if (N1.getOpcode() == ISD::XOR && isOneConstant(N1.getOperand(1)) &&\n             N1.getOperand(0).getOpcode() == ISD::SETCC) {\n    // (sub C, (xor (setcc), 1)) -> (add (setcc), C-1).\n    // Since setcc returns a bool the xor is equivalent to 1-setcc.\n    NewLHS = N1.getOperand(0);\n  } else\n    return SDValue();\n\n  SDValue NewRHS = DAG.getConstant(ImmValMinus1, DL, VT);\n  return DAG.getNode(ISD::ADD, DL, VT, NewLHS, NewRHS);\n}",
      "start_line": 12534,
      "end_line": 12572,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "getSetCC",
        "add",
        "getAPIntValue",
        "getSetCCInverse",
        "getOpcode",
        "isOneConstant",
        "get",
        "getValueType",
        "xor",
        "getConstant",
        "getOperand",
        "DL",
        "isInteger",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performSUBCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (SDValue V = combineSubOfBoolean(N, DAG))\n    return V;\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  // fold (sub 0, (setcc x, 0, setlt)) -> (sra x, xlen - 1)\n  if (isNullConstant(N0) && N1.getOpcode() == ISD::SETCC && N1.hasOneUse() &&\n      isNullConstant(N1.getOperand(1))) {\n    ISD::CondCode CCVal = cast<CondCodeSDNode>(N1.getOperand(2))->get();\n    if (CCVal == ISD::SETLT) {\n      EVT VT = N->getValueType(0);\n      SDLoc DL(N);\n      unsigned ShAmt = N0.getValueSizeInBits() - 1;\n      return DAG.getNode(ISD::SRA, DL, VT, N1.getOperand(0),\n                         DAG.getConstant(ShAmt, DL, VT));\n    }\n  }\n\n  // fold (sub x, (select lhs, rhs, cc, 0, y)) ->\n  //      (select lhs, rhs, cc, x, (sub x, y))\n  return combineSelectAndUse(N, N1, N0, DAG, /*AllOnes*/ false, Subtarget);\n}",
      "start_line": 12574,
      "end_line": 12597,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasOneUse",
        "getOpcode",
        "getValueSizeInBits",
        "get",
        "getValueType",
        "getConstant",
        "getOperand",
        "combineSelectAndUse",
        "DL",
        "isNullConstant",
        "getNode",
        "fold"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineDeMorganOfBoolean",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  bool IsAnd = N->getOpcode() == ISD::AND;\n\n  if (N0.getOpcode() != ISD::XOR || N1.getOpcode() != ISD::XOR)\n    return SDValue();\n\n  if (!N0.hasOneUse() || !N1.hasOneUse())\n    return SDValue();\n\n  SDValue N01 = N0.getOperand(1);\n  SDValue N11 = N1.getOperand(1);\n\n  // For AND, SimplifyDemandedBits may have turned one of the (xor X, 1) into\n  // (xor X, -1) based on the upper bits of the other operand being 0. If the\n  // operation is And, allow one of the Xors to use -1.\n  if (isOneConstant(N01)) {\n    if (!isOneConstant(N11) && !(IsAnd && isAllOnesConstant(N11)))\n      return SDValue();\n  } else if (isOneConstant(N11)) {\n    // N01 and N11 being 1 was already handled. Handle N11==1 and N01==-1.\n    if (!(IsAnd && isAllOnesConstant(N01)))\n      return SDValue();\n  } else\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n\n  SDValue N00 = N0.getOperand(0);\n  SDValue N10 = N1.getOperand(0);\n\n  // The LHS of the xors needs to be 0/1.\n  APInt Mask = APInt::getBitsSetFrom(VT.getSizeInBits(), 1);\n  if (!DAG.MaskedValueIsZero(N00, Mask) || !DAG.MaskedValueIsZero(N10, Mask))\n    return SDValue();\n\n  // Invert the opcode and insert a new xor.\n  SDLoc DL(N);\n  unsigned Opc = IsAnd ? ISD::OR : ISD::AND;\n  SDValue Logic = DAG.getNode(Opc, DL, VT, N00, N10);\n  return DAG.getNode(ISD::XOR, DL, VT, Logic, DAG.getConstant(1, DL, VT));\n}",
      "start_line": 12602,
      "end_line": 12644,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "MaskedValueIsZero",
        "SDValue",
        "hasOneUse",
        "the",
        "getOpcode",
        "isAllOnesConstant",
        "getValueType",
        "getOperand",
        "DL",
        "getBitsSetFrom",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performTRUNCATECombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue N0 = N->getOperand(0);\n  EVT VT = N->getValueType(0);\n\n  // Pre-promote (i1 (truncate (srl X, Y))) on RV64 with Zbs without zero\n  // extending X. This is safe since we only need the LSB after the shift and\n  // shift amounts larger than 31 would produce poison. If we wait until\n  // type legalization, we'll create RISCVISD::SRLW and we can't recover it\n  // to use a BEXT instruction.\n  if (!RV64LegalI32 && Subtarget.is64Bit() && Subtarget.hasStdExtZbs() && VT == MVT::i1 &&\n      N0.getValueType() == MVT::i32 && N0.getOpcode() == ISD::SRL &&\n      !isa<ConstantSDNode>(N0.getOperand(1)) && N0.hasOneUse()) {\n    SDLoc DL(N0);\n    SDValue Op0 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N0.getOperand(0));\n    SDValue Op1 = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i64, N0.getOperand(1));\n    SDValue Srl = DAG.getNode(ISD::SRL, DL, MVT::i64, Op0, Op1);\n    return DAG.getNode(ISD::TRUNCATE, SDLoc(N), VT, Srl);\n  }\n\n  return SDValue();\n}",
      "start_line": 12646,
      "end_line": 12667,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "getOpcode",
        "getValueType",
        "promote",
        "getOperand",
        "hasStdExtZbs",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performANDCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "TargetLowering::DAGCombinerInfo",
          "name": "&DCI"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = DCI.DAG;\n\n  SDValue N0 = N->getOperand(0);\n  // Pre-promote (i32 (and (srl X, Y), 1)) on RV64 with Zbs without zero\n  // extending X. This is safe since we only need the LSB after the shift and\n  // shift amounts larger than 31 would produce poison. If we wait until\n  // type legalization, we'll create RISCVISD::SRLW and we can't recover it\n  // to use a BEXT instruction.\n  if (!RV64LegalI32 && Subtarget.is64Bit() && Subtarget.hasStdExtZbs() &&\n      N->getValueType(0) == MVT::i32 && isOneConstant(N->getOperand(1)) &&\n      N0.getOpcode() == ISD::SRL && !isa<ConstantSDNode>(N0.getOperand(1)) &&\n      N0.hasOneUse()) {\n    SDLoc DL(N);\n    SDValue Op0 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N0.getOperand(0));\n    SDValue Op1 = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i64, N0.getOperand(1));\n    SDValue Srl = DAG.getNode(ISD::SRL, DL, MVT::i64, Op0, Op1);\n    SDValue And = DAG.getNode(ISD::AND, DL, MVT::i64, Srl,\n                              DAG.getConstant(1, DL, MVT::i64));\n    return DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, And);\n  }\n\n  if (SDValue V = combineBinOpToReduce(N, DAG, Subtarget))\n    return V;\n  if (SDValue V = combineBinOpOfExtractToReduceTree(N, DAG, Subtarget))\n    return V;\n\n  if (DCI.isAfterLegalizeDAG())\n    if (SDValue V = combineDeMorganOfBoolean(N, DAG))\n      return V;\n\n  // fold (and (select lhs, rhs, cc, -1, y), x) ->\n  //      (select lhs, rhs, cc, x, (and x, y))\n  return combineSelectAndUseCommutative(N, DAG, /*AllOnes*/ true, Subtarget);\n}",
      "start_line": 12672,
      "end_line": 12708,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasOneUse",
        "getOpcode",
        "isOneConstant",
        "getValueType",
        "promote",
        "getOperand",
        "combineSelectAndUseCommutative",
        "hasStdExtZbs",
        "DL",
        "getNode",
        "fold"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineOrOfCZERO",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SDValue",
          "name": "N0"
        },
        {
          "type": "SDValue",
          "name": "N1"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  assert(N->getOpcode() == ISD::OR && \"Unexpected opcode\");\n\n  if (N0.getOpcode() != RISCVISD::CZERO_EQZ ||\n      N1.getOpcode() != RISCVISD::CZERO_NEZ ||\n      !N0.hasOneUse() || !N1.hasOneUse())\n    return SDValue();\n\n  // Should have the same condition.\n  SDValue Cond = N0.getOperand(1);\n  if (Cond != N1.getOperand(1))\n    return SDValue();\n\n  SDValue TrueV = N0.getOperand(0);\n  SDValue FalseV = N1.getOperand(0);\n\n  if (TrueV.getOpcode() != ISD::XOR || FalseV.getOpcode() != ISD::XOR ||\n      TrueV.getOperand(1) != FalseV.getOperand(1) ||\n      !isOneConstant(TrueV.getOperand(1)) ||\n      !TrueV.hasOneUse() || !FalseV.hasOneUse())\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n  SDLoc DL(N);\n\n  SDValue NewN0 = DAG.getNode(RISCVISD::CZERO_EQZ, DL, VT, TrueV.getOperand(0),\n                              Cond);\n  SDValue NewN1 = DAG.getNode(RISCVISD::CZERO_NEZ, DL, VT, FalseV.getOperand(0),\n                              Cond);\n  SDValue NewOr = DAG.getNode(ISD::OR, DL, VT, NewN0, NewN1);\n  return DAG.getNode(ISD::XOR, DL, VT, NewOr, TrueV.getOperand(1));\n}",
      "start_line": 12712,
      "end_line": 12744,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "getOpcode",
        "isOneConstant",
        "getValueType",
        "getOperand",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performORCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "TargetLowering::DAGCombinerInfo",
          "name": "&DCI"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = DCI.DAG;\n\n  if (SDValue V = combineBinOpToReduce(N, DAG, Subtarget))\n    return V;\n  if (SDValue V = combineBinOpOfExtractToReduceTree(N, DAG, Subtarget))\n    return V;\n\n  if (DCI.isAfterLegalizeDAG())\n    if (SDValue V = combineDeMorganOfBoolean(N, DAG))\n      return V;\n\n  // Look for Or of CZERO_EQZ/NEZ with same condition which is the select idiom.\n  // We may be able to pull a common operation out of the true and false value.\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  if (SDValue V = combineOrOfCZERO(N, N0, N1, DAG))\n    return V;\n  if (SDValue V = combineOrOfCZERO(N, N1, N0, DAG))\n    return V;\n\n  // fold (or (select cond, 0, y), x) ->\n  //      (select cond, x, (or x, y))\n  return combineSelectAndUseCommutative(N, DAG, /*AllOnes*/ false, Subtarget);\n}",
      "start_line": 12746,
      "end_line": 12771,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand",
        "combineSelectAndUseCommutative",
        "fold"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performXORCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // Pre-promote (i32 (xor (shl -1, X), ~0)) on RV64 with Zbs so we can use\n  // (ADDI (BSET X0, X), -1). If we wait until/ type legalization, we'll create\n  // RISCVISD:::SLLW and we can't recover it to use a BSET instruction.\n  if (!RV64LegalI32 && Subtarget.is64Bit() && Subtarget.hasStdExtZbs() &&\n      N->getValueType(0) == MVT::i32 && isAllOnesConstant(N1) &&\n      N0.getOpcode() == ISD::SHL && isAllOnesConstant(N0.getOperand(0)) &&\n      !isa<ConstantSDNode>(N0.getOperand(1)) && N0.hasOneUse()) {\n    SDLoc DL(N);\n    SDValue Op0 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N0.getOperand(0));\n    SDValue Op1 = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i64, N0.getOperand(1));\n    SDValue Shl = DAG.getNode(ISD::SHL, DL, MVT::i64, Op0, Op1);\n    SDValue And = DAG.getNOT(DL, Shl, MVT::i64);\n    return DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, And);\n  }\n\n  // fold (xor (sllw 1, x), -1) -> (rolw ~1, x)\n  // NOTE: Assumes ROL being legal means ROLW is legal.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (N0.getOpcode() == RISCVISD::SLLW &&\n      isAllOnesConstant(N1) && isOneConstant(N0.getOperand(0)) &&\n      TLI.isOperationLegal(ISD::ROTL, MVT::i64)) {\n    SDLoc DL(N);\n    return DAG.getNode(RISCVISD::ROLW, DL, MVT::i64,\n                       DAG.getConstant(~1, DL, MVT::i64), N0.getOperand(1));\n  }\n\n  // Fold (xor (setcc constant, y, setlt), 1) -> (setcc y, constant + 1, setlt)\n  if (N0.getOpcode() == ISD::SETCC && isOneConstant(N1) && N0.hasOneUse()) {\n    auto *ConstN00 = dyn_cast<ConstantSDNode>(N0.getOperand(0));\n    ISD::CondCode CC = cast<CondCodeSDNode>(N0.getOperand(2))->get();\n    if (ConstN00 && CC == ISD::SETLT) {\n      EVT VT = N0.getValueType();\n      SDLoc DL(N0);\n      const APInt &Imm = ConstN00->getAPIntValue();\n      if ((Imm + 1).isSignedIntN(12))\n        return DAG.getSetCC(DL, VT, N0.getOperand(1),\n                            DAG.getConstant(Imm + 1, DL, VT), CC);\n    }\n  }\n\n  if (SDValue V = combineBinOpToReduce(N, DAG, Subtarget))\n    return V;\n  if (SDValue V = combineBinOpOfExtractToReduceTree(N, DAG, Subtarget))\n    return V;\n\n  // fold (xor (select cond, 0, y), x) ->\n  //      (select cond, x, (xor x, y))\n  return combineSelectAndUseCommutative(N, DAG, /*AllOnes*/ false, Subtarget);\n}",
      "start_line": 12773,
      "end_line": 12826,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNOT",
        "isSignedIntN",
        "getAPIntValue",
        "getConstant",
        "hasStdExtZbs",
        "isOneConstant",
        "promote",
        "getTargetLoweringInfo",
        "getSetCC",
        "getOpcode",
        "isAllOnesConstant",
        "getValueType",
        "getOperand",
        "fold",
        "hasOneUse",
        "Fold",
        "get",
        "combineSelectAndUseCommutative",
        "DL",
        "ADDI",
        "isOperationLegal",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performMULCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  EVT VT = N->getValueType(0);\n  if (!VT.isVector())\n    return SDValue();\n\n  SDLoc DL(N);\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  SDValue MulOper;\n  unsigned AddSubOpc;\n\n  // vmadd: (mul (add x, 1), y) -> (add (mul x, y), y)\n  //        (mul x, add (y, 1)) -> (add x, (mul x, y))\n  // vnmsub: (mul (sub 1, x), y) -> (sub y, (mul x, y))\n  //         (mul x, (sub 1, y)) -> (sub x, (mul x, y))\n  auto IsAddSubWith1 = [&](SDValue V) -> bool {\n    AddSubOpc = V->getOpcode();\n    if ((AddSubOpc == ISD::ADD || AddSubOpc == ISD::SUB) && V->hasOneUse()) {\n      SDValue Opnd = V->getOperand(1);\n      MulOper = V->getOperand(0);\n      if (AddSubOpc == ISD::SUB)\n        std::swap(Opnd, MulOper);\n      if (isOneOrOneSplat(Opnd))\n        return true;\n    }\n    return false;\n  };\n\n  if (IsAddSubWith1(N0)) {\n    SDValue MulVal = DAG.getNode(ISD::MUL, DL, VT, N1, MulOper);\n    return DAG.getNode(AddSubOpc, DL, VT, N1, MulVal);\n  }\n\n  if (IsAddSubWith1(N1)) {\n    SDValue MulVal = DAG.getNode(ISD::MUL, DL, VT, N0, MulOper);\n    return DAG.getNode(AddSubOpc, DL, VT, N0, MulVal);\n  }\n\n  return SDValue();\n}",
      "start_line": 12828,
      "end_line": 12867,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "add",
        "swap",
        "getOpcode",
        "getValueType",
        "getOperand",
        "DL",
        "getNode",
        "mul"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "narrowIndex",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "&N"
        },
        {
          "type": "ISD::MemIndexType",
          "name": "IndexType"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  if (isIndexTypeSigned(IndexType))\n    return false;\n\n  if (!N->hasOneUse())\n    return false;\n\n  EVT VT = N.getValueType();\n  SDLoc DL(N);\n\n  // In general, what we're doing here is seeing if we can sink a truncate to\n  // a smaller element type into the expression tree building our index.\n  // TODO: We can generalize this and handle a bunch more cases if useful.\n\n  // Narrow a buildvector to the narrowest element type.  This requires less\n  // work and less register pressure at high LMUL, and creates smaller constants\n  // which may be cheaper to materialize.\n  if (ISD::isBuildVectorOfConstantSDNodes(N.getNode())) {\n    KnownBits Known = DAG.computeKnownBits(N);\n    unsigned ActiveBits = std::max(8u, Known.countMaxActiveBits());\n    LLVMContext &C = *DAG.getContext();\n    EVT ResultVT = EVT::getIntegerVT(C, ActiveBits).getRoundIntegerType(C);\n    if (ResultVT.bitsLT(VT.getVectorElementType())) {\n      N = DAG.getNode(ISD::TRUNCATE, DL,\n                      VT.changeVectorElementType(ResultVT), N);\n      return true;\n    }\n  }\n\n  // Handle the pattern (shl (zext x to ty), C) and bits(x) + C < bits(ty).\n  if (N.getOpcode() != ISD::SHL)\n    return false;\n\n  SDValue N0 = N.getOperand(0);\n  if (N0.getOpcode() != ISD::ZERO_EXTEND &&\n      N0.getOpcode() != RISCVISD::VZEXT_VL)\n    return false;\n  if (!N0->hasOneUse())\n    return false;\n\n  APInt ShAmt;\n  SDValue N1 = N.getOperand(1);\n  if (!ISD::isConstantSplatVector(N1.getNode(), ShAmt))\n    return false;\n\n  SDValue Src = N0.getOperand(0);\n  EVT SrcVT = Src.getValueType();\n  unsigned SrcElen = SrcVT.getScalarSizeInBits();\n  unsigned ShAmtV = ShAmt.getZExtValue();\n  unsigned NewElen = PowerOf2Ceil(SrcElen + ShAmtV);\n  NewElen = std::max(NewElen, 8U);\n\n  // Skip if NewElen is not narrower than the original extended type.\n  if (NewElen >= N0.getValueType().getScalarSizeInBits())\n    return false;\n\n  EVT NewEltVT = EVT::getIntegerVT(*DAG.getContext(), NewElen);\n  EVT NewVT = SrcVT.changeVectorElementType(NewEltVT);\n\n  SDValue NewExt = DAG.getNode(N0->getOpcode(), DL, NewVT, N0->ops());\n  SDValue NewShAmtVec = DAG.getConstant(ShAmtV, DL, NewVT);\n  N = DAG.getNode(ISD::SHL, DL, NewVT, NewExt, NewShAmtVec);\n  return true;\n}",
      "start_line": 12871,
      "end_line": 12934,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getContext",
        "getZExtValue",
        "changeVectorElementType",
        "PowerOf2Ceil",
        "bits",
        "getOpcode",
        "getRoundIntegerType",
        "getConstant",
        "pattern",
        "getValueType",
        "ops",
        "getOperand",
        "getIntegerVT",
        "DL",
        "max",
        "computeKnownBits",
        "getNode",
        "getScalarSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performSETCCCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N->getValueType(0);\n  EVT OpVT = N0.getValueType();\n\n  if (OpVT != MVT::i64 || !Subtarget.is64Bit())\n    return SDValue();\n\n  // RHS needs to be a constant.\n  auto *N1C = dyn_cast<ConstantSDNode>(N1);\n  if (!N1C)\n    return SDValue();\n\n  // LHS needs to be (and X, 0xffffffff).\n  if (N0.getOpcode() != ISD::AND || !N0.hasOneUse() ||\n      !isa<ConstantSDNode>(N0.getOperand(1)) ||\n      N0.getConstantOperandVal(1) != UINT64_C(0xffffffff))\n    return SDValue();\n\n  // Looking for an equality compare.\n  ISD::CondCode Cond = cast<CondCodeSDNode>(N->getOperand(2))->get();\n  if (!isIntEqualitySetCC(Cond))\n    return SDValue();\n\n  // Don't do this if the sign bit is provably zero, it will be turned back into\n  // an AND.\n  APInt SignMask = APInt::getOneBitSet(64, 31);\n  if (DAG.MaskedValueIsZero(N0.getOperand(0), SignMask))\n    return SDValue();\n\n  const APInt &C1 = N1C->getAPIntValue();\n\n  SDLoc dl(N);\n  // If the constant is larger than 2^32 - 1 it is impossible for both sides\n  // to be equal.\n  if (C1.getActiveBits() > 32)\n    return DAG.getBoolConstant(Cond == ISD::SETNE, dl, VT, OpVT);\n\n  SDValue SExtOp = DAG.getNode(ISD::SIGN_EXTEND_INREG, N, OpVT,\n                               N0.getOperand(0), DAG.getValueType(MVT::i32));\n  return DAG.getSetCC(dl, VT, SExtOp, DAG.getConstant(C1.trunc(32).sext(64),\n                                                      dl, OpVT), Cond);\n}",
      "start_line": 12940,
      "end_line": 12984,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "getSetCC",
        "be",
        "getConstantOperandVal",
        "dl",
        "getAPIntValue",
        "UINT64_C",
        "sext",
        "get",
        "getValueType",
        "getBoolConstant",
        "getOperand",
        "getOneBitSet",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performSIGN_EXTEND_INREGCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue Src = N->getOperand(0);\n  EVT VT = N->getValueType(0);\n\n  // Fold (sext_inreg (fmv_x_anyexth X), i16) -> (fmv_x_signexth X)\n  if (Src.getOpcode() == RISCVISD::FMV_X_ANYEXTH &&\n      cast<VTSDNode>(N->getOperand(1))->getVT().bitsGE(MVT::i16))\n    return DAG.getNode(RISCVISD::FMV_X_SIGNEXTH, SDLoc(N), VT,\n                       Src.getOperand(0));\n\n  return SDValue();\n}",
      "start_line": 12986,
      "end_line": 12999,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "Fold",
        "getVT",
        "getValueType",
        "getOperand",
        "getNode",
        "bitsGE"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getSource",
      "return_type": "SDValue",
      "parameters": [],
      "body": "{\n    switch (OrigOperand.getOpcode()) {\n    case ISD::ZERO_EXTEND:\n    case ISD::SIGN_EXTEND:\n    case RISCVISD::VSEXT_VL:\n    case RISCVISD::VZEXT_VL:\n      return OrigOperand.getOperand(0);\n    default:\n      return OrigOperand;\n    }\n  }",
      "start_line": 13053,
      "end_line": 13063,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isSplat",
      "return_type": "bool",
      "parameters": [],
      "body": "{\n    return OrigOperand.getOpcode() == RISCVISD::VMV_V_X_VL;\n  }",
      "start_line": 13066,
      "end_line": 13068,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOpcode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getOrCreateExtendedOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Root"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "std::optional<bool>",
          "name": "SExt"
        }
      ],
      "body": "{\n    if (!SExt.has_value())\n      return OrigOperand;\n\n    MVT NarrowVT = getNarrowType(Root);\n\n    SDValue Source = getSource();\n    if (Source.getValueType() == NarrowVT)\n      return Source;\n\n    unsigned ExtOpc = *SExt ? RISCVISD::VSEXT_VL : RISCVISD::VZEXT_VL;\n\n    // If we need an extension, we should be changing the type.\n    SDLoc DL(Root);\n    auto [Mask, VL] = getMaskAndVL(Root, DAG, Subtarget);\n    switch (OrigOperand.getOpcode()) {\n    case ISD::ZERO_EXTEND:\n    case ISD::SIGN_EXTEND:\n    case RISCVISD::VSEXT_VL:\n    case RISCVISD::VZEXT_VL:\n      return DAG.getNode(ExtOpc, DL, NarrowVT, Source, Mask, VL);\n    case RISCVISD::VMV_V_X_VL:\n      return DAG.getNode(RISCVISD::VMV_V_X_VL, DL, NarrowVT,\n                         DAG.getUNDEF(NarrowVT), Source.getOperand(1), VL);\n    default:\n      // Other opcodes can only come from the original LHS of VW(ADD|SUB)_W_VL\n      // and that operand should already have the right NarrowVT so no\n      // extension should be required at this point.\n      llvm_unreachable(\"Unsupported opcode\");\n    }\n  }",
      "start_line": 13073,
      "end_line": 13105,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getNarrowType",
        "getOperand",
        "getSource",
        "DL",
        "VW",
        "getMaskAndVL",
        "llvm_unreachable",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getNarrowType",
      "return_type": "MVT",
      "parameters": [
        {
          "type": "const SDNode",
          "name": "*Root"
        }
      ],
      "body": "{\n    MVT VT = Root->getSimpleValueType(0);\n\n    // Determine the narrow size.\n    unsigned NarrowSize = VT.getScalarSizeInBits() / 2;\n    assert(NarrowSize >= 8 && \"Trying to extend something we can't represent\");\n    MVT NarrowVT = MVT::getVectorVT(MVT::getIntegerVT(NarrowSize),\n                                    VT.getVectorElementCount());\n    return NarrowVT;\n  }",
      "start_line": 13112,
      "end_line": 13121,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getVectorVT",
        "getScalarSizeInBits",
        "getSimpleValueType",
        "getVectorElementCount"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getSameExtensionOpcode",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        },
        {
          "type": "bool",
          "name": "IsSExt"
        }
      ],
      "body": "{\n    switch (Opcode) {\n    case ISD::ADD:\n    case RISCVISD::ADD_VL:\n    case RISCVISD::VWADD_W_VL:\n    case RISCVISD::VWADDU_W_VL:\n      return IsSExt ? RISCVISD::VWADD_VL : RISCVISD::VWADDU_VL;\n    case ISD::MUL:\n    case RISCVISD::MUL_VL:\n      return IsSExt ? RISCVISD::VWMUL_VL : RISCVISD::VWMULU_VL;\n    case ISD::SUB:\n    case RISCVISD::SUB_VL:\n    case RISCVISD::VWSUB_W_VL:\n    case RISCVISD::VWSUBU_W_VL:\n      return IsSExt ? RISCVISD::VWSUB_VL : RISCVISD::VWSUBU_VL;\n    default:\n      llvm_unreachable(\"Unexpected opcode\");\n    }\n  }",
      "start_line": 13130,
      "end_line": 13148,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getSameExtensionOpcode",
          "condition": "Opcode",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getSUOpcode",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        }
      ],
      "body": "{\n    assert((Opcode == RISCVISD::MUL_VL || Opcode == ISD::MUL) &&\n           \"SU is only supported for MUL\");\n    return RISCVISD::VWMULSU_VL;\n  }",
      "start_line": 13152,
      "end_line": 13156,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getWOpcode",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        },
        {
          "type": "bool",
          "name": "IsSExt"
        }
      ],
      "body": "{\n    switch (Opcode) {\n    case ISD::ADD:\n    case RISCVISD::ADD_VL:\n      return IsSExt ? RISCVISD::VWADD_W_VL : RISCVISD::VWADDU_W_VL;\n    case ISD::SUB:\n    case RISCVISD::SUB_VL:\n      return IsSExt ? RISCVISD::VWSUB_W_VL : RISCVISD::VWSUBU_W_VL;\n    default:\n      llvm_unreachable(\"Unexpected opcode\");\n    }\n  }",
      "start_line": 13160,
      "end_line": 13171,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getWOpcode",
          "condition": "Opcode",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "needToPromoteOtherUsers",
      "return_type": "bool",
      "parameters": [],
      "body": "{ return EnforceOneUse; }",
      "start_line": 13179,
      "end_line": 13179,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "fillUpExtensionSupport",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Root"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n    SupportsZExt = false;\n    SupportsSExt = false;\n    EnforceOneUse = true;\n    CheckMask = true;\n    unsigned Opc = OrigOperand.getOpcode();\n    switch (Opc) {\n    case ISD::ZERO_EXTEND:\n    case ISD::SIGN_EXTEND: {\n      MVT VT = OrigOperand.getSimpleValueType();\n      if (!VT.isVector())\n        break;\n\n      SDValue NarrowElt = OrigOperand.getOperand(0);\n      MVT NarrowVT = NarrowElt.getSimpleValueType();\n\n      unsigned ScalarBits = VT.getScalarSizeInBits();\n      unsigned NarrowScalarBits = NarrowVT.getScalarSizeInBits();\n\n      // Ensure the narrowing element type is legal\n      if (!Subtarget.getTargetLowering()->isTypeLegal(NarrowElt.getValueType()))\n        break;\n\n      // Ensure the extension's semantic is equivalent to rvv vzext or vsext.\n      if (ScalarBits != NarrowScalarBits * 2)\n        break;\n\n      SupportsZExt = Opc == ISD::ZERO_EXTEND;\n      SupportsSExt = Opc == ISD::SIGN_EXTEND;\n\n      SDLoc DL(Root);\n      std::tie(Mask, VL) = getDefaultScalableVLOps(VT, DL, DAG, Subtarget);\n      break;\n    }\n    case RISCVISD::VZEXT_VL:\n      SupportsZExt = true;\n      Mask = OrigOperand.getOperand(1);\n      VL = OrigOperand.getOperand(2);\n      break;\n    case RISCVISD::VSEXT_VL:\n      SupportsSExt = true;\n      Mask = OrigOperand.getOperand(1);\n      VL = OrigOperand.getOperand(2);\n      break;\n    case RISCVISD::VMV_V_X_VL: {\n      // Historically, we didn't care about splat values not disappearing during\n      // combines.\n      EnforceOneUse = false;\n      CheckMask = false;\n      VL = OrigOperand.getOperand(2);\n\n      // The operand is a splat of a scalar.\n\n      // The pasthru must be undef for tail agnostic.\n      if (!OrigOperand.getOperand(0).isUndef())\n        break;\n\n      // Get the scalar value.\n      SDValue Op = OrigOperand.getOperand(1);\n\n      // See if we have enough sign bits or zero bits in the scalar to use a\n      // widening opcode by splatting to smaller element size.\n      MVT VT = Root->getSimpleValueType(0);\n      unsigned EltBits = VT.getScalarSizeInBits();\n      unsigned ScalarBits = Op.getValueSizeInBits();\n      // Make sure we're getting all element bits from the scalar register.\n      // FIXME: Support implicit sign extension of vmv.v.x?\n      if (ScalarBits < EltBits)\n        break;\n\n      unsigned NarrowSize = VT.getScalarSizeInBits() / 2;\n      // If the narrow type cannot be expressed with a legal VMV,\n      // this is not a valid candidate.\n      if (NarrowSize < 8)\n        break;\n\n      if (DAG.ComputeMaxSignificantBits(Op) <= NarrowSize)\n        SupportsSExt = true;\n      if (DAG.MaskedValueIsZero(Op,\n                                APInt::getBitsSetFrom(ScalarBits, NarrowSize)))\n        SupportsZExt = true;\n      break;\n    }\n    default:\n      break;\n    }\n  }",
      "start_line": 13183,
      "end_line": 13270,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "fillUpExtensionSupport",
          "condition": "Opc",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "isTypeLegal",
        "getOpcode",
        "getValueSizeInBits",
        "getSimpleValueType",
        "isUndef",
        "getOperand",
        "DL",
        "tie",
        "getDefaultScalableVLOps",
        "getScalarSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isSupportedRoot",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const SDNode",
          "name": "*Root"
        },
        {
          "type": "const SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n    switch (Root->getOpcode()) {\n    case ISD::ADD:\n    case ISD::SUB:\n    case ISD::MUL: {\n      const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n      if (!TLI.isTypeLegal(Root->getValueType(0)))\n        return false;\n      return Root->getValueType(0).isScalableVector();\n    }\n    case RISCVISD::ADD_VL:\n    case RISCVISD::MUL_VL:\n    case RISCVISD::VWADD_W_VL:\n    case RISCVISD::VWADDU_W_VL:\n    case RISCVISD::SUB_VL:\n    case RISCVISD::VWSUB_W_VL:\n    case RISCVISD::VWSUBU_W_VL:\n      return true;\n    default:\n      return false;\n    }\n  }",
      "start_line": 13273,
      "end_line": 13294,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getTargetLoweringInfo",
        "isScalableVector",
        "getValueType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isVLCompatible",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "VL"
        }
      ],
      "body": "{\n    return this->VL != SDValue() && this->VL == VL;\n  }",
      "start_line": 13331,
      "end_line": 13333,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isMaskCompatible",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Mask"
        }
      ],
      "body": "{\n    return !CheckMask || (this->Mask != SDValue() && this->Mask == Mask);\n  }",
      "start_line": 13336,
      "end_line": 13338,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "areVLAndMaskCompatible",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Root"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n    auto [Mask, VL] = getMaskAndVL(Root, DAG, Subtarget);\n    return isMaskCompatible(Mask) && isVLCompatible(VL);\n  }",
      "start_line": 13359,
      "end_line": 13363,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isMaskCompatible",
        "getMaskAndVL",
        "isVLCompatible"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isCommutative",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const SDNode",
          "name": "*N"
        }
      ],
      "body": "{\n    switch (N->getOpcode()) {\n    case ISD::ADD:\n    case ISD::MUL:\n    case RISCVISD::ADD_VL:\n    case RISCVISD::MUL_VL:\n    case RISCVISD::VWADD_W_VL:\n    case RISCVISD::VWADDU_W_VL:\n      return true;\n    case ISD::SUB:\n    case RISCVISD::SUB_VL:\n    case RISCVISD::VWSUB_W_VL:\n    case RISCVISD::VWSUBU_W_VL:\n      return false;\n    default:\n      llvm_unreachable(\"Unexpected opcode\");\n    }\n  }",
      "start_line": 13367,
      "end_line": 13384,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "materialize",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n    SDValue Mask, VL, Merge;\n    std::tie(Mask, VL) =\n        NodeExtensionHelper::getMaskAndVL(Root, DAG, Subtarget);\n    switch (Root->getOpcode()) {\n    default:\n      Merge = Root->getOperand(2);\n      break;\n    case ISD::ADD:\n    case ISD::SUB:\n    case ISD::MUL:\n      Merge = DAG.getUNDEF(Root->getValueType(0));\n      break;\n    }\n    return DAG.getNode(TargetOpcode, SDLoc(Root), Root->getValueType(0),\n                       LHS.getOrCreateExtendedOp(Root, DAG, Subtarget, SExtLHS),\n                       RHS.getOrCreateExtendedOp(Root, DAG, Subtarget, SExtRHS),\n                       Merge, Mask, VL);\n  }",
      "start_line": 13421,
      "end_line": 13440,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOrCreateExtendedOp",
        "getValueType",
        "getUNDEF",
        "getOperand",
        "tie",
        "getMaskAndVL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineBinOp_VLToVWBinOp_VL",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "TargetLowering::DAGCombinerInfo",
          "name": "&DCI"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = DCI.DAG;\n\n  if (!NodeExtensionHelper::isSupportedRoot(N, DAG))\n    return SDValue();\n\n  SmallVector<SDNode *> Worklist;\n  SmallSet<SDNode *, 8> Inserted;\n  Worklist.push_back(N);\n  Inserted.insert(N);\n  SmallVector<CombineResult> CombinesToApply;\n\n  while (!Worklist.empty()) {\n    SDNode *Root = Worklist.pop_back_val();\n    if (!NodeExtensionHelper::isSupportedRoot(Root, DAG))\n      return SDValue();\n\n    NodeExtensionHelper LHS(N, 0, DAG, Subtarget);\n    NodeExtensionHelper RHS(N, 1, DAG, Subtarget);\n    auto AppendUsersIfNeeded = [&Worklist,\n                                &Inserted](const NodeExtensionHelper &Op) {\n      if (Op.needToPromoteOtherUsers()) {\n        for (SDNode *TheUse : Op.OrigOperand->uses()) {\n          if (Inserted.insert(TheUse).second)\n            Worklist.push_back(TheUse);\n        }\n      }\n    };\n\n    // Control the compile time by limiting the number of node we look at in\n    // total.\n    if (Inserted.size() > ExtensionMaxWebSize)\n      return SDValue();\n\n    SmallVector<NodeExtensionHelper::CombineToTry> FoldingStrategies =\n        NodeExtensionHelper::getSupportedFoldings(N);\n\n    assert(!FoldingStrategies.empty() && \"Nothing to be folded\");\n    bool Matched = false;\n    for (int Attempt = 0;\n         (Attempt != 1 + NodeExtensionHelper::isCommutative(N)) && !Matched;\n         ++Attempt) {\n\n      for (NodeExtensionHelper::CombineToTry FoldingStrategy :\n           FoldingStrategies) {\n        std::optional<CombineResult> Res =\n            FoldingStrategy(N, LHS, RHS, DAG, Subtarget);\n        if (Res) {\n          Matched = true;\n          CombinesToApply.push_back(*Res);\n          // All the inputs that are extended need to be folded, otherwise\n          // we would be leaving the old input (since it is may still be used),\n          // and the new one.\n          if (Res->SExtLHS.has_value())\n            AppendUsersIfNeeded(LHS);\n          if (Res->SExtRHS.has_value())\n            AppendUsersIfNeeded(RHS);\n          break;\n        }\n      }\n      std::swap(LHS, RHS);\n    }\n    // Right now we do an all or nothing approach.\n    if (!Matched)\n      return SDValue();\n  }\n  // Store the value for the replacement of the input node separately.\n  SDValue InputRootReplacement;\n  // We do the RAUW after we materialize all the combines, because some replaced\n  // nodes may be feeding some of the yet-to-be-replaced nodes. Put differently,\n  // some of these nodes may appear in the NodeExtensionHelpers of some of the\n  // yet-to-be-visited CombinesToApply roots.\n  SmallVector<std::pair<SDValue, SDValue>> ValuesToReplace;\n  ValuesToReplace.reserve(CombinesToApply.size());\n  for (CombineResult Res : CombinesToApply) {\n    SDValue NewValue = Res.materialize(DAG, Subtarget);\n    if (!InputRootReplacement) {\n      assert(Res.Root == N &&\n             \"First element is expected to be the current node\");\n      InputRootReplacement = NewValue;\n    } else {\n      ValuesToReplace.emplace_back(SDValue(Res.Root, 0), NewValue);\n    }\n  }\n  for (std::pair<SDValue, SDValue> OldNewValues : ValuesToReplace) {\n    DAG.ReplaceAllUsesOfValueWith(OldNewValues.first, OldNewValues.second);\n    DCI.AddToWorklist(OldNewValues.second.getNode());\n  }\n  return InputRootReplacement;\n}",
      "start_line": 13600,
      "end_line": 13691,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "ReplaceAllUsesOfValueWith",
        "SDValue",
        "pop_back_val",
        "swap",
        "AddToWorklist",
        "getSupportedFoldings",
        "reserve",
        "materialize",
        "FoldingStrategy",
        "emplace_back",
        "input",
        "insert",
        "LHS",
        "push_back",
        "AppendUsersIfNeeded",
        "RHS"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "tryMemPairCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "LSBaseSDNode",
          "name": "*LSNode1"
        },
        {
          "type": "LSBaseSDNode",
          "name": "*LSNode2"
        },
        {
          "type": "SDValue",
          "name": "BasePtr"
        },
        {
          "type": "uint64_t",
          "name": "Imm"
        }
      ],
      "body": "{\n  SmallPtrSet<const SDNode *, 32> Visited;\n  SmallVector<const SDNode *, 8> Worklist = {LSNode1, LSNode2};\n\n  if (SDNode::hasPredecessorHelper(LSNode1, Visited, Worklist) ||\n      SDNode::hasPredecessorHelper(LSNode2, Visited, Worklist))\n    return SDValue();\n\n  MachineFunction &MF = DAG.getMachineFunction();\n  const RISCVSubtarget &Subtarget = MF.getSubtarget<RISCVSubtarget>();\n\n  // The new operation has twice the width.\n  MVT XLenVT = Subtarget.getXLenVT();\n  EVT MemVT = LSNode1->getMemoryVT();\n  EVT NewMemVT = (MemVT == MVT::i32) ? MVT::i64 : MVT::i128;\n  MachineMemOperand *MMO = LSNode1->getMemOperand();\n  MachineMemOperand *NewMMO = MF.getMachineMemOperand(\n      MMO, MMO->getPointerInfo(), MemVT == MVT::i32 ? 8 : 16);\n\n  if (LSNode1->getOpcode() == ISD::LOAD) {\n    auto Ext = cast<LoadSDNode>(LSNode1)->getExtensionType();\n    unsigned Opcode;\n    if (MemVT == MVT::i32)\n      Opcode = (Ext == ISD::ZEXTLOAD) ? RISCVISD::TH_LWUD : RISCVISD::TH_LWD;\n    else\n      Opcode = RISCVISD::TH_LDD;\n\n    SDValue Res = DAG.getMemIntrinsicNode(\n        Opcode, SDLoc(LSNode1), DAG.getVTList({XLenVT, XLenVT, MVT::Other}),\n        {LSNode1->getChain(), BasePtr,\n         DAG.getConstant(Imm, SDLoc(LSNode1), XLenVT)},\n        NewMemVT, NewMMO);\n\n    SDValue Node1 =\n        DAG.getMergeValues({Res.getValue(0), Res.getValue(2)}, SDLoc(LSNode1));\n    SDValue Node2 =\n        DAG.getMergeValues({Res.getValue(1), Res.getValue(2)}, SDLoc(LSNode2));\n\n    DAG.ReplaceAllUsesWith(LSNode2, Node2.getNode());\n    return Node1;\n  } else {\n    unsigned Opcode = (MemVT == MVT::i32) ? RISCVISD::TH_SWD : RISCVISD::TH_SDD;\n\n    SDValue Res = DAG.getMemIntrinsicNode(\n        Opcode, SDLoc(LSNode1), DAG.getVTList(MVT::Other),\n        {LSNode1->getChain(), LSNode1->getOperand(1), LSNode2->getOperand(1),\n         BasePtr, DAG.getConstant(Imm, SDLoc(LSNode1), XLenVT)},\n        NewMemVT, NewMMO);\n\n    DAG.ReplaceAllUsesWith(LSNode2, Res.getNode());\n    return Res;\n  }\n}",
      "start_line": 13696,
      "end_line": 13750,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "getMemOperand",
        "getMergeValues",
        "getValue",
        "getVTList",
        "getChain",
        "getMachineFunction",
        "getMemoryVT",
        "getConstant",
        "getExtensionType",
        "getOperand",
        "getMemIntrinsicNode",
        "SDLoc",
        "getXLenVT",
        "ReplaceAllUsesWith",
        "getMachineMemOperand",
        "hasPredecessorHelper"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performMemPairCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "TargetLowering::DAGCombinerInfo",
          "name": "&DCI"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = DCI.DAG;\n  MachineFunction &MF = DAG.getMachineFunction();\n  const RISCVSubtarget &Subtarget = MF.getSubtarget<RISCVSubtarget>();\n\n  // Target does not support load/store pair.\n  if (!Subtarget.hasVendorXTHeadMemPair())\n    return SDValue();\n\n  LSBaseSDNode *LSNode1 = cast<LSBaseSDNode>(N);\n  EVT MemVT = LSNode1->getMemoryVT();\n  unsigned OpNum = LSNode1->getOpcode() == ISD::LOAD ? 1 : 2;\n\n  // No volatile, indexed or atomic loads/stores.\n  if (!LSNode1->isSimple() || LSNode1->isIndexed())\n    return SDValue();\n\n  // Function to get a base + constant representation from a memory value.\n  auto ExtractBaseAndOffset = [](SDValue Ptr) -> std::pair<SDValue, uint64_t> {\n    if (Ptr->getOpcode() == ISD::ADD)\n      if (auto *C1 = dyn_cast<ConstantSDNode>(Ptr->getOperand(1)))\n        return {Ptr->getOperand(0), C1->getZExtValue()};\n    return {Ptr, 0};\n  };\n\n  auto [Base1, Offset1] = ExtractBaseAndOffset(LSNode1->getOperand(OpNum));\n\n  SDValue Chain = N->getOperand(0);\n  for (SDNode::use_iterator UI = Chain->use_begin(), UE = Chain->use_end();\n       UI != UE; ++UI) {\n    SDUse &Use = UI.getUse();\n    if (Use.getUser() != N && Use.getResNo() == 0 &&\n        Use.getUser()->getOpcode() == N->getOpcode()) {\n      LSBaseSDNode *LSNode2 = cast<LSBaseSDNode>(Use.getUser());\n\n      // No volatile, indexed or atomic loads/stores.\n      if (!LSNode2->isSimple() || LSNode2->isIndexed())\n        continue;\n\n      // Check if LSNode1 and LSNode2 have the same type and extension.\n      if (LSNode1->getOpcode() == ISD::LOAD)\n        if (cast<LoadSDNode>(LSNode2)->getExtensionType() !=\n            cast<LoadSDNode>(LSNode1)->getExtensionType())\n          continue;\n\n      if (LSNode1->getMemoryVT() != LSNode2->getMemoryVT())\n        continue;\n\n      auto [Base2, Offset2] = ExtractBaseAndOffset(LSNode2->getOperand(OpNum));\n\n      // Check if the base pointer is the same for both instruction.\n      if (Base1 != Base2)\n        continue;\n\n      // Check if the offsets match the XTHeadMemPair encoding contraints.\n      bool Valid = false;\n      if (MemVT == MVT::i32) {\n        // Check for adjacent i32 values and a 2-bit index.\n        if ((Offset1 + 4 == Offset2) && isShiftedUInt<2, 3>(Offset1))\n          Valid = true;\n      } else if (MemVT == MVT::i64) {\n        // Check for adjacent i64 values and a 2-bit index.\n        if ((Offset1 + 8 == Offset2) && isShiftedUInt<2, 4>(Offset1))\n          Valid = true;\n      }\n\n      if (!Valid)\n        continue;\n\n      // Try to combine.\n      if (SDValue Res =\n              tryMemPairCombine(DAG, LSNode1, LSNode2, Base1, Offset1))\n        return Res;\n    }\n  }\n\n  return SDValue();\n}",
      "start_line": 13754,
      "end_line": 13832,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getZExtValue",
        "SDValue",
        "getMachineFunction",
        "getMemoryVT",
        "getOpcode",
        "getResNo",
        "use_end",
        "getExtensionType",
        "ExtractBaseAndOffset",
        "isIndexed",
        "getOperand",
        "getUse",
        "getUser"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "MemVT ==",
          "name": "MVT::i64"
        }
      ],
      "body": "{\n        // Check for adjacent i64 values and a 2-bit index.\n        if ((Offset1 + 8 == Offset2) && isShiftedUInt<2, 4>(Offset1))\n          Valid = true;\n      }",
      "start_line": 13815,
      "end_line": 13819,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performFP_TO_INTCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "TargetLowering::DAGCombinerInfo",
          "name": "&DCI"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = DCI.DAG;\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  SDValue Src = N->getOperand(0);\n\n  // Don't do this for strict-fp Src.\n  if (Src->isStrictFPOpcode() || Src->isTargetStrictFPOpcode())\n    return SDValue();\n\n  // Ensure the FP type is legal.\n  if (!TLI.isTypeLegal(Src.getValueType()))\n    return SDValue();\n\n  // Don't do this for f16 with Zfhmin and not Zfh.\n  if (Src.getValueType() == MVT::f16 && !Subtarget.hasStdExtZfh())\n    return SDValue();\n\n  RISCVFPRndMode::RoundingMode FRM = matchRoundingOp(Src.getOpcode());\n  // If the result is invalid, we didn't find a foldable instruction.\n  if (FRM == RISCVFPRndMode::Invalid)\n    return SDValue();\n\n  SDLoc DL(N);\n  bool IsSigned = N->getOpcode() == ISD::FP_TO_SINT;\n  EVT VT = N->getValueType(0);\n\n  if (VT.isVector() && TLI.isTypeLegal(VT)) {\n    MVT SrcVT = Src.getSimpleValueType();\n    MVT SrcContainerVT = SrcVT;\n    MVT ContainerVT = VT.getSimpleVT();\n    SDValue XVal = Src.getOperand(0);\n\n    // For widening and narrowing conversions we just combine it into a\n    // VFCVT_..._VL node, as there are no specific VFWCVT/VFNCVT VL nodes. They\n    // end up getting lowered to their appropriate pseudo instructions based on\n    // their operand types\n    if (VT.getScalarSizeInBits() > SrcVT.getScalarSizeInBits() * 2 ||\n        VT.getScalarSizeInBits() * 2 < SrcVT.getScalarSizeInBits())\n      return SDValue();\n\n    // Make fixed-length vectors scalable first\n    if (SrcVT.isFixedLengthVector()) {\n      SrcContainerVT = getContainerForFixedLengthVector(DAG, SrcVT, Subtarget);\n      XVal = convertToScalableVector(SrcContainerVT, XVal, DAG, Subtarget);\n      ContainerVT =\n          getContainerForFixedLengthVector(DAG, ContainerVT, Subtarget);\n    }\n\n    auto [Mask, VL] =\n        getDefaultVLOps(SrcVT, SrcContainerVT, DL, DAG, Subtarget);\n\n    SDValue FpToInt;\n    if (FRM == RISCVFPRndMode::RTZ) {\n      // Use the dedicated trunc static rounding mode if we're truncating so we\n      // don't need to generate calls to fsrmi/fsrm\n      unsigned Opc =\n          IsSigned ? RISCVISD::VFCVT_RTZ_X_F_VL : RISCVISD::VFCVT_RTZ_XU_F_VL;\n      FpToInt = DAG.getNode(Opc, DL, ContainerVT, XVal, Mask, VL);\n    } else if (FRM == RISCVFPRndMode::DYN) {\n      unsigned Opc =\n          IsSigned ? RISCVISD::VFCVT_X_F_VL : RISCVISD::VFCVT_XU_F_VL;\n      FpToInt = DAG.getNode(Opc, DL, ContainerVT, XVal, Mask, VL);\n    } else {\n      unsigned Opc =\n          IsSigned ? RISCVISD::VFCVT_RM_X_F_VL : RISCVISD::VFCVT_RM_XU_F_VL;\n      FpToInt = DAG.getNode(Opc, DL, ContainerVT, XVal, Mask,\n                            DAG.getTargetConstant(FRM, DL, XLenVT), VL);\n    }\n\n    // If converted from fixed-length to scalable, convert back\n    if (VT.isFixedLengthVector())\n      FpToInt = convertFromScalableVector(VT, FpToInt, DAG, Subtarget);\n\n    return FpToInt;\n  }\n\n  // Only handle XLen or i32 types. Other types narrower than XLen will\n  // eventually be legalized to XLenVT.\n  if (VT != MVT::i32 && VT != XLenVT)\n    return SDValue();\n\n  unsigned Opc;\n  if (VT == XLenVT)\n    Opc = IsSigned ? RISCVISD::FCVT_X : RISCVISD::FCVT_XU;\n  else\n    Opc = IsSigned ? RISCVISD::FCVT_W_RV64 : RISCVISD::FCVT_WU_RV64;\n\n  SDValue FpToInt = DAG.getNode(Opc, DL, XLenVT, Src.getOperand(0),\n                                DAG.getTargetConstant(FRM, DL, XLenVT));\n  return DAG.getNode(ISD::TRUNCATE, DL, VT, FpToInt);\n}",
      "start_line": 13841,
      "end_line": 13935,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getDefaultVLOps",
        "convertToScalableVector",
        "getSimpleValueType",
        "convertFromScalableVector",
        "getScalarSizeInBits",
        "getSimpleVT",
        "getTargetLoweringInfo",
        "SDValue",
        "matchRoundingOp",
        "isTypeLegal",
        "isTargetStrictFPOpcode",
        "getOpcode",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getOperand",
        "hasStdExtZfh",
        "DL",
        "getXLenVT",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "FRM ==",
          "name": "RISCVFPRndMode::DYN"
        }
      ],
      "body": "{\n      unsigned Opc =\n          IsSigned ? RISCVISD::VFCVT_X_F_VL : RISCVISD::VFCVT_XU_F_VL;\n      FpToInt = DAG.getNode(Opc, DL, ContainerVT, XVal, Mask, VL);\n    }",
      "start_line": 13903,
      "end_line": 13907,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performFP_TO_INT_SATCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "TargetLowering::DAGCombinerInfo",
          "name": "&DCI"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = DCI.DAG;\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  // Only handle XLen types. Other types narrower than XLen will eventually be\n  // legalized to XLenVT.\n  EVT DstVT = N->getValueType(0);\n  if (DstVT != XLenVT)\n    return SDValue();\n\n  SDValue Src = N->getOperand(0);\n\n  // Don't do this for strict-fp Src.\n  if (Src->isStrictFPOpcode() || Src->isTargetStrictFPOpcode())\n    return SDValue();\n\n  // Ensure the FP type is also legal.\n  if (!TLI.isTypeLegal(Src.getValueType()))\n    return SDValue();\n\n  // Don't do this for f16 with Zfhmin and not Zfh.\n  if (Src.getValueType() == MVT::f16 && !Subtarget.hasStdExtZfh())\n    return SDValue();\n\n  EVT SatVT = cast<VTSDNode>(N->getOperand(1))->getVT();\n\n  RISCVFPRndMode::RoundingMode FRM = matchRoundingOp(Src.getOpcode());\n  if (FRM == RISCVFPRndMode::Invalid)\n    return SDValue();\n\n  bool IsSigned = N->getOpcode() == ISD::FP_TO_SINT_SAT;\n\n  unsigned Opc;\n  if (SatVT == DstVT)\n    Opc = IsSigned ? RISCVISD::FCVT_X : RISCVISD::FCVT_XU;\n  else if (DstVT == MVT::i64 && SatVT == MVT::i32)\n    Opc = IsSigned ? RISCVISD::FCVT_W_RV64 : RISCVISD::FCVT_WU_RV64;\n  else\n    return SDValue();\n  // FIXME: Support other SatVTs by clamping before or after the conversion.\n\n  Src = Src.getOperand(0);\n\n  SDLoc DL(N);\n  SDValue FpToInt = DAG.getNode(Opc, DL, XLenVT, Src,\n                                DAG.getTargetConstant(FRM, DL, XLenVT));\n\n  // fcvt.wu.* sign extends bit 31 on RV64. FP_TO_UINT_SAT expects to zero\n  // extend.\n  if (Opc == RISCVISD::FCVT_WU_RV64)\n    FpToInt = DAG.getZeroExtendInReg(FpToInt, DL, MVT::i32);\n\n  // RISC-V FP-to-int conversions saturate to the destination register size, but\n  // don't produce 0 for nan.\n  SDValue ZeroInt = DAG.getConstant(0, DL, DstVT);\n  return DAG.getSelectCC(DL, Src, Src, ZeroInt, FpToInt, ISD::CondCode::SETUO);\n}",
      "start_line": 13944,
      "end_line": 14003,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasStdExtZfh",
        "matchRoundingOp",
        "isTargetStrictFPOpcode",
        "getOpcode",
        "getConstant",
        "getZeroExtendInReg",
        "getValueType",
        "getVT",
        "getSelectCC",
        "getOperand",
        "DL",
        "getXLenVT",
        "getTargetLoweringInfo",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performBITREVERSECombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert(Subtarget.hasStdExtZbkb() && \"Unexpected extension\");\n\n  SDValue Src = N->getOperand(0);\n  if (Src.getOpcode() != ISD::BSWAP)\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n  if (!VT.isScalarInteger() || VT.getSizeInBits() >= Subtarget.getXLen() ||\n      !llvm::has_single_bit<uint32_t>(VT.getSizeInBits()))\n    return SDValue();\n\n  SDLoc DL(N);\n  return DAG.getNode(RISCVISD::BREV8, DL, VT, Src.getOperand(0));\n}",
      "start_line": 14007,
      "end_line": 14022,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getValueType",
        "getOperand",
        "getSizeInBits",
        "getXLen",
        "DL",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "negateFMAOpcode",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Opcode"
        },
        {
          "type": "bool",
          "name": "NegMul"
        },
        {
          "type": "bool",
          "name": "NegAcc"
        }
      ],
      "body": "{\n  // Negating the multiply result changes ADD<->SUB and toggles 'N'.\n  if (NegMul) {\n    // clang-format off\n    switch (Opcode) {\n    default: llvm_unreachable(\"Unexpected opcode\");\n    case RISCVISD::VFMADD_VL:  Opcode = RISCVISD::VFNMSUB_VL; break;\n    case RISCVISD::VFNMSUB_VL: Opcode = RISCVISD::VFMADD_VL;  break;\n    case RISCVISD::VFNMADD_VL: Opcode = RISCVISD::VFMSUB_VL;  break;\n    case RISCVISD::VFMSUB_VL:  Opcode = RISCVISD::VFNMADD_VL; break;\n    case RISCVISD::STRICT_VFMADD_VL:  Opcode = RISCVISD::STRICT_VFNMSUB_VL; break;\n    case RISCVISD::STRICT_VFNMSUB_VL: Opcode = RISCVISD::STRICT_VFMADD_VL;  break;\n    case RISCVISD::STRICT_VFNMADD_VL: Opcode = RISCVISD::STRICT_VFMSUB_VL;  break;\n    case RISCVISD::STRICT_VFMSUB_VL:  Opcode = RISCVISD::STRICT_VFNMADD_VL; break;\n    }\n    // clang-format on\n  }\n\n  // Negating the accumulator changes ADD<->SUB.\n  if (NegAcc) {\n    // clang-format off\n    switch (Opcode) {\n    default: llvm_unreachable(\"Unexpected opcode\");\n    case RISCVISD::VFMADD_VL:  Opcode = RISCVISD::VFMSUB_VL;  break;\n    case RISCVISD::VFMSUB_VL:  Opcode = RISCVISD::VFMADD_VL;  break;\n    case RISCVISD::VFNMADD_VL: Opcode = RISCVISD::VFNMSUB_VL; break;\n    case RISCVISD::VFNMSUB_VL: Opcode = RISCVISD::VFNMADD_VL; break;\n    case RISCVISD::STRICT_VFMADD_VL:  Opcode = RISCVISD::STRICT_VFMSUB_VL;  break;\n    case RISCVISD::STRICT_VFMSUB_VL:  Opcode = RISCVISD::STRICT_VFMADD_VL;  break;\n    case RISCVISD::STRICT_VFNMADD_VL: Opcode = RISCVISD::STRICT_VFNMSUB_VL; break;\n    case RISCVISD::STRICT_VFNMSUB_VL: Opcode = RISCVISD::STRICT_VFNMADD_VL; break;\n    }\n    // clang-format on\n  }\n\n  return Opcode;\n}",
      "start_line": 14027,
      "end_line": 14063,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "negateFMAOpcode",
          "condition": "Opcode",
          "cases": [
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "negateFMAOpcode",
          "condition": "Opcode",
          "cases": [
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "switch",
      "return_type": "off",
      "parameters": [
        {
          "type": "Opcode",
          "name": ""
        }
      ],
      "body": "{\n    default: llvm_unreachable(\"Unexpected opcode\");\n    case RISCVISD::VFMADD_VL:  Opcode = RISCVISD::VFNMSUB_VL; break;\n    case RISCVISD::VFNMSUB_VL: Opcode = RISCVISD::VFMADD_VL;  break;\n    case RISCVISD::VFNMADD_VL: Opcode = RISCVISD::VFMSUB_VL;  break;\n    case RISCVISD::VFMSUB_VL:  Opcode = RISCVISD::VFNMADD_VL; break;\n    case RISCVISD::STRICT_VFMADD_VL:  Opcode = RISCVISD::STRICT_VFNMSUB_VL; break;\n    case RISCVISD::STRICT_VFNMSUB_VL: Opcode = RISCVISD::STRICT_VFMADD_VL;  break;\n    case RISCVISD::STRICT_VFNMADD_VL: Opcode = RISCVISD::STRICT_VFMSUB_VL;  break;\n    case RISCVISD::STRICT_VFMSUB_VL:  Opcode = RISCVISD::STRICT_VFNMADD_VL; break;\n    }",
      "start_line": 14030,
      "end_line": 14041,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "switch",
      "return_type": "off",
      "parameters": [
        {
          "type": "Opcode",
          "name": ""
        }
      ],
      "body": "{\n    default: llvm_unreachable(\"Unexpected opcode\");\n    case RISCVISD::VFMADD_VL:  Opcode = RISCVISD::VFMSUB_VL;  break;\n    case RISCVISD::VFMSUB_VL:  Opcode = RISCVISD::VFMADD_VL;  break;\n    case RISCVISD::VFNMADD_VL: Opcode = RISCVISD::VFNMSUB_VL; break;\n    case RISCVISD::VFNMSUB_VL: Opcode = RISCVISD::VFNMADD_VL; break;\n    case RISCVISD::STRICT_VFMADD_VL:  Opcode = RISCVISD::STRICT_VFMSUB_VL;  break;\n    case RISCVISD::STRICT_VFMSUB_VL:  Opcode = RISCVISD::STRICT_VFMADD_VL;  break;\n    case RISCVISD::STRICT_VFNMADD_VL: Opcode = RISCVISD::STRICT_VFNMSUB_VL; break;\n    case RISCVISD::STRICT_VFNMSUB_VL: Opcode = RISCVISD::STRICT_VFNMADD_VL; break;\n    }",
      "start_line": 14047,
      "end_line": 14058,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineVFMADD_VLWithVFNEG_VL",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  // Fold FNEG_VL into FMA opcodes.\n  // The first operand of strict-fp is chain.\n  unsigned Offset = N->isTargetStrictFPOpcode();\n  SDValue A = N->getOperand(0 + Offset);\n  SDValue B = N->getOperand(1 + Offset);\n  SDValue C = N->getOperand(2 + Offset);\n  SDValue Mask = N->getOperand(3 + Offset);\n  SDValue VL = N->getOperand(4 + Offset);\n\n  auto invertIfNegative = [&Mask, &VL](SDValue &V) {\n    if (V.getOpcode() == RISCVISD::FNEG_VL && V.getOperand(1) == Mask &&\n        V.getOperand(2) == VL) {\n      // Return the negated input.\n      V = V.getOperand(0);\n      return true;\n    }\n\n    return false;\n  };\n\n  bool NegA = invertIfNegative(A);\n  bool NegB = invertIfNegative(B);\n  bool NegC = invertIfNegative(C);\n\n  // If no operands are negated, we're done.\n  if (!NegA && !NegB && !NegC)\n    return SDValue();\n\n  unsigned NewOpcode = negateFMAOpcode(N->getOpcode(), NegA != NegB, NegC);\n  if (N->isTargetStrictFPOpcode())\n    return DAG.getNode(NewOpcode, SDLoc(N), N->getVTList(),\n                       {N->getOperand(0), A, B, C, Mask, VL});\n  return DAG.getNode(NewOpcode, SDLoc(N), N->getValueType(0), A, B, C, Mask,\n                     VL);\n}",
      "start_line": 14065,
      "end_line": 14100,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "negateFMAOpcode",
        "SDValue",
        "getVTList",
        "isTargetStrictFPOpcode",
        "getNode",
        "getValueType",
        "getOperand",
        "invertIfNegative"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performVFMADD_VLCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (SDValue V = combineVFMADD_VLWithVFNEG_VL(N, DAG))\n    return V;\n\n  if (N->getValueType(0).isScalableVector() &&\n      N->getValueType(0).getVectorElementType() == MVT::f32 &&\n      (Subtarget.hasVInstructionsF16Minimal() &&\n       !Subtarget.hasVInstructionsF16())) {\n    return SDValue();\n  }\n\n  // FIXME: Ignore strict opcodes for now.\n  if (N->isTargetStrictFPOpcode())\n    return SDValue();\n\n  // Try to form widening FMA.\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  SDValue Mask = N->getOperand(3);\n  SDValue VL = N->getOperand(4);\n\n  if (Op0.getOpcode() != RISCVISD::FP_EXTEND_VL ||\n      Op1.getOpcode() != RISCVISD::FP_EXTEND_VL)\n    return SDValue();\n\n  // TODO: Refactor to handle more complex cases similar to\n  // combineBinOp_VLToVWBinOp_VL.\n  if ((!Op0.hasOneUse() || !Op1.hasOneUse()) &&\n      (Op0 != Op1 || !Op0->hasNUsesOfValue(2, 0)))\n    return SDValue();\n\n  // Check the mask and VL are the same.\n  if (Op0.getOperand(1) != Mask || Op0.getOperand(2) != VL ||\n      Op1.getOperand(1) != Mask || Op1.getOperand(2) != VL)\n    return SDValue();\n\n  unsigned NewOpc;\n  switch (N->getOpcode()) {\n  default:\n    llvm_unreachable(\"Unexpected opcode\");\n  case RISCVISD::VFMADD_VL:\n    NewOpc = RISCVISD::VFWMADD_VL;\n    break;\n  case RISCVISD::VFNMSUB_VL:\n    NewOpc = RISCVISD::VFWNMSUB_VL;\n    break;\n  case RISCVISD::VFNMADD_VL:\n    NewOpc = RISCVISD::VFWNMADD_VL;\n    break;\n  case RISCVISD::VFMSUB_VL:\n    NewOpc = RISCVISD::VFWMSUB_VL;\n    break;\n  }\n\n  Op0 = Op0.getOperand(0);\n  Op1 = Op1.getOperand(0);\n\n  return DAG.getNode(NewOpc, SDLoc(N), N->getValueType(0), Op0, Op1,\n                     N->getOperand(2), Mask, VL);\n}",
      "start_line": 14102,
      "end_line": 14162,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "hasNUsesOfValue",
        "getOpcode",
        "getValueType",
        "getOperand",
        "hasVInstructionsF16Minimal",
        "getVectorElementType",
        "hasVInstructionsF16",
        "llvm_unreachable",
        "isScalableVector",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performVFMUL_VLCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (N->getValueType(0).isScalableVector() &&\n      N->getValueType(0).getVectorElementType() == MVT::f32 &&\n      (Subtarget.hasVInstructionsF16Minimal() &&\n       !Subtarget.hasVInstructionsF16())) {\n    return SDValue();\n  }\n\n  // FIXME: Ignore strict opcodes for now.\n  assert(!N->isTargetStrictFPOpcode() && \"Unexpected opcode\");\n\n  // Try to form widening multiply.\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  SDValue Merge = N->getOperand(2);\n  SDValue Mask = N->getOperand(3);\n  SDValue VL = N->getOperand(4);\n\n  if (Op0.getOpcode() != RISCVISD::FP_EXTEND_VL ||\n      Op1.getOpcode() != RISCVISD::FP_EXTEND_VL)\n    return SDValue();\n\n  // TODO: Refactor to handle more complex cases similar to\n  // combineBinOp_VLToVWBinOp_VL.\n  if ((!Op0.hasOneUse() || !Op1.hasOneUse()) &&\n      (Op0 != Op1 || !Op0->hasNUsesOfValue(2, 0)))\n    return SDValue();\n\n  // Check the mask and VL are the same.\n  if (Op0.getOperand(1) != Mask || Op0.getOperand(2) != VL ||\n      Op1.getOperand(1) != Mask || Op1.getOperand(2) != VL)\n    return SDValue();\n\n  Op0 = Op0.getOperand(0);\n  Op1 = Op1.getOperand(0);\n\n  return DAG.getNode(RISCVISD::VFWMUL_VL, SDLoc(N), N->getValueType(0), Op0,\n                     Op1, Merge, Mask, VL);\n}",
      "start_line": 14164,
      "end_line": 14203,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "hasNUsesOfValue",
        "getOpcode",
        "getValueType",
        "getOperand",
        "hasVInstructionsF16Minimal",
        "getVectorElementType",
        "hasVInstructionsF16",
        "isScalableVector",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performFADDSUB_VLCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (N->getValueType(0).isScalableVector() &&\n      N->getValueType(0).getVectorElementType() == MVT::f32 &&\n      (Subtarget.hasVInstructionsF16Minimal() &&\n       !Subtarget.hasVInstructionsF16())) {\n    return SDValue();\n  }\n\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  SDValue Merge = N->getOperand(2);\n  SDValue Mask = N->getOperand(3);\n  SDValue VL = N->getOperand(4);\n\n  bool IsAdd = N->getOpcode() == RISCVISD::FADD_VL;\n\n  // Look for foldable FP_EXTENDS.\n  bool Op0IsExtend =\n      Op0.getOpcode() == RISCVISD::FP_EXTEND_VL &&\n      (Op0.hasOneUse() || (Op0 == Op1 && Op0->hasNUsesOfValue(2, 0)));\n  bool Op1IsExtend =\n      (Op0 == Op1 && Op0IsExtend) ||\n      (Op1.getOpcode() == RISCVISD::FP_EXTEND_VL && Op1.hasOneUse());\n\n  // Check the mask and VL.\n  if (Op0IsExtend && (Op0.getOperand(1) != Mask || Op0.getOperand(2) != VL))\n    Op0IsExtend = false;\n  if (Op1IsExtend && (Op1.getOperand(1) != Mask || Op1.getOperand(2) != VL))\n    Op1IsExtend = false;\n\n  // Canonicalize.\n  if (!Op1IsExtend) {\n    // Sub requires at least operand 1 to be an extend.\n    if (!IsAdd)\n      return SDValue();\n\n    // Add is commutable, if the other operand is foldable, swap them.\n    if (!Op0IsExtend)\n      return SDValue();\n\n    std::swap(Op0, Op1);\n    std::swap(Op0IsExtend, Op1IsExtend);\n  }\n\n  // Op1 is a foldable extend. Op0 might be foldable.\n  Op1 = Op1.getOperand(0);\n  if (Op0IsExtend)\n    Op0 = Op0.getOperand(0);\n\n  unsigned Opc;\n  if (IsAdd)\n    Opc = Op0IsExtend ? RISCVISD::VFWADD_VL : RISCVISD::VFWADD_W_VL;\n  else\n    Opc = Op0IsExtend ? RISCVISD::VFWSUB_VL : RISCVISD::VFWSUB_W_VL;\n\n  return DAG.getNode(Opc, SDLoc(N), N->getValueType(0), Op0, Op1, Merge, Mask,\n                     VL);\n}",
      "start_line": 14205,
      "end_line": 14263,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "swap",
        "hasNUsesOfValue",
        "getOpcode",
        "getValueType",
        "getOperand",
        "hasVInstructionsF16Minimal",
        "getVectorElementType",
        "hasVInstructionsF16",
        "isScalableVector",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performSRACombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  assert(N->getOpcode() == ISD::SRA && \"Unexpected opcode\");\n\n  if (N->getValueType(0) != MVT::i64 || !Subtarget.is64Bit())\n    return SDValue();\n\n  if (!isa<ConstantSDNode>(N->getOperand(1)))\n    return SDValue();\n  uint64_t ShAmt = N->getConstantOperandVal(1);\n  if (ShAmt > 32)\n    return SDValue();\n\n  SDValue N0 = N->getOperand(0);\n\n  // Combine (sra (sext_inreg (shl X, C1), i32), C2) ->\n  // (sra (shl X, C1+32), C2+32) so it gets selected as SLLI+SRAI instead of\n  // SLLIW+SRAIW. SLLI+SRAI have compressed forms.\n  if (ShAmt < 32 &&\n      N0.getOpcode() == ISD::SIGN_EXTEND_INREG && N0.hasOneUse() &&\n      cast<VTSDNode>(N0.getOperand(1))->getVT() == MVT::i32 &&\n      N0.getOperand(0).getOpcode() == ISD::SHL && N0.getOperand(0).hasOneUse() &&\n      isa<ConstantSDNode>(N0.getOperand(0).getOperand(1))) {\n    uint64_t LShAmt = N0.getOperand(0).getConstantOperandVal(1);\n    if (LShAmt < 32) {\n      SDLoc ShlDL(N0.getOperand(0));\n      SDValue Shl = DAG.getNode(ISD::SHL, ShlDL, MVT::i64,\n                                N0.getOperand(0).getOperand(0),\n                                DAG.getConstant(LShAmt + 32, ShlDL, MVT::i64));\n      SDLoc DL(N);\n      return DAG.getNode(ISD::SRA, DL, MVT::i64, Shl,\n                         DAG.getConstant(ShAmt + 32, DL, MVT::i64));\n    }\n  }\n\n  // Combine (sra (shl X, 32), 32 - C) -> (shl (sext_inreg X, i32), C)\n  // FIXME: Should this be a generic combine? There's a similar combine on X86.\n  //\n  // Also try these folds where an add or sub is in the middle.\n  // (sra (add (shl X, 32), C1), 32 - C) -> (shl (sext_inreg (add X, C1), C)\n  // (sra (sub C1, (shl X, 32)), 32 - C) -> (shl (sext_inreg (sub C1, X), C)\n  SDValue Shl;\n  ConstantSDNode *AddC = nullptr;\n\n  // We might have an ADD or SUB between the SRA and SHL.\n  bool IsAdd = N0.getOpcode() == ISD::ADD;\n  if ((IsAdd || N0.getOpcode() == ISD::SUB)) {\n    // Other operand needs to be a constant we can modify.\n    AddC = dyn_cast<ConstantSDNode>(N0.getOperand(IsAdd ? 1 : 0));\n    if (!AddC)\n      return SDValue();\n\n    // AddC needs to have at least 32 trailing zeros.\n    if (AddC->getAPIntValue().countr_zero() < 32)\n      return SDValue();\n\n    // All users should be a shift by constant less than or equal to 32. This\n    // ensures we'll do this optimization for each of them to produce an\n    // add/sub+sext_inreg they can all share.\n    for (SDNode *U : N0->uses()) {\n      if (U->getOpcode() != ISD::SRA ||\n          !isa<ConstantSDNode>(U->getOperand(1)) ||\n          U->getConstantOperandVal(1) > 32)\n        return SDValue();\n    }\n\n    Shl = N0.getOperand(IsAdd ? 0 : 1);\n  } else {\n    // Not an ADD or SUB.\n    Shl = N0;\n  }\n\n  // Look for a shift left by 32.\n  if (Shl.getOpcode() != ISD::SHL || !isa<ConstantSDNode>(Shl.getOperand(1)) ||\n      Shl.getConstantOperandVal(1) != 32)\n    return SDValue();\n\n  // We if we didn't look through an add/sub, then the shl should have one use.\n  // If we did look through an add/sub, the sext_inreg we create is free so\n  // we're only creating 2 new instructions. It's enough to only remove the\n  // original sra+add/sub.\n  if (!AddC && !Shl.hasOneUse())\n    return SDValue();\n\n  SDLoc DL(N);\n  SDValue In = Shl.getOperand(0);\n\n  // If we looked through an ADD or SUB, we need to rebuild it with the shifted\n  // constant.\n  if (AddC) {\n    SDValue ShiftedAddC =\n        DAG.getConstant(AddC->getAPIntValue().lshr(32), DL, MVT::i64);\n    if (IsAdd)\n      In = DAG.getNode(ISD::ADD, DL, MVT::i64, In, ShiftedAddC);\n    else\n      In = DAG.getNode(ISD::SUB, DL, MVT::i64, ShiftedAddC, In);\n  }\n\n  SDValue SExt = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, In,\n                             DAG.getValueType(MVT::i32));\n  if (ShAmt == 32)\n    return SExt;\n\n  return DAG.getNode(\n      ISD::SHL, DL, MVT::i64, SExt,\n      DAG.getConstant(32 - ShAmt, DL, MVT::i64));\n}",
      "start_line": 14265,
      "end_line": 14371,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "countr_zero",
        "getConstantOperandVal",
        "sra",
        "lshr",
        "getOpcode",
        "Combine",
        "getConstant",
        "getVT",
        "is64Bit",
        "getOperand",
        "ShlDL",
        "DL",
        "getNode",
        "shl"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "tryDemorganOfBooleanCondition",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Cond"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  bool IsAnd = Cond.getOpcode() == ISD::AND;\n  if (!IsAnd && Cond.getOpcode() != ISD::OR)\n    return SDValue();\n\n  if (!Cond.hasOneUse())\n    return SDValue();\n\n  SDValue Setcc = Cond.getOperand(0);\n  SDValue Xor = Cond.getOperand(1);\n  // Canonicalize setcc to LHS.\n  if (Setcc.getOpcode() != ISD::SETCC)\n    std::swap(Setcc, Xor);\n  // LHS should be a setcc and RHS should be an xor.\n  if (Setcc.getOpcode() != ISD::SETCC || !Setcc.hasOneUse() ||\n      Xor.getOpcode() != ISD::XOR || !Xor.hasOneUse())\n    return SDValue();\n\n  // If the condition is an And, SimplifyDemandedBits may have changed\n  // (xor Z, 1) to (not Z).\n  SDValue Xor1 = Xor.getOperand(1);\n  if (!isOneConstant(Xor1) && !(IsAnd && isAllOnesConstant(Xor1)))\n    return SDValue();\n\n  EVT VT = Cond.getValueType();\n  SDValue Xor0 = Xor.getOperand(0);\n\n  // The LHS of the xor needs to be 0/1.\n  APInt Mask = APInt::getBitsSetFrom(VT.getSizeInBits(), 1);\n  if (!DAG.MaskedValueIsZero(Xor0, Mask))\n    return SDValue();\n\n  // We can only invert integer setccs.\n  EVT SetCCOpVT = Setcc.getOperand(0).getValueType();\n  if (!SetCCOpVT.isScalarInteger())\n    return SDValue();\n\n  ISD::CondCode CCVal = cast<CondCodeSDNode>(Setcc.getOperand(2))->get();\n  if (ISD::isIntEqualitySetCC(CCVal)) {\n    CCVal = ISD::getSetCCInverse(CCVal, SetCCOpVT);\n    Setcc = DAG.getSetCC(SDLoc(Setcc), VT, Setcc.getOperand(0),\n                         Setcc.getOperand(1), CCVal);\n  } else if (CCVal == ISD::SETLT && isNullConstant(Setcc.getOperand(0))) {\n    // Invert (setlt 0, X) by converting to (setlt X, 1).\n    Setcc = DAG.getSetCC(SDLoc(Setcc), VT, Setcc.getOperand(1),\n                         DAG.getConstant(1, SDLoc(Setcc), VT), CCVal);\n  } else if (CCVal == ISD::SETLT && isOneConstant(Setcc.getOperand(1))) {\n    // (setlt X, 1) by converting to (setlt 0, X).\n    Setcc = DAG.getSetCC(SDLoc(Setcc), VT,\n                         DAG.getConstant(0, SDLoc(Setcc), VT),\n                         Setcc.getOperand(0), CCVal);\n  } else\n    return SDValue();\n\n  unsigned Opc = IsAnd ? ISD::OR : ISD::AND;\n  return DAG.getNode(Opc, SDLoc(Cond), VT, Setcc, Xor.getOperand(0));\n}",
      "start_line": 14377,
      "end_line": 14433,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "getSetCC",
        "swap",
        "getSetCCInverse",
        "getOpcode",
        "to",
        "isAllOnesConstant",
        "get",
        "getValueType",
        "getConstant",
        "getOperand",
        "getBitsSetFrom",
        "Invert",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combine_CC",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "&LHS"
        },
        {
          "type": "SDValue",
          "name": "&RHS"
        },
        {
          "type": "SDValue",
          "name": "&CC"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  ISD::CondCode CCVal = cast<CondCodeSDNode>(CC)->get();\n\n  // As far as arithmetic right shift always saves the sign,\n  // shift can be omitted.\n  // Fold setlt (sra X, N), 0 -> setlt X, 0 and\n  // setge (sra X, N), 0 -> setge X, 0\n  if (isNullConstant(RHS) && (CCVal == ISD::SETGE || CCVal == ISD::SETLT) &&\n      LHS.getOpcode() == ISD::SRA) {\n    LHS = LHS.getOperand(0);\n    return true;\n  }\n\n  if (!ISD::isIntEqualitySetCC(CCVal))\n    return false;\n\n  // Fold ((setlt X, Y), 0, ne) -> (X, Y, lt)\n  // Sometimes the setcc is introduced after br_cc/select_cc has been formed.\n  if (LHS.getOpcode() == ISD::SETCC && isNullConstant(RHS) &&\n      LHS.getOperand(0).getValueType() == Subtarget.getXLenVT()) {\n    // If we're looking for eq 0 instead of ne 0, we need to invert the\n    // condition.\n    bool Invert = CCVal == ISD::SETEQ;\n    CCVal = cast<CondCodeSDNode>(LHS.getOperand(2))->get();\n    if (Invert)\n      CCVal = ISD::getSetCCInverse(CCVal, LHS.getValueType());\n\n    RHS = LHS.getOperand(1);\n    LHS = LHS.getOperand(0);\n    translateSetCCForBranch(DL, LHS, RHS, CCVal, DAG);\n\n    CC = DAG.getCondCode(CCVal);\n    return true;\n  }\n\n  // Fold ((xor X, Y), 0, eq/ne) -> (X, Y, eq/ne)\n  if (LHS.getOpcode() == ISD::XOR && isNullConstant(RHS)) {\n    RHS = LHS.getOperand(1);\n    LHS = LHS.getOperand(0);\n    return true;\n  }\n\n  // Fold ((srl (and X, 1<<C), C), 0, eq/ne) -> ((shl X, XLen-1-C), 0, ge/lt)\n  if (isNullConstant(RHS) && LHS.getOpcode() == ISD::SRL && LHS.hasOneUse() &&\n      LHS.getOperand(1).getOpcode() == ISD::Constant) {\n    SDValue LHS0 = LHS.getOperand(0);\n    if (LHS0.getOpcode() == ISD::AND &&\n        LHS0.getOperand(1).getOpcode() == ISD::Constant) {\n      uint64_t Mask = LHS0.getConstantOperandVal(1);\n      uint64_t ShAmt = LHS.getConstantOperandVal(1);\n      if (isPowerOf2_64(Mask) && Log2_64(Mask) == ShAmt) {\n        CCVal = CCVal == ISD::SETEQ ? ISD::SETGE : ISD::SETLT;\n        CC = DAG.getCondCode(CCVal);\n\n        ShAmt = LHS.getValueSizeInBits() - 1 - ShAmt;\n        LHS = LHS0.getOperand(0);\n        if (ShAmt != 0)\n          LHS =\n              DAG.getNode(ISD::SHL, DL, LHS.getValueType(), LHS0.getOperand(0),\n                          DAG.getConstant(ShAmt, DL, LHS.getValueType()));\n        return true;\n      }\n    }\n  }\n\n  // (X, 1, setne) -> // (X, 0, seteq) if we can prove X is 0/1.\n  // This can occur when legalizing some floating point comparisons.\n  APInt Mask = APInt::getBitsSetFrom(LHS.getValueSizeInBits(), 1);\n  if (isOneConstant(RHS) && DAG.MaskedValueIsZero(LHS, Mask)) {\n    CCVal = ISD::getSetCCInverse(CCVal, LHS.getValueType());\n    CC = DAG.getCondCode(CCVal);\n    RHS = DAG.getConstant(0, DL, LHS.getValueType());\n    return true;\n  }\n\n  if (isNullConstant(RHS)) {\n    if (SDValue NewCond = tryDemorganOfBooleanCondition(LHS, DAG)) {\n      CCVal = ISD::getSetCCInverse(CCVal, LHS.getValueType());\n      CC = DAG.getCondCode(CCVal);\n      LHS = NewCond;\n      return true;\n    }\n  }\n\n  return false;\n}",
      "start_line": 14436,
      "end_line": 14522,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "MaskedValueIsZero",
        "getSetCCInverse",
        "getValueSizeInBits",
        "getConstant",
        "getConstantOperandVal",
        "getBitsSetFrom",
        "setge",
        "translateSetCCForBranch",
        "getOpcode",
        "getValueType",
        "getOperand",
        "isNullConstant",
        "getCondCode",
        "hasOneUse",
        "Fold",
        "Log2_64",
        "get",
        "setlt",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "tryFoldSelectIntoOp",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SDValue",
          "name": "TrueVal"
        },
        {
          "type": "SDValue",
          "name": "FalseVal"
        },
        {
          "type": "bool",
          "name": "Swapped"
        }
      ],
      "body": "{\n  bool Commutative = true;\n  unsigned Opc = TrueVal.getOpcode();\n  switch (Opc) {\n  default:\n    return SDValue();\n  case ISD::SHL:\n  case ISD::SRA:\n  case ISD::SRL:\n  case ISD::SUB:\n    Commutative = false;\n    break;\n  case ISD::ADD:\n  case ISD::OR:\n  case ISD::XOR:\n    break;\n  }\n\n  if (!TrueVal.hasOneUse() || isa<ConstantSDNode>(FalseVal))\n    return SDValue();\n\n  unsigned OpToFold;\n  if (FalseVal == TrueVal.getOperand(0))\n    OpToFold = 0;\n  else if (Commutative && FalseVal == TrueVal.getOperand(1))\n    OpToFold = 1;\n  else\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n  SDLoc DL(N);\n  SDValue OtherOp = TrueVal.getOperand(1 - OpToFold);\n  EVT OtherOpVT = OtherOp.getValueType();\n  SDValue IdentityOperand =\n      DAG.getNeutralElement(Opc, DL, OtherOpVT, N->getFlags());\n  if (!Commutative)\n    IdentityOperand = DAG.getConstant(0, DL, OtherOpVT);\n  assert(IdentityOperand && \"No identity operand!\");\n\n  if (Swapped)\n    std::swap(OtherOp, IdentityOperand);\n  SDValue NewSel =\n      DAG.getSelect(DL, OtherOpVT, N->getOperand(0), OtherOp, IdentityOperand);\n  return DAG.getNode(TrueVal.getOpcode(), DL, VT, FalseVal, NewSel);\n}",
      "start_line": 14529,
      "end_line": 14575,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "tryFoldSelectIntoOp",
          "condition": "Opc",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "SDValue()"
        }
      ],
      "calls": [
        "SDValue",
        "swap",
        "getSelect",
        "getOpcode",
        "getConstant",
        "getValueType",
        "getOperand",
        "DL",
        "getNeutralElement",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "foldSelectOfCTTZOrCTLZ",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  SDValue Cond = N->getOperand(0);\n\n  // This represents either CTTZ or CTLZ instruction.\n  SDValue CountZeroes;\n\n  SDValue ValOnZero;\n\n  if (Cond.getOpcode() != ISD::SETCC)\n    return SDValue();\n\n  if (!isNullConstant(Cond->getOperand(1)))\n    return SDValue();\n\n  ISD::CondCode CCVal = cast<CondCodeSDNode>(Cond->getOperand(2))->get();\n  if (CCVal == ISD::CondCode::SETEQ) {\n    CountZeroes = N->getOperand(2);\n    ValOnZero = N->getOperand(1);\n  } else if (CCVal == ISD::CondCode::SETNE) {\n    CountZeroes = N->getOperand(1);\n    ValOnZero = N->getOperand(2);\n  } else {\n    return SDValue();\n  }\n\n  if (CountZeroes.getOpcode() == ISD::TRUNCATE ||\n      CountZeroes.getOpcode() == ISD::ZERO_EXTEND)\n    CountZeroes = CountZeroes.getOperand(0);\n\n  if (CountZeroes.getOpcode() != ISD::CTTZ &&\n      CountZeroes.getOpcode() != ISD::CTTZ_ZERO_UNDEF &&\n      CountZeroes.getOpcode() != ISD::CTLZ &&\n      CountZeroes.getOpcode() != ISD::CTLZ_ZERO_UNDEF)\n    return SDValue();\n\n  if (!isNullConstant(ValOnZero))\n    return SDValue();\n\n  SDValue CountZeroesArgument = CountZeroes->getOperand(0);\n  if (Cond->getOperand(0) != CountZeroesArgument)\n    return SDValue();\n\n  if (CountZeroes.getOpcode() == ISD::CTTZ_ZERO_UNDEF) {\n    CountZeroes = DAG.getNode(ISD::CTTZ, SDLoc(CountZeroes),\n                              CountZeroes.getValueType(), CountZeroesArgument);\n  } else if (CountZeroes.getOpcode() == ISD::CTLZ_ZERO_UNDEF) {\n    CountZeroes = DAG.getNode(ISD::CTLZ, SDLoc(CountZeroes),\n                              CountZeroes.getValueType(), CountZeroesArgument);\n  }\n\n  unsigned BitWidth = CountZeroes.getValueSizeInBits();\n  SDValue BitWidthMinusOne =\n      DAG.getConstant(BitWidth - 1, SDLoc(N), CountZeroes.getValueType());\n\n  auto AndNode = DAG.getNode(ISD::AND, SDLoc(N), CountZeroes.getValueType(),\n                             CountZeroes, BitWidthMinusOne);\n  return DAG.getZExtOrTrunc(AndNode, SDLoc(N), N->getValueType(0));\n}",
      "start_line": 14579,
      "end_line": 14636,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "getZExtOrTrunc",
        "getOpcode",
        "getValueSizeInBits",
        "get",
        "getValueType",
        "getConstant",
        "getOperand",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "CCVal ==",
          "name": "ISD::CondCode::SETNE"
        }
      ],
      "body": "{\n    CountZeroes = N->getOperand(1);\n    ValOnZero = N->getOperand(2);\n  }",
      "start_line": 14597,
      "end_line": 14600,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getOperand"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "useInversedSetcc",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  SDValue Cond = N->getOperand(0);\n  SDValue True = N->getOperand(1);\n  SDValue False = N->getOperand(2);\n  SDLoc DL(N);\n  EVT VT = N->getValueType(0);\n  EVT CondVT = Cond.getValueType();\n\n  if (Cond.getOpcode() != ISD::SETCC || !Cond.hasOneUse())\n    return SDValue();\n\n  // Replace (setcc eq (and x, C)) with (setcc ne (and x, C))) to generate\n  // BEXTI, where C is power of 2.\n  if (Subtarget.hasStdExtZbs() && VT.isScalarInteger() &&\n      (Subtarget.hasStdExtZicond() || Subtarget.hasVendorXVentanaCondOps())) {\n    SDValue LHS = Cond.getOperand(0);\n    SDValue RHS = Cond.getOperand(1);\n    ISD::CondCode CC = cast<CondCodeSDNode>(Cond.getOperand(2))->get();\n    if (CC == ISD::SETEQ && LHS.getOpcode() == ISD::AND &&\n        isa<ConstantSDNode>(LHS.getOperand(1)) && isNullConstant(RHS)) {\n      const APInt &MaskVal = LHS.getConstantOperandAPInt(1);\n      if (MaskVal.isPowerOf2() && !MaskVal.isSignedIntN(12))\n        return DAG.getSelect(DL, VT,\n                             DAG.getSetCC(DL, CondVT, LHS, RHS, ISD::SETNE),\n                             False, True);\n    }\n  }\n  return SDValue();\n}",
      "start_line": 14638,
      "end_line": 14667,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getConstantOperandAPInt",
        "SDValue",
        "hasOneUse",
        "isSignedIntN",
        "hasStdExtZicond",
        "Replace",
        "getSelect",
        "isScalarInteger",
        "get",
        "getValueType",
        "getOperand",
        "with",
        "isNullConstant",
        "DL",
        "hasVendorXVentanaCondOps"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performSELECTCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  if (SDValue Folded = foldSelectOfCTTZOrCTLZ(N, DAG))\n    return Folded;\n\n  if (SDValue V = useInversedSetcc(N, DAG, Subtarget))\n    return V;\n\n  if (Subtarget.hasConditionalMoveFusion())\n    return SDValue();\n\n  SDValue TrueVal = N->getOperand(1);\n  SDValue FalseVal = N->getOperand(2);\n  if (SDValue V = tryFoldSelectIntoOp(N, DAG, TrueVal, FalseVal, /*Swapped*/false))\n    return V;\n  return tryFoldSelectIntoOp(N, DAG, FalseVal, TrueVal, /*Swapped*/true);\n}",
      "start_line": 14669,
      "end_line": 14685,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getOperand",
        "tryFoldSelectIntoOp"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performBUILD_VECTORCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "const RISCVTargetLowering",
          "name": "&TLI"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  EVT VT = N->getValueType(0);\n\n  assert(!VT.isScalableVector() && \"unexpected build vector\");\n\n  if (VT.getVectorNumElements() == 1)\n    return SDValue();\n\n  const unsigned Opcode = N->op_begin()->getNode()->getOpcode();\n  if (!TLI.isBinOp(Opcode))\n    return SDValue();\n\n  if (!TLI.isOperationLegalOrCustom(Opcode, VT) || !TLI.isTypeLegal(VT))\n    return SDValue();\n\n  SmallVector<SDValue> LHSOps;\n  SmallVector<SDValue> RHSOps;\n  for (SDValue Op : N->ops()) {\n    if (Op.isUndef()) {\n      // We can't form a divide or remainder from undef.\n      if (!DAG.isSafeToSpeculativelyExecute(Opcode))\n        return SDValue();\n\n      LHSOps.push_back(Op);\n      RHSOps.push_back(Op);\n      continue;\n    }\n\n    // TODO: We can handle operations which have an neutral rhs value\n    // (e.g. x + 0, a * 1 or a << 0), but we then have to keep track\n    // of profit in a more explicit manner.\n    if (Op.getOpcode() != Opcode || !Op.hasOneUse())\n      return SDValue();\n\n    LHSOps.push_back(Op.getOperand(0));\n    if (!isa<ConstantSDNode>(Op.getOperand(1)) &&\n        !isa<ConstantFPSDNode>(Op.getOperand(1)))\n      return SDValue();\n    // FIXME: Return failure if the RHS type doesn't match the LHS. Shifts may\n    // have different LHS and RHS types.\n    if (Op.getOperand(0).getValueType() != Op.getOperand(1).getValueType())\n      return SDValue();\n    RHSOps.push_back(Op.getOperand(1));\n  }\n\n  return DAG.getNode(Opcode, DL, VT, DAG.getBuildVector(VT, DL, LHSOps),\n                     DAG.getBuildVector(VT, DL, RHSOps));\n}",
      "start_line": 14692,
      "end_line": 14742,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "isTypeLegal",
        "op_begin",
        "getOpcode",
        "getBuildVector",
        "getValueType",
        "getOperand",
        "DL",
        "push_back",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performINSERT_VECTOR_ELTCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "const RISCVTargetLowering",
          "name": "&TLI"
        }
      ],
      "body": "{\n  SDValue InVec = N->getOperand(0);\n  SDValue InVal = N->getOperand(1);\n  SDValue EltNo = N->getOperand(2);\n  SDLoc DL(N);\n\n  EVT VT = InVec.getValueType();\n  if (VT.isScalableVector())\n    return SDValue();\n\n  if (!InVec.hasOneUse())\n    return SDValue();\n\n  // Given insert_vector_elt (binop a, VecC), (same_binop b, C2), Elt\n  // move the insert_vector_elts into the arms of the binop.  Note that\n  // the new RHS must be a constant.\n  const unsigned InVecOpcode = InVec->getOpcode();\n  if (InVecOpcode == InVal->getOpcode() && TLI.isBinOp(InVecOpcode) &&\n      InVal.hasOneUse()) {\n    SDValue InVecLHS = InVec->getOperand(0);\n    SDValue InVecRHS = InVec->getOperand(1);\n    SDValue InValLHS = InVal->getOperand(0);\n    SDValue InValRHS = InVal->getOperand(1);\n\n    if (!ISD::isBuildVectorOfConstantSDNodes(InVecRHS.getNode()))\n      return SDValue();\n    if (!isa<ConstantSDNode>(InValRHS) && !isa<ConstantFPSDNode>(InValRHS))\n      return SDValue();\n    // FIXME: Return failure if the RHS type doesn't match the LHS. Shifts may\n    // have different LHS and RHS types.\n    if (InVec.getOperand(0).getValueType() != InVec.getOperand(1).getValueType())\n      return SDValue();\n    SDValue LHS = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT,\n                              InVecLHS, InValLHS, EltNo);\n    SDValue RHS = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT,\n                              InVecRHS, InValRHS, EltNo);\n    return DAG.getNode(InVecOpcode, DL, VT, LHS, RHS);\n  }\n\n  // Given insert_vector_elt (concat_vectors ...), InVal, Elt\n  // move the insert_vector_elt to the source operand of the concat_vector.\n  if (InVec.getOpcode() != ISD::CONCAT_VECTORS)\n    return SDValue();\n\n  auto *IndexC = dyn_cast<ConstantSDNode>(EltNo);\n  if (!IndexC)\n    return SDValue();\n  unsigned Elt = IndexC->getZExtValue();\n\n  EVT ConcatVT = InVec.getOperand(0).getValueType();\n  if (ConcatVT.getVectorElementType() != InVal.getValueType())\n    return SDValue();\n  unsigned ConcatNumElts = ConcatVT.getVectorNumElements();\n  SDValue NewIdx = DAG.getConstant(Elt % ConcatNumElts, DL,\n                                   EltNo.getValueType());\n\n  unsigned ConcatOpIdx = Elt / ConcatNumElts;\n  SDValue ConcatOp = InVec.getOperand(ConcatOpIdx);\n  ConcatOp = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, ConcatVT,\n                         ConcatOp, InVal, NewIdx);\n\n  SmallVector<SDValue> ConcatOps;\n  ConcatOps.append(InVec->op_begin(), InVec->op_end());\n  ConcatOps[ConcatOpIdx] = ConcatOp;\n  return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, ConcatOps);\n}",
      "start_line": 14744,
      "end_line": 14811,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getZExtValue",
        "SDValue",
        "hasOneUse",
        "op_end",
        "getOpcode",
        "getConstant",
        "append",
        "getValueType",
        "getOperand",
        "isBinOp",
        "getVectorNumElements",
        "DL",
        "insert_vector_elt",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performCONCAT_VECTORSCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        },
        {
          "type": "const RISCVTargetLowering",
          "name": "&TLI"
        }
      ],
      "body": "{\n  SDLoc DL(N);\n  EVT VT = N->getValueType(0);\n\n  // Only perform this combine on legal MVTs.\n  if (!TLI.isTypeLegal(VT))\n    return SDValue();\n\n  // TODO: Potentially extend this to scalable vectors\n  if (VT.isScalableVector())\n    return SDValue();\n\n  auto *BaseLd = dyn_cast<LoadSDNode>(N->getOperand(0));\n  if (!BaseLd || !BaseLd->isSimple() || !ISD::isNormalLoad(BaseLd) ||\n      !SDValue(BaseLd, 0).hasOneUse())\n    return SDValue();\n\n  EVT BaseLdVT = BaseLd->getValueType(0);\n\n  // Go through the loads and check that they're strided\n  SmallVector<LoadSDNode *> Lds;\n  Lds.push_back(BaseLd);\n  Align Align = BaseLd->getAlign();\n  for (SDValue Op : N->ops().drop_front()) {\n    auto *Ld = dyn_cast<LoadSDNode>(Op);\n    if (!Ld || !Ld->isSimple() || !Op.hasOneUse() ||\n        Ld->getChain() != BaseLd->getChain() || !ISD::isNormalLoad(Ld) ||\n        Ld->getValueType(0) != BaseLdVT)\n      return SDValue();\n\n    Lds.push_back(Ld);\n\n    // The common alignment is the most restrictive (smallest) of all the loads\n    Align = std::min(Align, Ld->getAlign());\n  }\n\n  using PtrDiff = std::pair<std::variant<int64_t, SDValue>, bool>;\n  auto GetPtrDiff = [&DAG](LoadSDNode *Ld1,\n                           LoadSDNode *Ld2) -> std::optional<PtrDiff> {\n    // If the load ptrs can be decomposed into a common (Base + Index) with a\n    // common constant stride, then return the constant stride.\n    BaseIndexOffset BIO1 = BaseIndexOffset::match(Ld1, DAG);\n    BaseIndexOffset BIO2 = BaseIndexOffset::match(Ld2, DAG);\n    if (BIO1.equalBaseIndex(BIO2, DAG))\n      return {{BIO2.getOffset() - BIO1.getOffset(), false}};\n\n    // Otherwise try to match (add LastPtr, Stride) or (add NextPtr, Stride)\n    SDValue P1 = Ld1->getBasePtr();\n    SDValue P2 = Ld2->getBasePtr();\n    if (P2.getOpcode() == ISD::ADD && P2.getOperand(0) == P1)\n      return {{P2.getOperand(1), false}};\n    if (P1.getOpcode() == ISD::ADD && P1.getOperand(0) == P2)\n      return {{P1.getOperand(1), true}};\n\n    return std::nullopt;\n  };\n\n  // Get the distance between the first and second loads\n  auto BaseDiff = GetPtrDiff(Lds[0], Lds[1]);\n  if (!BaseDiff)\n    return SDValue();\n\n  // Check all the loads are the same distance apart\n  for (auto *It = Lds.begin() + 1; It != Lds.end() - 1; It++)\n    if (GetPtrDiff(*It, *std::next(It)) != BaseDiff)\n      return SDValue();\n\n  // TODO: At this point, we've successfully matched a generalized gather\n  // load.  Maybe we should emit that, and then move the specialized\n  // matchers above and below into a DAG combine?\n\n  // Get the widened scalar type, e.g. v4i8 -> i64\n  unsigned WideScalarBitWidth =\n      BaseLdVT.getScalarSizeInBits() * BaseLdVT.getVectorNumElements();\n  MVT WideScalarVT = MVT::getIntegerVT(WideScalarBitWidth);\n\n  // Get the vector type for the strided load, e.g. 4 x v4i8 -> v4i64\n  MVT WideVecVT = MVT::getVectorVT(WideScalarVT, N->getNumOperands());\n  if (!TLI.isTypeLegal(WideVecVT))\n    return SDValue();\n\n  // Check that the operation is legal\n  if (!TLI.isLegalStridedLoadStore(WideVecVT, Align))\n    return SDValue();\n\n  auto [StrideVariant, MustNegateStride] = *BaseDiff;\n  SDValue Stride = std::holds_alternative<SDValue>(StrideVariant)\n                       ? std::get<SDValue>(StrideVariant)\n                       : DAG.getConstant(std::get<int64_t>(StrideVariant), DL,\n                                         Lds[0]->getOffset().getValueType());\n  if (MustNegateStride)\n    Stride = DAG.getNegative(Stride, DL, Stride.getValueType());\n\n  SDVTList VTs = DAG.getVTList({WideVecVT, MVT::Other});\n  SDValue IntID =\n    DAG.getTargetConstant(Intrinsic::riscv_masked_strided_load, DL,\n                          Subtarget.getXLenVT());\n\n  SDValue AllOneMask =\n    DAG.getSplat(WideVecVT.changeVectorElementType(MVT::i1), DL,\n                 DAG.getConstant(1, DL, MVT::i1));\n\n  SDValue Ops[] = {BaseLd->getChain(),   IntID,  DAG.getUNDEF(WideVecVT),\n                   BaseLd->getBasePtr(), Stride, AllOneMask};\n\n  uint64_t MemSize;\n  if (auto *ConstStride = dyn_cast<ConstantSDNode>(Stride);\n      ConstStride && ConstStride->getSExtValue() >= 0)\n    // total size = (elsize * n) + (stride - elsize) * (n-1)\n    //            = elsize + stride * (n-1)\n    MemSize = WideScalarVT.getSizeInBits() +\n              ConstStride->getSExtValue() * (N->getNumOperands() - 1);\n  else\n    // If Stride isn't constant, then we can't know how much it will load\n    MemSize = MemoryLocation::UnknownSize;\n\n  MachineMemOperand *MMO = DAG.getMachineFunction().getMachineMemOperand(\n      BaseLd->getPointerInfo(), BaseLd->getMemOperand()->getFlags(), MemSize,\n      Align);\n\n  SDValue StridedLoad = DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs,\n                                                Ops, WideVecVT, MMO);\n  for (SDValue Ld : N->ops())\n    DAG.makeEquivalentMemoryOrdering(cast<LoadSDNode>(Ld), StridedLoad);\n\n  return DAG.getBitcast(VT.getSimpleVT(), StridedLoad);\n}",
      "start_line": 14817,
      "end_line": 14945,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOffset",
        "getChain",
        "restrictive",
        "match",
        "getConstant",
        "getSizeInBits",
        "getBitcast",
        "getScalarSizeInBits",
        "getNegative",
        "min",
        "getNumOperands",
        "getMachineFunction",
        "getFlags",
        "end",
        "getUNDEF",
        "getVectorNumElements",
        "getMemIntrinsicNode",
        "getVectorVT",
        "makeEquivalentMemoryOrdering",
        "or",
        "push_back",
        "getAlign",
        "getMachineMemOperand",
        "getSplat",
        "SDValue",
        "getMemOperand",
        "getVTList",
        "getValueType",
        "getOperand",
        "drop_front",
        "common",
        "hasOneUse",
        "getBasePtr",
        "GetPtrDiff",
        "isNormalLoad",
        "DL",
        "getIntegerVT",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "combineToVWMACC",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n\n  assert(N->getOpcode() == RISCVISD::ADD_VL || N->getOpcode() == ISD::ADD);\n\n  if (N->getValueType(0).isFixedLengthVector())\n    return SDValue();\n\n  SDValue Addend = N->getOperand(0);\n  SDValue MulOp = N->getOperand(1);\n\n  if (N->getOpcode() == RISCVISD::ADD_VL) {\n    SDValue AddMergeOp = N->getOperand(2);\n    if (!AddMergeOp.isUndef())\n      return SDValue();\n  }\n\n  auto IsVWMulOpc = [](unsigned Opc) {\n    switch (Opc) {\n    case RISCVISD::VWMUL_VL:\n    case RISCVISD::VWMULU_VL:\n    case RISCVISD::VWMULSU_VL:\n      return true;\n    default:\n      return false;\n    }\n  };\n\n  if (!IsVWMulOpc(MulOp.getOpcode()))\n    std::swap(Addend, MulOp);\n\n  if (!IsVWMulOpc(MulOp.getOpcode()))\n    return SDValue();\n\n  SDValue MulMergeOp = MulOp.getOperand(2);\n\n  if (!MulMergeOp.isUndef())\n    return SDValue();\n\n  auto [AddMask, AddVL] = [](SDNode *N, SelectionDAG &DAG,\n                             const RISCVSubtarget &Subtarget) {\n    if (N->getOpcode() == ISD::ADD) {\n      SDLoc DL(N);\n      return getDefaultScalableVLOps(N->getSimpleValueType(0), DL, DAG,\n                                     Subtarget);\n    }\n    return std::make_pair(N->getOperand(3), N->getOperand(4));\n  }(N, DAG, Subtarget);\n\n  SDValue MulMask = MulOp.getOperand(3);\n  SDValue MulVL = MulOp.getOperand(4);\n\n  if (AddMask != MulMask || AddVL != MulVL)\n    return SDValue();\n\n  unsigned Opc = RISCVISD::VWMACC_VL + MulOp.getOpcode() - RISCVISD::VWMUL_VL;\n  static_assert(RISCVISD::VWMACC_VL + 1 == RISCVISD::VWMACCU_VL,\n                \"Unexpected opcode after VWMACC_VL\");\n  static_assert(RISCVISD::VWMACC_VL + 2 == RISCVISD::VWMACCSU_VL,\n                \"Unexpected opcode after VWMACC_VL!\");\n  static_assert(RISCVISD::VWMUL_VL + 1 == RISCVISD::VWMULU_VL,\n                \"Unexpected opcode after VWMUL_VL!\");\n  static_assert(RISCVISD::VWMUL_VL + 2 == RISCVISD::VWMULSU_VL,\n                \"Unexpected opcode after VWMUL_VL!\");\n\n  SDLoc DL(N);\n  EVT VT = N->getValueType(0);\n  SDValue Ops[] = {MulOp.getOperand(0), MulOp.getOperand(1), Addend, AddMask,\n                   AddVL};\n  return DAG.getNode(Opc, DL, VT, Ops);\n}",
      "start_line": 14947,
      "end_line": 15017,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "combineToVWMACC",
          "condition": "Opc",
          "cases": [
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "false"
        }
      ],
      "calls": [
        "SDValue",
        "static_assert",
        "swap",
        "getNode",
        "getOpcode",
        "make_pair",
        "getValueType",
        "getOperand",
        "DL",
        "getDefaultScalableVLOps",
        "isFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "legalizeScatterGatherIndexType",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDLoc",
          "name": "DL"
        },
        {
          "type": "SDValue",
          "name": "&Index"
        },
        {
          "type": "ISD::MemIndexType",
          "name": "&IndexType"
        },
        {
          "type": "RISCVTargetLowering::DAGCombinerInfo",
          "name": "&DCI"
        }
      ],
      "body": "{\n  if (!DCI.isBeforeLegalize())\n    return false;\n\n  SelectionDAG &DAG = DCI.DAG;\n  const MVT XLenVT =\n    DAG.getMachineFunction().getSubtarget<RISCVSubtarget>().getXLenVT();\n\n  const EVT IndexVT = Index.getValueType();\n\n  // RISC-V indexed loads only support the \"unsigned unscaled\" addressing\n  // mode, so anything else must be manually legalized.\n  if (!isIndexTypeSigned(IndexType))\n    return false;\n\n  if (IndexVT.getVectorElementType().bitsLT(XLenVT)) {\n    // Any index legalization should first promote to XLenVT, so we don't lose\n    // bits when scaling. This may create an illegal index type so we let\n    // LLVM's legalization take care of the splitting.\n    // FIXME: LLVM can't split VP_GATHER or VP_SCATTER yet.\n    Index = DAG.getNode(ISD::SIGN_EXTEND, DL,\n                        IndexVT.changeVectorElementType(XLenVT), Index);\n  }\n  IndexType = ISD::UNSIGNED_SCALED;\n  return true;\n}",
      "start_line": 15019,
      "end_line": 15046,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMachineFunction",
        "bitsLT",
        "getValueType",
        "getXLenVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "matchIndexAsShuffle",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "Index"
        },
        {
          "type": "SDValue",
          "name": "Mask"
        },
        {
          "type": "SmallVector<int>",
          "name": "&ShuffleMask"
        }
      ],
      "body": "{\n  if (!ISD::isConstantSplatVectorAllOnes(Mask.getNode()))\n    return false;\n  if (!ISD::isBuildVectorOfConstantSDNodes(Index.getNode()))\n    return false;\n\n  const unsigned ElementSize = VT.getScalarStoreSize();\n  const unsigned NumElems = VT.getVectorNumElements();\n\n  // Create the shuffle mask and check all bits active\n  assert(ShuffleMask.empty());\n  BitVector ActiveLanes(NumElems);\n  for (unsigned i = 0; i < Index->getNumOperands(); i++) {\n    // TODO: We've found an active bit of UB, and could be\n    // more aggressive here if desired.\n    if (Index->getOperand(i)->isUndef())\n      return false;\n    uint64_t C = Index->getConstantOperandVal(i);\n    if (C % ElementSize != 0)\n      return false;\n    C = C / ElementSize;\n    if (C >= NumElems)\n      return false;\n    ShuffleMask.push_back(C);\n    ActiveLanes.set(C);\n  }\n  return ActiveLanes.all();\n}",
      "start_line": 15052,
      "end_line": 15080,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getConstantOperandVal",
        "ActiveLanes",
        "isUndef",
        "getVectorNumElements",
        "set",
        "all",
        "getScalarStoreSize",
        "push_back"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "matchIndexAsWiderOp",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "Index"
        },
        {
          "type": "SDValue",
          "name": "Mask"
        },
        {
          "type": "Align",
          "name": "BaseAlign"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&ST"
        }
      ],
      "body": "{\n  if (!ISD::isConstantSplatVectorAllOnes(Mask.getNode()))\n    return false;\n  if (!ISD::isBuildVectorOfConstantSDNodes(Index.getNode()))\n    return false;\n\n  // Attempt a doubling.  If we can use a element type 4x or 8x in\n  // size, this will happen via multiply iterations of the transform.\n  const unsigned NumElems = VT.getVectorNumElements();\n  if (NumElems % 2 != 0)\n    return false;\n\n  const unsigned ElementSize = VT.getScalarStoreSize();\n  const unsigned WiderElementSize = ElementSize * 2;\n  if (WiderElementSize > ST.getELen()/8)\n    return false;\n\n  if (!ST.hasFastUnalignedAccess() && BaseAlign < WiderElementSize)\n    return false;\n\n  for (unsigned i = 0; i < Index->getNumOperands(); i++) {\n    // TODO: We've found an active bit of UB, and could be\n    // more aggressive here if desired.\n    if (Index->getOperand(i)->isUndef())\n      return false;\n    // TODO: This offset check is too strict if we support fully\n    // misaligned memory operations.\n    uint64_t C = Index->getConstantOperandVal(i);\n    if (i % 2 == 0) {\n      if (C % WiderElementSize != 0)\n        return false;\n      continue;\n    }\n    uint64_t Last = Index->getConstantOperandVal(i-1);\n    if (C != Last + ElementSize)\n      return false;\n  }\n  return true;\n}",
      "start_line": 15087,
      "end_line": 15126,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isUndef",
        "getConstantOperandVal",
        "getVectorNumElements",
        "getScalarStoreSize"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "PerformDAGCombine",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "DAGCombinerInfo",
          "name": "&DCI"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = DCI.DAG;\n  const MVT XLenVT = Subtarget.getXLenVT();\n  SDLoc DL(N);\n\n  // Helper to call SimplifyDemandedBits on an operand of N where only some low\n  // bits are demanded. N will be added to the Worklist if it was not deleted.\n  // Caller should return SDValue(N, 0) if this returns true.\n  auto SimplifyDemandedLowBitsHelper = [&](unsigned OpNo, unsigned LowBits) {\n    SDValue Op = N->getOperand(OpNo);\n    APInt Mask = APInt::getLowBitsSet(Op.getValueSizeInBits(), LowBits);\n    if (!SimplifyDemandedBits(Op, Mask, DCI))\n      return false;\n\n    if (N->getOpcode() != ISD::DELETED_NODE)\n      DCI.AddToWorklist(N);\n    return true;\n  };\n\n  switch (N->getOpcode()) {\n  default:\n    break;\n  case RISCVISD::SplitF64: {\n    SDValue Op0 = N->getOperand(0);\n    // If the input to SplitF64 is just BuildPairF64 then the operation is\n    // redundant. Instead, use BuildPairF64's operands directly.\n    if (Op0->getOpcode() == RISCVISD::BuildPairF64)\n      return DCI.CombineTo(N, Op0.getOperand(0), Op0.getOperand(1));\n\n    if (Op0->isUndef()) {\n      SDValue Lo = DAG.getUNDEF(MVT::i32);\n      SDValue Hi = DAG.getUNDEF(MVT::i32);\n      return DCI.CombineTo(N, Lo, Hi);\n    }\n\n    // It's cheaper to materialise two 32-bit integers than to load a double\n    // from the constant pool and transfer it to integer registers through the\n    // stack.\n    if (ConstantFPSDNode *C = dyn_cast<ConstantFPSDNode>(Op0)) {\n      APInt V = C->getValueAPF().bitcastToAPInt();\n      SDValue Lo = DAG.getConstant(V.trunc(32), DL, MVT::i32);\n      SDValue Hi = DAG.getConstant(V.lshr(32).trunc(32), DL, MVT::i32);\n      return DCI.CombineTo(N, Lo, Hi);\n    }\n\n    // This is a target-specific version of a DAGCombine performed in\n    // DAGCombiner::visitBITCAST. It performs the equivalent of:\n    // fold (bitconvert (fneg x)) -> (xor (bitconvert x), signbit)\n    // fold (bitconvert (fabs x)) -> (and (bitconvert x), (not signbit))\n    if (!(Op0.getOpcode() == ISD::FNEG || Op0.getOpcode() == ISD::FABS) ||\n        !Op0.getNode()->hasOneUse())\n      break;\n    SDValue NewSplitF64 =\n        DAG.getNode(RISCVISD::SplitF64, DL, DAG.getVTList(MVT::i32, MVT::i32),\n                    Op0.getOperand(0));\n    SDValue Lo = NewSplitF64.getValue(0);\n    SDValue Hi = NewSplitF64.getValue(1);\n    APInt SignBit = APInt::getSignMask(32);\n    if (Op0.getOpcode() == ISD::FNEG) {\n      SDValue NewHi = DAG.getNode(ISD::XOR, DL, MVT::i32, Hi,\n                                  DAG.getConstant(SignBit, DL, MVT::i32));\n      return DCI.CombineTo(N, Lo, NewHi);\n    }\n    assert(Op0.getOpcode() == ISD::FABS);\n    SDValue NewHi = DAG.getNode(ISD::AND, DL, MVT::i32, Hi,\n                                DAG.getConstant(~SignBit, DL, MVT::i32));\n    return DCI.CombineTo(N, Lo, NewHi);\n  }\n  case RISCVISD::SLLW:\n  case RISCVISD::SRAW:\n  case RISCVISD::SRLW:\n  case RISCVISD::RORW:\n  case RISCVISD::ROLW: {\n    // Only the lower 32 bits of LHS and lower 5 bits of RHS are read.\n    if (SimplifyDemandedLowBitsHelper(0, 32) ||\n        SimplifyDemandedLowBitsHelper(1, 5))\n      return SDValue(N, 0);\n\n    break;\n  }\n  case RISCVISD::CLZW:\n  case RISCVISD::CTZW: {\n    // Only the lower 32 bits of the first operand are read\n    if (SimplifyDemandedLowBitsHelper(0, 32))\n      return SDValue(N, 0);\n    break;\n  }\n  case RISCVISD::FMV_W_X_RV64: {\n    // If the input to FMV_W_X_RV64 is just FMV_X_ANYEXTW_RV64 the the\n    // conversion is unnecessary and can be replaced with the\n    // FMV_X_ANYEXTW_RV64 operand.\n    SDValue Op0 = N->getOperand(0);\n    if (Op0.getOpcode() == RISCVISD::FMV_X_ANYEXTW_RV64)\n      return Op0.getOperand(0);\n    break;\n  }\n  case RISCVISD::FMV_X_ANYEXTH:\n  case RISCVISD::FMV_X_ANYEXTW_RV64: {\n    SDLoc DL(N);\n    SDValue Op0 = N->getOperand(0);\n    MVT VT = N->getSimpleValueType(0);\n    // If the input to FMV_X_ANYEXTW_RV64 is just FMV_W_X_RV64 then the\n    // conversion is unnecessary and can be replaced with the FMV_W_X_RV64\n    // operand. Similar for FMV_X_ANYEXTH and FMV_H_X.\n    if ((N->getOpcode() == RISCVISD::FMV_X_ANYEXTW_RV64 &&\n         Op0->getOpcode() == RISCVISD::FMV_W_X_RV64) ||\n        (N->getOpcode() == RISCVISD::FMV_X_ANYEXTH &&\n         Op0->getOpcode() == RISCVISD::FMV_H_X)) {\n      assert(Op0.getOperand(0).getValueType() == VT &&\n             \"Unexpected value type!\");\n      return Op0.getOperand(0);\n    }\n\n    // This is a target-specific version of a DAGCombine performed in\n    // DAGCombiner::visitBITCAST. It performs the equivalent of:\n    // fold (bitconvert (fneg x)) -> (xor (bitconvert x), signbit)\n    // fold (bitconvert (fabs x)) -> (and (bitconvert x), (not signbit))\n    if (!(Op0.getOpcode() == ISD::FNEG || Op0.getOpcode() == ISD::FABS) ||\n        !Op0.getNode()->hasOneUse())\n      break;\n    SDValue NewFMV = DAG.getNode(N->getOpcode(), DL, VT, Op0.getOperand(0));\n    unsigned FPBits = N->getOpcode() == RISCVISD::FMV_X_ANYEXTW_RV64 ? 32 : 16;\n    APInt SignBit = APInt::getSignMask(FPBits).sext(VT.getSizeInBits());\n    if (Op0.getOpcode() == ISD::FNEG)\n      return DAG.getNode(ISD::XOR, DL, VT, NewFMV,\n                         DAG.getConstant(SignBit, DL, VT));\n\n    assert(Op0.getOpcode() == ISD::FABS);\n    return DAG.getNode(ISD::AND, DL, VT, NewFMV,\n                       DAG.getConstant(~SignBit, DL, VT));\n  }\n  case ISD::ADD: {\n    if (SDValue V = combineBinOp_VLToVWBinOp_VL(N, DCI, Subtarget))\n      return V;\n    if (SDValue V = combineToVWMACC(N, DAG, Subtarget))\n      return V;\n    return performADDCombine(N, DAG, Subtarget);\n  }\n  case ISD::SUB: {\n    if (SDValue V = combineBinOp_VLToVWBinOp_VL(N, DCI, Subtarget))\n      return V;\n    return performSUBCombine(N, DAG, Subtarget);\n  }\n  case ISD::AND:\n    return performANDCombine(N, DCI, Subtarget);\n  case ISD::OR:\n    return performORCombine(N, DCI, Subtarget);\n  case ISD::XOR:\n    return performXORCombine(N, DAG, Subtarget);\n  case ISD::MUL:\n    if (SDValue V = combineBinOp_VLToVWBinOp_VL(N, DCI, Subtarget))\n      return V;\n    return performMULCombine(N, DAG);\n  case ISD::FADD:\n  case ISD::UMAX:\n  case ISD::UMIN:\n  case ISD::SMAX:\n  case ISD::SMIN:\n  case ISD::FMAXNUM:\n  case ISD::FMINNUM: {\n    if (SDValue V = combineBinOpToReduce(N, DAG, Subtarget))\n      return V;\n    if (SDValue V = combineBinOpOfExtractToReduceTree(N, DAG, Subtarget))\n      return V;\n    return SDValue();\n  }\n  case ISD::SETCC:\n    return performSETCCCombine(N, DAG, Subtarget);\n  case ISD::SIGN_EXTEND_INREG:\n    return performSIGN_EXTEND_INREGCombine(N, DAG, Subtarget);\n  case ISD::ZERO_EXTEND:\n    // Fold (zero_extend (fp_to_uint X)) to prevent forming fcvt+zexti32 during\n    // type legalization. This is safe because fp_to_uint produces poison if\n    // it overflows.\n    if (N->getValueType(0) == MVT::i64 && Subtarget.is64Bit()) {\n      SDValue Src = N->getOperand(0);\n      if (Src.getOpcode() == ISD::FP_TO_UINT &&\n          isTypeLegal(Src.getOperand(0).getValueType()))\n        return DAG.getNode(ISD::FP_TO_UINT, SDLoc(N), MVT::i64,\n                           Src.getOperand(0));\n      if (Src.getOpcode() == ISD::STRICT_FP_TO_UINT && Src.hasOneUse() &&\n          isTypeLegal(Src.getOperand(1).getValueType())) {\n        SDVTList VTs = DAG.getVTList(MVT::i64, MVT::Other);\n        SDValue Res = DAG.getNode(ISD::STRICT_FP_TO_UINT, SDLoc(N), VTs,\n                                  Src.getOperand(0), Src.getOperand(1));\n        DCI.CombineTo(N, Res);\n        DAG.ReplaceAllUsesOfValueWith(Src.getValue(1), Res.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(Src.getNode());\n        return SDValue(N, 0); // Return N so it doesn't get rechecked.\n      }\n    }\n    return SDValue();\n  case RISCVISD::TRUNCATE_VECTOR_VL: {\n    // trunc (sra sext (X), zext (Y)) -> sra (X, smin (Y, scalarsize(Y) - 1))\n    // This would be benefit for the cases where X and Y are both the same value\n    // type of low precision vectors. Since the truncate would be lowered into\n    // n-levels TRUNCATE_VECTOR_VL to satisfy RVV's SEW*2->SEW truncate\n    // restriction, such pattern would be expanded into a series of \"vsetvli\"\n    // and \"vnsrl\" instructions later to reach this point.\n    auto IsTruncNode = [](SDValue V) {\n      if (V.getOpcode() != RISCVISD::TRUNCATE_VECTOR_VL)\n        return false;\n      SDValue VL = V.getOperand(2);\n      auto *C = dyn_cast<ConstantSDNode>(VL);\n      // Assume all TRUNCATE_VECTOR_VL nodes use VLMAX for VMSET_VL operand\n      bool IsVLMAXForVMSET = (C && C->isAllOnes()) ||\n                             (isa<RegisterSDNode>(VL) &&\n                              cast<RegisterSDNode>(VL)->getReg() == RISCV::X0);\n      return V.getOperand(1).getOpcode() == RISCVISD::VMSET_VL &&\n             IsVLMAXForVMSET;\n    };\n\n    SDValue Op = N->getOperand(0);\n\n    // We need to first find the inner level of TRUNCATE_VECTOR_VL node\n    // to distinguish such pattern.\n    while (IsTruncNode(Op)) {\n      if (!Op.hasOneUse())\n        return SDValue();\n      Op = Op.getOperand(0);\n    }\n\n    if (Op.getOpcode() == ISD::SRA && Op.hasOneUse()) {\n      SDValue N0 = Op.getOperand(0);\n      SDValue N1 = Op.getOperand(1);\n      if (N0.getOpcode() == ISD::SIGN_EXTEND && N0.hasOneUse() &&\n          N1.getOpcode() == ISD::ZERO_EXTEND && N1.hasOneUse()) {\n        SDValue N00 = N0.getOperand(0);\n        SDValue N10 = N1.getOperand(0);\n        if (N00.getValueType().isVector() &&\n            N00.getValueType() == N10.getValueType() &&\n            N->getValueType(0) == N10.getValueType()) {\n          unsigned MaxShAmt = N10.getValueType().getScalarSizeInBits() - 1;\n          SDValue SMin = DAG.getNode(\n              ISD::SMIN, SDLoc(N1), N->getValueType(0), N10,\n              DAG.getConstant(MaxShAmt, SDLoc(N1), N->getValueType(0)));\n          return DAG.getNode(ISD::SRA, SDLoc(N), N->getValueType(0), N00, SMin);\n        }\n      }\n    }\n    break;\n  }\n  case ISD::TRUNCATE:\n    return performTRUNCATECombine(N, DAG, Subtarget);\n  case ISD::SELECT:\n    return performSELECTCombine(N, DAG, Subtarget);\n  case RISCVISD::CZERO_EQZ:\n  case RISCVISD::CZERO_NEZ:\n    // czero_eq X, (xor Y, 1) -> czero_ne X, Y if Y is 0 or 1.\n    // czero_ne X, (xor Y, 1) -> czero_eq X, Y if Y is 0 or 1.\n    if (N->getOperand(1).getOpcode() == ISD::XOR &&\n        isOneConstant(N->getOperand(1).getOperand(1))) {\n      SDValue Cond = N->getOperand(1).getOperand(0);\n      APInt Mask = APInt::getBitsSetFrom(Cond.getValueSizeInBits(), 1);\n      if (DAG.MaskedValueIsZero(Cond, Mask)) {\n        unsigned NewOpc = N->getOpcode() == RISCVISD::CZERO_EQZ\n                              ? RISCVISD::CZERO_NEZ\n                              : RISCVISD::CZERO_EQZ;\n        return DAG.getNode(NewOpc, SDLoc(N), N->getValueType(0),\n                           N->getOperand(0), Cond);\n      }\n    }\n    return SDValue();\n\n  case RISCVISD::SELECT_CC: {\n    // Transform\n    SDValue LHS = N->getOperand(0);\n    SDValue RHS = N->getOperand(1);\n    SDValue CC = N->getOperand(2);\n    ISD::CondCode CCVal = cast<CondCodeSDNode>(CC)->get();\n    SDValue TrueV = N->getOperand(3);\n    SDValue FalseV = N->getOperand(4);\n    SDLoc DL(N);\n    EVT VT = N->getValueType(0);\n\n    // If the True and False values are the same, we don't need a select_cc.\n    if (TrueV == FalseV)\n      return TrueV;\n\n    // (select (x < 0), y, z)  -> x >> (XLEN - 1) & (y - z) + z\n    // (select (x >= 0), y, z) -> x >> (XLEN - 1) & (z - y) + y\n    if (!Subtarget.hasShortForwardBranchOpt() && isa<ConstantSDNode>(TrueV) &&\n        isa<ConstantSDNode>(FalseV) && isNullConstant(RHS) &&\n        (CCVal == ISD::CondCode::SETLT || CCVal == ISD::CondCode::SETGE)) {\n      if (CCVal == ISD::CondCode::SETGE)\n        std::swap(TrueV, FalseV);\n\n      int64_t TrueSImm = cast<ConstantSDNode>(TrueV)->getSExtValue();\n      int64_t FalseSImm = cast<ConstantSDNode>(FalseV)->getSExtValue();\n      // Only handle simm12, if it is not in this range, it can be considered as\n      // register.\n      if (isInt<12>(TrueSImm) && isInt<12>(FalseSImm) &&\n          isInt<12>(TrueSImm - FalseSImm)) {\n        SDValue SRA =\n            DAG.getNode(ISD::SRA, DL, VT, LHS,\n                        DAG.getConstant(Subtarget.getXLen() - 1, DL, VT));\n        SDValue AND =\n            DAG.getNode(ISD::AND, DL, VT, SRA,\n                        DAG.getConstant(TrueSImm - FalseSImm, DL, VT));\n        return DAG.getNode(ISD::ADD, DL, VT, AND, FalseV);\n      }\n\n      if (CCVal == ISD::CondCode::SETGE)\n        std::swap(TrueV, FalseV);\n    }\n\n    if (combine_CC(LHS, RHS, CC, DL, DAG, Subtarget))\n      return DAG.getNode(RISCVISD::SELECT_CC, DL, N->getValueType(0),\n                         {LHS, RHS, CC, TrueV, FalseV});\n\n    if (!Subtarget.hasConditionalMoveFusion()) {\n      // (select c, -1, y) -> -c | y\n      if (isAllOnesConstant(TrueV)) {\n        SDValue C = DAG.getSetCC(DL, VT, LHS, RHS, CCVal);\n        SDValue Neg = DAG.getNegative(C, DL, VT);\n        return DAG.getNode(ISD::OR, DL, VT, Neg, FalseV);\n      }\n      // (select c, y, -1) -> -!c | y\n      if (isAllOnesConstant(FalseV)) {\n        SDValue C =\n            DAG.getSetCC(DL, VT, LHS, RHS, ISD::getSetCCInverse(CCVal, VT));\n        SDValue Neg = DAG.getNegative(C, DL, VT);\n        return DAG.getNode(ISD::OR, DL, VT, Neg, TrueV);\n      }\n\n      // (select c, 0, y) -> -!c & y\n      if (isNullConstant(TrueV)) {\n        SDValue C =\n            DAG.getSetCC(DL, VT, LHS, RHS, ISD::getSetCCInverse(CCVal, VT));\n        SDValue Neg = DAG.getNegative(C, DL, VT);\n        return DAG.getNode(ISD::AND, DL, VT, Neg, FalseV);\n      }\n      // (select c, y, 0) -> -c & y\n      if (isNullConstant(FalseV)) {\n        SDValue C = DAG.getSetCC(DL, VT, LHS, RHS, CCVal);\n        SDValue Neg = DAG.getNegative(C, DL, VT);\n        return DAG.getNode(ISD::AND, DL, VT, Neg, TrueV);\n      }\n      // (riscvisd::select_cc x, 0, ne, x, 1) -> (add x, (setcc x, 0, eq))\n      // (riscvisd::select_cc x, 0, eq, 1, x) -> (add x, (setcc x, 0, eq))\n      if (((isOneConstant(FalseV) && LHS == TrueV &&\n            CCVal == ISD::CondCode::SETNE) ||\n           (isOneConstant(TrueV) && LHS == FalseV &&\n            CCVal == ISD::CondCode::SETEQ)) &&\n          isNullConstant(RHS)) {\n        // freeze it to be safe.\n        LHS = DAG.getFreeze(LHS);\n        SDValue C = DAG.getSetCC(DL, VT, LHS, RHS, ISD::CondCode::SETEQ);\n        return DAG.getNode(ISD::ADD, DL, VT, LHS, C);\n      }\n    }\n\n    // If both true/false are an xor with 1, pull through the select.\n    // This can occur after op legalization if both operands are setccs that\n    // require an xor to invert.\n    // FIXME: Generalize to other binary ops with identical operand?\n    if (TrueV.getOpcode() == ISD::XOR && FalseV.getOpcode() == ISD::XOR &&\n        TrueV.getOperand(1) == FalseV.getOperand(1) &&\n        isOneConstant(TrueV.getOperand(1)) &&\n        TrueV.hasOneUse() && FalseV.hasOneUse()) {\n      SDValue NewSel = DAG.getNode(RISCVISD::SELECT_CC, DL, VT, LHS, RHS, CC,\n                                   TrueV.getOperand(0), FalseV.getOperand(0));\n      return DAG.getNode(ISD::XOR, DL, VT, NewSel, TrueV.getOperand(1));\n    }\n\n    return SDValue();\n  }\n  case RISCVISD::BR_CC: {\n    SDValue LHS = N->getOperand(1);\n    SDValue RHS = N->getOperand(2);\n    SDValue CC = N->getOperand(3);\n    SDLoc DL(N);\n\n    if (combine_CC(LHS, RHS, CC, DL, DAG, Subtarget))\n      return DAG.getNode(RISCVISD::BR_CC, DL, N->getValueType(0),\n                         N->getOperand(0), LHS, RHS, CC, N->getOperand(4));\n\n    return SDValue();\n  }\n  case ISD::BITREVERSE:\n    return performBITREVERSECombine(N, DAG, Subtarget);\n  case ISD::FP_TO_SINT:\n  case ISD::FP_TO_UINT:\n    return performFP_TO_INTCombine(N, DCI, Subtarget);\n  case ISD::FP_TO_SINT_SAT:\n  case ISD::FP_TO_UINT_SAT:\n    return performFP_TO_INT_SATCombine(N, DCI, Subtarget);\n  case ISD::FCOPYSIGN: {\n    EVT VT = N->getValueType(0);\n    if (!VT.isVector())\n      break;\n    // There is a form of VFSGNJ which injects the negated sign of its second\n    // operand. Try and bubble any FNEG up after the extend/round to produce\n    // this optimized pattern. Avoid modifying cases where FP_ROUND and\n    // TRUNC=1.\n    SDValue In2 = N->getOperand(1);\n    // Avoid cases where the extend/round has multiple uses, as duplicating\n    // those is typically more expensive than removing a fneg.\n    if (!In2.hasOneUse())\n      break;\n    if (In2.getOpcode() != ISD::FP_EXTEND &&\n        (In2.getOpcode() != ISD::FP_ROUND || In2.getConstantOperandVal(1) != 0))\n      break;\n    In2 = In2.getOperand(0);\n    if (In2.getOpcode() != ISD::FNEG)\n      break;\n    SDLoc DL(N);\n    SDValue NewFPExtRound = DAG.getFPExtendOrRound(In2.getOperand(0), DL, VT);\n    return DAG.getNode(ISD::FCOPYSIGN, DL, VT, N->getOperand(0),\n                       DAG.getNode(ISD::FNEG, DL, VT, NewFPExtRound));\n  }\n  case ISD::MGATHER: {\n    const auto *MGN = dyn_cast<MaskedGatherSDNode>(N);\n    const EVT VT = N->getValueType(0);\n    SDValue Index = MGN->getIndex();\n    SDValue ScaleOp = MGN->getScale();\n    ISD::MemIndexType IndexType = MGN->getIndexType();\n    assert(!MGN->isIndexScaled() &&\n           \"Scaled gather/scatter should not be formed\");\n\n    SDLoc DL(N);\n    if (legalizeScatterGatherIndexType(DL, Index, IndexType, DCI))\n      return DAG.getMaskedGather(\n          N->getVTList(), MGN->getMemoryVT(), DL,\n          {MGN->getChain(), MGN->getPassThru(), MGN->getMask(),\n           MGN->getBasePtr(), Index, ScaleOp},\n          MGN->getMemOperand(), IndexType, MGN->getExtensionType());\n\n    if (narrowIndex(Index, IndexType, DAG))\n      return DAG.getMaskedGather(\n          N->getVTList(), MGN->getMemoryVT(), DL,\n          {MGN->getChain(), MGN->getPassThru(), MGN->getMask(),\n           MGN->getBasePtr(), Index, ScaleOp},\n          MGN->getMemOperand(), IndexType, MGN->getExtensionType());\n\n    if (Index.getOpcode() == ISD::BUILD_VECTOR &&\n        MGN->getExtensionType() == ISD::NON_EXTLOAD && isTypeLegal(VT)) {\n      // The sequence will be XLenVT, not the type of Index. Tell\n      // isSimpleVIDSequence this so we avoid overflow.\n      if (std::optional<VIDSequence> SimpleVID =\n              isSimpleVIDSequence(Index, Subtarget.getXLen());\n          SimpleVID && SimpleVID->StepDenominator == 1) {\n        const int64_t StepNumerator = SimpleVID->StepNumerator;\n        const int64_t Addend = SimpleVID->Addend;\n\n        // Note: We don't need to check alignment here since (by assumption\n        // from the existance of the gather), our offsets must be sufficiently\n        // aligned.\n\n        const EVT PtrVT = getPointerTy(DAG.getDataLayout());\n        assert(MGN->getBasePtr()->getValueType(0) == PtrVT);\n        assert(IndexType == ISD::UNSIGNED_SCALED);\n        SDValue BasePtr = DAG.getNode(ISD::ADD, DL, PtrVT, MGN->getBasePtr(),\n                                      DAG.getConstant(Addend, DL, PtrVT));\n\n        SDVTList VTs = DAG.getVTList({VT, MVT::Other});\n        SDValue IntID =\n          DAG.getTargetConstant(Intrinsic::riscv_masked_strided_load, DL,\n                                XLenVT);\n        SDValue Ops[] =\n          {MGN->getChain(), IntID, MGN->getPassThru(), BasePtr,\n           DAG.getConstant(StepNumerator, DL, XLenVT), MGN->getMask()};\n        return DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs,\n                                       Ops, VT, MGN->getMemOperand());\n      }\n    }\n\n    SmallVector<int> ShuffleMask;\n    if (MGN->getExtensionType() == ISD::NON_EXTLOAD &&\n        matchIndexAsShuffle(VT, Index, MGN->getMask(), ShuffleMask)) {\n      SDValue Load = DAG.getMaskedLoad(VT, DL, MGN->getChain(),\n                                       MGN->getBasePtr(), DAG.getUNDEF(XLenVT),\n                                       MGN->getMask(), DAG.getUNDEF(VT),\n                                       MGN->getMemoryVT(), MGN->getMemOperand(),\n                                       ISD::UNINDEXED, ISD::NON_EXTLOAD);\n      SDValue Shuffle =\n        DAG.getVectorShuffle(VT, DL, Load, DAG.getUNDEF(VT), ShuffleMask);\n      return DAG.getMergeValues({Shuffle, Load.getValue(1)}, DL);\n    }\n\n    if (MGN->getExtensionType() == ISD::NON_EXTLOAD &&\n        matchIndexAsWiderOp(VT, Index, MGN->getMask(),\n                            MGN->getMemOperand()->getBaseAlign(), Subtarget)) {\n      SmallVector<SDValue> NewIndices;\n      for (unsigned i = 0; i < Index->getNumOperands(); i += 2)\n        NewIndices.push_back(Index.getOperand(i));\n      EVT IndexVT = Index.getValueType()\n        .getHalfNumVectorElementsVT(*DAG.getContext());\n      Index = DAG.getBuildVector(IndexVT, DL, NewIndices);\n\n      unsigned ElementSize = VT.getScalarStoreSize();\n      EVT WideScalarVT = MVT::getIntegerVT(ElementSize * 8 * 2);\n      auto EltCnt = VT.getVectorElementCount();\n      assert(EltCnt.isKnownEven() && \"Splitting vector, but not in half!\");\n      EVT WideVT = EVT::getVectorVT(*DAG.getContext(), WideScalarVT,\n                                    EltCnt.divideCoefficientBy(2));\n      SDValue Passthru = DAG.getBitcast(WideVT, MGN->getPassThru());\n      EVT MaskVT = EVT::getVectorVT(*DAG.getContext(), MVT::i1,\n                                    EltCnt.divideCoefficientBy(2));\n      SDValue Mask = DAG.getSplat(MaskVT, DL, DAG.getConstant(1, DL, MVT::i1));\n\n      SDValue Gather =\n        DAG.getMaskedGather(DAG.getVTList(WideVT, MVT::Other), WideVT, DL,\n                            {MGN->getChain(), Passthru, Mask, MGN->getBasePtr(),\n                             Index, ScaleOp},\n                            MGN->getMemOperand(), IndexType, ISD::NON_EXTLOAD);\n      SDValue Result = DAG.getBitcast(VT, Gather.getValue(0));\n      return DAG.getMergeValues({Result, Gather.getValue(1)}, DL);\n    }\n    break;\n  }\n  case ISD::MSCATTER:{\n    const auto *MSN = dyn_cast<MaskedScatterSDNode>(N);\n    SDValue Index = MSN->getIndex();\n    SDValue ScaleOp = MSN->getScale();\n    ISD::MemIndexType IndexType = MSN->getIndexType();\n    assert(!MSN->isIndexScaled() &&\n           \"Scaled gather/scatter should not be formed\");\n\n    SDLoc DL(N);\n    if (legalizeScatterGatherIndexType(DL, Index, IndexType, DCI))\n      return DAG.getMaskedScatter(\n          N->getVTList(), MSN->getMemoryVT(), DL,\n          {MSN->getChain(), MSN->getValue(), MSN->getMask(), MSN->getBasePtr(),\n           Index, ScaleOp},\n          MSN->getMemOperand(), IndexType, MSN->isTruncatingStore());\n\n    if (narrowIndex(Index, IndexType, DAG))\n      return DAG.getMaskedScatter(\n          N->getVTList(), MSN->getMemoryVT(), DL,\n          {MSN->getChain(), MSN->getValue(), MSN->getMask(), MSN->getBasePtr(),\n           Index, ScaleOp},\n          MSN->getMemOperand(), IndexType, MSN->isTruncatingStore());\n\n    EVT VT = MSN->getValue()->getValueType(0);\n    SmallVector<int> ShuffleMask;\n    if (!MSN->isTruncatingStore() &&\n        matchIndexAsShuffle(VT, Index, MSN->getMask(), ShuffleMask)) {\n      SDValue Shuffle = DAG.getVectorShuffle(VT, DL, MSN->getValue(),\n                                             DAG.getUNDEF(VT), ShuffleMask);\n      return DAG.getMaskedStore(MSN->getChain(), DL, Shuffle, MSN->getBasePtr(),\n                                DAG.getUNDEF(XLenVT), MSN->getMask(),\n                                MSN->getMemoryVT(), MSN->getMemOperand(),\n                                ISD::UNINDEXED, false);\n    }\n    break;\n  }\n  case ISD::VP_GATHER: {\n    const auto *VPGN = dyn_cast<VPGatherSDNode>(N);\n    SDValue Index = VPGN->getIndex();\n    SDValue ScaleOp = VPGN->getScale();\n    ISD::MemIndexType IndexType = VPGN->getIndexType();\n    assert(!VPGN->isIndexScaled() &&\n           \"Scaled gather/scatter should not be formed\");\n\n    SDLoc DL(N);\n    if (legalizeScatterGatherIndexType(DL, Index, IndexType, DCI))\n      return DAG.getGatherVP(N->getVTList(), VPGN->getMemoryVT(), DL,\n                             {VPGN->getChain(), VPGN->getBasePtr(), Index,\n                              ScaleOp, VPGN->getMask(),\n                              VPGN->getVectorLength()},\n                             VPGN->getMemOperand(), IndexType);\n\n    if (narrowIndex(Index, IndexType, DAG))\n      return DAG.getGatherVP(N->getVTList(), VPGN->getMemoryVT(), DL,\n                             {VPGN->getChain(), VPGN->getBasePtr(), Index,\n                              ScaleOp, VPGN->getMask(),\n                              VPGN->getVectorLength()},\n                             VPGN->getMemOperand(), IndexType);\n\n    break;\n  }\n  case ISD::VP_SCATTER: {\n    const auto *VPSN = dyn_cast<VPScatterSDNode>(N);\n    SDValue Index = VPSN->getIndex();\n    SDValue ScaleOp = VPSN->getScale();\n    ISD::MemIndexType IndexType = VPSN->getIndexType();\n    assert(!VPSN->isIndexScaled() &&\n           \"Scaled gather/scatter should not be formed\");\n\n    SDLoc DL(N);\n    if (legalizeScatterGatherIndexType(DL, Index, IndexType, DCI))\n      return DAG.getScatterVP(N->getVTList(), VPSN->getMemoryVT(), DL,\n                              {VPSN->getChain(), VPSN->getValue(),\n                               VPSN->getBasePtr(), Index, ScaleOp,\n                               VPSN->getMask(), VPSN->getVectorLength()},\n                              VPSN->getMemOperand(), IndexType);\n\n    if (narrowIndex(Index, IndexType, DAG))\n      return DAG.getScatterVP(N->getVTList(), VPSN->getMemoryVT(), DL,\n                              {VPSN->getChain(), VPSN->getValue(),\n                               VPSN->getBasePtr(), Index, ScaleOp,\n                               VPSN->getMask(), VPSN->getVectorLength()},\n                              VPSN->getMemOperand(), IndexType);\n    break;\n  }\n  case RISCVISD::SRA_VL:\n  case RISCVISD::SRL_VL:\n  case RISCVISD::SHL_VL: {\n    SDValue ShAmt = N->getOperand(1);\n    if (ShAmt.getOpcode() == RISCVISD::SPLAT_VECTOR_SPLIT_I64_VL) {\n      // We don't need the upper 32 bits of a 64-bit element for a shift amount.\n      SDLoc DL(N);\n      SDValue VL = N->getOperand(4);\n      EVT VT = N->getValueType(0);\n      ShAmt = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, DAG.getUNDEF(VT),\n                          ShAmt.getOperand(1), VL);\n      return DAG.getNode(N->getOpcode(), DL, VT, N->getOperand(0), ShAmt,\n                         N->getOperand(2), N->getOperand(3), N->getOperand(4));\n    }\n    break;\n  }\n  case ISD::SRA:\n    if (SDValue V = performSRACombine(N, DAG, Subtarget))\n      return V;\n    [[fallthrough]];\n  case ISD::SRL:\n  case ISD::SHL: {\n    SDValue ShAmt = N->getOperand(1);\n    if (ShAmt.getOpcode() == RISCVISD::SPLAT_VECTOR_SPLIT_I64_VL) {\n      // We don't need the upper 32 bits of a 64-bit element for a shift amount.\n      SDLoc DL(N);\n      EVT VT = N->getValueType(0);\n      ShAmt = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, DAG.getUNDEF(VT),\n                          ShAmt.getOperand(1),\n                          DAG.getRegister(RISCV::X0, Subtarget.getXLenVT()));\n      return DAG.getNode(N->getOpcode(), DL, VT, N->getOperand(0), ShAmt);\n    }\n    break;\n  }\n  case RISCVISD::ADD_VL:\n    if (SDValue V = combineBinOp_VLToVWBinOp_VL(N, DCI, Subtarget))\n      return V;\n    return combineToVWMACC(N, DAG, Subtarget);\n  case RISCVISD::SUB_VL:\n  case RISCVISD::VWADD_W_VL:\n  case RISCVISD::VWADDU_W_VL:\n  case RISCVISD::VWSUB_W_VL:\n  case RISCVISD::VWSUBU_W_VL:\n  case RISCVISD::MUL_VL:\n    return combineBinOp_VLToVWBinOp_VL(N, DCI, Subtarget);\n  case RISCVISD::VFMADD_VL:\n  case RISCVISD::VFNMADD_VL:\n  case RISCVISD::VFMSUB_VL:\n  case RISCVISD::VFNMSUB_VL:\n  case RISCVISD::STRICT_VFMADD_VL:\n  case RISCVISD::STRICT_VFNMADD_VL:\n  case RISCVISD::STRICT_VFMSUB_VL:\n  case RISCVISD::STRICT_VFNMSUB_VL:\n    return performVFMADD_VLCombine(N, DAG, Subtarget);\n  case RISCVISD::FMUL_VL:\n    return performVFMUL_VLCombine(N, DAG, Subtarget);\n  case RISCVISD::FADD_VL:\n  case RISCVISD::FSUB_VL:\n    return performFADDSUB_VLCombine(N, DAG, Subtarget);\n  case ISD::LOAD:\n  case ISD::STORE: {\n    if (DCI.isAfterLegalizeDAG())\n      if (SDValue V = performMemPairCombine(N, DCI))\n        return V;\n\n    if (N->getOpcode() != ISD::STORE)\n      break;\n\n    auto *Store = cast<StoreSDNode>(N);\n    SDValue Chain = Store->getChain();\n    EVT MemVT = Store->getMemoryVT();\n    SDValue Val = Store->getValue();\n    SDLoc DL(N);\n\n    bool IsScalarizable =\n        MemVT.isFixedLengthVector() && ISD::isNormalStore(Store) &&\n        Store->isSimple() &&\n        MemVT.getVectorElementType().bitsLE(Subtarget.getXLenVT()) &&\n        isPowerOf2_64(MemVT.getSizeInBits()) &&\n        MemVT.getSizeInBits() <= Subtarget.getXLen();\n\n    // If sufficiently aligned we can scalarize stores of constant vectors of\n    // any power-of-two size up to XLen bits, provided that they aren't too\n    // expensive to materialize.\n    //   vsetivli   zero, 2, e8, m1, ta, ma\n    //   vmv.v.i    v8, 4\n    //   vse64.v    v8, (a0)\n    // ->\n    //   li     a1, 1028\n    //   sh     a1, 0(a0)\n    if (DCI.isBeforeLegalize() && IsScalarizable &&\n        ISD::isBuildVectorOfConstantSDNodes(Val.getNode())) {\n      // Get the constant vector bits\n      APInt NewC(Val.getValueSizeInBits(), 0);\n      uint64_t EltSize = Val.getScalarValueSizeInBits();\n      for (unsigned i = 0; i < Val.getNumOperands(); i++) {\n        if (Val.getOperand(i).isUndef())\n          continue;\n        NewC.insertBits(Val.getConstantOperandAPInt(i).trunc(EltSize),\n                        i * EltSize);\n      }\n      MVT NewVT = MVT::getIntegerVT(MemVT.getSizeInBits());\n\n      if (RISCVMatInt::getIntMatCost(NewC, Subtarget.getXLen(), Subtarget,\n                                     true) <= 2 &&\n          allowsMemoryAccessForAlignment(*DAG.getContext(), DAG.getDataLayout(),\n                                         NewVT, *Store->getMemOperand())) {\n        SDValue NewV = DAG.getConstant(NewC, DL, NewVT);\n        return DAG.getStore(Chain, DL, NewV, Store->getBasePtr(),\n                            Store->getPointerInfo(), Store->getOriginalAlign(),\n                            Store->getMemOperand()->getFlags());\n      }\n    }\n\n    // Similarly, if sufficiently aligned we can scalarize vector copies, e.g.\n    //   vsetivli   zero, 2, e16, m1, ta, ma\n    //   vle16.v    v8, (a0)\n    //   vse16.v    v8, (a1)\n    if (auto *L = dyn_cast<LoadSDNode>(Val);\n        L && DCI.isBeforeLegalize() && IsScalarizable && L->isSimple() &&\n        L->hasNUsesOfValue(1, 0) && L->hasNUsesOfValue(1, 1) &&\n        Store->getChain() == SDValue(L, 1) && ISD::isNormalLoad(L) &&\n        L->getMemoryVT() == MemVT) {\n      MVT NewVT = MVT::getIntegerVT(MemVT.getSizeInBits());\n      if (allowsMemoryAccessForAlignment(*DAG.getContext(), DAG.getDataLayout(),\n                                         NewVT, *Store->getMemOperand()) &&\n          allowsMemoryAccessForAlignment(*DAG.getContext(), DAG.getDataLayout(),\n                                         NewVT, *L->getMemOperand())) {\n        SDValue NewL = DAG.getLoad(NewVT, DL, L->getChain(), L->getBasePtr(),\n                                   L->getPointerInfo(), L->getOriginalAlign(),\n                                   L->getMemOperand()->getFlags());\n        return DAG.getStore(Chain, DL, NewL, Store->getBasePtr(),\n                            Store->getPointerInfo(), Store->getOriginalAlign(),\n                            Store->getMemOperand()->getFlags());\n      }\n    }\n\n    // Combine store of vmv.x.s/vfmv.f.s to vse with VL of 1.\n    // vfmv.f.s is represented as extract element from 0. Match it late to avoid\n    // any illegal types.\n    if (Val.getOpcode() == RISCVISD::VMV_X_S ||\n        (DCI.isAfterLegalizeDAG() &&\n         Val.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n         isNullConstant(Val.getOperand(1)))) {\n      SDValue Src = Val.getOperand(0);\n      MVT VecVT = Src.getSimpleValueType();\n      // VecVT should be scalable and memory VT should match the element type.\n      if (!Store->isIndexed() && VecVT.isScalableVector() &&\n          MemVT == VecVT.getVectorElementType()) {\n        SDLoc DL(N);\n        MVT MaskVT = getMaskTypeFor(VecVT);\n        return DAG.getStoreVP(\n            Store->getChain(), DL, Src, Store->getBasePtr(), Store->getOffset(),\n            DAG.getConstant(1, DL, MaskVT),\n            DAG.getConstant(1, DL, Subtarget.getXLenVT()), MemVT,\n            Store->getMemOperand(), Store->getAddressingMode(),\n            Store->isTruncatingStore(), /*IsCompress*/ false);\n      }\n    }\n\n    break;\n  }\n  case ISD::SPLAT_VECTOR: {\n    EVT VT = N->getValueType(0);\n    // Only perform this combine on legal MVT types.\n    if (!isTypeLegal(VT))\n      break;\n    if (auto Gather = matchSplatAsGather(N->getOperand(0), VT.getSimpleVT(), N,\n                                         DAG, Subtarget))\n      return Gather;\n    break;\n  }\n  case ISD::BUILD_VECTOR:\n    if (SDValue V = performBUILD_VECTORCombine(N, DAG, Subtarget, *this))\n      return V;\n    break;\n  case ISD::CONCAT_VECTORS:\n    if (SDValue V = performCONCAT_VECTORSCombine(N, DAG, Subtarget, *this))\n      return V;\n    break;\n  case ISD::INSERT_VECTOR_ELT:\n    if (SDValue V = performINSERT_VECTOR_ELTCombine(N, DAG, Subtarget, *this))\n      return V;\n    break;\n  case RISCVISD::VFMV_V_F_VL: {\n    const MVT VT = N->getSimpleValueType(0);\n    SDValue Passthru = N->getOperand(0);\n    SDValue Scalar = N->getOperand(1);\n    SDValue VL = N->getOperand(2);\n\n    // If VL is 1, we can use vfmv.s.f.\n    if (isOneConstant(VL))\n      return DAG.getNode(RISCVISD::VFMV_S_F_VL, DL, VT, Passthru, Scalar, VL);\n    break;\n  }\n  case RISCVISD::VMV_V_X_VL: {\n    const MVT VT = N->getSimpleValueType(0);\n    SDValue Passthru = N->getOperand(0);\n    SDValue Scalar = N->getOperand(1);\n    SDValue VL = N->getOperand(2);\n\n    // Tail agnostic VMV.V.X only demands the vector element bitwidth from the\n    // scalar input.\n    unsigned ScalarSize = Scalar.getValueSizeInBits();\n    unsigned EltWidth = VT.getScalarSizeInBits();\n    if (ScalarSize > EltWidth && Passthru.isUndef())\n      if (SimplifyDemandedLowBitsHelper(1, EltWidth))\n        return SDValue(N, 0);\n\n    // If VL is 1 and the scalar value won't benefit from immediate, we can\n    // use vmv.s.x.\n    ConstantSDNode *Const = dyn_cast<ConstantSDNode>(Scalar);\n    if (isOneConstant(VL) &&\n        (!Const || Const->isZero() ||\n         !Const->getAPIntValue().sextOrTrunc(EltWidth).isSignedIntN(5)))\n      return DAG.getNode(RISCVISD::VMV_S_X_VL, DL, VT, Passthru, Scalar, VL);\n\n    break;\n  }\n  case RISCVISD::VFMV_S_F_VL: {\n    SDValue Src = N->getOperand(1);\n    // Try to remove vector->scalar->vector if the scalar->vector is inserting\n    // into an undef vector.\n    // TODO: Could use a vslide or vmv.v.v for non-undef.\n    if (N->getOperand(0).isUndef() &&\n        Src.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n        isNullConstant(Src.getOperand(1)) &&\n        Src.getOperand(0).getValueType().isScalableVector()) {\n      EVT VT = N->getValueType(0);\n      EVT SrcVT = Src.getOperand(0).getValueType();\n      assert(SrcVT.getVectorElementType() == VT.getVectorElementType());\n      // Widths match, just return the original vector.\n      if (SrcVT == VT)\n        return Src.getOperand(0);\n      // TODO: Use insert_subvector/extract_subvector to change widen/narrow?\n    }\n    [[fallthrough]];\n  }\n  case RISCVISD::VMV_S_X_VL: {\n    const MVT VT = N->getSimpleValueType(0);\n    SDValue Passthru = N->getOperand(0);\n    SDValue Scalar = N->getOperand(1);\n    SDValue VL = N->getOperand(2);\n\n    // Use M1 or smaller to avoid over constraining register allocation\n    const MVT M1VT = getLMUL1VT(VT);\n    if (M1VT.bitsLT(VT)) {\n      SDValue M1Passthru =\n          DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, M1VT, Passthru,\n                      DAG.getVectorIdxConstant(0, DL));\n      SDValue Result =\n          DAG.getNode(N->getOpcode(), DL, M1VT, M1Passthru, Scalar, VL);\n      Result = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, Passthru, Result,\n                           DAG.getConstant(0, DL, XLenVT));\n      return Result;\n    }\n\n    // We use a vmv.v.i if possible.  We limit this to LMUL1.  LMUL2 or\n    // higher would involve overly constraining the register allocator for\n    // no purpose.\n    if (ConstantSDNode *Const = dyn_cast<ConstantSDNode>(Scalar);\n        Const && !Const->isZero() && isInt<5>(Const->getSExtValue()) &&\n        VT.bitsLE(getLMUL1VT(VT)) && Passthru.isUndef())\n      return DAG.getNode(RISCVISD::VMV_V_X_VL, DL, VT, Passthru, Scalar, VL);\n\n    break;\n  }\n  case RISCVISD::VMV_X_S: {\n    SDValue Vec = N->getOperand(0);\n    MVT VecVT = N->getOperand(0).getSimpleValueType();\n    const MVT M1VT = getLMUL1VT(VecVT);\n    if (M1VT.bitsLT(VecVT)) {\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, M1VT, Vec,\n                        DAG.getVectorIdxConstant(0, DL));\n      return DAG.getNode(RISCVISD::VMV_X_S, DL, N->getSimpleValueType(0), Vec);\n    }\n    break;\n  }\n  case ISD::INTRINSIC_VOID:\n  case ISD::INTRINSIC_W_CHAIN:\n  case ISD::INTRINSIC_WO_CHAIN: {\n    unsigned IntOpNo = N->getOpcode() == ISD::INTRINSIC_WO_CHAIN ? 0 : 1;\n    unsigned IntNo = N->getConstantOperandVal(IntOpNo);\n    switch (IntNo) {\n      // By default we do not combine any intrinsic.\n    default:\n      return SDValue();\n    case Intrinsic::riscv_masked_strided_load: {\n      MVT VT = N->getSimpleValueType(0);\n      auto *Load = cast<MemIntrinsicSDNode>(N);\n      SDValue PassThru = N->getOperand(2);\n      SDValue Base = N->getOperand(3);\n      SDValue Stride = N->getOperand(4);\n      SDValue Mask = N->getOperand(5);\n\n      // If the stride is equal to the element size in bytes,  we can use\n      // a masked.load.\n      const unsigned ElementSize = VT.getScalarStoreSize();\n      if (auto *StrideC = dyn_cast<ConstantSDNode>(Stride);\n          StrideC && StrideC->getZExtValue() == ElementSize)\n        return DAG.getMaskedLoad(VT, DL, Load->getChain(), Base,\n                                 DAG.getUNDEF(XLenVT), Mask, PassThru,\n                                 Load->getMemoryVT(), Load->getMemOperand(),\n                                 ISD::UNINDEXED, ISD::NON_EXTLOAD);\n      return SDValue();\n    }\n    case Intrinsic::riscv_masked_strided_store: {\n      auto *Store = cast<MemIntrinsicSDNode>(N);\n      SDValue Value = N->getOperand(2);\n      SDValue Base = N->getOperand(3);\n      SDValue Stride = N->getOperand(4);\n      SDValue Mask = N->getOperand(5);\n\n      // If the stride is equal to the element size in bytes,  we can use\n      // a masked.store.\n      const unsigned ElementSize = Value.getValueType().getScalarStoreSize();\n      if (auto *StrideC = dyn_cast<ConstantSDNode>(Stride);\n          StrideC && StrideC->getZExtValue() == ElementSize)\n        return DAG.getMaskedStore(Store->getChain(), DL, Value, Base,\n                                  DAG.getUNDEF(XLenVT), Mask,\n                                  Store->getMemoryVT(), Store->getMemOperand(),\n                                  ISD::UNINDEXED, false);\n      return SDValue();\n    }\n    case Intrinsic::riscv_vcpop:\n    case Intrinsic::riscv_vcpop_mask:\n    case Intrinsic::riscv_vfirst:\n    case Intrinsic::riscv_vfirst_mask: {\n      SDValue VL = N->getOperand(2);\n      if (IntNo == Intrinsic::riscv_vcpop_mask ||\n          IntNo == Intrinsic::riscv_vfirst_mask)\n        VL = N->getOperand(3);\n      if (!isNullConstant(VL))\n        return SDValue();\n      // If VL is 0, vcpop -> li 0, vfirst -> li -1.\n      SDLoc DL(N);\n      EVT VT = N->getValueType(0);\n      if (IntNo == Intrinsic::riscv_vfirst ||\n          IntNo == Intrinsic::riscv_vfirst_mask)\n        return DAG.getConstant(-1, DL, VT);\n      return DAG.getConstant(0, DL, VT);\n    }\n    }\n  }\n  case ISD::BITCAST: {\n    assert(Subtarget.useRVVForFixedLengthVectors());\n    SDValue N0 = N->getOperand(0);\n    EVT VT = N->getValueType(0);\n    EVT SrcVT = N0.getValueType();\n    // If this is a bitcast between a MVT::v4i1/v2i1/v1i1 and an illegal integer\n    // type, widen both sides to avoid a trip through memory.\n    if ((SrcVT == MVT::v1i1 || SrcVT == MVT::v2i1 || SrcVT == MVT::v4i1) &&\n        VT.isScalarInteger()) {\n      unsigned NumConcats = 8 / SrcVT.getVectorNumElements();\n      SmallVector<SDValue, 4> Ops(NumConcats, DAG.getUNDEF(SrcVT));\n      Ops[0] = N0;\n      SDLoc DL(N);\n      N0 = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v8i1, Ops);\n      N0 = DAG.getBitcast(MVT::i8, N0);\n      return DAG.getNode(ISD::TRUNCATE, DL, VT, N0);\n    }\n\n    return SDValue();\n  }\n  }\n\n  return SDValue();\n}",
      "start_line": 15129,
      "end_line": 16093,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "PerformDAGCombine",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": "SDValue()"
        }
      ],
      "calls": [
        "0",
        "since",
        "bitsLE",
        "getConstant",
        "is64Bit",
        "getSizeInBits",
        "getAddressingMode",
        "select",
        "isSimple",
        "getOriginalAlign",
        "bitcastToAPInt",
        "performFP_TO_INT_SATCombine",
        "performSUBCombine",
        "getVectorLength",
        "isVector",
        "getScale",
        "performFP_TO_INTCombine",
        "getMemIntrinsicNode",
        "getSimpleVT",
        "recursivelyDeleteUnusedNodes",
        "isPowerOf2_64",
        "getMaskedLoad",
        "getValueType",
        "getXLen",
        "getSignMask",
        "combineToVWMACC",
        "fold",
        "getBasePtr",
        "performTRUNCATECombine",
        "getMaskedStore",
        "getBaseAlign",
        "getScatterVP",
        "getTargetConstant",
        "isSignedIntN",
        "getDataLayout",
        "getScalarValueSizeInBits",
        "getFPExtendOrRound",
        "getIndex",
        "getHalfNumVectorElementsVT",
        "performSELECTCombine",
        "getVectorShuffle",
        "getPassThru",
        "getVectorElementCount",
        "push_back",
        "isFixedLengthVector",
        "allowsMemoryAccessForAlignment",
        "getValue",
        "swap",
        "getLoad",
        "getOpcode",
        "CombineTo",
        "getExtensionType",
        "getOperand",
        "performANDCombine",
        "divideCoefficientBy",
        "performBITREVERSECombine",
        "performORCombine",
        "combineBinOp_VLToVWBinOp_VL",
        "matchIndexAsShuffle",
        "getIntegerVT",
        "isNormalLoad",
        "getReg",
        "getNode",
        "SimplifyDemandedLowBitsHelper",
        "isBeforeLegalize",
        "getOffset",
        "getMemoryVT",
        "matchIndexAsWiderOp",
        "getStore",
        "performSIGN_EXTEND_INREGCombine",
        "getLowBitsSet",
        "getFreeze",
        "isScalableVector",
        "performMULCombine",
        "sra",
        "insertBits",
        "getMask",
        "getFlags",
        "isOneConstant",
        "getValueAPF",
        "isUndef",
        "getUNDEF",
        "getVectorVT",
        "getBitsSetFrom",
        "getBuildVector",
        "getMaskedScatter",
        "getScalarStoreSize",
        "getPointerTy",
        "isAfterLegalizeDAG",
        "getZExtValue",
        "SDValue",
        "getMemOperand",
        "getSetCC",
        "isTypeLegal",
        "getVTList",
        "zext",
        "performVFMADD_VLCombine",
        "performFADDSUB_VLCombine",
        "sext",
        "getRegister",
        "isZero",
        "isTruncatingStore",
        "getStoreVP",
        "getLMUL1VT",
        "getMergeValues",
        "getMaskTypeFor",
        "performSETCCCombine",
        "getIndexType",
        "sextOrTrunc",
        "Fold",
        "get",
        "xor",
        "DL",
        "isNullConstant",
        "getXLenVT",
        "getSExtValue",
        "and",
        "getChain",
        "getAPIntValue",
        "getValueSizeInBits",
        "getSimpleValueType",
        "isScalarInteger",
        "NewC",
        "getBitcast",
        "isAllOnes",
        "getScalarSizeInBits",
        "trunc",
        "ReplaceAllUsesOfValueWith",
        "getGatherVP",
        "getNegative",
        "getConstantOperandVal",
        "performXORCombine",
        "isNormalStore",
        "getVectorNumElements",
        "performVFMUL_VLCombine",
        "getVectorElementType",
        "getSplat",
        "isBuildVectorOfConstantSDNodes",
        "hasNUsesOfValue",
        "getPointerInfo",
        "hasOneUse",
        "Ops",
        "AddToWorklist",
        "getMaskedGather",
        "performADDCombine"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldTransformSignedTruncationCheck",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "XVT"
        },
        {
          "type": "unsigned",
          "name": "KeptBits"
        }
      ],
      "body": "{\n  // For vectors, we don't have a preference..\n  if (XVT.isVector())\n    return false;\n\n  if (XVT != MVT::i32 && XVT != MVT::i64)\n    return false;\n\n  // We can use sext.w for RV64 or an srai 31 on RV32.\n  if (KeptBits == 32 || KeptBits == 64)\n    return true;\n\n  // With Zbb we can use sext.h/sext.b.\n  return Subtarget.hasStdExtZbb() &&\n         ((KeptBits == 8 && XVT == MVT::i64 && !Subtarget.is64Bit()) ||\n          KeptBits == 16);\n}",
      "start_line": 16095,
      "end_line": 16112,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZbb",
        "is64Bit"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isDesirableToCommuteWithShift",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const SDNode",
          "name": "*N"
        },
        {
          "type": "CombineLevel",
          "name": "Level"
        }
      ],
      "body": "{\n  assert((N->getOpcode() == ISD::SHL || N->getOpcode() == ISD::SRA ||\n          N->getOpcode() == ISD::SRL) &&\n         \"Expected shift op\");\n\n  // The following folds are only desirable if `(OP _, c1 << c2)` can be\n  // materialised in fewer instructions than `(OP _, c1)`:\n  //\n  //   (shl (add x, c1), c2) -> (add (shl x, c2), c1 << c2)\n  //   (shl (or x, c1), c2) -> (or (shl x, c2), c1 << c2)\n  SDValue N0 = N->getOperand(0);\n  EVT Ty = N0.getValueType();\n  if (Ty.isScalarInteger() &&\n      (N0.getOpcode() == ISD::ADD || N0.getOpcode() == ISD::OR)) {\n    auto *C1 = dyn_cast<ConstantSDNode>(N0->getOperand(1));\n    auto *C2 = dyn_cast<ConstantSDNode>(N->getOperand(1));\n    if (C1 && C2) {\n      const APInt &C1Int = C1->getAPIntValue();\n      APInt ShiftedC1Int = C1Int << C2->getAPIntValue();\n\n      // We can materialise `c1 << c2` into an add immediate, so it's \"free\",\n      // and the combine should happen, to potentially allow further combines\n      // later.\n      if (ShiftedC1Int.getSignificantBits() <= 64 &&\n          isLegalAddImmediate(ShiftedC1Int.getSExtValue()))\n        return true;\n\n      // We can materialise `c1` in an add immediate, so it's \"free\", and the\n      // combine should be prevented.\n      if (C1Int.getSignificantBits() <= 64 &&\n          isLegalAddImmediate(C1Int.getSExtValue()))\n        return false;\n\n      // Neither constant will fit into an immediate, so find materialisation\n      // costs.\n      int C1Cost =\n          RISCVMatInt::getIntMatCost(C1Int, Ty.getSizeInBits(), Subtarget,\n                                     /*CompressionCost*/ true);\n      int ShiftedC1Cost = RISCVMatInt::getIntMatCost(\n          ShiftedC1Int, Ty.getSizeInBits(), Subtarget,\n          /*CompressionCost*/ true);\n\n      // Materialising `c1` is cheaper than materialising `c1 << c2`, so the\n      // combine should be prevented.\n      if (C1Cost < ShiftedC1Cost)\n        return false;\n    }\n  }\n  return true;\n}",
      "start_line": 16114,
      "end_line": 16164,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "add",
        "isLegalAddImmediate",
        "getAPIntValue",
        "getOpcode",
        "getIntMatCost",
        "getValueType",
        "getOperand",
        "or",
        "shl"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "targetShrinkDemandedConstant",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "const APInt",
          "name": "&DemandedBits"
        },
        {
          "type": "const APInt",
          "name": "&DemandedElts"
        },
        {
          "type": "TargetLoweringOpt",
          "name": "&TLO"
        }
      ],
      "body": "{\n  // Delay this optimization as late as possible.\n  if (!TLO.LegalOps)\n    return false;\n\n  EVT VT = Op.getValueType();\n  if (VT.isVector())\n    return false;\n\n  unsigned Opcode = Op.getOpcode();\n  if (Opcode != ISD::AND && Opcode != ISD::OR && Opcode != ISD::XOR)\n    return false;\n\n  ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op.getOperand(1));\n  if (!C)\n    return false;\n\n  const APInt &Mask = C->getAPIntValue();\n\n  // Clear all non-demanded bits initially.\n  APInt ShrunkMask = Mask & DemandedBits;\n\n  // Try to make a smaller immediate by setting undemanded bits.\n\n  APInt ExpandedMask = Mask | ~DemandedBits;\n\n  auto IsLegalMask = [ShrunkMask, ExpandedMask](const APInt &Mask) -> bool {\n    return ShrunkMask.isSubsetOf(Mask) && Mask.isSubsetOf(ExpandedMask);\n  };\n  auto UseMask = [Mask, Op, &TLO](const APInt &NewMask) -> bool {\n    if (NewMask == Mask)\n      return true;\n    SDLoc DL(Op);\n    SDValue NewC = TLO.DAG.getConstant(NewMask, DL, Op.getValueType());\n    SDValue NewOp = TLO.DAG.getNode(Op.getOpcode(), DL, Op.getValueType(),\n                                    Op.getOperand(0), NewC);\n    return TLO.CombineTo(Op, NewOp);\n  };\n\n  // If the shrunk mask fits in sign extended 12 bits, let the target\n  // independent code apply it.\n  if (ShrunkMask.isSignedIntN(12))\n    return false;\n\n  // And has a few special cases for zext.\n  if (Opcode == ISD::AND) {\n    // Preserve (and X, 0xffff), if zext.h exists use zext.h,\n    // otherwise use SLLI + SRLI.\n    APInt NewMask = APInt(Mask.getBitWidth(), 0xffff);\n    if (IsLegalMask(NewMask))\n      return UseMask(NewMask);\n\n    // Try to preserve (and X, 0xffffffff), the (zext_inreg X, i32) pattern.\n    if (VT == MVT::i64) {\n      APInt NewMask = APInt(64, 0xffffffff);\n      if (IsLegalMask(NewMask))\n        return UseMask(NewMask);\n    }\n  }\n\n  // For the remaining optimizations, we need to be able to make a negative\n  // number through a combination of mask and undemanded bits.\n  if (!ExpandedMask.isNegative())\n    return false;\n\n  // What is the fewest number of bits we need to represent the negative number.\n  unsigned MinSignedBits = ExpandedMask.getSignificantBits();\n\n  // Try to make a 12 bit negative immediate. If that fails try to make a 32\n  // bit negative immediate unless the shrunk immediate already fits in 32 bits.\n  // If we can't create a simm12, we shouldn't change opaque constants.\n  APInt NewMask = ShrunkMask;\n  if (MinSignedBits <= 12)\n    NewMask.setBitsFrom(11);\n  else if (!C->isOpaque() && MinSignedBits <= 32 && !ShrunkMask.isSignedIntN(32))\n    NewMask.setBitsFrom(31);\n  else\n    return false;\n\n  // Check that our new mask is a subset of the demanded mask.\n  assert(IsLegalMask(NewMask));\n  return UseMask(NewMask);\n}",
      "start_line": 16166,
      "end_line": 16250,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isSignedIntN",
        "the",
        "isSubsetOf",
        "getAPIntValue",
        "getOpcode",
        "CombineTo",
        "getConstant",
        "UseMask",
        "getValueType",
        "getSignificantBits",
        "getOperand",
        "setBitsFrom",
        "DL",
        "Preserve",
        "APInt",
        "getNode",
        "preserve"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "computeGREVOrGORC",
      "return_type": "uint64_t",
      "parameters": [
        {
          "type": "uint64_t",
          "name": "x"
        },
        {
          "type": "unsigned",
          "name": "ShAmt"
        },
        {
          "type": "bool",
          "name": "IsGORC"
        }
      ],
      "body": "{\n  static const uint64_t GREVMasks[] = {\n      0x5555555555555555ULL, 0x3333333333333333ULL, 0x0F0F0F0F0F0F0F0FULL,\n      0x00FF00FF00FF00FFULL, 0x0000FFFF0000FFFFULL, 0x00000000FFFFFFFFULL};\n\n  for (unsigned Stage = 0; Stage != 6; ++Stage) {\n    unsigned Shift = 1 << Stage;\n    if (ShAmt & Shift) {\n      uint64_t Mask = GREVMasks[Stage];\n      uint64_t Res = ((x & Mask) << Shift) | ((x >> Shift) & Mask);\n      if (IsGORC)\n        Res |= x;\n      x = Res;\n    }\n  }\n\n  return x;\n}",
      "start_line": 16252,
      "end_line": 16269,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "computeKnownBitsForTargetNode",
      "return_type": "void",
      "parameters": [
        {
          "type": "const SDValue",
          "name": "Op"
        },
        {
          "type": "KnownBits",
          "name": "&Known"
        },
        {
          "type": "const APInt",
          "name": "&DemandedElts"
        },
        {
          "type": "const SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned",
          "name": "Depth"
        }
      ],
      "body": "{\n  unsigned BitWidth = Known.getBitWidth();\n  unsigned Opc = Op.getOpcode();\n  assert((Opc >= ISD::BUILTIN_OP_END ||\n          Opc == ISD::INTRINSIC_WO_CHAIN ||\n          Opc == ISD::INTRINSIC_W_CHAIN ||\n          Opc == ISD::INTRINSIC_VOID) &&\n         \"Should use MaskedValueIsZero if you don't know whether Op\"\n         \" is a target node!\");\n\n  Known.resetAll();\n  switch (Opc) {\n  default: break;\n  case RISCVISD::SELECT_CC: {\n    Known = DAG.computeKnownBits(Op.getOperand(4), Depth + 1);\n    // If we don't know any bits, early out.\n    if (Known.isUnknown())\n      break;\n    KnownBits Known2 = DAG.computeKnownBits(Op.getOperand(3), Depth + 1);\n\n    // Only known if known in both the LHS and RHS.\n    Known = Known.intersectWith(Known2);\n    break;\n  }\n  case RISCVISD::CZERO_EQZ:\n  case RISCVISD::CZERO_NEZ:\n    Known = DAG.computeKnownBits(Op.getOperand(0), Depth + 1);\n    // Result is either all zero or operand 0. We can propagate zeros, but not\n    // ones.\n    Known.One.clearAllBits();\n    break;\n  case RISCVISD::REMUW: {\n    KnownBits Known2;\n    Known = DAG.computeKnownBits(Op.getOperand(0), DemandedElts, Depth + 1);\n    Known2 = DAG.computeKnownBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    // We only care about the lower 32 bits.\n    Known = KnownBits::urem(Known.trunc(32), Known2.trunc(32));\n    // Restore the original width by sign extending.\n    Known = Known.sext(BitWidth);\n    break;\n  }\n  case RISCVISD::DIVUW: {\n    KnownBits Known2;\n    Known = DAG.computeKnownBits(Op.getOperand(0), DemandedElts, Depth + 1);\n    Known2 = DAG.computeKnownBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    // We only care about the lower 32 bits.\n    Known = KnownBits::udiv(Known.trunc(32), Known2.trunc(32));\n    // Restore the original width by sign extending.\n    Known = Known.sext(BitWidth);\n    break;\n  }\n  case RISCVISD::SLLW: {\n    KnownBits Known2;\n    Known = DAG.computeKnownBits(Op.getOperand(0), DemandedElts, Depth + 1);\n    Known2 = DAG.computeKnownBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    Known = KnownBits::shl(Known.trunc(32), Known2.trunc(5).zext(32));\n    // Restore the original width by sign extending.\n    Known = Known.sext(BitWidth);\n    break;\n  }\n  case RISCVISD::CTZW: {\n    KnownBits Known2 = DAG.computeKnownBits(Op.getOperand(0), Depth + 1);\n    unsigned PossibleTZ = Known2.trunc(32).countMaxTrailingZeros();\n    unsigned LowBits = llvm::bit_width(PossibleTZ);\n    Known.Zero.setBitsFrom(LowBits);\n    break;\n  }\n  case RISCVISD::CLZW: {\n    KnownBits Known2 = DAG.computeKnownBits(Op.getOperand(0), Depth + 1);\n    unsigned PossibleLZ = Known2.trunc(32).countMaxLeadingZeros();\n    unsigned LowBits = llvm::bit_width(PossibleLZ);\n    Known.Zero.setBitsFrom(LowBits);\n    break;\n  }\n  case RISCVISD::BREV8:\n  case RISCVISD::ORC_B: {\n    // FIXME: This is based on the non-ratified Zbp GREV and GORC where a\n    // control value of 7 is equivalent to brev8 and orc.b.\n    Known = DAG.computeKnownBits(Op.getOperand(0), Depth + 1);\n    bool IsGORC = Op.getOpcode() == RISCVISD::ORC_B;\n    // To compute zeros, we need to invert the value and invert it back after.\n    Known.Zero =\n        ~computeGREVOrGORC(~Known.Zero.getZExtValue(), 7, IsGORC);\n    Known.One = computeGREVOrGORC(Known.One.getZExtValue(), 7, IsGORC);\n    break;\n  }\n  case RISCVISD::READ_VLENB: {\n    // We can use the minimum and maximum VLEN values to bound VLENB.  We\n    // know VLEN must be a power of two.\n    const unsigned MinVLenB = Subtarget.getRealMinVLen() / 8;\n    const unsigned MaxVLenB = Subtarget.getRealMaxVLen() / 8;\n    assert(MinVLenB > 0 && \"READ_VLENB without vector extension enabled?\");\n    Known.Zero.setLowBits(Log2_32(MinVLenB));\n    Known.Zero.setBitsFrom(Log2_32(MaxVLenB)+1);\n    if (MaxVLenB == MinVLenB)\n      Known.One.setBit(Log2_32(MinVLenB));\n    break;\n  }\n  case RISCVISD::FCLASS: {\n    // fclass will only set one of the low 10 bits.\n    Known.Zero.setBitsFrom(10);\n    break;\n  }\n  case ISD::INTRINSIC_W_CHAIN:\n  case ISD::INTRINSIC_WO_CHAIN: {\n    unsigned IntNo =\n        Op.getConstantOperandVal(Opc == ISD::INTRINSIC_WO_CHAIN ? 0 : 1);\n    switch (IntNo) {\n    default:\n      // We can't do anything for most intrinsics.\n      break;\n    case Intrinsic::riscv_vsetvli:\n    case Intrinsic::riscv_vsetvlimax: {\n      bool HasAVL = IntNo == Intrinsic::riscv_vsetvli;\n      unsigned VSEW = Op.getConstantOperandVal(HasAVL + 1);\n      RISCVII::VLMUL VLMUL =\n          static_cast<RISCVII::VLMUL>(Op.getConstantOperandVal(HasAVL + 2));\n      unsigned SEW = RISCVVType::decodeVSEW(VSEW);\n      auto [LMul, Fractional] = RISCVVType::decodeVLMUL(VLMUL);\n      uint64_t MaxVL = Subtarget.getRealMaxVLen() / SEW;\n      MaxVL = (Fractional) ? MaxVL / LMul : MaxVL * LMul;\n\n      // Result of vsetvli must be not larger than AVL.\n      if (HasAVL && isa<ConstantSDNode>(Op.getOperand(1)))\n        MaxVL = std::min(MaxVL, Op.getConstantOperandVal(1));\n\n      unsigned KnownZeroFirstBit = Log2_32(MaxVL) + 1;\n      if (BitWidth > KnownZeroFirstBit)\n        Known.Zero.setBitsFrom(KnownZeroFirstBit);\n      break;\n    }\n    }\n    break;\n  }\n  }\n}",
      "start_line": 16271,
      "end_line": 16410,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "computeKnownBitsForTargetNode",
          "condition": "Opc",
          "cases": [
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "computeKnownBitsForTargetNode",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "computeGREVOrGORC",
        "getRealMaxVLen",
        "getRealMinVLen",
        "clearAllBits",
        "urem",
        "computeKnownBits",
        "decodeVLMUL",
        "decodeVSEW",
        "trunc",
        "Log2_32",
        "getConstantOperandVal",
        "min",
        "countMaxLeadingZeros",
        "resetAll",
        "shl",
        "udiv",
        "getBitWidth",
        "zext",
        "setLowBits",
        "getOpcode",
        "sext",
        "intersectWith",
        "bit_width",
        "setBit",
        "setBitsFrom",
        "countMaxTrailingZeros"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "ComputeNumSignBitsForTargetNode",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "const APInt",
          "name": "&DemandedElts"
        },
        {
          "type": "const SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "unsigned",
          "name": "Depth"
        }
      ],
      "body": "{\n  switch (Op.getOpcode()) {\n  default:\n    break;\n  case RISCVISD::SELECT_CC: {\n    unsigned Tmp =\n        DAG.ComputeNumSignBits(Op.getOperand(3), DemandedElts, Depth + 1);\n    if (Tmp == 1) return 1;  // Early out.\n    unsigned Tmp2 =\n        DAG.ComputeNumSignBits(Op.getOperand(4), DemandedElts, Depth + 1);\n    return std::min(Tmp, Tmp2);\n  }\n  case RISCVISD::CZERO_EQZ:\n  case RISCVISD::CZERO_NEZ:\n    // Output is either all zero or operand 0. We can propagate sign bit count\n    // from operand 0.\n    return DAG.ComputeNumSignBits(Op.getOperand(0), DemandedElts, Depth + 1);\n  case RISCVISD::ABSW: {\n    // We expand this at isel to negw+max. The result will have 33 sign bits\n    // if the input has at least 33 sign bits.\n    unsigned Tmp =\n        DAG.ComputeNumSignBits(Op.getOperand(0), DemandedElts, Depth + 1);\n    if (Tmp < 33) return 1;\n    return 33;\n  }\n  case RISCVISD::SLLW:\n  case RISCVISD::SRAW:\n  case RISCVISD::SRLW:\n  case RISCVISD::DIVW:\n  case RISCVISD::DIVUW:\n  case RISCVISD::REMUW:\n  case RISCVISD::ROLW:\n  case RISCVISD::RORW:\n  case RISCVISD::FCVT_W_RV64:\n  case RISCVISD::FCVT_WU_RV64:\n  case RISCVISD::STRICT_FCVT_W_RV64:\n  case RISCVISD::STRICT_FCVT_WU_RV64:\n    // TODO: As the result is sign-extended, this is conservatively correct. A\n    // more precise answer could be calculated for SRAW depending on known\n    // bits in the shift amount.\n    return 33;\n  case RISCVISD::VMV_X_S: {\n    // The number of sign bits of the scalar result is computed by obtaining the\n    // element type of the input vector operand, subtracting its width from the\n    // XLEN, and then adding one (sign bit within the element type). If the\n    // element type is wider than XLen, the least-significant XLEN bits are\n    // taken.\n    unsigned XLen = Subtarget.getXLen();\n    unsigned EltBits = Op.getOperand(0).getScalarValueSizeInBits();\n    if (EltBits <= XLen)\n      return XLen - EltBits + 1;\n    break;\n  }\n  case ISD::INTRINSIC_W_CHAIN: {\n    unsigned IntNo = Op.getConstantOperandVal(1);\n    switch (IntNo) {\n    default:\n      break;\n    case Intrinsic::riscv_masked_atomicrmw_xchg_i64:\n    case Intrinsic::riscv_masked_atomicrmw_add_i64:\n    case Intrinsic::riscv_masked_atomicrmw_sub_i64:\n    case Intrinsic::riscv_masked_atomicrmw_nand_i64:\n    case Intrinsic::riscv_masked_atomicrmw_max_i64:\n    case Intrinsic::riscv_masked_atomicrmw_min_i64:\n    case Intrinsic::riscv_masked_atomicrmw_umax_i64:\n    case Intrinsic::riscv_masked_atomicrmw_umin_i64:\n    case Intrinsic::riscv_masked_cmpxchg_i64:\n      // riscv_masked_{atomicrmw_*,cmpxchg} intrinsics represent an emulated\n      // narrow atomic operation. These are implemented using atomic\n      // operations at the minimum supported atomicrmw/cmpxchg width whose\n      // result is then sign extended to XLEN. With +A, the minimum width is\n      // 32 for both 64 and 32.\n      assert(Subtarget.getXLen() == 64);\n      assert(getMinCmpXchgSizeInBits() == 32);\n      assert(Subtarget.hasStdExtA());\n      return 33;\n    }\n    break;\n  }\n  }\n\n  return 1;\n}",
      "start_line": 16412,
      "end_line": 16496,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "ComputeNumSignBitsForTargetNode",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getConstantOperandVal",
        "min",
        "getOperand",
        "ComputeNumSignBits",
        "getXLen",
        "one",
        "getScalarValueSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTargetConstantFromLoad",
      "return_type": "Constant *",
      "parameters": [
        {
          "type": "LoadSDNode",
          "name": "*Ld"
        }
      ],
      "body": "{\n  assert(Ld && \"Unexpected null LoadSDNode\");\n  if (!ISD::isNormalLoad(Ld))\n    return nullptr;\n\n  SDValue Ptr = Ld->getBasePtr();\n\n  // Only constant pools with no offset are supported.\n  auto GetSupportedConstantPool = [](SDValue Ptr) -> ConstantPoolSDNode * {\n    auto *CNode = dyn_cast<ConstantPoolSDNode>(Ptr);\n    if (!CNode || CNode->isMachineConstantPoolEntry() ||\n        CNode->getOffset() != 0)\n      return nullptr;\n\n    return CNode;\n  };\n\n  // Simple case, LLA.\n  if (Ptr.getOpcode() == RISCVISD::LLA) {\n    auto *CNode = GetSupportedConstantPool(Ptr);\n    if (!CNode || CNode->getTargetFlags() != 0)\n      return nullptr;\n\n    return CNode->getConstVal();\n  }\n\n  // Look for a HI and ADD_LO pair.\n  if (Ptr.getOpcode() != RISCVISD::ADD_LO ||\n      Ptr.getOperand(0).getOpcode() != RISCVISD::HI)\n    return nullptr;\n\n  auto *CNodeLo = GetSupportedConstantPool(Ptr.getOperand(1));\n  auto *CNodeHi = GetSupportedConstantPool(Ptr.getOperand(0).getOperand(0));\n\n  if (!CNodeLo || CNodeLo->getTargetFlags() != RISCVII::MO_LO ||\n      !CNodeHi || CNodeHi->getTargetFlags() != RISCVII::MO_HI)\n    return nullptr;\n\n  if (CNodeLo->getConstVal() != CNodeHi->getConstVal())\n    return nullptr;\n\n  return CNodeLo->getConstVal();\n}",
      "start_line": 16498,
      "end_line": 16541,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getBasePtr",
        "getOffset",
        "getOpcode",
        "getOperand",
        "getConstVal",
        "GetSupportedConstantPool",
        "getTargetFlags"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isSelectPseudo",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MachineInstr",
          "name": "&MI"
        }
      ],
      "body": "{\n  switch (MI.getOpcode()) {\n  default:\n    return false;\n  case RISCV::Select_GPR_Using_CC_GPR:\n  case RISCV::Select_FPR16_Using_CC_GPR:\n  case RISCV::Select_FPR16INX_Using_CC_GPR:\n  case RISCV::Select_FPR32_Using_CC_GPR:\n  case RISCV::Select_FPR32INX_Using_CC_GPR:\n  case RISCV::Select_FPR64_Using_CC_GPR:\n  case RISCV::Select_FPR64INX_Using_CC_GPR:\n  case RISCV::Select_FPR64IN32X_Using_CC_GPR:\n    return true;\n  }\n}",
      "start_line": 16684,
      "end_line": 16698,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "EmitLoweredCascadedSelect",
      "return_type": "MachineBasicBlock *",
      "parameters": [
        {
          "type": "MachineInstr",
          "name": "&First"
        },
        {
          "type": "MachineInstr",
          "name": "&Second"
        },
        {
          "type": "MachineBasicBlock",
          "name": "*ThisMBB"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  // Select_FPRX_ (rs1, rs2, imm, rs4, (Select_FPRX_ rs1, rs2, imm, rs4, rs5)\n  // Without this, custom-inserter would have generated:\n  //\n  //   A\n  //   | \\\n  //   |  B\n  //   | /\n  //   C\n  //   | \\\n  //   |  D\n  //   | /\n  //   E\n  //\n  // A: X = ...; Y = ...\n  // B: empty\n  // C: Z = PHI [X, A], [Y, B]\n  // D: empty\n  // E: PHI [X, C], [Z, D]\n  //\n  // If we lower both Select_FPRX_ in a single step, we can instead generate:\n  //\n  //   A\n  //   | \\\n  //   |  C\n  //   | /|\n  //   |/ |\n  //   |  |\n  //   |  D\n  //   | /\n  //   E\n  //\n  // A: X = ...; Y = ...\n  // D: empty\n  // E: PHI [X, A], [X, C], [Y, D]\n\n  const RISCVInstrInfo &TII = *Subtarget.getInstrInfo();\n  const DebugLoc &DL = First.getDebugLoc();\n  const BasicBlock *LLVM_BB = ThisMBB->getBasicBlock();\n  MachineFunction *F = ThisMBB->getParent();\n  MachineBasicBlock *FirstMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *SecondMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *SinkMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  MachineFunction::iterator It = ++ThisMBB->getIterator();\n  F->insert(It, FirstMBB);\n  F->insert(It, SecondMBB);\n  F->insert(It, SinkMBB);\n\n  // Transfer the remainder of ThisMBB and its successor edges to SinkMBB.\n  SinkMBB->splice(SinkMBB->begin(), ThisMBB,\n                  std::next(MachineBasicBlock::iterator(First)),\n                  ThisMBB->end());\n  SinkMBB->transferSuccessorsAndUpdatePHIs(ThisMBB);\n\n  // Fallthrough block for ThisMBB.\n  ThisMBB->addSuccessor(FirstMBB);\n  // Fallthrough block for FirstMBB.\n  FirstMBB->addSuccessor(SecondMBB);\n  ThisMBB->addSuccessor(SinkMBB);\n  FirstMBB->addSuccessor(SinkMBB);\n  // This is fallthrough.\n  SecondMBB->addSuccessor(SinkMBB);\n\n  auto FirstCC = static_cast<RISCVCC::CondCode>(First.getOperand(3).getImm());\n  Register FLHS = First.getOperand(1).getReg();\n  Register FRHS = First.getOperand(2).getReg();\n  // Insert appropriate branch.\n  BuildMI(FirstMBB, DL, TII.getBrCond(FirstCC))\n      .addReg(FLHS)\n      .addReg(FRHS)\n      .addMBB(SinkMBB);\n\n  Register SLHS = Second.getOperand(1).getReg();\n  Register SRHS = Second.getOperand(2).getReg();\n  Register Op1Reg4 = First.getOperand(4).getReg();\n  Register Op1Reg5 = First.getOperand(5).getReg();\n\n  auto SecondCC = static_cast<RISCVCC::CondCode>(Second.getOperand(3).getImm());\n  // Insert appropriate branch.\n  BuildMI(ThisMBB, DL, TII.getBrCond(SecondCC))\n      .addReg(SLHS)\n      .addReg(SRHS)\n      .addMBB(SinkMBB);\n\n  Register DestReg = Second.getOperand(0).getReg();\n  Register Op2Reg4 = Second.getOperand(4).getReg();\n  BuildMI(*SinkMBB, SinkMBB->begin(), DL, TII.get(RISCV::PHI), DestReg)\n      .addReg(Op2Reg4)\n      .addMBB(ThisMBB)\n      .addReg(Op1Reg4)\n      .addMBB(FirstMBB)\n      .addReg(Op1Reg5)\n      .addMBB(SecondMBB);\n\n  // Now remove the Select_FPRX_s.\n  First.eraseFromParent();\n  Second.eraseFromParent();\n  return SinkMBB;\n}",
      "start_line": 16736,
      "end_line": 16837,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "Select_FPRX_",
        "addSuccessor",
        "getParent",
        "getIterator",
        "BuildMI",
        "getImm",
        "splice",
        "end",
        "addMBB",
        "eraseFromParent",
        "addReg",
        "getDebugLoc",
        "getOperand",
        "next",
        "insert",
        "getInstrInfo",
        "transferSuccessorsAndUpdatePHIs",
        "getBasicBlock",
        "get",
        "CreateMachineBasicBlock",
        "getReg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "EmitInstrWithCustomInserter",
      "return_type": "MachineBasicBlock *",
      "parameters": [
        {
          "type": "MachineInstr",
          "name": "&MI"
        },
        {
          "type": "MachineBasicBlock",
          "name": "*BB"
        }
      ],
      "body": "{\n  switch (MI.getOpcode()) {\n  default:\n    llvm_unreachable(\"Unexpected instr type to insert\");\n  case RISCV::ReadCycleWide:\n    assert(!Subtarget.is64Bit() &&\n           \"ReadCycleWrite is only to be used on riscv32\");\n    return emitReadCycleWidePseudo(MI, BB);\n  case RISCV::Select_GPR_Using_CC_GPR:\n  case RISCV::Select_FPR16_Using_CC_GPR:\n  case RISCV::Select_FPR16INX_Using_CC_GPR:\n  case RISCV::Select_FPR32_Using_CC_GPR:\n  case RISCV::Select_FPR32INX_Using_CC_GPR:\n  case RISCV::Select_FPR64_Using_CC_GPR:\n  case RISCV::Select_FPR64INX_Using_CC_GPR:\n  case RISCV::Select_FPR64IN32X_Using_CC_GPR:\n    return emitSelectPseudo(MI, BB, Subtarget);\n  case RISCV::BuildPairF64Pseudo:\n  case RISCV::BuildPairF64Pseudo_INX:\n    return emitBuildPairF64Pseudo(MI, BB, Subtarget);\n  case RISCV::SplitF64Pseudo:\n  case RISCV::SplitF64Pseudo_INX:\n    return emitSplitF64Pseudo(MI, BB, Subtarget);\n  case RISCV::PseudoQuietFLE_H:\n    return emitQuietFCMP(MI, BB, RISCV::FLE_H, RISCV::FEQ_H, Subtarget);\n  case RISCV::PseudoQuietFLE_H_INX:\n    return emitQuietFCMP(MI, BB, RISCV::FLE_H_INX, RISCV::FEQ_H_INX, Subtarget);\n  case RISCV::PseudoQuietFLT_H:\n    return emitQuietFCMP(MI, BB, RISCV::FLT_H, RISCV::FEQ_H, Subtarget);\n  case RISCV::PseudoQuietFLT_H_INX:\n    return emitQuietFCMP(MI, BB, RISCV::FLT_H_INX, RISCV::FEQ_H_INX, Subtarget);\n  case RISCV::PseudoQuietFLE_S:\n    return emitQuietFCMP(MI, BB, RISCV::FLE_S, RISCV::FEQ_S, Subtarget);\n  case RISCV::PseudoQuietFLE_S_INX:\n    return emitQuietFCMP(MI, BB, RISCV::FLE_S_INX, RISCV::FEQ_S_INX, Subtarget);\n  case RISCV::PseudoQuietFLT_S:\n    return emitQuietFCMP(MI, BB, RISCV::FLT_S, RISCV::FEQ_S, Subtarget);\n  case RISCV::PseudoQuietFLT_S_INX:\n    return emitQuietFCMP(MI, BB, RISCV::FLT_S_INX, RISCV::FEQ_S_INX, Subtarget);\n  case RISCV::PseudoQuietFLE_D:\n    return emitQuietFCMP(MI, BB, RISCV::FLE_D, RISCV::FEQ_D, Subtarget);\n  case RISCV::PseudoQuietFLE_D_INX:\n    return emitQuietFCMP(MI, BB, RISCV::FLE_D_INX, RISCV::FEQ_D_INX, Subtarget);\n  case RISCV::PseudoQuietFLE_D_IN32X:\n    return emitQuietFCMP(MI, BB, RISCV::FLE_D_IN32X, RISCV::FEQ_D_IN32X,\n                         Subtarget);\n  case RISCV::PseudoQuietFLT_D:\n    return emitQuietFCMP(MI, BB, RISCV::FLT_D, RISCV::FEQ_D, Subtarget);\n  case RISCV::PseudoQuietFLT_D_INX:\n    return emitQuietFCMP(MI, BB, RISCV::FLT_D_INX, RISCV::FEQ_D_INX, Subtarget);\n  case RISCV::PseudoQuietFLT_D_IN32X:\n    return emitQuietFCMP(MI, BB, RISCV::FLT_D_IN32X, RISCV::FEQ_D_IN32X,\n                         Subtarget);\n\n  case RISCV::PseudoVFROUND_NOEXCEPT_V_M1_MASK:\n    return emitVFROUND_NOEXCEPT_MASK(MI, BB, RISCV::PseudoVFCVT_X_F_V_M1_MASK,\n                                     RISCV::PseudoVFCVT_F_X_V_M1_MASK);\n  case RISCV::PseudoVFROUND_NOEXCEPT_V_M2_MASK:\n    return emitVFROUND_NOEXCEPT_MASK(MI, BB, RISCV::PseudoVFCVT_X_F_V_M2_MASK,\n                                     RISCV::PseudoVFCVT_F_X_V_M2_MASK);\n  case RISCV::PseudoVFROUND_NOEXCEPT_V_M4_MASK:\n    return emitVFROUND_NOEXCEPT_MASK(MI, BB, RISCV::PseudoVFCVT_X_F_V_M4_MASK,\n                                     RISCV::PseudoVFCVT_F_X_V_M4_MASK);\n  case RISCV::PseudoVFROUND_NOEXCEPT_V_M8_MASK:\n    return emitVFROUND_NOEXCEPT_MASK(MI, BB, RISCV::PseudoVFCVT_X_F_V_M8_MASK,\n                                     RISCV::PseudoVFCVT_F_X_V_M8_MASK);\n  case RISCV::PseudoVFROUND_NOEXCEPT_V_MF2_MASK:\n    return emitVFROUND_NOEXCEPT_MASK(MI, BB, RISCV::PseudoVFCVT_X_F_V_MF2_MASK,\n                                     RISCV::PseudoVFCVT_F_X_V_MF2_MASK);\n  case RISCV::PseudoVFROUND_NOEXCEPT_V_MF4_MASK:\n    return emitVFROUND_NOEXCEPT_MASK(MI, BB, RISCV::PseudoVFCVT_X_F_V_MF4_MASK,\n                                     RISCV::PseudoVFCVT_F_X_V_MF4_MASK);\n  case RISCV::PseudoFROUND_H:\n  case RISCV::PseudoFROUND_H_INX:\n  case RISCV::PseudoFROUND_S:\n  case RISCV::PseudoFROUND_S_INX:\n  case RISCV::PseudoFROUND_D:\n  case RISCV::PseudoFROUND_D_INX:\n  case RISCV::PseudoFROUND_D_IN32X:\n    return emitFROUND(MI, BB, Subtarget);\n  case TargetOpcode::STATEPOINT:\n  case TargetOpcode::STACKMAP:\n  case TargetOpcode::PATCHPOINT:\n    if (!Subtarget.is64Bit())\n      report_fatal_error(\"STACKMAP, PATCHPOINT and STATEPOINT are only \"\n                         \"supported on 64-bit targets\");\n    return emitPatchPoint(MI, BB);\n  }\n}",
      "start_line": 17162,
      "end_line": 17252,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "emitSplitF64Pseudo",
        "emitBuildPairF64Pseudo",
        "emitFROUND",
        "emitReadCycleWidePseudo",
        "emitVFROUND_NOEXCEPT_MASK",
        "emitSelectPseudo",
        "emitQuietFCMP",
        "report_fatal_error",
        "emitPatchPoint",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "AdjustInstrPostInstrSelection",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineInstr",
          "name": "&MI"
        },
        {
          "type": "SDNode",
          "name": "*Node"
        }
      ],
      "body": "{\n  // Add FRM dependency to any instructions with dynamic rounding mode.\n  int Idx = RISCV::getNamedOperandIdx(MI.getOpcode(), RISCV::OpName::frm);\n  if (Idx < 0) {\n    // Vector pseudos have FRM index indicated by TSFlags.\n    Idx = RISCVII::getFRMOpNum(MI.getDesc());\n    if (Idx < 0)\n      return;\n  }\n  if (MI.getOperand(Idx).getImm() != RISCVFPRndMode::DYN)\n    return;\n  // If the instruction already reads FRM, don't add another read.\n  if (MI.readsRegister(RISCV::FRM))\n    return;\n  MI.addOperand(\n      MachineOperand::CreateReg(RISCV::FRM, /*isDef*/ false, /*isImp*/ true));\n}",
      "start_line": 17254,
      "end_line": 17271,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "addOperand",
        "getFRMOpNum",
        "getImm",
        "getNamedOperandIdx"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "CC_RISCVAssign2XLen",
      "return_type": "bool",
      "parameters": [
        {
          "type": "unsigned",
          "name": "XLen"
        },
        {
          "type": "CCState",
          "name": "&State"
        },
        {
          "type": "CCValAssign",
          "name": "VA1"
        },
        {
          "type": "ISD::ArgFlagsTy",
          "name": "ArgFlags1"
        },
        {
          "type": "unsigned",
          "name": "ValNo2"
        },
        {
          "type": "MVT",
          "name": "ValVT2"
        },
        {
          "type": "MVT",
          "name": "LocVT2"
        },
        {
          "type": "ISD::ArgFlagsTy",
          "name": "ArgFlags2"
        },
        {
          "type": "bool",
          "name": "EABI"
        }
      ],
      "body": "{\n  unsigned XLenInBytes = XLen / 8;\n  const RISCVSubtarget &STI =\n      State.getMachineFunction().getSubtarget<RISCVSubtarget>();\n  ArrayRef<MCPhysReg> ArgGPRs = RISCV::getArgGPRs(STI.getTargetABI());\n\n  if (Register Reg = State.AllocateReg(ArgGPRs)) {\n    // At least one half can be passed via register.\n    State.addLoc(CCValAssign::getReg(VA1.getValNo(), VA1.getValVT(), Reg,\n                                     VA1.getLocVT(), CCValAssign::Full));\n  } else {\n    // Both halves must be passed on the stack, with proper alignment.\n    // TODO: To be compatible with GCC's behaviors, we force them to have 4-byte\n    // alignment. This behavior may be changed when RV32E/ILP32E is ratified.\n    Align StackAlign(XLenInBytes);\n    if (!EABI || XLen != 32)\n      StackAlign = std::max(StackAlign, ArgFlags1.getNonZeroOrigAlign());\n    State.addLoc(\n        CCValAssign::getMem(VA1.getValNo(), VA1.getValVT(),\n                            State.AllocateStack(XLenInBytes, StackAlign),\n                            VA1.getLocVT(), CCValAssign::Full));\n    State.addLoc(CCValAssign::getMem(\n        ValNo2, ValVT2, State.AllocateStack(XLenInBytes, Align(XLenInBytes)),\n        LocVT2, CCValAssign::Full));\n    return false;\n  }\n\n  if (Register Reg = State.AllocateReg(ArgGPRs)) {\n    // The second half can also be passed via register.\n    State.addLoc(\n        CCValAssign::getReg(ValNo2, ValVT2, Reg, LocVT2, CCValAssign::Full));\n  } else {\n    // The second half is passed via the stack, without additional alignment.\n    State.addLoc(CCValAssign::getMem(\n        ValNo2, ValVT2, State.AllocateStack(XLenInBytes, Align(XLenInBytes)),\n        LocVT2, CCValAssign::Full));\n  }\n\n  return false;\n}",
      "start_line": 17359,
      "end_line": 17401,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getArgGPRs",
        "getMachineFunction",
        "getLocVT",
        "StackAlign",
        "AllocateStack",
        "getValVT",
        "max",
        "addLoc"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "allocateRVVReg",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "MVT",
          "name": "ValVT"
        },
        {
          "type": "unsigned",
          "name": "ValNo"
        },
        {
          "type": "std::optional<unsigned>",
          "name": "FirstMaskArgument"
        },
        {
          "type": "CCState",
          "name": "&State"
        },
        {
          "type": "const RISCVTargetLowering",
          "name": "&TLI"
        }
      ],
      "body": "{\n  const TargetRegisterClass *RC = TLI.getRegClassFor(ValVT);\n  if (RC == &RISCV::VRRegClass) {\n    // Assign the first mask argument to V0.\n    // This is an interim calling convention and it may be changed in the\n    // future.\n    if (FirstMaskArgument && ValNo == *FirstMaskArgument)\n      return State.AllocateReg(RISCV::V0);\n    return State.AllocateReg(ArgVRs);\n  }\n  if (RC == &RISCV::VRM2RegClass)\n    return State.AllocateReg(ArgVRM2s);\n  if (RC == &RISCV::VRM4RegClass)\n    return State.AllocateReg(ArgVRM4s);\n  if (RC == &RISCV::VRM8RegClass)\n    return State.AllocateReg(ArgVRM8s);\n  llvm_unreachable(\"Unhandled register class for ValueType\");\n}",
      "start_line": 17403,
      "end_line": 17422,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getRegClassFor",
        "AllocateReg",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "CC_RISCV",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const DataLayout",
          "name": "&DL"
        },
        {
          "type": "RISCVABI::ABI",
          "name": "ABI"
        },
        {
          "type": "unsigned",
          "name": "ValNo"
        },
        {
          "type": "MVT",
          "name": "ValVT"
        },
        {
          "type": "MVT",
          "name": "LocVT"
        },
        {
          "type": "CCValAssign::LocInfo",
          "name": "LocInfo"
        },
        {
          "type": "ISD::ArgFlagsTy",
          "name": "ArgFlags"
        },
        {
          "type": "CCState",
          "name": "&State"
        },
        {
          "type": "bool",
          "name": "IsFixed"
        },
        {
          "type": "bool",
          "name": "IsRet"
        },
        {
          "type": "Type",
          "name": "*OrigTy"
        },
        {
          "type": "const RISCVTargetLowering",
          "name": "&TLI"
        },
        {
          "type": "std::optional<unsigned>",
          "name": "FirstMaskArgument"
        }
      ],
      "body": "{\n  unsigned XLen = DL.getLargestLegalIntTypeSizeInBits();\n  assert(XLen == 32 || XLen == 64);\n  MVT XLenVT = XLen == 32 ? MVT::i32 : MVT::i64;\n\n  // Static chain parameter must not be passed in normal argument registers,\n  // so we assign t2 for it as done in GCC's __builtin_call_with_static_chain\n  if (ArgFlags.isNest()) {\n    if (unsigned Reg = State.AllocateReg(RISCV::X7)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  // Any return value split in to more than two values can't be returned\n  // directly. Vectors are returned via the available vector registers.\n  if (!LocVT.isVector() && IsRet && ValNo > 1)\n    return true;\n\n  // UseGPRForF16_F32 if targeting one of the soft-float ABIs, if passing a\n  // variadic argument, or if no F16/F32 argument registers are available.\n  bool UseGPRForF16_F32 = true;\n  // UseGPRForF64 if targeting soft-float ABIs or an FLEN=32 ABI, if passing a\n  // variadic argument, or if no F64 argument registers are available.\n  bool UseGPRForF64 = true;\n\n  switch (ABI) {\n  default:\n    llvm_unreachable(\"Unexpected ABI\");\n  case RISCVABI::ABI_ILP32:\n  case RISCVABI::ABI_ILP32E:\n  case RISCVABI::ABI_LP64:\n  case RISCVABI::ABI_LP64E:\n    break;\n  case RISCVABI::ABI_ILP32F:\n  case RISCVABI::ABI_LP64F:\n    UseGPRForF16_F32 = !IsFixed;\n    break;\n  case RISCVABI::ABI_ILP32D:\n  case RISCVABI::ABI_LP64D:\n    UseGPRForF16_F32 = !IsFixed;\n    UseGPRForF64 = !IsFixed;\n    break;\n  }\n\n  // FPR16, FPR32, and FPR64 alias each other.\n  if (State.getFirstUnallocated(ArgFPR32s) == std::size(ArgFPR32s)) {\n    UseGPRForF16_F32 = true;\n    UseGPRForF64 = true;\n  }\n\n  // From this point on, rely on UseGPRForF16_F32, UseGPRForF64 and\n  // similar local variables rather than directly checking against the target\n  // ABI.\n\n  if (UseGPRForF16_F32 &&\n      (ValVT == MVT::f16 || ValVT == MVT::bf16 || ValVT == MVT::f32)) {\n    LocVT = XLenVT;\n    LocInfo = CCValAssign::BCvt;\n  } else if (UseGPRForF64 && XLen == 64 && ValVT == MVT::f64) {\n    LocVT = MVT::i64;\n    LocInfo = CCValAssign::BCvt;\n  }\n\n  ArrayRef<MCPhysReg> ArgGPRs = RISCV::getArgGPRs(ABI);\n\n  // If this is a variadic argument, the RISC-V calling convention requires\n  // that it is assigned an 'even' or 'aligned' register if it has 8-byte\n  // alignment (RV32) or 16-byte alignment (RV64). An aligned register should\n  // be used regardless of whether the original argument was split during\n  // legalisation or not. The argument will not be passed by registers if the\n  // original type is larger than 2*XLEN, so the register alignment rule does\n  // not apply.\n  // TODO: To be compatible with GCC's behaviors, we don't align registers\n  // currently if we are using ILP32E calling convention. This behavior may be\n  // changed when RV32E/ILP32E is ratified.\n  unsigned TwoXLenInBytes = (2 * XLen) / 8;\n  if (!IsFixed && ArgFlags.getNonZeroOrigAlign() == TwoXLenInBytes &&\n      DL.getTypeAllocSize(OrigTy) == TwoXLenInBytes &&\n      ABI != RISCVABI::ABI_ILP32E) {\n    unsigned RegIdx = State.getFirstUnallocated(ArgGPRs);\n    // Skip 'odd' register if necessary.\n    if (RegIdx != std::size(ArgGPRs) && RegIdx % 2 == 1)\n      State.AllocateReg(ArgGPRs);\n  }\n\n  SmallVectorImpl<CCValAssign> &PendingLocs = State.getPendingLocs();\n  SmallVectorImpl<ISD::ArgFlagsTy> &PendingArgFlags =\n      State.getPendingArgFlags();\n\n  assert(PendingLocs.size() == PendingArgFlags.size() &&\n         \"PendingLocs and PendingArgFlags out of sync\");\n\n  // Handle passing f64 on RV32D with a soft float ABI or when floating point\n  // registers are exhausted.\n  if (UseGPRForF64 && XLen == 32 && ValVT == MVT::f64) {\n    assert(PendingLocs.empty() && \"Can't lower f64 if it is split\");\n    // Depending on available argument GPRS, f64 may be passed in a pair of\n    // GPRs, split between a GPR and the stack, or passed completely on the\n    // stack. LowerCall/LowerFormalArguments/LowerReturn must recognise these\n    // cases.\n    Register Reg = State.AllocateReg(ArgGPRs);\n    if (!Reg) {\n      unsigned StackOffset = State.AllocateStack(8, Align(8));\n      State.addLoc(\n          CCValAssign::getMem(ValNo, ValVT, StackOffset, LocVT, LocInfo));\n      return false;\n    }\n    LocVT = MVT::i32;\n    State.addLoc(CCValAssign::getCustomReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n    Register HiReg = State.AllocateReg(ArgGPRs);\n    if (HiReg) {\n      State.addLoc(\n          CCValAssign::getCustomReg(ValNo, ValVT, HiReg, LocVT, LocInfo));\n    } else {\n      unsigned StackOffset = State.AllocateStack(4, Align(4));\n      State.addLoc(\n          CCValAssign::getCustomMem(ValNo, ValVT, StackOffset, LocVT, LocInfo));\n    }\n    return false;\n  }\n\n  // Fixed-length vectors are located in the corresponding scalable-vector\n  // container types.\n  if (ValVT.isFixedLengthVector())\n    LocVT = TLI.getContainerForFixedLengthVector(LocVT);\n\n  // Split arguments might be passed indirectly, so keep track of the pending\n  // values. Split vectors are passed via a mix of registers and indirectly, so\n  // treat them as we would any other argument.\n  if (ValVT.isScalarInteger() && (ArgFlags.isSplit() || !PendingLocs.empty())) {\n    LocVT = XLenVT;\n    LocInfo = CCValAssign::Indirect;\n    PendingLocs.push_back(\n        CCValAssign::getPending(ValNo, ValVT, LocVT, LocInfo));\n    PendingArgFlags.push_back(ArgFlags);\n    if (!ArgFlags.isSplitEnd()) {\n      return false;\n    }\n  }\n\n  // If the split argument only had two elements, it should be passed directly\n  // in registers or on the stack.\n  if (ValVT.isScalarInteger() && ArgFlags.isSplitEnd() &&\n      PendingLocs.size() <= 2) {\n    assert(PendingLocs.size() == 2 && \"Unexpected PendingLocs.size()\");\n    // Apply the normal calling convention rules to the first half of the\n    // split argument.\n    CCValAssign VA = PendingLocs[0];\n    ISD::ArgFlagsTy AF = PendingArgFlags[0];\n    PendingLocs.clear();\n    PendingArgFlags.clear();\n    return CC_RISCVAssign2XLen(\n        XLen, State, VA, AF, ValNo, ValVT, LocVT, ArgFlags,\n        ABI == RISCVABI::ABI_ILP32E || ABI == RISCVABI::ABI_LP64E);\n  }\n\n  // Allocate to a register if possible, or else a stack slot.\n  Register Reg;\n  unsigned StoreSizeBytes = XLen / 8;\n  Align StackAlign = Align(XLen / 8);\n\n  if ((ValVT == MVT::f16 || ValVT == MVT::bf16) && !UseGPRForF16_F32)\n    Reg = State.AllocateReg(ArgFPR16s);\n  else if (ValVT == MVT::f32 && !UseGPRForF16_F32)\n    Reg = State.AllocateReg(ArgFPR32s);\n  else if (ValVT == MVT::f64 && !UseGPRForF64)\n    Reg = State.AllocateReg(ArgFPR64s);\n  else if (ValVT.isVector()) {\n    Reg = allocateRVVReg(ValVT, ValNo, FirstMaskArgument, State, TLI);\n    if (!Reg) {\n      // For return values, the vector must be passed fully via registers or\n      // via the stack.\n      // FIXME: The proposed vector ABI only mandates v8-v15 for return values,\n      // but we're using all of them.\n      if (IsRet)\n        return true;\n      // Try using a GPR to pass the address\n      if ((Reg = State.AllocateReg(ArgGPRs))) {\n        LocVT = XLenVT;\n        LocInfo = CCValAssign::Indirect;\n      } else if (ValVT.isScalableVector()) {\n        LocVT = XLenVT;\n        LocInfo = CCValAssign::Indirect;\n      } else {\n        // Pass fixed-length vectors on the stack.\n        LocVT = ValVT;\n        StoreSizeBytes = ValVT.getStoreSize();\n        // Align vectors to their element sizes, being careful for vXi1\n        // vectors.\n        StackAlign = MaybeAlign(ValVT.getScalarSizeInBits() / 8).valueOrOne();\n      }\n    }\n  } else {\n    Reg = State.AllocateReg(ArgGPRs);\n  }\n\n  unsigned StackOffset =\n      Reg ? 0 : State.AllocateStack(StoreSizeBytes, StackAlign);\n\n  // If we reach this point and PendingLocs is non-empty, we must be at the\n  // end of a split argument that must be passed indirectly.\n  if (!PendingLocs.empty()) {\n    assert(ArgFlags.isSplitEnd() && \"Expected ArgFlags.isSplitEnd()\");\n    assert(PendingLocs.size() > 2 && \"Unexpected PendingLocs.size()\");\n\n    for (auto &It : PendingLocs) {\n      if (Reg)\n        It.convertToReg(Reg);\n      else\n        It.convertToMem(StackOffset);\n      State.addLoc(It);\n    }\n    PendingLocs.clear();\n    PendingArgFlags.clear();\n    return false;\n  }\n\n  assert((!UseGPRForF16_F32 || !UseGPRForF64 || LocVT == XLenVT ||\n          (TLI.getSubtarget().hasVInstructions() && ValVT.isVector())) &&\n         \"Expected an XLenVT or vector types at this stage\");\n\n  if (Reg) {\n    State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n    return false;\n  }\n\n  // When a scalar floating-point value is passed on the stack, no\n  // bit-conversion is needed.\n  if (ValVT.isFloatingPoint() && LocInfo != CCValAssign::Indirect) {\n    assert(!ValVT.isVector());\n    LocVT = ValVT;\n    LocInfo = CCValAssign::Full;\n  }\n  State.addLoc(CCValAssign::getMem(ValNo, ValVT, StackOffset, LocVT, LocInfo));\n  return false;\n}",
      "start_line": 17425,
      "end_line": 17665,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "CC_RISCV",
          "condition": "ABI",
          "cases": [
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVABI",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getArgGPRs",
        "hasVInstructions",
        "convertToReg",
        "MaybeAlign",
        "valueOrOne",
        "AllocateReg",
        "getFirstUnallocated",
        "isVector",
        "getPendingArgFlags",
        "size",
        "addLoc",
        "push_back",
        "isSplitEnd",
        "CC_RISCVAssign2XLen",
        "allocateRVVReg",
        "convertToMem",
        "getContainerForFixedLengthVector",
        "getTypeAllocSize",
        "getPendingLocs",
        "isSplit",
        "getStoreSize",
        "clear",
        "empty",
        "Align",
        "AllocateStack",
        "getLargestLegalIntTypeSizeInBits",
        "llvm_unreachable",
        "alignment"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "UseGPRForF64 && XLen == 64 && ValVT ==",
          "name": "MVT::f64"
        }
      ],
      "body": "{\n    LocVT = MVT::i64;\n    LocInfo = CCValAssign::BCvt;\n  }",
      "start_line": 17488,
      "end_line": 17491,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "analyzeInputArgs",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "CCState",
          "name": "&CCInfo"
        },
        {
          "type": "const SmallVectorImpl<ISD::InputArg>",
          "name": "&Ins"
        },
        {
          "type": "bool",
          "name": "IsRet"
        },
        {
          "type": "RISCVCCAssignFn",
          "name": "Fn"
        }
      ],
      "body": "{\n  unsigned NumArgs = Ins.size();\n  FunctionType *FType = MF.getFunction().getFunctionType();\n\n  std::optional<unsigned> FirstMaskArgument;\n  if (Subtarget.hasVInstructions())\n    FirstMaskArgument = preAssignMask(Ins);\n\n  for (unsigned i = 0; i != NumArgs; ++i) {\n    MVT ArgVT = Ins[i].VT;\n    ISD::ArgFlagsTy ArgFlags = Ins[i].Flags;\n\n    Type *ArgTy = nullptr;\n    if (IsRet)\n      ArgTy = FType->getReturnType();\n    else if (Ins[i].isOrigArg())\n      ArgTy = FType->getParamType(Ins[i].getOrigArgIndex());\n\n    RISCVABI::ABI ABI = MF.getSubtarget<RISCVSubtarget>().getTargetABI();\n    if (Fn(MF.getDataLayout(), ABI, i, ArgVT, ArgVT, CCValAssign::Full,\n           ArgFlags, CCInfo, /*IsFixed=*/true, IsRet, ArgTy, *this,\n           FirstMaskArgument)) {\n      LLVM_DEBUG(dbgs() << \"InputArg #\" << i << \" has unhandled type \"\n                        << ArgVT << '\\n');\n      llvm_unreachable(nullptr);\n    }\n  }\n}",
      "start_line": 17677,
      "end_line": 17707,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "LLVM_DEBUG",
        "getFunctionType",
        "getFunction",
        "getReturnType",
        "getParamType",
        "size",
        "llvm_unreachable",
        "preAssignMask",
        "getTargetABI"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "analyzeOutputArgs",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "CCState",
          "name": "&CCInfo"
        },
        {
          "type": "const SmallVectorImpl<ISD::OutputArg>",
          "name": "&Outs"
        },
        {
          "type": "bool",
          "name": "IsRet"
        },
        {
          "type": "CallLoweringInfo",
          "name": "*CLI"
        },
        {
          "type": "RISCVCCAssignFn",
          "name": "Fn"
        }
      ],
      "body": "{\n  unsigned NumArgs = Outs.size();\n\n  std::optional<unsigned> FirstMaskArgument;\n  if (Subtarget.hasVInstructions())\n    FirstMaskArgument = preAssignMask(Outs);\n\n  for (unsigned i = 0; i != NumArgs; i++) {\n    MVT ArgVT = Outs[i].VT;\n    ISD::ArgFlagsTy ArgFlags = Outs[i].Flags;\n    Type *OrigTy = CLI ? CLI->getArgs()[Outs[i].OrigArgIndex].Ty : nullptr;\n\n    RISCVABI::ABI ABI = MF.getSubtarget<RISCVSubtarget>().getTargetABI();\n    if (Fn(MF.getDataLayout(), ABI, i, ArgVT, ArgVT, CCValAssign::Full,\n           ArgFlags, CCInfo, Outs[i].IsFixed, IsRet, OrigTy, *this,\n           FirstMaskArgument)) {\n      LLVM_DEBUG(dbgs() << \"OutputArg #\" << i << \" has unhandled type \"\n                        << ArgVT << \"\\n\");\n      llvm_unreachable(nullptr);\n    }\n  }\n}",
      "start_line": 17709,
      "end_line": 17733,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getArgs",
        "LLVM_DEBUG",
        "size",
        "llvm_unreachable",
        "preAssignMask",
        "getTargetABI"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "convertLocVTToValVT",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SDValue",
          "name": "Val"
        },
        {
          "type": "const CCValAssign",
          "name": "&VA"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  switch (VA.getLocInfo()) {\n  default:\n    llvm_unreachable(\"Unexpected CCValAssign::LocInfo\");\n  case CCValAssign::Full:\n    if (VA.getValVT().isFixedLengthVector() && VA.getLocVT().isScalableVector())\n      Val = convertFromScalableVector(VA.getValVT(), Val, DAG, Subtarget);\n    break;\n  case CCValAssign::BCvt:\n    if (VA.getLocVT().isInteger() &&\n        (VA.getValVT() == MVT::f16 || VA.getValVT() == MVT::bf16)) {\n      Val = DAG.getNode(RISCVISD::FMV_H_X, DL, VA.getValVT(), Val);\n    } else if (VA.getLocVT() == MVT::i64 && VA.getValVT() == MVT::f32) {\n      if (RV64LegalI32) {\n        Val = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Val);\n        Val = DAG.getNode(ISD::BITCAST, DL, MVT::f32, Val);\n      } else {\n        Val = DAG.getNode(RISCVISD::FMV_W_X_RV64, DL, MVT::f32, Val);\n      }\n    } else {\n      Val = DAG.getNode(ISD::BITCAST, DL, VA.getValVT(), Val);\n    }\n    break;\n  }\n  return Val;\n}",
      "start_line": 17737,
      "end_line": 17764,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLocVT",
        "getNode",
        "convertFromScalableVector",
        "isInteger",
        "getValVT",
        "llvm_unreachable",
        "isScalableVector",
        "isFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "unpackFromRegLoc",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SDValue",
          "name": "Chain"
        },
        {
          "type": "const CCValAssign",
          "name": "&VA"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "const ISD::InputArg",
          "name": "&In"
        },
        {
          "type": "const RISCVTargetLowering",
          "name": "&TLI"
        }
      ],
      "body": "{\n  MachineFunction &MF = DAG.getMachineFunction();\n  MachineRegisterInfo &RegInfo = MF.getRegInfo();\n  EVT LocVT = VA.getLocVT();\n  SDValue Val;\n  const TargetRegisterClass *RC = TLI.getRegClassFor(LocVT.getSimpleVT());\n  Register VReg = RegInfo.createVirtualRegister(RC);\n  RegInfo.addLiveIn(VA.getLocReg(), VReg);\n  Val = DAG.getCopyFromReg(Chain, DL, VReg, LocVT);\n\n  // If input is sign extended from 32 bits, note it for the SExtWRemoval pass.\n  if (In.isOrigArg()) {\n    Argument *OrigArg = MF.getFunction().getArg(In.getOrigArgIndex());\n    if (OrigArg->getType()->isIntegerTy()) {\n      unsigned BitWidth = OrigArg->getType()->getIntegerBitWidth();\n      // An input zero extended from i31 can also be considered sign extended.\n      if ((BitWidth <= 32 && In.Flags.isSExt()) ||\n          (BitWidth < 32 && In.Flags.isZExt())) {\n        RISCVMachineFunctionInfo *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n        RVFI->addSExt32Register(VReg);\n      }\n    }\n  }\n\n  if (VA.getLocInfo() == CCValAssign::Indirect)\n    return Val;\n\n  return convertLocVTToValVT(DAG, Val, VA, DL, TLI.getSubtarget());\n}",
      "start_line": 17768,
      "end_line": 17799,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCopyFromReg",
        "isZExt",
        "getRegInfo",
        "createVirtualRegister",
        "getArg",
        "isIntegerTy",
        "addSExt32Register",
        "getMachineFunction",
        "getLocVT",
        "addLiveIn",
        "getType",
        "getRegClassFor",
        "getIntegerBitWidth",
        "getFunction",
        "convertLocVTToValVT"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "convertValVTToLocVT",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SDValue",
          "name": "Val"
        },
        {
          "type": "const CCValAssign",
          "name": "&VA"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  EVT LocVT = VA.getLocVT();\n\n  switch (VA.getLocInfo()) {\n  default:\n    llvm_unreachable(\"Unexpected CCValAssign::LocInfo\");\n  case CCValAssign::Full:\n    if (VA.getValVT().isFixedLengthVector() && LocVT.isScalableVector())\n      Val = convertToScalableVector(LocVT, Val, DAG, Subtarget);\n    break;\n  case CCValAssign::BCvt:\n    if (LocVT.isInteger() &&\n        (VA.getValVT() == MVT::f16 || VA.getValVT() == MVT::bf16)) {\n      Val = DAG.getNode(RISCVISD::FMV_X_ANYEXTH, DL, LocVT, Val);\n    } else if (LocVT == MVT::i64 && VA.getValVT() == MVT::f32) {\n      if (RV64LegalI32) {\n        Val = DAG.getNode(ISD::BITCAST, DL, MVT::i32, Val);\n        Val = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Val);\n      } else {\n        Val = DAG.getNode(RISCVISD::FMV_X_ANYEXTW_RV64, DL, MVT::i64, Val);\n      }\n    } else {\n      Val = DAG.getNode(ISD::BITCAST, DL, LocVT, Val);\n    }\n    break;\n  }\n  return Val;\n}",
      "start_line": 17801,
      "end_line": 17830,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLocVT",
        "getNode",
        "convertToScalableVector",
        "getValVT",
        "llvm_unreachable",
        "isScalableVector",
        "isFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "unpackFromMemLoc",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SDValue",
          "name": "Chain"
        },
        {
          "type": "const CCValAssign",
          "name": "&VA"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        }
      ],
      "body": "{\n  MachineFunction &MF = DAG.getMachineFunction();\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  EVT LocVT = VA.getLocVT();\n  EVT ValVT = VA.getValVT();\n  EVT PtrVT = MVT::getIntegerVT(DAG.getDataLayout().getPointerSizeInBits(0));\n  if (ValVT.isScalableVector()) {\n    // When the value is a scalable vector, we save the pointer which points to\n    // the scalable vector value in the stack. The ValVT will be the pointer\n    // type, instead of the scalable vector type.\n    ValVT = LocVT;\n  }\n  int FI = MFI.CreateFixedObject(ValVT.getStoreSize(), VA.getLocMemOffset(),\n                                 /*IsImmutable=*/true);\n  SDValue FIN = DAG.getFrameIndex(FI, PtrVT);\n  SDValue Val;\n\n  ISD::LoadExtType ExtType;\n  switch (VA.getLocInfo()) {\n  default:\n    llvm_unreachable(\"Unexpected CCValAssign::LocInfo\");\n  case CCValAssign::Full:\n  case CCValAssign::Indirect:\n  case CCValAssign::BCvt:\n    ExtType = ISD::NON_EXTLOAD;\n    break;\n  }\n  Val = DAG.getExtLoad(\n      ExtType, DL, LocVT, Chain, FIN,\n      MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI), ValVT);\n  return Val;\n}",
      "start_line": 17834,
      "end_line": 17866,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "CreateFixedObject",
        "getFrameIndex",
        "getExtLoad",
        "getPointerSizeInBits",
        "getMachineFunction",
        "getLocVT",
        "getFrameInfo",
        "getLocMemOffset",
        "getIntegerVT",
        "getValVT",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "unpackF64OnRV32DSoftABI",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SDValue",
          "name": "Chain"
        },
        {
          "type": "const CCValAssign",
          "name": "&VA"
        },
        {
          "type": "const CCValAssign",
          "name": "&HiVA"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        }
      ],
      "body": "{\n  assert(VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f64 &&\n         \"Unexpected VA\");\n  MachineFunction &MF = DAG.getMachineFunction();\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  MachineRegisterInfo &RegInfo = MF.getRegInfo();\n\n  assert(VA.isRegLoc() && \"Expected register VA assignment\");\n\n  Register LoVReg = RegInfo.createVirtualRegister(&RISCV::GPRRegClass);\n  RegInfo.addLiveIn(VA.getLocReg(), LoVReg);\n  SDValue Lo = DAG.getCopyFromReg(Chain, DL, LoVReg, MVT::i32);\n  SDValue Hi;\n  if (HiVA.isMemLoc()) {\n    // Second half of f64 is passed on the stack.\n    int FI = MFI.CreateFixedObject(4, HiVA.getLocMemOffset(),\n                                   /*IsImmutable=*/true);\n    SDValue FIN = DAG.getFrameIndex(FI, MVT::i32);\n    Hi = DAG.getLoad(MVT::i32, DL, Chain, FIN,\n                     MachinePointerInfo::getFixedStack(MF, FI));\n  } else {\n    // Second half of f64 is passed in another GPR.\n    Register HiVReg = RegInfo.createVirtualRegister(&RISCV::GPRRegClass);\n    RegInfo.addLiveIn(HiVA.getLocReg(), HiVReg);\n    Hi = DAG.getCopyFromReg(Chain, DL, HiVReg, MVT::i32);\n  }\n  return DAG.getNode(RISCVISD::BuildPairF64, DL, MVT::f64, Lo, Hi);\n}",
      "start_line": 17868,
      "end_line": 17898,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCopyFromReg",
        "CreateFixedObject",
        "getRegInfo",
        "createVirtualRegister",
        "getFrameIndex",
        "getMachineFunction",
        "getLoad",
        "addLiveIn",
        "getFrameInfo",
        "getValVT",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "CC_RISCV_FastCC",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const DataLayout",
          "name": "&DL"
        },
        {
          "type": "RISCVABI::ABI",
          "name": "ABI"
        },
        {
          "type": "unsigned",
          "name": "ValNo"
        },
        {
          "type": "MVT",
          "name": "ValVT"
        },
        {
          "type": "MVT",
          "name": "LocVT"
        },
        {
          "type": "CCValAssign::LocInfo",
          "name": "LocInfo"
        },
        {
          "type": "ISD::ArgFlagsTy",
          "name": "ArgFlags"
        },
        {
          "type": "CCState",
          "name": "&State"
        },
        {
          "type": "bool",
          "name": "IsFixed"
        },
        {
          "type": "bool",
          "name": "IsRet"
        },
        {
          "type": "Type",
          "name": "*OrigTy"
        },
        {
          "type": "const RISCVTargetLowering",
          "name": "&TLI"
        },
        {
          "type": "std::optional<unsigned>",
          "name": "FirstMaskArgument"
        }
      ],
      "body": "{\n  if (LocVT == MVT::i32 || LocVT == MVT::i64) {\n    if (unsigned Reg = State.AllocateReg(getFastCCArgGPRs(ABI))) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  const RISCVSubtarget &Subtarget = TLI.getSubtarget();\n\n  if (LocVT == MVT::f16 &&\n      (Subtarget.hasStdExtZfh() || Subtarget.hasStdExtZfhmin())) {\n    static const MCPhysReg FPR16List[] = {\n        RISCV::F10_H, RISCV::F11_H, RISCV::F12_H, RISCV::F13_H, RISCV::F14_H,\n        RISCV::F15_H, RISCV::F16_H, RISCV::F17_H, RISCV::F0_H,  RISCV::F1_H,\n        RISCV::F2_H,  RISCV::F3_H,  RISCV::F4_H,  RISCV::F5_H,  RISCV::F6_H,\n        RISCV::F7_H,  RISCV::F28_H, RISCV::F29_H, RISCV::F30_H, RISCV::F31_H};\n    if (unsigned Reg = State.AllocateReg(FPR16List)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  if (LocVT == MVT::f32 && Subtarget.hasStdExtF()) {\n    static const MCPhysReg FPR32List[] = {\n        RISCV::F10_F, RISCV::F11_F, RISCV::F12_F, RISCV::F13_F, RISCV::F14_F,\n        RISCV::F15_F, RISCV::F16_F, RISCV::F17_F, RISCV::F0_F,  RISCV::F1_F,\n        RISCV::F2_F,  RISCV::F3_F,  RISCV::F4_F,  RISCV::F5_F,  RISCV::F6_F,\n        RISCV::F7_F,  RISCV::F28_F, RISCV::F29_F, RISCV::F30_F, RISCV::F31_F};\n    if (unsigned Reg = State.AllocateReg(FPR32List)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  if (LocVT == MVT::f64 && Subtarget.hasStdExtD()) {\n    static const MCPhysReg FPR64List[] = {\n        RISCV::F10_D, RISCV::F11_D, RISCV::F12_D, RISCV::F13_D, RISCV::F14_D,\n        RISCV::F15_D, RISCV::F16_D, RISCV::F17_D, RISCV::F0_D,  RISCV::F1_D,\n        RISCV::F2_D,  RISCV::F3_D,  RISCV::F4_D,  RISCV::F5_D,  RISCV::F6_D,\n        RISCV::F7_D,  RISCV::F28_D, RISCV::F29_D, RISCV::F30_D, RISCV::F31_D};\n    if (unsigned Reg = State.AllocateReg(FPR64List)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  // Check if there is an available GPR before hitting the stack.\n  if ((LocVT == MVT::f16 &&\n       (Subtarget.hasStdExtZhinx() || Subtarget.hasStdExtZhinxmin())) ||\n      (LocVT == MVT::f32 && Subtarget.hasStdExtZfinx()) ||\n      (LocVT == MVT::f64 && Subtarget.is64Bit() &&\n       Subtarget.hasStdExtZdinx())) {\n    if (unsigned Reg = State.AllocateReg(getFastCCArgGPRs(ABI))) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  if (LocVT == MVT::f16) {\n    unsigned Offset2 = State.AllocateStack(2, Align(2));\n    State.addLoc(CCValAssign::getMem(ValNo, ValVT, Offset2, LocVT, LocInfo));\n    return false;\n  }\n\n  if (LocVT == MVT::i32 || LocVT == MVT::f32) {\n    unsigned Offset4 = State.AllocateStack(4, Align(4));\n    State.addLoc(CCValAssign::getMem(ValNo, ValVT, Offset4, LocVT, LocInfo));\n    return false;\n  }\n\n  if (LocVT == MVT::i64 || LocVT == MVT::f64) {\n    unsigned Offset5 = State.AllocateStack(8, Align(8));\n    State.addLoc(CCValAssign::getMem(ValNo, ValVT, Offset5, LocVT, LocInfo));\n    return false;\n  }\n\n  if (LocVT.isVector()) {\n    if (unsigned Reg =\n            allocateRVVReg(ValVT, ValNo, FirstMaskArgument, State, TLI)) {\n      // Fixed-length vectors are located in the corresponding scalable-vector\n      // container types.\n      if (ValVT.isFixedLengthVector())\n        LocVT = TLI.getContainerForFixedLengthVector(LocVT);\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n    } else {\n      // Try and pass the address via a \"fast\" GPR.\n      if (unsigned GPRReg = State.AllocateReg(getFastCCArgGPRs(ABI))) {\n        LocInfo = CCValAssign::Indirect;\n        LocVT = TLI.getSubtarget().getXLenVT();\n        State.addLoc(CCValAssign::getReg(ValNo, ValVT, GPRReg, LocVT, LocInfo));\n      } else if (ValVT.isFixedLengthVector()) {\n        auto StackAlign =\n            MaybeAlign(ValVT.getScalarSizeInBits() / 8).valueOrOne();\n        unsigned StackOffset =\n            State.AllocateStack(ValVT.getStoreSize(), StackAlign);\n        State.addLoc(\n            CCValAssign::getMem(ValNo, ValVT, StackOffset, LocVT, LocInfo));\n      } else {\n        // Can't pass scalable vectors on the stack.\n        return true;\n      }\n    }\n\n    return false;\n  }\n\n  return true; // CC didn't match.\n}",
      "start_line": 17902,
      "end_line": 18016,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "valueOrOne",
        "hasStdExtZfinx",
        "getSubtarget",
        "hasStdExtZdinx",
        "getContainerForFixedLengthVector",
        "is64Bit",
        "MaybeAlign",
        "hasStdExtZhinxmin",
        "hasStdExtZfhmin",
        "AllocateStack",
        "getXLenVT",
        "addLoc"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "CC_RISCV_GHC",
      "return_type": "bool",
      "parameters": [
        {
          "type": "unsigned",
          "name": "ValNo"
        },
        {
          "type": "MVT",
          "name": "ValVT"
        },
        {
          "type": "MVT",
          "name": "LocVT"
        },
        {
          "type": "CCValAssign::LocInfo",
          "name": "LocInfo"
        },
        {
          "type": "ISD::ArgFlagsTy",
          "name": "ArgFlags"
        },
        {
          "type": "CCState",
          "name": "&State"
        }
      ],
      "body": "{\n  if (ArgFlags.isNest()) {\n    report_fatal_error(\n        \"Attribute 'nest' is not supported in GHC calling convention\");\n  }\n\n  static const MCPhysReg GPRList[] = {\n      RISCV::X9,  RISCV::X18, RISCV::X19, RISCV::X20, RISCV::X21, RISCV::X22,\n      RISCV::X23, RISCV::X24, RISCV::X25, RISCV::X26, RISCV::X27};\n\n  if (LocVT == MVT::i32 || LocVT == MVT::i64) {\n    // Pass in STG registers: Base, Sp, Hp, R1, R2, R3, R4, R5, R6, R7, SpLim\n    //                        s1    s2  s3  s4  s5  s6  s7  s8  s9  s10 s11\n    if (unsigned Reg = State.AllocateReg(GPRList)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  const RISCVSubtarget &Subtarget =\n      State.getMachineFunction().getSubtarget<RISCVSubtarget>();\n\n  if (LocVT == MVT::f32 && Subtarget.hasStdExtF()) {\n    // Pass in STG registers: F1, ..., F6\n    //                        fs0 ... fs5\n    static const MCPhysReg FPR32List[] = {RISCV::F8_F, RISCV::F9_F,\n                                          RISCV::F18_F, RISCV::F19_F,\n                                          RISCV::F20_F, RISCV::F21_F};\n    if (unsigned Reg = State.AllocateReg(FPR32List)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  if (LocVT == MVT::f64 && Subtarget.hasStdExtD()) {\n    // Pass in STG registers: D1, ..., D6\n    //                        fs6 ... fs11\n    static const MCPhysReg FPR64List[] = {RISCV::F22_D, RISCV::F23_D,\n                                          RISCV::F24_D, RISCV::F25_D,\n                                          RISCV::F26_D, RISCV::F27_D};\n    if (unsigned Reg = State.AllocateReg(FPR64List)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  if ((LocVT == MVT::f32 && Subtarget.hasStdExtZfinx()) ||\n      (LocVT == MVT::f64 && Subtarget.hasStdExtZdinx() &&\n       Subtarget.is64Bit())) {\n    if (unsigned Reg = State.AllocateReg(GPRList)) {\n      State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));\n      return false;\n    }\n  }\n\n  report_fatal_error(\"No registers left in GHC calling convention\");\n  return true;\n}",
      "start_line": 18018,
      "end_line": 18077,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMachineFunction",
        "hasStdExtZdinx",
        "is64Bit",
        "report_fatal_error",
        "addLoc"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerFormalArguments",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Chain"
        },
        {
          "type": "CallingConv::ID",
          "name": "CallConv"
        },
        {
          "type": "bool",
          "name": "IsVarArg"
        },
        {
          "type": "const SmallVectorImpl<ISD::InputArg>",
          "name": "&Ins"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SmallVectorImpl<SDValue>",
          "name": "&InVals"
        }
      ],
      "body": "{\n\n  MachineFunction &MF = DAG.getMachineFunction();\n\n  switch (CallConv) {\n  default:\n    report_fatal_error(\"Unsupported calling convention\");\n  case CallingConv::C:\n  case CallingConv::Fast:\n  case CallingConv::SPIR_KERNEL:\n  case CallingConv::GRAAL:\n    break;\n  case CallingConv::GHC:\n    if (Subtarget.isRVE())\n      report_fatal_error(\"GHC calling convention is not supported on RVE!\");\n    if (!Subtarget.hasStdExtFOrZfinx() || !Subtarget.hasStdExtDOrZdinx())\n      report_fatal_error(\"GHC calling convention requires the (Zfinx/F) and \"\n                         \"(Zdinx/D) instruction set extensions\");\n  }\n\n  const Function &Func = MF.getFunction();\n  if (Func.hasFnAttribute(\"interrupt\")) {\n    if (!Func.arg_empty())\n      report_fatal_error(\n        \"Functions with the interrupt attribute cannot have arguments!\");\n\n    StringRef Kind =\n      MF.getFunction().getFnAttribute(\"interrupt\").getValueAsString();\n\n    if (!(Kind == \"user\" || Kind == \"supervisor\" || Kind == \"machine\"))\n      report_fatal_error(\n        \"Function interrupt attribute argument not supported!\");\n  }\n\n  EVT PtrVT = getPointerTy(DAG.getDataLayout());\n  MVT XLenVT = Subtarget.getXLenVT();\n  unsigned XLenInBytes = Subtarget.getXLen() / 8;\n  // Used with vargs to acumulate store chains.\n  std::vector<SDValue> OutChains;\n\n  // Assign locations to all of the incoming arguments.\n  SmallVector<CCValAssign, 16> ArgLocs;\n  CCState CCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.getContext());\n\n  if (CallConv == CallingConv::GHC)\n    CCInfo.AnalyzeFormalArguments(Ins, RISCV::CC_RISCV_GHC);\n  else\n    analyzeInputArgs(MF, CCInfo, Ins, /*IsRet=*/false,\n                     CallConv == CallingConv::Fast ? RISCV::CC_RISCV_FastCC\n                                                   : RISCV::CC_RISCV);\n\n  for (unsigned i = 0, e = ArgLocs.size(), InsIdx = 0; i != e; ++i, ++InsIdx) {\n    CCValAssign &VA = ArgLocs[i];\n    SDValue ArgValue;\n    // Passing f64 on RV32D with a soft float ABI must be handled as a special\n    // case.\n    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f64) {\n      assert(VA.needsCustom());\n      ArgValue = unpackF64OnRV32DSoftABI(DAG, Chain, VA, ArgLocs[++i], DL);\n    } else if (VA.isRegLoc())\n      ArgValue = unpackFromRegLoc(DAG, Chain, VA, DL, Ins[InsIdx], *this);\n    else\n      ArgValue = unpackFromMemLoc(DAG, Chain, VA, DL);\n\n    if (VA.getLocInfo() == CCValAssign::Indirect) {\n      // If the original argument was split and passed by reference (e.g. i128\n      // on RV32), we need to load all parts of it here (using the same\n      // address). Vectors may be partly split to registers and partly to the\n      // stack, in which case the base address is partly offset and subsequent\n      // stores are relative to that.\n      InVals.push_back(DAG.getLoad(VA.getValVT(), DL, Chain, ArgValue,\n                                   MachinePointerInfo()));\n      unsigned ArgIndex = Ins[InsIdx].OrigArgIndex;\n      unsigned ArgPartOffset = Ins[InsIdx].PartOffset;\n      assert(VA.getValVT().isVector() || ArgPartOffset == 0);\n      while (i + 1 != e && Ins[InsIdx + 1].OrigArgIndex == ArgIndex) {\n        CCValAssign &PartVA = ArgLocs[i + 1];\n        unsigned PartOffset = Ins[InsIdx + 1].PartOffset - ArgPartOffset;\n        SDValue Offset = DAG.getIntPtrConstant(PartOffset, DL);\n        if (PartVA.getValVT().isScalableVector())\n          Offset = DAG.getNode(ISD::VSCALE, DL, XLenVT, Offset);\n        SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, ArgValue, Offset);\n        InVals.push_back(DAG.getLoad(PartVA.getValVT(), DL, Chain, Address,\n                                     MachinePointerInfo()));\n        ++i;\n        ++InsIdx;\n      }\n      continue;\n    }\n    InVals.push_back(ArgValue);\n  }\n\n  if (any_of(ArgLocs,\n             [](CCValAssign &VA) { return VA.getLocVT().isScalableVector(); }))\n    MF.getInfo<RISCVMachineFunctionInfo>()->setIsVectorCall();\n\n  if (IsVarArg) {\n    ArrayRef<MCPhysReg> ArgRegs = RISCV::getArgGPRs(Subtarget.getTargetABI());\n    unsigned Idx = CCInfo.getFirstUnallocated(ArgRegs);\n    const TargetRegisterClass *RC = &RISCV::GPRRegClass;\n    MachineFrameInfo &MFI = MF.getFrameInfo();\n    MachineRegisterInfo &RegInfo = MF.getRegInfo();\n    RISCVMachineFunctionInfo *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n\n    // Size of the vararg save area. For now, the varargs save area is either\n    // zero or large enough to hold a0-a7.\n    int VarArgsSaveSize = XLenInBytes * (ArgRegs.size() - Idx);\n    int FI;\n\n    // If all registers are allocated, then all varargs must be passed on the\n    // stack and we don't need to save any argregs.\n    if (VarArgsSaveSize == 0) {\n      int VaArgOffset = CCInfo.getStackSize();\n      FI = MFI.CreateFixedObject(XLenInBytes, VaArgOffset, true);\n    } else {\n      int VaArgOffset = -VarArgsSaveSize;\n      FI = MFI.CreateFixedObject(VarArgsSaveSize, VaArgOffset, true);\n\n      // If saving an odd number of registers then create an extra stack slot to\n      // ensure that the frame pointer is 2*XLEN-aligned, which in turn ensures\n      // offsets to even-numbered registered remain 2*XLEN-aligned.\n      if (Idx % 2) {\n        MFI.CreateFixedObject(\n            XLenInBytes, VaArgOffset - static_cast<int>(XLenInBytes), true);\n        VarArgsSaveSize += XLenInBytes;\n      }\n\n      SDValue FIN = DAG.getFrameIndex(FI, PtrVT);\n\n      // Copy the integer registers that may have been used for passing varargs\n      // to the vararg save area.\n      for (unsigned I = Idx; I < ArgRegs.size(); ++I) {\n        const Register Reg = RegInfo.createVirtualRegister(RC);\n        RegInfo.addLiveIn(ArgRegs[I], Reg);\n        SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, XLenVT);\n        SDValue Store = DAG.getStore(\n            Chain, DL, ArgValue, FIN,\n            MachinePointerInfo::getFixedStack(MF, FI, (I - Idx) * XLenInBytes));\n        OutChains.push_back(Store);\n        FIN =\n            DAG.getMemBasePlusOffset(FIN, TypeSize::getFixed(XLenInBytes), DL);\n      }\n    }\n\n    // Record the frame index of the first variable argument\n    // which is a value necessary to VASTART.\n    RVFI->setVarArgsFrameIndex(FI);\n    RVFI->setVarArgsSaveSize(VarArgsSaveSize);\n  }\n\n  // All stores are grouped in one node to allow the matching between\n  // the size of Ins and InVals. This only happens for vararg functions.\n  if (!OutChains.empty()) {\n    OutChains.push_back(Chain);\n    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, OutChains);\n  }\n\n  return Chain;\n}",
      "start_line": 18080,
      "end_line": 18241,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "LowerFormalArguments",
          "condition": "CallConv",
          "cases": [
            {
              "label": "CallingConv",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "CallingConv",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "CallingConv",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "CallingConv",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "CallingConv",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "unpackFromRegLoc",
        "AnalyzeFormalArguments",
        "getArgGPRs",
        "setVarArgsFrameIndex",
        "getStore",
        "unpackF64OnRV32DSoftABI",
        "unpackFromMemLoc",
        "setIsVectorCall",
        "getFnAttribute",
        "MachinePointerInfo",
        "isScalableVector",
        "getCopyFromReg",
        "CreateFixedObject",
        "createVirtualRegister",
        "getFirstUnallocated",
        "getMachineFunction",
        "getLocVT",
        "getFrameInfo",
        "isVector",
        "here",
        "getIntPtrConstant",
        "getValVT",
        "size",
        "push_back",
        "getPointerTy",
        "CCInfo",
        "reference",
        "setVarArgsSaveSize",
        "getStackSize",
        "addLiveIn",
        "getXLen",
        "getValueAsString",
        "getRegInfo",
        "hasStdExtDOrZdinx",
        "getFrameIndex",
        "analyzeInputArgs",
        "getMemBasePlusOffset",
        "getXLenVT",
        "report_fatal_error",
        "getNode",
        "getFunction"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isEligibleForTailCallOptimization",
      "return_type": "bool",
      "parameters": [
        {
          "type": "CCState",
          "name": "&CCInfo"
        },
        {
          "type": "CallLoweringInfo",
          "name": "&CLI"
        },
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "const",
          "name": "SmallVector<CCValAssign"
        },
        {
          "type": "16>",
          "name": "&ArgLocs"
        }
      ],
      "body": "{\n\n  auto CalleeCC = CLI.CallConv;\n  auto &Outs = CLI.Outs;\n  auto &Caller = MF.getFunction();\n  auto CallerCC = Caller.getCallingConv();\n\n  // Exception-handling functions need a special set of instructions to\n  // indicate a return to the hardware. Tail-calling another function would\n  // probably break this.\n  // TODO: The \"interrupt\" attribute isn't currently defined by RISC-V. This\n  // should be expanded as new function attributes are introduced.\n  if (Caller.hasFnAttribute(\"interrupt\"))\n    return false;\n\n  // Do not tail call opt if the stack is used to pass parameters.\n  if (CCInfo.getStackSize() != 0)\n    return false;\n\n  // Do not tail call opt if any parameters need to be passed indirectly.\n  // Since long doubles (fp128) and i128 are larger than 2*XLEN, they are\n  // passed indirectly. So the address of the value will be passed in a\n  // register, or if not available, then the address is put on the stack. In\n  // order to pass indirectly, space on the stack often needs to be allocated\n  // in order to store the value. In this case the CCInfo.getNextStackOffset()\n  // != 0 check is not enough and we need to check if any CCValAssign ArgsLocs\n  // are passed CCValAssign::Indirect.\n  for (auto &VA : ArgLocs)\n    if (VA.getLocInfo() == CCValAssign::Indirect)\n      return false;\n\n  // Do not tail call opt if either caller or callee uses struct return\n  // semantics.\n  auto IsCallerStructRet = Caller.hasStructRetAttr();\n  auto IsCalleeStructRet = Outs.empty() ? false : Outs[0].Flags.isSRet();\n  if (IsCallerStructRet || IsCalleeStructRet)\n    return false;\n\n  // The callee has to preserve all registers the caller needs to preserve.\n  const RISCVRegisterInfo *TRI = Subtarget.getRegisterInfo();\n  const uint32_t *CallerPreserved = TRI->getCallPreservedMask(MF, CallerCC);\n  if (CalleeCC != CallerCC) {\n    const uint32_t *CalleePreserved = TRI->getCallPreservedMask(MF, CalleeCC);\n    if (!TRI->regmaskSubsetEqual(CallerPreserved, CalleePreserved))\n      return false;\n  }\n\n  // Byval parameters hand the function a pointer directly into the stack area\n  // we want to reuse during a tail call. Working around this *is* possible\n  // but less efficient and uglier in LowerCall.\n  for (auto &Arg : Outs)\n    if (Arg.Flags.isByVal())\n      return false;\n\n  return true;\n}",
      "start_line": 18246,
      "end_line": 18303,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCallPreservedMask",
        "getCallingConv",
        "getNextStackOffset",
        "hasStructRetAttr",
        "empty",
        "getRegisterInfo",
        "doubles",
        "isSRet",
        "getFunction"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getPrefTypeAlign",
      "return_type": "Align",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  return DAG.getDataLayout().getPrefTypeAlign(\n      VT.getTypeForEVT(*DAG.getContext()));\n}",
      "start_line": 18305,
      "end_line": 18308,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getDataLayout",
        "getPrefTypeAlign"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerCall",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "CallLoweringInfo",
          "name": "&CLI"
        },
        {
          "type": "SmallVectorImpl<SDValue>",
          "name": "&InVals"
        }
      ],
      "body": "{\n  SelectionDAG &DAG = CLI.DAG;\n  SDLoc &DL = CLI.DL;\n  SmallVectorImpl<ISD::OutputArg> &Outs = CLI.Outs;\n  SmallVectorImpl<SDValue> &OutVals = CLI.OutVals;\n  SmallVectorImpl<ISD::InputArg> &Ins = CLI.Ins;\n  SDValue Chain = CLI.Chain;\n  SDValue Callee = CLI.Callee;\n  bool &IsTailCall = CLI.IsTailCall;\n  CallingConv::ID CallConv = CLI.CallConv;\n  bool IsVarArg = CLI.IsVarArg;\n  EVT PtrVT = getPointerTy(DAG.getDataLayout());\n  MVT XLenVT = Subtarget.getXLenVT();\n\n  MachineFunction &MF = DAG.getMachineFunction();\n\n  // Analyze the operands of the call, assigning locations to each operand.\n  SmallVector<CCValAssign, 16> ArgLocs;\n  CCState ArgCCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.getContext());\n\n  if (CallConv == CallingConv::GHC) {\n    if (Subtarget.isRVE())\n      report_fatal_error(\"GHC calling convention is not supported on RVE!\");\n    ArgCCInfo.AnalyzeCallOperands(Outs, RISCV::CC_RISCV_GHC);\n  } else\n    analyzeOutputArgs(MF, ArgCCInfo, Outs, /*IsRet=*/false, &CLI,\n                      CallConv == CallingConv::Fast ? RISCV::CC_RISCV_FastCC\n                                                    : RISCV::CC_RISCV);\n\n  // Check if it's really possible to do a tail call.\n  if (IsTailCall)\n    IsTailCall = isEligibleForTailCallOptimization(ArgCCInfo, CLI, MF, ArgLocs);\n\n  if (IsTailCall)\n    ++NumTailCalls;\n  else if (CLI.CB && CLI.CB->isMustTailCall())\n    report_fatal_error(\"failed to perform tail call elimination on a call \"\n                       \"site marked musttail\");\n\n  // Get a count of how many bytes are to be pushed on the stack.\n  unsigned NumBytes = ArgCCInfo.getStackSize();\n\n  // Create local copies for byval args\n  SmallVector<SDValue, 8> ByValArgs;\n  for (unsigned i = 0, e = Outs.size(); i != e; ++i) {\n    ISD::ArgFlagsTy Flags = Outs[i].Flags;\n    if (!Flags.isByVal())\n      continue;\n\n    SDValue Arg = OutVals[i];\n    unsigned Size = Flags.getByValSize();\n    Align Alignment = Flags.getNonZeroByValAlign();\n\n    int FI =\n        MF.getFrameInfo().CreateStackObject(Size, Alignment, /*isSS=*/false);\n    SDValue FIPtr = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));\n    SDValue SizeNode = DAG.getConstant(Size, DL, XLenVT);\n\n    Chain = DAG.getMemcpy(Chain, DL, FIPtr, Arg, SizeNode, Alignment,\n                          /*IsVolatile=*/false,\n                          /*AlwaysInline=*/false, IsTailCall,\n                          MachinePointerInfo(), MachinePointerInfo());\n    ByValArgs.push_back(FIPtr);\n  }\n\n  if (!IsTailCall)\n    Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, CLI.DL);\n\n  // Copy argument values to their designated locations.\n  SmallVector<std::pair<Register, SDValue>, 8> RegsToPass;\n  SmallVector<SDValue, 8> MemOpChains;\n  SDValue StackPtr;\n  for (unsigned i = 0, j = 0, e = ArgLocs.size(), OutIdx = 0; i != e;\n       ++i, ++OutIdx) {\n    CCValAssign &VA = ArgLocs[i];\n    SDValue ArgValue = OutVals[OutIdx];\n    ISD::ArgFlagsTy Flags = Outs[OutIdx].Flags;\n\n    // Handle passing f64 on RV32D with a soft float ABI as a special case.\n    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f64) {\n      assert(VA.isRegLoc() && \"Expected register VA assignment\");\n      assert(VA.needsCustom());\n      SDValue SplitF64 = DAG.getNode(\n          RISCVISD::SplitF64, DL, DAG.getVTList(MVT::i32, MVT::i32), ArgValue);\n      SDValue Lo = SplitF64.getValue(0);\n      SDValue Hi = SplitF64.getValue(1);\n\n      Register RegLo = VA.getLocReg();\n      RegsToPass.push_back(std::make_pair(RegLo, Lo));\n\n      // Get the CCValAssign for the Hi part.\n      CCValAssign &HiVA = ArgLocs[++i];\n\n      if (HiVA.isMemLoc()) {\n        // Second half of f64 is passed on the stack.\n        if (!StackPtr.getNode())\n          StackPtr = DAG.getCopyFromReg(Chain, DL, RISCV::X2, PtrVT);\n        SDValue Address =\n            DAG.getNode(ISD::ADD, DL, PtrVT, StackPtr,\n                        DAG.getIntPtrConstant(HiVA.getLocMemOffset(), DL));\n        // Emit the store.\n        MemOpChains.push_back(\n            DAG.getStore(Chain, DL, Hi, Address, MachinePointerInfo()));\n      } else {\n        // Second half of f64 is passed in another GPR.\n        Register RegHigh = HiVA.getLocReg();\n        RegsToPass.push_back(std::make_pair(RegHigh, Hi));\n      }\n      continue;\n    }\n\n    // Promote the value if needed.\n    // For now, only handle fully promoted and indirect arguments.\n    if (VA.getLocInfo() == CCValAssign::Indirect) {\n      // Store the argument in a stack slot and pass its address.\n      Align StackAlign =\n          std::max(getPrefTypeAlign(Outs[OutIdx].ArgVT, DAG),\n                   getPrefTypeAlign(ArgValue.getValueType(), DAG));\n      TypeSize StoredSize = ArgValue.getValueType().getStoreSize();\n      // If the original argument was split (e.g. i128), we need\n      // to store the required parts of it here (and pass just one address).\n      // Vectors may be partly split to registers and partly to the stack, in\n      // which case the base address is partly offset and subsequent stores are\n      // relative to that.\n      unsigned ArgIndex = Outs[OutIdx].OrigArgIndex;\n      unsigned ArgPartOffset = Outs[OutIdx].PartOffset;\n      assert(VA.getValVT().isVector() || ArgPartOffset == 0);\n      // Calculate the total size to store. We don't have access to what we're\n      // actually storing other than performing the loop and collecting the\n      // info.\n      SmallVector<std::pair<SDValue, SDValue>> Parts;\n      while (i + 1 != e && Outs[OutIdx + 1].OrigArgIndex == ArgIndex) {\n        SDValue PartValue = OutVals[OutIdx + 1];\n        unsigned PartOffset = Outs[OutIdx + 1].PartOffset - ArgPartOffset;\n        SDValue Offset = DAG.getIntPtrConstant(PartOffset, DL);\n        EVT PartVT = PartValue.getValueType();\n        if (PartVT.isScalableVector())\n          Offset = DAG.getNode(ISD::VSCALE, DL, XLenVT, Offset);\n        StoredSize += PartVT.getStoreSize();\n        StackAlign = std::max(StackAlign, getPrefTypeAlign(PartVT, DAG));\n        Parts.push_back(std::make_pair(PartValue, Offset));\n        ++i;\n        ++OutIdx;\n      }\n      SDValue SpillSlot = DAG.CreateStackTemporary(StoredSize, StackAlign);\n      int FI = cast<FrameIndexSDNode>(SpillSlot)->getIndex();\n      MemOpChains.push_back(\n          DAG.getStore(Chain, DL, ArgValue, SpillSlot,\n                       MachinePointerInfo::getFixedStack(MF, FI)));\n      for (const auto &Part : Parts) {\n        SDValue PartValue = Part.first;\n        SDValue PartOffset = Part.second;\n        SDValue Address =\n            DAG.getNode(ISD::ADD, DL, PtrVT, SpillSlot, PartOffset);\n        MemOpChains.push_back(\n            DAG.getStore(Chain, DL, PartValue, Address,\n                         MachinePointerInfo::getFixedStack(MF, FI)));\n      }\n      ArgValue = SpillSlot;\n    } else {\n      ArgValue = convertValVTToLocVT(DAG, ArgValue, VA, DL, Subtarget);\n    }\n\n    // Use local copy if it is a byval arg.\n    if (Flags.isByVal())\n      ArgValue = ByValArgs[j++];\n\n    if (VA.isRegLoc()) {\n      // Queue up the argument copies and emit them at the end.\n      RegsToPass.push_back(std::make_pair(VA.getLocReg(), ArgValue));\n    } else {\n      assert(VA.isMemLoc() && \"Argument not register or memory\");\n      assert(!IsTailCall && \"Tail call not allowed if stack is used \"\n                            \"for passing parameters\");\n\n      // Work out the address of the stack slot.\n      if (!StackPtr.getNode())\n        StackPtr = DAG.getCopyFromReg(Chain, DL, RISCV::X2, PtrVT);\n      SDValue Address =\n          DAG.getNode(ISD::ADD, DL, PtrVT, StackPtr,\n                      DAG.getIntPtrConstant(VA.getLocMemOffset(), DL));\n\n      // Emit the store.\n      MemOpChains.push_back(\n          DAG.getStore(Chain, DL, ArgValue, Address, MachinePointerInfo()));\n    }\n  }\n\n  // Join the stores, which are independent of one another.\n  if (!MemOpChains.empty())\n    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, MemOpChains);\n\n  SDValue Glue;\n\n  // Build a sequence of copy-to-reg nodes, chained and glued together.\n  for (auto &Reg : RegsToPass) {\n    Chain = DAG.getCopyToReg(Chain, DL, Reg.first, Reg.second, Glue);\n    Glue = Chain.getValue(1);\n  }\n\n  // Validate that none of the argument registers have been marked as\n  // reserved, if so report an error. Do the same for the return address if this\n  // is not a tailcall.\n  validateCCReservedRegs(RegsToPass, MF);\n  if (!IsTailCall &&\n      MF.getSubtarget<RISCVSubtarget>().isRegisterReservedByUser(RISCV::X1))\n    MF.getFunction().getContext().diagnose(DiagnosticInfoUnsupported{\n        MF.getFunction(),\n        \"Return address register required, but has been reserved.\"});\n\n  // If the callee is a GlobalAddress/ExternalSymbol node, turn it into a\n  // TargetGlobalAddress/TargetExternalSymbol node so that legalize won't\n  // split it and then direct call can be matched by PseudoCALL.\n  if (GlobalAddressSDNode *S = dyn_cast<GlobalAddressSDNode>(Callee)) {\n    const GlobalValue *GV = S->getGlobal();\n    Callee = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0, RISCVII::MO_CALL);\n  } else if (ExternalSymbolSDNode *S = dyn_cast<ExternalSymbolSDNode>(Callee)) {\n    Callee = DAG.getTargetExternalSymbol(S->getSymbol(), PtrVT, RISCVII::MO_CALL);\n  }\n\n  // The first call operand is the chain and the second is the target address.\n  SmallVector<SDValue, 8> Ops;\n  Ops.push_back(Chain);\n  Ops.push_back(Callee);\n\n  // Add argument registers to the end of the list so that they are\n  // known live into the call.\n  for (auto &Reg : RegsToPass)\n    Ops.push_back(DAG.getRegister(Reg.first, Reg.second.getValueType()));\n\n  if (!IsTailCall) {\n    // Add a register mask operand representing the call-preserved registers.\n    const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();\n    const uint32_t *Mask = TRI->getCallPreservedMask(MF, CallConv);\n    assert(Mask && \"Missing call preserved mask for calling convention\");\n    Ops.push_back(DAG.getRegisterMask(Mask));\n  }\n\n  // Glue the call to the argument copies, if any.\n  if (Glue.getNode())\n    Ops.push_back(Glue);\n\n  assert((!CLI.CFIType || CLI.CB->isIndirectCall()) &&\n         \"Unexpected CFI type for a direct call\");\n\n  // Emit the call.\n  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);\n\n  if (IsTailCall) {\n    MF.getFrameInfo().setHasTailCall();\n    SDValue Ret = DAG.getNode(RISCVISD::TAIL, DL, NodeTys, Ops);\n    if (CLI.CFIType)\n      Ret.getNode()->setCFIType(CLI.CFIType->getZExtValue());\n    DAG.addNoMergeSiteInfo(Ret.getNode(), CLI.NoMerge);\n    return Ret;\n  }\n\n  Chain = DAG.getNode(RISCVISD::CALL, DL, NodeTys, Ops);\n  if (CLI.CFIType)\n    Chain.getNode()->setCFIType(CLI.CFIType->getZExtValue());\n  DAG.addNoMergeSiteInfo(Chain.getNode(), CLI.NoMerge);\n  Glue = Chain.getValue(1);\n\n  // Mark the end of the call, which is glued to the call itself.\n  Chain = DAG.getCALLSEQ_END(Chain, NumBytes, 0, Glue, DL);\n  Glue = Chain.getValue(1);\n\n  // Assign locations to each value returned by this call.\n  SmallVector<CCValAssign, 16> RVLocs;\n  CCState RetCCInfo(CallConv, IsVarArg, MF, RVLocs, *DAG.getContext());\n  analyzeInputArgs(MF, RetCCInfo, Ins, /*IsRet=*/true, RISCV::CC_RISCV);\n\n  // Copy all of the result registers out of their specified physreg.\n  for (unsigned i = 0, e = RVLocs.size(); i != e; ++i) {\n    auto &VA = RVLocs[i];\n    // Copy the value out\n    SDValue RetValue =\n        DAG.getCopyFromReg(Chain, DL, VA.getLocReg(), VA.getLocVT(), Glue);\n    // Glue the RetValue to the end of the call sequence\n    Chain = RetValue.getValue(1);\n    Glue = RetValue.getValue(2);\n\n    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f64) {\n      assert(VA.needsCustom());\n      SDValue RetValue2 = DAG.getCopyFromReg(Chain, DL, RVLocs[++i].getLocReg(),\n                                             MVT::i32, Glue);\n      Chain = RetValue2.getValue(1);\n      Glue = RetValue2.getValue(2);\n      RetValue = DAG.getNode(RISCVISD::BuildPairF64, DL, MVT::f64, RetValue,\n                             RetValue2);\n    }\n\n    RetValue = convertLocVTToValVT(DAG, RetValue, VA, DL, Subtarget);\n\n    InVals.push_back(RetValue);\n  }\n\n  return Chain;\n}",
      "start_line": 18312,
      "end_line": 18611,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "getCallPreservedMask",
        "getConstant",
        "getLocReg",
        "getTargetGlobalAddress",
        "RetCCInfo",
        "addNoMergeSiteInfo",
        "getCALLSEQ_START",
        "MachinePointerInfo",
        "getIndex",
        "getCopyFromReg",
        "getMachineFunction",
        "diagnose",
        "analyzeOutputArgs",
        "CreateStackObject",
        "getLocVT",
        "getFrameInfo",
        "isVector",
        "setCFIType",
        "here",
        "getIntPtrConstant",
        "getPrefTypeAlign",
        "getValVT",
        "max",
        "validateCCReservedRegs",
        "getTargetExternalSymbol",
        "push_back",
        "getPointerTy",
        "getGlobal",
        "getCopyToReg",
        "setHasTailCall",
        "getValue",
        "convertValVTToLocVT",
        "isRegisterReservedByUser",
        "getVTList",
        "getStackSize",
        "getValueType",
        "CreateStackTemporary",
        "ArgCCInfo",
        "getRegisterInfo",
        "getStoreSize",
        "getCALLSEQ_END",
        "getFrameIndex",
        "AnalyzeCallOperands",
        "analyzeInputArgs",
        "getNonZeroByValAlign",
        "split",
        "getByValSize",
        "getMemcpy",
        "isEligibleForTailCallOptimization",
        "getXLenVT",
        "report_fatal_error",
        "getNode",
        "getFunction",
        "convertLocVTToValVT"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "CanLowerReturn",
      "return_type": "bool",
      "parameters": [
        {
          "type": "CallingConv::ID",
          "name": "CallConv"
        },
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "bool",
          "name": "IsVarArg"
        },
        {
          "type": "const SmallVectorImpl<ISD::OutputArg>",
          "name": "&Outs"
        },
        {
          "type": "LLVMContext",
          "name": "&Context"
        }
      ],
      "body": "{\n  SmallVector<CCValAssign, 16> RVLocs;\n  CCState CCInfo(CallConv, IsVarArg, MF, RVLocs, Context);\n\n  std::optional<unsigned> FirstMaskArgument;\n  if (Subtarget.hasVInstructions())\n    FirstMaskArgument = preAssignMask(Outs);\n\n  for (unsigned i = 0, e = Outs.size(); i != e; ++i) {\n    MVT VT = Outs[i].VT;\n    ISD::ArgFlagsTy ArgFlags = Outs[i].Flags;\n    RISCVABI::ABI ABI = MF.getSubtarget<RISCVSubtarget>().getTargetABI();\n    if (RISCV::CC_RISCV(MF.getDataLayout(), ABI, i, VT, VT, CCValAssign::Full,\n                 ArgFlags, CCInfo, /*IsFixed=*/true, /*IsRet=*/true, nullptr,\n                 *this, FirstMaskArgument))\n      return false;\n  }\n  return true;\n}",
      "start_line": 18613,
      "end_line": 18633,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "CCInfo",
        "preAssignMask",
        "getTargetABI"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerReturn",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Chain"
        },
        {
          "type": "CallingConv::ID",
          "name": "CallConv"
        },
        {
          "type": "bool",
          "name": "IsVarArg"
        },
        {
          "type": "const SmallVectorImpl<ISD::OutputArg>",
          "name": "&Outs"
        },
        {
          "type": "const SmallVectorImpl<SDValue>",
          "name": "&OutVals"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  MachineFunction &MF = DAG.getMachineFunction();\n  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();\n\n  // Stores the assignment of the return value to a location.\n  SmallVector<CCValAssign, 16> RVLocs;\n\n  // Info about the registers and stack slot.\n  CCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction(), RVLocs,\n                 *DAG.getContext());\n\n  analyzeOutputArgs(DAG.getMachineFunction(), CCInfo, Outs, /*IsRet=*/true,\n                    nullptr, RISCV::CC_RISCV);\n\n  if (CallConv == CallingConv::GHC && !RVLocs.empty())\n    report_fatal_error(\"GHC functions return void only\");\n\n  SDValue Glue;\n  SmallVector<SDValue, 4> RetOps(1, Chain);\n\n  // Copy the result values into the output registers.\n  for (unsigned i = 0, e = RVLocs.size(), OutIdx = 0; i < e; ++i, ++OutIdx) {\n    SDValue Val = OutVals[OutIdx];\n    CCValAssign &VA = RVLocs[i];\n    assert(VA.isRegLoc() && \"Can only return in registers!\");\n\n    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f64) {\n      // Handle returning f64 on RV32D with a soft float ABI.\n      assert(VA.isRegLoc() && \"Expected return via registers\");\n      assert(VA.needsCustom());\n      SDValue SplitF64 = DAG.getNode(RISCVISD::SplitF64, DL,\n                                     DAG.getVTList(MVT::i32, MVT::i32), Val);\n      SDValue Lo = SplitF64.getValue(0);\n      SDValue Hi = SplitF64.getValue(1);\n      Register RegLo = VA.getLocReg();\n      Register RegHi = RVLocs[++i].getLocReg();\n\n      if (STI.isRegisterReservedByUser(RegLo) ||\n          STI.isRegisterReservedByUser(RegHi))\n        MF.getFunction().getContext().diagnose(DiagnosticInfoUnsupported{\n            MF.getFunction(),\n            \"Return value register required, but has been reserved.\"});\n\n      Chain = DAG.getCopyToReg(Chain, DL, RegLo, Lo, Glue);\n      Glue = Chain.getValue(1);\n      RetOps.push_back(DAG.getRegister(RegLo, MVT::i32));\n      Chain = DAG.getCopyToReg(Chain, DL, RegHi, Hi, Glue);\n      Glue = Chain.getValue(1);\n      RetOps.push_back(DAG.getRegister(RegHi, MVT::i32));\n    } else {\n      // Handle a 'normal' return.\n      Val = convertValVTToLocVT(DAG, Val, VA, DL, Subtarget);\n      Chain = DAG.getCopyToReg(Chain, DL, VA.getLocReg(), Val, Glue);\n\n      if (STI.isRegisterReservedByUser(VA.getLocReg()))\n        MF.getFunction().getContext().diagnose(DiagnosticInfoUnsupported{\n            MF.getFunction(),\n            \"Return value register required, but has been reserved.\"});\n\n      // Guarantee that all emitted copies are stuck together.\n      Glue = Chain.getValue(1);\n      RetOps.push_back(DAG.getRegister(VA.getLocReg(), VA.getLocVT()));\n    }\n  }\n\n  RetOps[0] = Chain; // Update chain.\n\n  // Add the glue node if we have it.\n  if (Glue.getNode()) {\n    RetOps.push_back(Glue);\n  }\n\n  if (any_of(RVLocs,\n             [](CCValAssign &VA) { return VA.getLocVT().isScalableVector(); }))\n    MF.getInfo<RISCVMachineFunctionInfo>()->setIsVectorCall();\n\n  unsigned RetOpc = RISCVISD::RET_GLUE;\n  // Interrupt service routines use different return instructions.\n  const Function &Func = DAG.getMachineFunction().getFunction();\n  if (Func.hasFnAttribute(\"interrupt\")) {\n    if (!Func.getReturnType()->isVoidTy())\n      report_fatal_error(\n          \"Functions with the interrupt attribute must have void return type!\");\n\n    MachineFunction &MF = DAG.getMachineFunction();\n    StringRef Kind =\n      MF.getFunction().getFnAttribute(\"interrupt\").getValueAsString();\n\n    if (Kind == \"supervisor\")\n      RetOpc = RISCVISD::SRET_GLUE;\n    else\n      RetOpc = RISCVISD::MRET_GLUE;\n  }\n\n  return DAG.getNode(RetOpc, DL, MVT::Other, RetOps);\n}",
      "start_line": 18635,
      "end_line": 18735,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "getLocReg",
        "setIsVectorCall",
        "isScalableVector",
        "getFnAttribute",
        "getMachineFunction",
        "diagnose",
        "analyzeOutputArgs",
        "getLocVT",
        "getValVT",
        "push_back",
        "getCopyToReg",
        "CCInfo",
        "getValue",
        "isRegisterReservedByUser",
        "convertValVTToLocVT",
        "RetOps",
        "isVoidTy",
        "getValueAsString",
        "report_fatal_error",
        "getNode",
        "getFunction"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "validateCCReservedRegs",
      "return_type": "void",
      "parameters": [
        {
          "type": "const",
          "name": "SmallVectorImpl<std::pair<llvm::Register"
        },
        {
          "type": "llvm::SDValue>>",
          "name": "&Regs"
        },
        {
          "type": "MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  const Function &F = MF.getFunction();\n  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();\n\n  if (llvm::any_of(Regs, [&STI](auto Reg) {\n        return STI.isRegisterReservedByUser(Reg.first);\n      }))\n    F.getContext().diagnose(DiagnosticInfoUnsupported{\n        F, \"Argument register required, but has been reserved.\"});\n}",
      "start_line": 18737,
      "end_line": 18748,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "isRegisterReservedByUser",
        "getFunction",
        "diagnose"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isUsedByReturnOnly",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SDValue",
          "name": "&Chain"
        }
      ],
      "body": "{\n  if (N->getNumValues() != 1)\n    return false;\n  if (!N->hasNUsesOfValue(1, 0))\n    return false;\n\n  SDNode *Copy = *N->use_begin();\n\n  if (Copy->getOpcode() == ISD::BITCAST) {\n    return isUsedByReturnOnly(Copy, Chain);\n  }\n\n  // TODO: Handle additional opcodes in order to support tail-calling libcalls\n  // with soft float ABIs.\n  if (Copy->getOpcode() != ISD::CopyToReg) {\n    return false;\n  }\n\n  // If the ISD::CopyToReg has a glue operand, we conservatively assume it\n  // isn't safe to perform a tail call.\n  if (Copy->getOperand(Copy->getNumOperands() - 1).getValueType() == MVT::Glue)\n    return false;\n\n  // The copy must be used by a RISCVISD::RET_GLUE, and nothing else.\n  bool HasRet = false;\n  for (SDNode *Node : Copy->uses()) {\n    if (Node->getOpcode() != RISCVISD::RET_GLUE)\n      return false;\n    HasRet = true;\n  }\n  if (!HasRet)\n    return false;\n\n  Chain = Copy->getOperand(0);\n  return true;\n}",
      "start_line": 18752,
      "end_line": 18787,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "use_begin",
        "getOperand",
        "isUsedByReturnOnly",
        "getValueType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "mayBeEmittedAsTailCall",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const CallInst",
          "name": "*CI"
        }
      ],
      "body": "{\n  return CI->isTailCall();\n}",
      "start_line": 18789,
      "end_line": 18791,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isTailCall"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getConstraintType",
      "return_type": "ConstraintType",
      "parameters": [
        {
          "type": "StringRef",
          "name": "Constraint"
        }
      ],
      "body": "{\n  if (Constraint.size() == 1) {\n    switch (Constraint[0]) {\n    default:\n      break;\n    case 'f':\n      return C_RegisterClass;\n    case 'I':\n    case 'J':\n    case 'K':\n      return C_Immediate;\n    case 'A':\n      return C_Memory;\n    case 'S': // A symbolic address\n      return C_Other;\n    }\n  } else {\n    if (Constraint == \"vr\" || Constraint == \"vm\")\n      return C_RegisterClass;\n  }\n  return TargetLowering::getConstraintType(Constraint);\n}",
      "start_line": 19025,
      "end_line": 19047,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "getConstraintType",
          "condition": "Constraint[0]",
          "cases": [
            {
              "label": "'f'",
              "return": "C_RegisterClass",
              "fallthrough": false
            },
            {
              "label": "'I'",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "'J'",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "'K'",
              "return": "C_Immediate",
              "fallthrough": false
            },
            {
              "label": "'A'",
              "return": "C_Memory",
              "fallthrough": false
            },
            {
              "label": "'S'",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getConstraintType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "Constraint ==",
          "name": "\"vr\""
        }
      ],
      "body": "{\n    for (const auto *RC : {&RISCV::VRRegClass, &RISCV::VRM2RegClass,\n                           &RISCV::VRM4RegClass, &RISCV::VRM8RegClass}) {\n      if (TRI->isTypeLegalForClass(*RC, VT.SimpleTy))\n        return std::make_pair(0U, RC);\n    }\n  }",
      "start_line": 19079,
      "end_line": 19085,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "make_pair"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "Constraint ==",
          "name": "\"vm\""
        }
      ],
      "body": "{\n    if (TRI->isTypeLegalForClass(RISCV::VMV0RegClass, VT.SimpleTy))\n      return std::make_pair(0U, &RISCV::VMV0RegClass);\n  }",
      "start_line": 19085,
      "end_line": 19088,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "make_pair"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getInlineAsmMemConstraint",
      "return_type": "ConstraintCode",
      "parameters": [
        {
          "type": "StringRef",
          "name": "ConstraintCode"
        }
      ],
      "body": "{\n  // Currently only support length 1 constraints.\n  if (ConstraintCode.size() == 1) {\n    switch (ConstraintCode[0]) {\n    case 'A':\n      return InlineAsm::ConstraintCode::A;\n    default:\n      break;\n    }\n  }\n\n  return TargetLowering::getInlineAsmMemConstraint(ConstraintCode);\n}",
      "start_line": 19254,
      "end_line": 19267,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "getInlineAsmMemConstraint",
          "condition": "ConstraintCode[0]",
          "cases": [
            {
              "label": "'A'",
              "return": "InlineAsm::ConstraintCode::A",
              "fallthrough": false
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getInlineAsmMemConstraint"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "LowerAsmOperandForConstraint",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Op"
        },
        {
          "type": "StringRef",
          "name": "Constraint"
        },
        {
          "type": "std::vector<SDValue>",
          "name": "&Ops"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  // Currently only support length 1 constraints.\n  if (Constraint.size() == 1) {\n    switch (Constraint[0]) {\n    case 'I':\n      // Validate & create a 12-bit signed immediate operand.\n      if (auto *C = dyn_cast<ConstantSDNode>(Op)) {\n        uint64_t CVal = C->getSExtValue();\n        if (isInt<12>(CVal))\n          Ops.push_back(\n              DAG.getTargetConstant(CVal, SDLoc(Op), Subtarget.getXLenVT()));\n      }\n      return;\n    case 'J':\n      // Validate & create an integer zero operand.\n      if (isNullConstant(Op))\n        Ops.push_back(\n            DAG.getTargetConstant(0, SDLoc(Op), Subtarget.getXLenVT()));\n      return;\n    case 'K':\n      // Validate & create a 5-bit unsigned immediate operand.\n      if (auto *C = dyn_cast<ConstantSDNode>(Op)) {\n        uint64_t CVal = C->getZExtValue();\n        if (isUInt<5>(CVal))\n          Ops.push_back(\n              DAG.getTargetConstant(CVal, SDLoc(Op), Subtarget.getXLenVT()));\n      }\n      return;\n    case 'S':\n      if (const auto *GA = dyn_cast<GlobalAddressSDNode>(Op)) {\n        Ops.push_back(DAG.getTargetGlobalAddress(GA->getGlobal(), SDLoc(Op),\n                                                 GA->getValueType(0)));\n      } else if (const auto *BA = dyn_cast<BlockAddressSDNode>(Op)) {\n        Ops.push_back(DAG.getTargetBlockAddress(BA->getBlockAddress(),\n                                                BA->getValueType(0)));\n      }\n      return;\n    default:\n      break;\n    }\n  }\n  TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);\n}",
      "start_line": 19269,
      "end_line": 19313,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "LowerAsmOperandForConstraint",
          "condition": "Constraint[0]",
          "cases": [
            {
              "label": "'I'",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "'J'",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "'K'",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "'S'",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getZExtValue",
        "getValueType",
        "SDLoc",
        "push_back",
        "getXLenVT",
        "LowerAsmOperandForConstraint",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldExpandAtomicRMWInIR",
      "return_type": "AtomicExpansionKind",
      "parameters": [
        {
          "type": "AtomicRMWInst",
          "name": "*AI"
        }
      ],
      "body": "{\n  // atomicrmw {fadd,fsub} must be expanded to use compare-exchange, as floating\n  // point operations can't be used in an lr/sc sequence without breaking the\n  // forward-progress guarantee.\n  if (AI->isFloatingPointOperation() ||\n      AI->getOperation() == AtomicRMWInst::UIncWrap ||\n      AI->getOperation() == AtomicRMWInst::UDecWrap)\n    return AtomicExpansionKind::CmpXChg;\n\n  // Don't expand forced atomics, we want to have __sync libcalls instead.\n  if (Subtarget.hasForcedAtomics())\n    return AtomicExpansionKind::None;\n\n  unsigned Size = AI->getType()->getPrimitiveSizeInBits();\n  if (Size == 8 || Size == 16)\n    return AtomicExpansionKind::MaskedIntrinsic;\n  return AtomicExpansionKind::None;\n}",
      "start_line": 19348,
      "end_line": 19366,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperation",
        "getPrimitiveSizeInBits",
        "getType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getIntrinsicForMaskedAtomicRMWBinOp",
      "return_type": "ID",
      "parameters": [
        {
          "type": "unsigned",
          "name": "XLen"
        },
        {
          "type": "AtomicRMWInst::BinOp",
          "name": "BinOp"
        }
      ],
      "body": "{\n  if (XLen == 32) {\n    switch (BinOp) {\n    default:\n      llvm_unreachable(\"Unexpected AtomicRMW BinOp\");\n    case AtomicRMWInst::Xchg:\n      return Intrinsic::riscv_masked_atomicrmw_xchg_i32;\n    case AtomicRMWInst::Add:\n      return Intrinsic::riscv_masked_atomicrmw_add_i32;\n    case AtomicRMWInst::Sub:\n      return Intrinsic::riscv_masked_atomicrmw_sub_i32;\n    case AtomicRMWInst::Nand:\n      return Intrinsic::riscv_masked_atomicrmw_nand_i32;\n    case AtomicRMWInst::Max:\n      return Intrinsic::riscv_masked_atomicrmw_max_i32;\n    case AtomicRMWInst::Min:\n      return Intrinsic::riscv_masked_atomicrmw_min_i32;\n    case AtomicRMWInst::UMax:\n      return Intrinsic::riscv_masked_atomicrmw_umax_i32;\n    case AtomicRMWInst::UMin:\n      return Intrinsic::riscv_masked_atomicrmw_umin_i32;\n    }\n  }\n\n  if (XLen == 64) {\n    switch (BinOp) {\n    default:\n      llvm_unreachable(\"Unexpected AtomicRMW BinOp\");\n    case AtomicRMWInst::Xchg:\n      return Intrinsic::riscv_masked_atomicrmw_xchg_i64;\n    case AtomicRMWInst::Add:\n      return Intrinsic::riscv_masked_atomicrmw_add_i64;\n    case AtomicRMWInst::Sub:\n      return Intrinsic::riscv_masked_atomicrmw_sub_i64;\n    case AtomicRMWInst::Nand:\n      return Intrinsic::riscv_masked_atomicrmw_nand_i64;\n    case AtomicRMWInst::Max:\n      return Intrinsic::riscv_masked_atomicrmw_max_i64;\n    case AtomicRMWInst::Min:\n      return Intrinsic::riscv_masked_atomicrmw_min_i64;\n    case AtomicRMWInst::UMax:\n      return Intrinsic::riscv_masked_atomicrmw_umax_i64;\n    case AtomicRMWInst::UMin:\n      return Intrinsic::riscv_masked_atomicrmw_umin_i64;\n    }\n  }\n\n  llvm_unreachable(\"Unexpected XLen\\n\");\n}",
      "start_line": 19368,
      "end_line": 19417,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "getIntrinsicForMaskedAtomicRMWBinOp",
          "condition": "BinOp",
          "cases": [
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "getIntrinsicForMaskedAtomicRMWBinOp",
          "condition": "BinOp",
          "cases": [
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "AtomicRMWInst",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldExpandAtomicCmpXchgInIR",
      "return_type": "AtomicExpansionKind",
      "parameters": [
        {
          "type": "AtomicCmpXchgInst",
          "name": "*CI"
        }
      ],
      "body": "{\n  // Don't expand forced atomics, we want to have __sync libcalls instead.\n  if (Subtarget.hasForcedAtomics())\n    return AtomicExpansionKind::None;\n\n  unsigned Size = CI->getCompareOperand()->getType()->getPrimitiveSizeInBits();\n  if (Size == 8 || Size == 16)\n    return AtomicExpansionKind::MaskedIntrinsic;\n  return AtomicExpansionKind::None;\n}",
      "start_line": 19478,
      "end_line": 19489,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getType",
        "getPrimitiveSizeInBits",
        "getCompareOperand"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldRemoveExtendFromGSIndex",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Extend"
        },
        {
          "type": "EVT",
          "name": "DataVT"
        }
      ],
      "body": "{\n  // We have indexed loads for all legal index types.  Indices are always\n  // zero extended\n  return Extend.getOpcode() == ISD::ZERO_EXTEND &&\n    isTypeLegal(Extend.getValueType()) &&\n    isTypeLegal(Extend.getOperand(0).getValueType());\n}",
      "start_line": 19513,
      "end_line": 19520,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOpcode",
        "isTypeLegal",
        "getValueType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldConvertFpToSat",
      "return_type": "bool",
      "parameters": [
        {
          "type": "unsigned",
          "name": "Op"
        },
        {
          "type": "EVT",
          "name": "FPVT"
        },
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  if (!isOperationLegalOrCustom(Op, VT) || !FPVT.isSimple())\n    return false;\n\n  switch (FPVT.getSimpleVT().SimpleTy) {\n  case MVT::f16:\n    return Subtarget.hasStdExtZfhmin();\n  case MVT::f32:\n    return Subtarget.hasStdExtF();\n  case MVT::f64:\n    return Subtarget.hasStdExtD();\n  default:\n    return false;\n  }\n}",
      "start_line": 19522,
      "end_line": 19537,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtF",
        "hasStdExtZfhmin",
        "isSimple",
        "hasStdExtD"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getJumpTableEncoding",
      "return_type": "unsigned",
      "parameters": [],
      "body": "{\n  // If we are using the small code model, we can reduce size of jump table\n  // entry to 4 bytes.\n  if (Subtarget.is64Bit() && !isPositionIndependent() &&\n      getTargetMachine().getCodeModel() == CodeModel::Small) {\n    return MachineJumpTableInfo::EK_Custom32;\n  }\n  return TargetLowering::getJumpTableEncoding();\n}",
      "start_line": 19539,
      "end_line": 19547,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getTargetMachine",
        "getCodeModel",
        "isPositionIndependent",
        "getJumpTableEncoding"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isVScaleKnownToBeAPowerOfTwo",
      "return_type": "bool",
      "parameters": [],
      "body": "{\n  // We define vscale to be VLEN/RVVBitsPerBlock.  VLEN is always a power\n  // of two >= 64, and RVVBitsPerBlock is 64.  Thus, vscale must be\n  // a power of two as well.\n  // FIXME: This doesn't work for zve32, but that's already broken\n  // elsewhere for the same reason.\n  assert(Subtarget.getRealMinVLen() >= 64 && \"zve32* unsupported\");\n  static_assert(RISCV::RVVBitsPerBlock == 64,\n                \"RVVBitsPerBlock changed, audit needed\");\n  return true;\n}",
      "start_line": 19557,
      "end_line": 19567,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "static_assert"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getIndexedAddressParts",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Op"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        },
        {
          "type": "ISD::MemIndexedMode",
          "name": "&AM"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  // Target does not support indexed loads.\n  if (!Subtarget.hasVendorXTHeadMemIdx())\n    return false;\n\n  if (Op->getOpcode() != ISD::ADD && Op->getOpcode() != ISD::SUB)\n    return false;\n\n  Base = Op->getOperand(0);\n  if (ConstantSDNode *RHS = dyn_cast<ConstantSDNode>(Op->getOperand(1))) {\n    int64_t RHSC = RHS->getSExtValue();\n    if (Op->getOpcode() == ISD::SUB)\n      RHSC = -(uint64_t)RHSC;\n\n    // The constants that can be encoded in the THeadMemIdx instructions\n    // are of the form (sign_extend(imm5) << imm2).\n    bool isLegalIndexedOffset = false;\n    for (unsigned i = 0; i < 4; i++)\n      if (isInt<5>(RHSC >> i) && ((RHSC % (1LL << i)) == 0)) {\n        isLegalIndexedOffset = true;\n        break;\n      }\n\n    if (!isLegalIndexedOffset)\n      return false;\n\n    Offset = Op->getOperand(1);\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 19569,
      "end_line": 19603,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand",
        "getOpcode",
        "getSExtValue",
        "form"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getPreIndexedAddressParts",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        },
        {
          "type": "ISD::MemIndexedMode",
          "name": "&AM"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  EVT VT;\n  SDValue Ptr;\n  if (LoadSDNode *LD = dyn_cast<LoadSDNode>(N)) {\n    VT = LD->getMemoryVT();\n    Ptr = LD->getBasePtr();\n  } else if (StoreSDNode *ST = dyn_cast<StoreSDNode>(N)) {\n    VT = ST->getMemoryVT();\n    Ptr = ST->getBasePtr();\n  } else\n    return false;\n\n  if (!getIndexedAddressParts(Ptr.getNode(), Base, Offset, AM, DAG))\n    return false;\n\n  AM = ISD::PRE_INC;\n  return true;\n}",
      "start_line": 19605,
      "end_line": 19625,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMemoryVT",
        "getBasePtr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getPostIndexedAddressParts",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "SDNode",
          "name": "*Op"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        },
        {
          "type": "ISD::MemIndexedMode",
          "name": "&AM"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        }
      ],
      "body": "{\n  EVT VT;\n  SDValue Ptr;\n  if (LoadSDNode *LD = dyn_cast<LoadSDNode>(N)) {\n    VT = LD->getMemoryVT();\n    Ptr = LD->getBasePtr();\n  } else if (StoreSDNode *ST = dyn_cast<StoreSDNode>(N)) {\n    VT = ST->getMemoryVT();\n    Ptr = ST->getBasePtr();\n  } else\n    return false;\n\n  if (!getIndexedAddressParts(Op, Base, Offset, AM, DAG))\n    return false;\n  // Post-indexing updates the base, so it's not a valid transform\n  // if that's not the same as the load's pointer.\n  if (Ptr != Base)\n    return false;\n\n  AM = ISD::POST_INC;\n  return true;\n}",
      "start_line": 19627,
      "end_line": 19652,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMemoryVT",
        "getBasePtr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isFMAFasterThanFMulAndFAdd",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  EVT SVT = VT.getScalarType();\n\n  if (!SVT.isSimple())\n    return false;\n\n  switch (SVT.getSimpleVT().SimpleTy) {\n  case MVT::f16:\n    return VT.isVector() ? Subtarget.hasVInstructionsF16()\n                         : Subtarget.hasStdExtZfhOrZhinx();\n  case MVT::f32:\n    return Subtarget.hasStdExtFOrZfinx();\n  case MVT::f64:\n    return Subtarget.hasStdExtDOrZdinx();\n  default:\n    break;\n  }\n\n  return false;\n}",
      "start_line": 19654,
      "end_line": 19674,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtDOrZdinx",
        "hasStdExtZfhOrZhinx",
        "isVector",
        "hasStdExtFOrZfinx",
        "hasVInstructionsF16",
        "getScalarType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getExtendForAtomicCmpSwapArg",
      "return_type": "NodeType",
      "parameters": [],
      "body": "{\n  // Zacas will use amocas.w which does not require extension.\n  return Subtarget.hasStdExtZacas() ? ISD::ANY_EXTEND : ISD::SIGN_EXTEND;\n}",
      "start_line": 19676,
      "end_line": 19679,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZacas"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getExceptionPointerRegister",
      "return_type": "Register",
      "parameters": [
        {
          "type": "const Constant",
          "name": "*PersonalityFn"
        }
      ],
      "body": "{\n  return RISCV::X10;\n}",
      "start_line": 19681,
      "end_line": 19684,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getExceptionSelectorRegister",
      "return_type": "Register",
      "parameters": [
        {
          "type": "const Constant",
          "name": "*PersonalityFn"
        }
      ],
      "body": "{\n  return RISCV::X11;\n}",
      "start_line": 19686,
      "end_line": 19689,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldExtendTypeInLibCall",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "Type"
        }
      ],
      "body": "{\n  // Return false to suppress the unnecessary extensions if the LibCall\n  // arguments or return value is a float narrower than XLEN on a soft FP ABI.\n  if (Subtarget.isSoftFPABI() && (Type.isFloatingPoint() && !Type.isVector() &&\n                                  Type.getSizeInBits() < Subtarget.getXLen()))\n    return false;\n\n  return true;\n}",
      "start_line": 19691,
      "end_line": 19699,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isVector",
        "getSizeInBits",
        "getXLen",
        "isFloatingPoint"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldSignExtendTypeInLibCall",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "Type"
        },
        {
          "type": "bool",
          "name": "IsSigned"
        }
      ],
      "body": "{\n  if (Subtarget.is64Bit() && Type == MVT::i32)\n    return true;\n\n  return IsSigned;\n}",
      "start_line": 19701,
      "end_line": 19706,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "decomposeMulByConstant",
      "return_type": "bool",
      "parameters": [
        {
          "type": "LLVMContext",
          "name": "&Context"
        },
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "SDValue",
          "name": "C"
        }
      ],
      "body": "{\n  // Check integral scalar types.\n  const bool HasExtMOrZmmul =\n      Subtarget.hasStdExtM() || Subtarget.hasStdExtZmmul();\n  if (!VT.isScalarInteger())\n    return false;\n\n  // Omit the optimization if the sub target has the M extension and the data\n  // size exceeds XLen.\n  if (HasExtMOrZmmul && VT.getSizeInBits() > Subtarget.getXLen())\n    return false;\n\n  if (auto *ConstNode = dyn_cast<ConstantSDNode>(C.getNode())) {\n    // Break the MUL to a SLLI and an ADD/SUB.\n    const APInt &Imm = ConstNode->getAPIntValue();\n    if ((Imm + 1).isPowerOf2() || (Imm - 1).isPowerOf2() ||\n        (1 - Imm).isPowerOf2() || (-1 - Imm).isPowerOf2())\n      return true;\n\n    // Optimize the MUL to (SH*ADD x, (SLLI x, bits)) if Imm is not simm12.\n    if (Subtarget.hasStdExtZba() && !Imm.isSignedIntN(12) &&\n        ((Imm - 2).isPowerOf2() || (Imm - 4).isPowerOf2() ||\n         (Imm - 8).isPowerOf2()))\n      return true;\n\n    // Break the MUL to two SLLI instructions and an ADD/SUB, if Imm needs\n    // a pair of LUI/ADDI.\n    if (!Imm.isSignedIntN(12) && Imm.countr_zero() < 12 &&\n        ConstNode->hasOneUse()) {\n      APInt ImmS = Imm.ashr(Imm.countr_zero());\n      if ((ImmS + 1).isPowerOf2() || (ImmS - 1).isPowerOf2() ||\n          (1 - ImmS).isPowerOf2())\n        return true;\n    }\n  }\n\n  return false;\n}",
      "start_line": 19708,
      "end_line": 19746,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isSignedIntN",
        "hasStdExtZmmul",
        "countr_zero",
        "hasOneUse",
        "isPowerOf2",
        "getAPIntValue",
        "to",
        "getXLen",
        "ashr",
        "hasStdExtM"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isMulAddWithConstProfitable",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "AddNode"
        },
        {
          "type": "SDValue",
          "name": "ConstNode"
        }
      ],
      "body": "{\n  // Let the DAGCombiner decide for vectors.\n  EVT VT = AddNode.getValueType();\n  if (VT.isVector())\n    return true;\n\n  // Let the DAGCombiner decide for larger types.\n  if (VT.getScalarSizeInBits() > Subtarget.getXLen())\n    return true;\n\n  // It is worse if c1 is simm12 while c1*c2 is not.\n  ConstantSDNode *C1Node = cast<ConstantSDNode>(AddNode.getOperand(1));\n  ConstantSDNode *C2Node = cast<ConstantSDNode>(ConstNode);\n  const APInt &C1 = C1Node->getAPIntValue();\n  const APInt &C2 = C2Node->getAPIntValue();\n  if (C1.isSignedIntN(12) && !(C1 * C2).isSignedIntN(12))\n    return false;\n\n  // Default to true and let the DAGCombiner decide.\n  return true;\n}",
      "start_line": 19748,
      "end_line": 19769,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isSignedIntN",
        "getAPIntValue",
        "getValueType",
        "getOperand",
        "getXLen"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "allowsMisalignedMemoryAccesses",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "unsigned",
          "name": "AddrSpace"
        },
        {
          "type": "Align",
          "name": "Alignment"
        },
        {
          "type": "MachineMemOperand::Flags",
          "name": "Flags"
        },
        {
          "type": "unsigned",
          "name": "*Fast"
        }
      ],
      "body": "{\n  if (!VT.isVector()) {\n    if (Fast)\n      *Fast = Subtarget.hasFastUnalignedAccess() ||\n              Subtarget.enableUnalignedScalarMem();\n    return Subtarget.hasFastUnalignedAccess() ||\n           Subtarget.enableUnalignedScalarMem();\n  }\n\n  // All vector implementations must support element alignment\n  EVT ElemVT = VT.getVectorElementType();\n  if (Alignment >= ElemVT.getStoreSize()) {\n    if (Fast)\n      *Fast = 1;\n    return true;\n  }\n\n  // Note: We lower an unmasked unaligned vector access to an equally sized\n  // e8 element type access.  Given this, we effectively support all unmasked\n  // misaligned accesses.  TODO: Work through the codegen implications of\n  // allowing such accesses to be formed, and considered fast.\n  if (Fast)\n    *Fast = Subtarget.hasFastUnalignedAccess();\n  return Subtarget.hasFastUnalignedAccess();\n}",
      "start_line": 19771,
      "end_line": 19797,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasFastUnalignedAccess",
        "getVectorElementType",
        "enableUnalignedScalarMem"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getOptimalMemOpType",
      "return_type": "EVT",
      "parameters": [
        {
          "type": "const MemOp",
          "name": "&Op"
        },
        {
          "type": "const AttributeList",
          "name": "&FuncAttributes"
        }
      ],
      "body": "{\n  if (!Subtarget.hasVInstructions())\n    return MVT::Other;\n\n  if (FuncAttributes.hasFnAttr(Attribute::NoImplicitFloat))\n    return MVT::Other;\n\n  // We use LMUL1 memory operations here for a non-obvious reason.  Our caller\n  // has an expansion threshold, and we want the number of hardware memory\n  // operations to correspond roughly to that threshold.  LMUL>1 operations\n  // are typically expanded linearly internally, and thus correspond to more\n  // than one actual memory operation.  Note that store merging and load\n  // combining will typically form larger LMUL operations from the LMUL1\n  // operations emitted here, and that's okay because combining isn't\n  // introducing new memory operations; it's just merging existing ones.\n  const unsigned MinVLenInBytes = Subtarget.getRealMinVLen()/8;\n  if (Op.size() < MinVLenInBytes)\n    // TODO: Figure out short memops.  For the moment, do the default thing\n    // which ends up using scalar sequences.\n    return MVT::Other;\n\n  // Prefer i8 for non-zero memset as it allows us to avoid materializing\n  // a large scalar constant and instead use vmv.v.x/i to do the\n  // broadcast.  For everything else, prefer ELenVT to minimize VL and thus\n  // maximize the chance we can encode the size in the vsetvli.\n  MVT ELenVT = MVT::getIntegerVT(Subtarget.getELen());\n  MVT PreferredVT = (Op.isMemset() && !Op.isZeroMemset()) ? MVT::i8 : ELenVT;\n\n  // Do we have sufficient alignment for our preferred VT?  If not, revert\n  // to largest size allowed by our alignment criteria.\n  if (PreferredVT != MVT::i8 && !Subtarget.hasFastUnalignedAccess()) {\n    Align RequiredAlign(PreferredVT.getStoreSize());\n    if (Op.isFixedDstAlign())\n      RequiredAlign = std::min(RequiredAlign, Op.getDstAlign());\n    if (Op.isMemcpy())\n      RequiredAlign = std::min(RequiredAlign, Op.getSrcAlign());\n    PreferredVT = MVT::getIntegerVT(RequiredAlign.value() * 8);\n  }\n  return MVT::getVectorVT(PreferredVT, MinVLenInBytes/PreferredVT.getStoreSize());\n}",
      "start_line": 19800,
      "end_line": 19840,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isMemset",
        "min",
        "getRealMinVLen",
        "getIntegerVT",
        "getVectorVT",
        "RequiredAlign",
        "isZeroMemset"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "splitValueIntoRegisterParts",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "SDValue",
          "name": "Val"
        },
        {
          "type": "SDValue",
          "name": "*Parts"
        },
        {
          "type": "unsigned",
          "name": "NumParts"
        },
        {
          "type": "MVT",
          "name": "PartVT"
        },
        {
          "type": "std::optional<CallingConv::ID>",
          "name": "CC"
        }
      ],
      "body": "{\n  bool IsABIRegCopy = CC.has_value();\n  EVT ValueVT = Val.getValueType();\n  if (IsABIRegCopy && (ValueVT == MVT::f16 || ValueVT == MVT::bf16) &&\n      PartVT == MVT::f32) {\n    // Cast the [b]f16 to i16, extend to i32, pad with ones to make a float\n    // nan, and cast to f32.\n    Val = DAG.getNode(ISD::BITCAST, DL, MVT::i16, Val);\n    Val = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i32, Val);\n    Val = DAG.getNode(ISD::OR, DL, MVT::i32, Val,\n                      DAG.getConstant(0xFFFF0000, DL, MVT::i32));\n    Val = DAG.getNode(ISD::BITCAST, DL, MVT::f32, Val);\n    Parts[0] = Val;\n    return true;\n  }\n\n  if (ValueVT.isScalableVector() && PartVT.isScalableVector()) {\n    LLVMContext &Context = *DAG.getContext();\n    EVT ValueEltVT = ValueVT.getVectorElementType();\n    EVT PartEltVT = PartVT.getVectorElementType();\n    unsigned ValueVTBitSize = ValueVT.getSizeInBits().getKnownMinValue();\n    unsigned PartVTBitSize = PartVT.getSizeInBits().getKnownMinValue();\n    if (PartVTBitSize % ValueVTBitSize == 0) {\n      assert(PartVTBitSize >= ValueVTBitSize);\n      // If the element types are different, bitcast to the same element type of\n      // PartVT first.\n      // Give an example here, we want copy a <vscale x 1 x i8> value to\n      // <vscale x 4 x i16>.\n      // We need to convert <vscale x 1 x i8> to <vscale x 8 x i8> by insert\n      // subvector, then we can bitcast to <vscale x 4 x i16>.\n      if (ValueEltVT != PartEltVT) {\n        if (PartVTBitSize > ValueVTBitSize) {\n          unsigned Count = PartVTBitSize / ValueEltVT.getFixedSizeInBits();\n          assert(Count != 0 && \"The number of element should not be zero.\");\n          EVT SameEltTypeVT =\n              EVT::getVectorVT(Context, ValueEltVT, Count, /*IsScalable=*/true);\n          Val = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, SameEltTypeVT,\n                            DAG.getUNDEF(SameEltTypeVT), Val,\n                            DAG.getVectorIdxConstant(0, DL));\n        }\n        Val = DAG.getNode(ISD::BITCAST, DL, PartVT, Val);\n      } else {\n        Val =\n            DAG.getNode(ISD::INSERT_SUBVECTOR, DL, PartVT, DAG.getUNDEF(PartVT),\n                        Val, DAG.getVectorIdxConstant(0, DL));\n      }\n      Parts[0] = Val;\n      return true;\n    }\n  }\n  return false;\n}",
      "start_line": 19842,
      "end_line": 19895,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "getFixedSizeInBits",
        "getValueType",
        "getVectorIdxConstant",
        "getSizeInBits",
        "getVectorElementType",
        "getKnownMinValue",
        "getVectorVT",
        "has_value",
        "isScalableVector",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "joinRegisterPartsIntoValue",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "const SDValue",
          "name": "*Parts"
        },
        {
          "type": "unsigned",
          "name": "NumParts"
        },
        {
          "type": "MVT",
          "name": "PartVT"
        },
        {
          "type": "EVT",
          "name": "ValueVT"
        },
        {
          "type": "std::optional<CallingConv::ID>",
          "name": "CC"
        }
      ],
      "body": "{\n  bool IsABIRegCopy = CC.has_value();\n  if (IsABIRegCopy && (ValueVT == MVT::f16 || ValueVT == MVT::bf16) &&\n      PartVT == MVT::f32) {\n    SDValue Val = Parts[0];\n\n    // Cast the f32 to i32, truncate to i16, and cast back to [b]f16.\n    Val = DAG.getNode(ISD::BITCAST, DL, MVT::i32, Val);\n    Val = DAG.getNode(ISD::TRUNCATE, DL, MVT::i16, Val);\n    Val = DAG.getNode(ISD::BITCAST, DL, ValueVT, Val);\n    return Val;\n  }\n\n  if (ValueVT.isScalableVector() && PartVT.isScalableVector()) {\n    LLVMContext &Context = *DAG.getContext();\n    SDValue Val = Parts[0];\n    EVT ValueEltVT = ValueVT.getVectorElementType();\n    EVT PartEltVT = PartVT.getVectorElementType();\n    unsigned ValueVTBitSize = ValueVT.getSizeInBits().getKnownMinValue();\n    unsigned PartVTBitSize = PartVT.getSizeInBits().getKnownMinValue();\n    if (PartVTBitSize % ValueVTBitSize == 0) {\n      assert(PartVTBitSize >= ValueVTBitSize);\n      EVT SameEltTypeVT = ValueVT;\n      // If the element types are different, convert it to the same element type\n      // of PartVT.\n      // Give an example here, we want copy a <vscale x 1 x i8> value from\n      // <vscale x 4 x i16>.\n      // We need to convert <vscale x 4 x i16> to <vscale x 8 x i8> first,\n      // then we can extract <vscale x 1 x i8>.\n      if (ValueEltVT != PartEltVT) {\n        unsigned Count = PartVTBitSize / ValueEltVT.getFixedSizeInBits();\n        assert(Count != 0 && \"The number of element should not be zero.\");\n        SameEltTypeVT =\n            EVT::getVectorVT(Context, ValueEltVT, Count, /*IsScalable=*/true);\n        Val = DAG.getNode(ISD::BITCAST, DL, SameEltTypeVT, Val);\n      }\n      Val = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ValueVT, Val,\n                        DAG.getVectorIdxConstant(0, DL));\n      return Val;\n    }\n  }\n  return SDValue();\n}",
      "start_line": 19897,
      "end_line": 19941,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "SDValue",
        "getFixedSizeInBits",
        "getSizeInBits",
        "getVectorElementType",
        "getKnownMinValue",
        "getVectorVT",
        "has_value",
        "isScalableVector",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isIntDivCheap",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "AttributeList",
          "name": "Attr"
        }
      ],
      "body": "{\n  // When aggressively optimizing for code size, we prefer to use a div\n  // instruction, as it is usually smaller than the alternative sequence.\n  // TODO: Add vector division?\n  bool OptSize = Attr.hasFnAttr(Attribute::MinSize);\n  return OptSize && !VT.isVector();\n}",
      "start_line": 19943,
      "end_line": 19949,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isVector",
        "hasFnAttr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "preferScalarizeSplat",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        }
      ],
      "body": "{\n  // Scalarize zero_ext and sign_ext might stop match to widening instruction in\n  // some situation.\n  unsigned Opc = N->getOpcode();\n  if (Opc == ISD::ZERO_EXTEND || Opc == ISD::SIGN_EXTEND)\n    return false;\n  return true;\n}",
      "start_line": 19951,
      "end_line": 19958,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOpcode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isLegalInterleavedAccessType",
      "return_type": "bool",
      "parameters": [
        {
          "type": "VectorType",
          "name": "*VTy"
        },
        {
          "type": "unsigned",
          "name": "Factor"
        },
        {
          "type": "Align",
          "name": "Alignment"
        },
        {
          "type": "unsigned",
          "name": "AddrSpace"
        },
        {
          "type": "const DataLayout",
          "name": "&DL"
        }
      ],
      "body": "{\n  EVT VT = getValueType(DL, VTy);\n  // Don't lower vlseg/vsseg for vector types that can't be split.\n  if (!isTypeLegal(VT))\n    return false;\n\n  if (!isLegalElementTypeForRVV(VT.getScalarType()) ||\n      !allowsMemoryAccessForAlignment(VTy->getContext(), DL, VT, AddrSpace,\n                                      Alignment))\n    return false;\n\n  MVT ContainerVT = VT.getSimpleVT();\n\n  if (auto *FVTy = dyn_cast<FixedVectorType>(VTy)) {\n    if (!Subtarget.useRVVForFixedLengthVectors())\n      return false;\n    // Sometimes the interleaved access pass picks up splats as interleaves of\n    // one element. Don't lower these.\n    if (FVTy->getNumElements() < 2)\n      return false;\n\n    ContainerVT = getContainerForFixedLengthVector(VT.getSimpleVT());\n  }\n\n  // Need to make sure that EMUL * NFIELDS \u2264 8\n  auto [LMUL, Fractional] = RISCVVType::decodeVLMUL(getLMUL(ContainerVT));\n  if (Fractional)\n    return true;\n  return Factor * LMUL <= 8;\n}",
      "start_line": 19977,
      "end_line": 20008,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "allowsMemoryAccessForAlignment",
        "getContainerForFixedLengthVector",
        "getValueType",
        "getSimpleVT",
        "decodeVLMUL"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isLegalStridedLoadStore",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "DataType"
        },
        {
          "type": "Align",
          "name": "Alignment"
        }
      ],
      "body": "{\n  if (!Subtarget.hasVInstructions())\n    return false;\n\n  // Only support fixed vectors if we know the minimum vector size.\n  if (DataType.isFixedLengthVector() && !Subtarget.useRVVForFixedLengthVectors())\n    return false;\n\n  EVT ScalarType = DataType.getScalarType();\n  if (!isLegalElementTypeForRVV(ScalarType))\n    return false;\n\n  if (!Subtarget.hasFastUnalignedAccess() &&\n      Alignment < ScalarType.getStoreSize())\n    return false;\n\n  return true;\n}",
      "start_line": 20010,
      "end_line": 20028,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "useRVVForFixedLengthVectors",
        "getStoreSize",
        "getScalarType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerInterleavedLoad",
      "return_type": "bool",
      "parameters": [
        {
          "type": "LoadInst",
          "name": "*LI"
        },
        {
          "type": "ArrayRef<ShuffleVectorInst *>",
          "name": "Shuffles"
        },
        {
          "type": "ArrayRef<unsigned>",
          "name": "Indices"
        },
        {
          "type": "unsigned",
          "name": "Factor"
        }
      ],
      "body": "{\n  IRBuilder<> Builder(LI);\n\n  auto *VTy = cast<FixedVectorType>(Shuffles[0]->getType());\n  if (!isLegalInterleavedAccessType(VTy, Factor, LI->getAlign(),\n                                    LI->getPointerAddressSpace(),\n                                    LI->getModule()->getDataLayout()))\n    return false;\n\n  auto *XLenTy = Type::getIntNTy(LI->getContext(), Subtarget.getXLen());\n\n  Function *VlsegNFunc =\n      Intrinsic::getDeclaration(LI->getModule(), FixedVlsegIntrIds[Factor - 2],\n                                {VTy, LI->getPointerOperandType(), XLenTy});\n\n  Value *VL = ConstantInt::get(XLenTy, VTy->getNumElements());\n\n  CallInst *VlsegN =\n      Builder.CreateCall(VlsegNFunc, {LI->getPointerOperand(), VL});\n\n  for (unsigned i = 0; i < Shuffles.size(); i++) {\n    Value *SubVec = Builder.CreateExtractValue(VlsegN, Indices[i]);\n    Shuffles[i]->replaceAllUsesWith(SubVec);\n  }\n\n  return true;\n}",
      "start_line": 20048,
      "end_line": 20076,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getModule",
        "getPointerOperandType",
        "getDeclaration",
        "replaceAllUsesWith",
        "getIntNTy",
        "get",
        "CreateCall",
        "getDataLayout",
        "getPointerAddressSpace",
        "Builder",
        "getType",
        "getXLen",
        "CreateExtractValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerInterleavedStore",
      "return_type": "bool",
      "parameters": [
        {
          "type": "StoreInst",
          "name": "*SI"
        },
        {
          "type": "ShuffleVectorInst",
          "name": "*SVI"
        },
        {
          "type": "unsigned",
          "name": "Factor"
        }
      ],
      "body": "{\n  IRBuilder<> Builder(SI);\n  auto *ShuffleVTy = cast<FixedVectorType>(SVI->getType());\n  // Given SVI : <n*factor x ty>, then VTy : <n x ty>\n  auto *VTy = FixedVectorType::get(ShuffleVTy->getElementType(),\n                                   ShuffleVTy->getNumElements() / Factor);\n  if (!isLegalInterleavedAccessType(VTy, Factor, SI->getAlign(),\n                                    SI->getPointerAddressSpace(),\n                                    SI->getModule()->getDataLayout()))\n    return false;\n\n  auto *XLenTy = Type::getIntNTy(SI->getContext(), Subtarget.getXLen());\n\n  Function *VssegNFunc =\n      Intrinsic::getDeclaration(SI->getModule(), FixedVssegIntrIds[Factor - 2],\n                                {VTy, SI->getPointerOperandType(), XLenTy});\n\n  auto Mask = SVI->getShuffleMask();\n  SmallVector<Value *, 10> Ops;\n\n  for (unsigned i = 0; i < Factor; i++) {\n    Value *Shuffle = Builder.CreateShuffleVector(\n        SVI->getOperand(0), SVI->getOperand(1),\n        createSequentialMask(Mask[i], VTy->getNumElements(), 0));\n    Ops.push_back(Shuffle);\n  }\n  // This VL should be OK (should be executable in one vsseg instruction,\n  // potentially under larger LMULs) because we checked that the fixed vector\n  // type fits in isLegalInterleavedAccessType\n  Value *VL = ConstantInt::get(XLenTy, VTy->getNumElements());\n  Ops.append({SI->getPointerOperand(), VL});\n\n  Builder.CreateCall(VssegNFunc, Ops);\n\n  return true;\n}",
      "start_line": 20100,
      "end_line": 20137,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getPointerOperandType",
        "getDataLayout",
        "getDeclaration",
        "getIntNTy",
        "CreateCall",
        "createSequentialMask",
        "getShuffleMask",
        "push_back",
        "getModule",
        "CreateShuffleVector",
        "append",
        "getOperand",
        "getPointerAddressSpace",
        "getXLen",
        "getNumElements",
        "get",
        "Builder",
        "getType",
        "OK"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerDeinterleaveIntrinsicToLoad",
      "return_type": "bool",
      "parameters": [
        {
          "type": "IntrinsicInst",
          "name": "*DI"
        },
        {
          "type": "LoadInst",
          "name": "*LI"
        }
      ],
      "body": "{\n  assert(LI->isSimple());\n  IRBuilder<> Builder(LI);\n\n  // Only deinterleave2 supported at present.\n  if (DI->getIntrinsicID() != Intrinsic::experimental_vector_deinterleave2)\n    return false;\n\n  unsigned Factor = 2;\n\n  VectorType *VTy = cast<VectorType>(DI->getOperand(0)->getType());\n  VectorType *ResVTy = cast<VectorType>(DI->getType()->getContainedType(0));\n\n  if (!isLegalInterleavedAccessType(ResVTy, Factor, LI->getAlign(),\n                                    LI->getPointerAddressSpace(),\n                                    LI->getModule()->getDataLayout()))\n    return false;\n\n  Function *VlsegNFunc;\n  Value *VL;\n  Type *XLenTy = Type::getIntNTy(LI->getContext(), Subtarget.getXLen());\n  SmallVector<Value *, 10> Ops;\n\n  if (auto *FVTy = dyn_cast<FixedVectorType>(VTy)) {\n    VlsegNFunc = Intrinsic::getDeclaration(\n        LI->getModule(), FixedVlsegIntrIds[Factor - 2],\n        {ResVTy, LI->getPointerOperandType(), XLenTy});\n    VL = ConstantInt::get(XLenTy, FVTy->getNumElements());\n  } else {\n    static const Intrinsic::ID IntrIds[] = {\n        Intrinsic::riscv_vlseg2, Intrinsic::riscv_vlseg3,\n        Intrinsic::riscv_vlseg4, Intrinsic::riscv_vlseg5,\n        Intrinsic::riscv_vlseg6, Intrinsic::riscv_vlseg7,\n        Intrinsic::riscv_vlseg8};\n\n    VlsegNFunc = Intrinsic::getDeclaration(LI->getModule(), IntrIds[Factor - 2],\n                                           {ResVTy, XLenTy});\n    VL = Constant::getAllOnesValue(XLenTy);\n    Ops.append(Factor, PoisonValue::get(ResVTy));\n  }\n\n  Ops.append({LI->getPointerOperand(), VL});\n\n  Value *Vlseg = Builder.CreateCall(VlsegNFunc, Ops);\n  DI->replaceAllUsesWith(Vlseg);\n\n  return true;\n}",
      "start_line": 20139,
      "end_line": 20187,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getModule",
        "getPointerOperandType",
        "getDeclaration",
        "getAllOnesValue",
        "replaceAllUsesWith",
        "getIntNTy",
        "get",
        "append",
        "getDataLayout",
        "CreateCall",
        "getOperand",
        "getPointerAddressSpace",
        "Builder",
        "getType",
        "getXLen",
        "getContainedType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerInterleaveIntrinsicToStore",
      "return_type": "bool",
      "parameters": [
        {
          "type": "IntrinsicInst",
          "name": "*II"
        },
        {
          "type": "StoreInst",
          "name": "*SI"
        }
      ],
      "body": "{\n  assert(SI->isSimple());\n  IRBuilder<> Builder(SI);\n\n  // Only interleave2 supported at present.\n  if (II->getIntrinsicID() != Intrinsic::experimental_vector_interleave2)\n    return false;\n\n  unsigned Factor = 2;\n\n  VectorType *VTy = cast<VectorType>(II->getType());\n  VectorType *InVTy = cast<VectorType>(II->getOperand(0)->getType());\n\n  if (!isLegalInterleavedAccessType(InVTy, Factor, SI->getAlign(),\n                                    SI->getPointerAddressSpace(),\n                                    SI->getModule()->getDataLayout()))\n    return false;\n\n  Function *VssegNFunc;\n  Value *VL;\n  Type *XLenTy = Type::getIntNTy(SI->getContext(), Subtarget.getXLen());\n\n  if (auto *FVTy = dyn_cast<FixedVectorType>(VTy)) {\n    VssegNFunc = Intrinsic::getDeclaration(\n        SI->getModule(), FixedVssegIntrIds[Factor - 2],\n        {InVTy, SI->getPointerOperandType(), XLenTy});\n    VL = ConstantInt::get(XLenTy, FVTy->getNumElements());\n  } else {\n    static const Intrinsic::ID IntrIds[] = {\n        Intrinsic::riscv_vsseg2, Intrinsic::riscv_vsseg3,\n        Intrinsic::riscv_vsseg4, Intrinsic::riscv_vsseg5,\n        Intrinsic::riscv_vsseg6, Intrinsic::riscv_vsseg7,\n        Intrinsic::riscv_vsseg8};\n\n    VssegNFunc = Intrinsic::getDeclaration(SI->getModule(), IntrIds[Factor - 2],\n                                           {InVTy, XLenTy});\n    VL = Constant::getAllOnesValue(XLenTy);\n  }\n\n  Builder.CreateCall(VssegNFunc, {II->getOperand(0), II->getOperand(1),\n                                  SI->getPointerOperand(), VL});\n\n  return true;\n}",
      "start_line": 20189,
      "end_line": 20233,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getModule",
        "getPointerOperandType",
        "getDeclaration",
        "getPointerOperand",
        "getAllOnesValue",
        "getIntNTy",
        "get",
        "CreateCall",
        "getDataLayout",
        "getOperand",
        "getPointerAddressSpace",
        "Builder",
        "getType",
        "getXLen"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "EmitKCFICheck",
      "return_type": "MachineInstr *",
      "parameters": [
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::instr_iterator",
          "name": "&MBBI"
        },
        {
          "type": "const TargetInstrInfo",
          "name": "*TII"
        }
      ],
      "body": "{\n  assert(MBBI->isCall() && MBBI->getCFIType() &&\n         \"Invalid call instruction for a KCFI check\");\n  assert(is_contained({RISCV::PseudoCALLIndirect, RISCV::PseudoTAILIndirect},\n                      MBBI->getOpcode()));\n\n  MachineOperand &Target = MBBI->getOperand(0);\n  Target.setIsRenamable(false);\n\n  return BuildMI(MBB, MBBI, MBBI->getDebugLoc(), TII->get(RISCV::KCFI_CHECK))\n      .addReg(Target.getReg())\n      .addImm(MBBI->getCFIType())\n      .getInstr();\n}",
      "start_line": 20235,
      "end_line": 20251,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "BuildMI",
        "addReg",
        "get",
        "getOperand",
        "getCFIType",
        "getInstr",
        "setIsRenamable",
        "addImm"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRegisterByName",
      "return_type": "Register",
      "parameters": [
        {
          "type": "const char",
          "name": "*RegName"
        },
        {
          "type": "LLT",
          "name": "VT"
        },
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  Register Reg = MatchRegisterAltName(RegName);\n  if (Reg == RISCV::NoRegister)\n    Reg = MatchRegisterName(RegName);\n  if (Reg == RISCV::NoRegister)\n    report_fatal_error(\n        Twine(\"Invalid register name \\\"\" + StringRef(RegName) + \"\\\".\"));\n  BitVector ReservedRegs = Subtarget.getRegisterInfo()->getReservedRegs(MF);\n  if (!ReservedRegs.test(Reg) && !Subtarget.isRegisterReservedByUser(Reg))\n    report_fatal_error(Twine(\"Trying to obtain non-reserved register \\\"\" +\n                             StringRef(RegName) + \"\\\".\"));\n  return Reg;\n}",
      "start_line": 20256,
      "end_line": 20270,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isRegisterReservedByUser",
        "getRegisterInfo",
        "MatchRegisterAltName",
        "getReservedRegs",
        "MatchRegisterName",
        "report_fatal_error"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTargetMMOFlags",
      "return_type": "Flags",
      "parameters": [
        {
          "type": "const Instruction",
          "name": "&I"
        }
      ],
      "body": "{\n  const MDNode *NontemporalInfo = I.getMetadata(LLVMContext::MD_nontemporal);\n\n  if (NontemporalInfo == nullptr)\n    return MachineMemOperand::MONone;\n\n  // 1 for default value work as __RISCV_NTLH_ALL\n  // 2 -> __RISCV_NTLH_INNERMOST_PRIVATE\n  // 3 -> __RISCV_NTLH_ALL_PRIVATE\n  // 4 -> __RISCV_NTLH_INNERMOST_SHARED\n  // 5 -> __RISCV_NTLH_ALL\n  int NontemporalLevel = 5;\n  const MDNode *RISCVNontemporalInfo =\n      I.getMetadata(\"riscv-nontemporal-domain\");\n  if (RISCVNontemporalInfo != nullptr)\n    NontemporalLevel =\n        cast<ConstantInt>(\n            cast<ConstantAsMetadata>(RISCVNontemporalInfo->getOperand(0))\n                ->getValue())\n            ->getZExtValue();\n\n  assert((1 <= NontemporalLevel && NontemporalLevel <= 5) &&\n         \"RISC-V target doesn't support this non-temporal domain.\");\n\n  NontemporalLevel -= 2;\n  MachineMemOperand::Flags Flags = MachineMemOperand::MONone;\n  if (NontemporalLevel & 0b1)\n    Flags |= MONontemporalBit0;\n  if (NontemporalLevel & 0b10)\n    Flags |= MONontemporalBit1;\n\n  return Flags;\n}",
      "start_line": 20272,
      "end_line": 20305,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getZExtValue",
        "getOperand",
        "getValue",
        "getMetadata"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getTargetMMOFlags",
      "return_type": "Flags",
      "parameters": [
        {
          "type": "const MemSDNode",
          "name": "&Node"
        }
      ],
      "body": "{\n\n  MachineMemOperand::Flags NodeFlags = Node.getMemOperand()->getFlags();\n  MachineMemOperand::Flags TargetFlags = MachineMemOperand::MONone;\n  TargetFlags |= (NodeFlags & MONontemporalBit0);\n  TargetFlags |= (NodeFlags & MONontemporalBit1);\n\n  return TargetFlags;\n}",
      "start_line": 20307,
      "end_line": 20316,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMemOperand",
        "getFlags"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "areTwoSDNodeTargetMMOFlagsMergeable",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MemSDNode",
          "name": "&NodeX"
        },
        {
          "type": "const MemSDNode",
          "name": "&NodeY"
        }
      ],
      "body": "{\n  return getTargetMMOFlags(NodeX) == getTargetMMOFlags(NodeY);\n}",
      "start_line": 20318,
      "end_line": 20321,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getTargetMMOFlags"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isCtpopFast",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        }
      ],
      "body": "{\n  if (VT.isScalableVector())\n    return isTypeLegal(VT) && Subtarget.hasStdExtZvbb();\n  if (VT.isFixedLengthVector() && Subtarget.hasStdExtZvbb())\n    return true;\n  return Subtarget.hasStdExtZbb() &&\n         (VT == MVT::i32 || VT == MVT::i64 || VT.isFixedLengthVector());\n}",
      "start_line": 20323,
      "end_line": 20330,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtZbb",
        "isTypeLegal",
        "hasStdExtZvbb",
        "isFixedLengthVector"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getCustomCtpopCost",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "ISD::CondCode",
          "name": "Cond"
        }
      ],
      "body": "{\n  return isCtpopFast(VT) ? 0 : 1;\n}",
      "start_line": 20332,
      "end_line": 20335,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isCtpopFast"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "fallBackToDAGISel",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const Instruction",
          "name": "&Inst"
        }
      ],
      "body": "{\n\n  // GISel support is in progress or complete for G_ADD, G_SUB, G_AND, G_OR, and\n  // G_XOR.\n  unsigned Op = Inst.getOpcode();\n  if (Op == Instruction::Add || Op == Instruction::Sub ||\n      Op == Instruction::And || Op == Instruction::Or || Op == Instruction::Xor)\n    return false;\n\n  if (Inst.getType()->isScalableTy())\n    return true;\n\n  for (unsigned i = 0; i < Inst.getNumOperands(); ++i)\n    if (Inst.getOperand(i)->getType()->isScalableTy() &&\n        !isa<ReturnInst>(&Inst))\n      return true;\n\n  if (const AllocaInst *AI = dyn_cast<AllocaInst>(&Inst)) {\n    if (AI->getAllocatedType()->isScalableTy())\n      return true;\n  }\n\n  return false;\n}",
      "start_line": 20337,
      "end_line": 20360,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isScalableTy",
        "getOpcode",
        "getType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "BuildSDIVPow2",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "const APInt",
          "name": "&Divisor"
        },
        {
          "type": "SelectionDAG",
          "name": "&DAG"
        },
        {
          "type": "SmallVectorImpl<SDNode *>",
          "name": "&Created"
        }
      ],
      "body": "{\n  AttributeList Attr = DAG.getMachineFunction().getFunction().getAttributes();\n  if (isIntDivCheap(N->getValueType(0), Attr))\n    return SDValue(N, 0); // Lower SDIV as SDIV\n\n  // Only perform this transform if short forward branch opt is supported.\n  if (!Subtarget.hasShortForwardBranchOpt())\n    return SDValue();\n  EVT VT = N->getValueType(0);\n  if (!(VT == MVT::i32 || (VT == MVT::i64 && Subtarget.is64Bit())))\n    return SDValue();\n\n  // Ensure 2**k-1 < 2048 so that we can just emit a single addi/addiw.\n  if (Divisor.sgt(2048) || Divisor.slt(-2048))\n    return SDValue();\n  return TargetLowering::buildSDIVPow2WithCMov(N, Divisor, DAG, Created);\n}",
      "start_line": 20362,
      "end_line": 20381,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getMachineFunction",
        "slt",
        "getValueType",
        "getAttributes",
        "buildSDIVPow2WithCMov",
        "getFunction"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "shouldFoldSelectWithSingleBitTest",
      "return_type": "bool",
      "parameters": [
        {
          "type": "EVT",
          "name": "VT"
        },
        {
          "type": "const APInt",
          "name": "&AndMask"
        }
      ],
      "body": "{\n  if (Subtarget.hasStdExtZicond() || Subtarget.hasVendorXVentanaCondOps())\n    return !Subtarget.hasStdExtZbs() && AndMask.ugt(1024);\n  return TargetLowering::shouldFoldSelectWithSingleBitTest(VT, AndMask);\n}",
      "start_line": 20383,
      "end_line": 20388,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasVendorXVentanaCondOps",
        "ugt",
        "shouldFoldSelectWithSingleBitTest",
        "hasStdExtZbs"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getMinimumJumpTableEntries",
      "return_type": "unsigned",
      "parameters": [],
      "body": "{\n  return Subtarget.getMinimumJumpTableEntries();\n}",
      "start_line": 20390,
      "end_line": 20392,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMinimumJumpTableEntries"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "PreprocessISelDAG",
      "return_type": "void",
      "parameters": [],
      "body": "{\n  SelectionDAG::allnodes_iterator Position = CurDAG->allnodes_end();\n\n  bool MadeChange = false;\n  while (Position != CurDAG->allnodes_begin()) {\n    SDNode *N = &*--Position;\n    if (N->use_empty())\n      continue;\n\n    SDValue Result;\n    switch (N->getOpcode()) {\n    case ISD::SPLAT_VECTOR: {\n      // Convert integer SPLAT_VECTOR to VMV_V_X_VL and floating-point\n      // SPLAT_VECTOR to VFMV_V_F_VL to reduce isel burden.\n      MVT VT = N->getSimpleValueType(0);\n      unsigned Opc =\n          VT.isInteger() ? RISCVISD::VMV_V_X_VL : RISCVISD::VFMV_V_F_VL;\n      SDLoc DL(N);\n      SDValue VL = CurDAG->getRegister(RISCV::X0, Subtarget->getXLenVT());\n      SDValue Src = N->getOperand(0);\n      if (VT.isInteger())\n        Src = CurDAG->getNode(ISD::ANY_EXTEND, DL, Subtarget->getXLenVT(),\n                              N->getOperand(0));\n      Result = CurDAG->getNode(Opc, DL, VT, CurDAG->getUNDEF(VT), Src, VL);\n      break;\n    }\n    case RISCVISD::SPLAT_VECTOR_SPLIT_I64_VL: {\n      // Lower SPLAT_VECTOR_SPLIT_I64 to two scalar stores and a stride 0 vector\n      // load. Done after lowering and combining so that we have a chance to\n      // optimize this to VMV_V_X_VL when the upper bits aren't needed.\n      assert(N->getNumOperands() == 4 && \"Unexpected number of operands\");\n      MVT VT = N->getSimpleValueType(0);\n      SDValue Passthru = N->getOperand(0);\n      SDValue Lo = N->getOperand(1);\n      SDValue Hi = N->getOperand(2);\n      SDValue VL = N->getOperand(3);\n      assert(VT.getVectorElementType() == MVT::i64 && VT.isScalableVector() &&\n             Lo.getValueType() == MVT::i32 && Hi.getValueType() == MVT::i32 &&\n             \"Unexpected VTs!\");\n      MachineFunction &MF = CurDAG->getMachineFunction();\n      SDLoc DL(N);\n\n      // Create temporary stack for each expanding node.\n      SDValue StackSlot =\n          CurDAG->CreateStackTemporary(TypeSize::getFixed(8), Align(8));\n      int FI = cast<FrameIndexSDNode>(StackSlot.getNode())->getIndex();\n      MachinePointerInfo MPI = MachinePointerInfo::getFixedStack(MF, FI);\n\n      SDValue Chain = CurDAG->getEntryNode();\n      Lo = CurDAG->getStore(Chain, DL, Lo, StackSlot, MPI, Align(8));\n\n      SDValue OffsetSlot =\n          CurDAG->getMemBasePlusOffset(StackSlot, TypeSize::getFixed(4), DL);\n      Hi = CurDAG->getStore(Chain, DL, Hi, OffsetSlot, MPI.getWithOffset(4),\n                            Align(8));\n\n      Chain = CurDAG->getNode(ISD::TokenFactor, DL, MVT::Other, Lo, Hi);\n\n      SDVTList VTs = CurDAG->getVTList({VT, MVT::Other});\n      SDValue IntID =\n          CurDAG->getTargetConstant(Intrinsic::riscv_vlse, DL, MVT::i64);\n      SDValue Ops[] = {Chain,\n                       IntID,\n                       Passthru,\n                       StackSlot,\n                       CurDAG->getRegister(RISCV::X0, MVT::i64),\n                       VL};\n\n      Result = CurDAG->getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, VTs, Ops,\n                                           MVT::i64, MPI, Align(8),\n                                           MachineMemOperand::MOLoad);\n      break;\n    }\n    }\n\n    if (Result) {\n      LLVM_DEBUG(dbgs() << \"RISC-V DAG preprocessing replacing:\\nOld:    \");\n      LLVM_DEBUG(N->dump(CurDAG));\n      LLVM_DEBUG(dbgs() << \"\\nNew: \");\n      LLVM_DEBUG(Result->dump(CurDAG));\n      LLVM_DEBUG(dbgs() << \"\\n\");\n\n      CurDAG->ReplaceAllUsesOfValueWith(SDValue(N, 0), Result);\n      MadeChange = true;\n    }\n  }\n\n  if (MadeChange)\n    CurDAG->RemoveDeadNodes();\n}",
      "start_line": 50,
      "end_line": 139,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getFixedStack",
        "getStore",
        "getSimpleValueType",
        "isScalableVector",
        "getIndex",
        "ReplaceAllUsesOfValueWith",
        "getMachineFunction",
        "RemoveDeadNodes",
        "getMemIntrinsicNode",
        "allnodes_end",
        "getVTList",
        "LLVM_DEBUG",
        "getEntryNode",
        "getRegister",
        "getValueType",
        "getOperand",
        "CreateStackTemporary",
        "isInteger",
        "getMemBasePlusOffset",
        "Align",
        "DL",
        "getNode",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "PostprocessISelDAG",
      "return_type": "void",
      "parameters": [],
      "body": "{\n  HandleSDNode Dummy(CurDAG->getRoot());\n  SelectionDAG::allnodes_iterator Position = CurDAG->allnodes_end();\n\n  bool MadeChange = false;\n  while (Position != CurDAG->allnodes_begin()) {\n    SDNode *N = &*--Position;\n    // Skip dead nodes and any non-machine opcodes.\n    if (N->use_empty() || !N->isMachineOpcode())\n      continue;\n\n    MadeChange |= doPeepholeSExtW(N);\n\n    // FIXME: This is here only because the VMerge transform doesn't\n    // know how to handle masked true inputs.  Once that has been moved\n    // to post-ISEL, this can be deleted as well.\n    MadeChange |= doPeepholeMaskedRVV(cast<MachineSDNode>(N));\n  }\n\n  CurDAG->setRoot(Dummy.getValue());\n\n  MadeChange |= doPeepholeMergeVVMFold();\n\n  // After we're done with everything else, convert IMPLICIT_DEF\n  // passthru operands to NoRegister.  This is required to workaround\n  // an optimization deficiency in MachineCSE.  This really should\n  // be merged back into each of the patterns (i.e. there's no good\n  // reason not to go directly to NoReg), but is being done this way\n  // to allow easy backporting.\n  MadeChange |= doPeepholeNoRegPassThru();\n\n  if (MadeChange)\n    CurDAG->RemoveDeadNodes();\n}",
      "start_line": 141,
      "end_line": 174,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "setRoot",
        "doPeepholeMaskedRVV",
        "RemoveDeadNodes",
        "isMachineOpcode",
        "Dummy",
        "patterns",
        "doPeepholeNoRegPassThru",
        "allnodes_end",
        "doPeepholeMergeVVMFold",
        "doPeepholeSExtW"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectImmSeq",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "*CurDAG"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "const MVT",
          "name": "VT"
        },
        {
          "type": "RISCVMatInt::InstSeq",
          "name": "&Seq"
        }
      ],
      "body": "{\n  SDValue SrcReg = CurDAG->getRegister(RISCV::X0, VT);\n  for (const RISCVMatInt::Inst &Inst : Seq) {\n    SDValue SDImm = CurDAG->getTargetConstant(Inst.getImm(), DL, VT);\n    SDNode *Result = nullptr;\n    switch (Inst.getOpndKind()) {\n    case RISCVMatInt::Imm:\n      Result = CurDAG->getMachineNode(Inst.getOpcode(), DL, VT, SDImm);\n      break;\n    case RISCVMatInt::RegX0:\n      Result = CurDAG->getMachineNode(Inst.getOpcode(), DL, VT, SrcReg,\n                                      CurDAG->getRegister(RISCV::X0, VT));\n      break;\n    case RISCVMatInt::RegReg:\n      Result = CurDAG->getMachineNode(Inst.getOpcode(), DL, VT, SrcReg, SrcReg);\n      break;\n    case RISCVMatInt::RegImm:\n      Result = CurDAG->getMachineNode(Inst.getOpcode(), DL, VT, SrcReg, SDImm);\n      break;\n    }\n\n    // Only the first instruction has X0 as its source.\n    SrcReg = SDValue(Result, 0);\n  }\n\n  return SrcReg;\n}",
      "start_line": 176,
      "end_line": 203,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "getMachineNode",
        "getRegister",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectImm",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "*CurDAG"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "const MVT",
          "name": "VT"
        },
        {
          "type": "int64_t",
          "name": "Imm"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "&Subtarget"
        }
      ],
      "body": "{\n  RISCVMatInt::InstSeq Seq = RISCVMatInt::generateInstSeq(Imm, Subtarget);\n\n  // Use a rematerializable pseudo instruction for short sequences if enabled.\n  if (Seq.size() == 2 && UsePseudoMovImm)\n    return SDValue(\n        CurDAG->getMachineNode(RISCV::PseudoMovImm, DL, VT,\n                               CurDAG->getTargetConstant(Imm, DL, VT)),\n        0);\n\n  // See if we can create this constant as (ADD (SLLI X, C), X) where X is at\n  // worst an LUI+ADDIW. This will require an extra register, but avoids a\n  // constant pool.\n  // If we have Zba we can use (ADD_UW X, (SLLI X, 32)) to handle cases where\n  // low and high 32 bits are the same and bit 31 and 63 are set.\n  if (Seq.size() > 3) {\n    unsigned ShiftAmt, AddOpc;\n    RISCVMatInt::InstSeq SeqLo =\n        RISCVMatInt::generateTwoRegInstSeq(Imm, Subtarget, ShiftAmt, AddOpc);\n    if (!SeqLo.empty() && (SeqLo.size() + 2) < Seq.size()) {\n      SDValue Lo = selectImmSeq(CurDAG, DL, VT, SeqLo);\n\n      SDValue SLLI = SDValue(\n          CurDAG->getMachineNode(RISCV::SLLI, DL, VT, Lo,\n                                 CurDAG->getTargetConstant(ShiftAmt, DL, VT)),\n          0);\n      return SDValue(CurDAG->getMachineNode(AddOpc, DL, VT, Lo, SLLI), 0);\n    }\n  }\n\n  // Otherwise, use the original sequence.\n  return selectImmSeq(CurDAG, DL, VT, Seq);\n}",
      "start_line": 205,
      "end_line": 238,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SDValue",
        "generateInstSeq",
        "generateTwoRegInstSeq",
        "selectImmSeq",
        "use",
        "size",
        "as"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "createTuple",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "&CurDAG"
        },
        {
          "type": "ArrayRef<SDValue>",
          "name": "Regs"
        },
        {
          "type": "unsigned",
          "name": "NF"
        },
        {
          "type": "RISCVII::VLMUL",
          "name": "LMUL"
        }
      ],
      "body": "{\n  static const unsigned M1TupleRegClassIDs[] = {\n      RISCV::VRN2M1RegClassID, RISCV::VRN3M1RegClassID, RISCV::VRN4M1RegClassID,\n      RISCV::VRN5M1RegClassID, RISCV::VRN6M1RegClassID, RISCV::VRN7M1RegClassID,\n      RISCV::VRN8M1RegClassID};\n  static const unsigned M2TupleRegClassIDs[] = {RISCV::VRN2M2RegClassID,\n                                                RISCV::VRN3M2RegClassID,\n                                                RISCV::VRN4M2RegClassID};\n\n  assert(Regs.size() >= 2 && Regs.size() <= 8);\n\n  unsigned RegClassID;\n  unsigned SubReg0;\n  switch (LMUL) {\n  default:\n    llvm_unreachable(\"Invalid LMUL.\");\n  case RISCVII::VLMUL::LMUL_F8:\n  case RISCVII::VLMUL::LMUL_F4:\n  case RISCVII::VLMUL::LMUL_F2:\n  case RISCVII::VLMUL::LMUL_1:\n    static_assert(RISCV::sub_vrm1_7 == RISCV::sub_vrm1_0 + 7,\n                  \"Unexpected subreg numbering\");\n    SubReg0 = RISCV::sub_vrm1_0;\n    RegClassID = M1TupleRegClassIDs[NF - 2];\n    break;\n  case RISCVII::VLMUL::LMUL_2:\n    static_assert(RISCV::sub_vrm2_3 == RISCV::sub_vrm2_0 + 3,\n                  \"Unexpected subreg numbering\");\n    SubReg0 = RISCV::sub_vrm2_0;\n    RegClassID = M2TupleRegClassIDs[NF - 2];\n    break;\n  case RISCVII::VLMUL::LMUL_4:\n    static_assert(RISCV::sub_vrm4_1 == RISCV::sub_vrm4_0 + 1,\n                  \"Unexpected subreg numbering\");\n    SubReg0 = RISCV::sub_vrm4_0;\n    RegClassID = RISCV::VRN2M4RegClassID;\n    break;\n  }\n\n  SDLoc DL(Regs[0]);\n  SmallVector<SDValue, 8> Ops;\n\n  Ops.push_back(CurDAG.getTargetConstant(RegClassID, DL, MVT::i32));\n\n  for (unsigned I = 0; I < Regs.size(); ++I) {\n    Ops.push_back(Regs[I]);\n    Ops.push_back(CurDAG.getTargetConstant(SubReg0 + I, DL, MVT::i32));\n  }\n  SDNode *N =\n      CurDAG.getMachineNode(TargetOpcode::REG_SEQUENCE, DL, MVT::Untyped, Ops);\n  return SDValue(N, 0);\n}",
      "start_line": 240,
      "end_line": 292,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "createTuple",
          "condition": "LMUL",
          "cases": [
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "SDValue",
        "static_assert",
        "getMachineNode",
        "DL",
        "size",
        "llvm_unreachable",
        "push_back"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "addVectorLoadStoreOperands",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        },
        {
          "type": "unsigned",
          "name": "Log2SEW"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "unsigned",
          "name": "CurOp"
        },
        {
          "type": "bool",
          "name": "IsMasked"
        },
        {
          "type": "bool",
          "name": "IsStridedOrIndexed"
        },
        {
          "type": "SmallVectorImpl<SDValue>",
          "name": "&Operands"
        },
        {
          "type": "bool",
          "name": "IsLoad"
        },
        {
          "type": "MVT",
          "name": "*IndexVT"
        }
      ],
      "body": "{\n  SDValue Chain = Node->getOperand(0);\n  SDValue Glue;\n\n  Operands.push_back(Node->getOperand(CurOp++)); // Base pointer.\n\n  if (IsStridedOrIndexed) {\n    Operands.push_back(Node->getOperand(CurOp++)); // Index.\n    if (IndexVT)\n      *IndexVT = Operands.back()->getSimpleValueType(0);\n  }\n\n  if (IsMasked) {\n    // Mask needs to be copied to V0.\n    SDValue Mask = Node->getOperand(CurOp++);\n    Chain = CurDAG->getCopyToReg(Chain, DL, RISCV::V0, Mask, SDValue());\n    Glue = Chain.getValue(1);\n    Operands.push_back(CurDAG->getRegister(RISCV::V0, Mask.getValueType()));\n  }\n  SDValue VL;\n  selectVLOp(Node->getOperand(CurOp++), VL);\n  Operands.push_back(VL);\n\n  MVT XLenVT = Subtarget->getXLenVT();\n  SDValue SEWOp = CurDAG->getTargetConstant(Log2SEW, DL, XLenVT);\n  Operands.push_back(SEWOp);\n\n  // At the IR layer, all the masked load intrinsics have policy operands,\n  // none of the others do.  All have passthru operands.  For our pseudos,\n  // all loads have policy operands.\n  if (IsLoad) {\n    uint64_t Policy = RISCVII::MASK_AGNOSTIC;\n    if (IsMasked)\n      Policy = Node->getConstantOperandVal(CurOp++);\n    SDValue PolicyOp = CurDAG->getTargetConstant(Policy, DL, XLenVT);\n    Operands.push_back(PolicyOp);\n  }\n\n  Operands.push_back(Chain); // Chain.\n  if (Glue)\n    Operands.push_back(Glue);\n}",
      "start_line": 294,
      "end_line": 338,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCopyToReg",
        "selectVLOp",
        "getValue",
        "getConstantOperandVal",
        "back",
        "getSimpleValueType",
        "getOperand",
        "getXLenVT",
        "push_back",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVLSEG",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        },
        {
          "type": "bool",
          "name": "IsMasked"
        },
        {
          "type": "bool",
          "name": "IsStrided"
        }
      ],
      "body": "{\n  SDLoc DL(Node);\n  unsigned NF = Node->getNumValues() - 1;\n  MVT VT = Node->getSimpleValueType(0);\n  unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n  RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n\n  unsigned CurOp = 2;\n  SmallVector<SDValue, 8> Operands;\n\n  SmallVector<SDValue, 8> Regs(Node->op_begin() + CurOp,\n                               Node->op_begin() + CurOp + NF);\n  SDValue Merge = createTuple(*CurDAG, Regs, NF, LMUL);\n  Operands.push_back(Merge);\n  CurOp += NF;\n\n  addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked, IsStrided,\n                             Operands, /*IsLoad=*/true);\n\n  const RISCV::VLSEGPseudo *P =\n      RISCV::getVLSEGPseudo(NF, IsMasked, IsStrided, /*FF*/ false, Log2SEW,\n                            static_cast<unsigned>(LMUL));\n  MachineSDNode *Load =\n      CurDAG->getMachineNode(P->Pseudo, DL, MVT::Untyped, MVT::Other, Operands);\n\n  if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n    CurDAG->setNodeMemRefs(Load, {MemOp->getMemOperand()});\n\n  SDValue SuperReg = SDValue(Load, 0);\n  for (unsigned I = 0; I < NF; ++I) {\n    unsigned SubRegIdx = RISCVTargetLowering::getSubregIndexByMVT(VT, I);\n    ReplaceUses(SDValue(Node, I),\n                CurDAG->getTargetExtractSubreg(SubRegIdx, DL, VT, SuperReg));\n  }\n\n  ReplaceUses(SDValue(Node, NF), SDValue(Load, 1));\n  CurDAG->RemoveDeadNode(Node);\n}",
      "start_line": 340,
      "end_line": 378,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getNumValues",
        "setNodeMemRefs",
        "getMachineNode",
        "SDValue",
        "ReplaceUses",
        "createTuple",
        "getLMUL",
        "op_begin",
        "getTargetExtractSubreg",
        "getSimpleValueType",
        "addVectorLoadStoreOperands",
        "RemoveDeadNode",
        "getVLSEGPseudo",
        "DL",
        "getSubregIndexByMVT",
        "Regs",
        "push_back",
        "Log2_32"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVLSEGFF",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        },
        {
          "type": "bool",
          "name": "IsMasked"
        }
      ],
      "body": "{\n  SDLoc DL(Node);\n  unsigned NF = Node->getNumValues() - 2; // Do not count VL and Chain.\n  MVT VT = Node->getSimpleValueType(0);\n  MVT XLenVT = Subtarget->getXLenVT();\n  unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n  RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n\n  unsigned CurOp = 2;\n  SmallVector<SDValue, 7> Operands;\n\n  SmallVector<SDValue, 8> Regs(Node->op_begin() + CurOp,\n                               Node->op_begin() + CurOp + NF);\n  SDValue MaskedOff = createTuple(*CurDAG, Regs, NF, LMUL);\n  Operands.push_back(MaskedOff);\n  CurOp += NF;\n\n  addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked,\n                             /*IsStridedOrIndexed*/ false, Operands,\n                             /*IsLoad=*/true);\n\n  const RISCV::VLSEGPseudo *P =\n      RISCV::getVLSEGPseudo(NF, IsMasked, /*Strided*/ false, /*FF*/ true,\n                            Log2SEW, static_cast<unsigned>(LMUL));\n  MachineSDNode *Load = CurDAG->getMachineNode(P->Pseudo, DL, MVT::Untyped,\n                                               XLenVT, MVT::Other, Operands);\n\n  if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n    CurDAG->setNodeMemRefs(Load, {MemOp->getMemOperand()});\n\n  SDValue SuperReg = SDValue(Load, 0);\n  for (unsigned I = 0; I < NF; ++I) {\n    unsigned SubRegIdx = RISCVTargetLowering::getSubregIndexByMVT(VT, I);\n    ReplaceUses(SDValue(Node, I),\n                CurDAG->getTargetExtractSubreg(SubRegIdx, DL, VT, SuperReg));\n  }\n\n  ReplaceUses(SDValue(Node, NF), SDValue(Load, 1));     // VL\n  ReplaceUses(SDValue(Node, NF + 1), SDValue(Load, 2)); // Chain\n  CurDAG->RemoveDeadNode(Node);\n}",
      "start_line": 380,
      "end_line": 420,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMachineNode",
        "createTuple",
        "getLMUL",
        "getTargetExtractSubreg",
        "getSimpleValueType",
        "Regs",
        "Log2_32",
        "setNodeMemRefs",
        "op_begin",
        "addVectorLoadStoreOperands",
        "getSubregIndexByMVT",
        "push_back",
        "getNumValues",
        "SDValue",
        "RemoveDeadNode",
        "ReplaceUses",
        "getVLSEGPseudo",
        "DL",
        "getXLenVT"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVLXSEG",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        },
        {
          "type": "bool",
          "name": "IsMasked"
        },
        {
          "type": "bool",
          "name": "IsOrdered"
        }
      ],
      "body": "{\n  SDLoc DL(Node);\n  unsigned NF = Node->getNumValues() - 1;\n  MVT VT = Node->getSimpleValueType(0);\n  unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n  RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n\n  unsigned CurOp = 2;\n  SmallVector<SDValue, 8> Operands;\n\n  SmallVector<SDValue, 8> Regs(Node->op_begin() + CurOp,\n                               Node->op_begin() + CurOp + NF);\n  SDValue MaskedOff = createTuple(*CurDAG, Regs, NF, LMUL);\n  Operands.push_back(MaskedOff);\n  CurOp += NF;\n\n  MVT IndexVT;\n  addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked,\n                             /*IsStridedOrIndexed*/ true, Operands,\n                             /*IsLoad=*/true, &IndexVT);\n\n  assert(VT.getVectorElementCount() == IndexVT.getVectorElementCount() &&\n         \"Element count mismatch\");\n\n  RISCVII::VLMUL IndexLMUL = RISCVTargetLowering::getLMUL(IndexVT);\n  unsigned IndexLog2EEW = Log2_32(IndexVT.getScalarSizeInBits());\n  if (IndexLog2EEW == 6 && !Subtarget->is64Bit()) {\n    report_fatal_error(\"The V extension does not support EEW=64 for index \"\n                       \"values when XLEN=32\");\n  }\n  const RISCV::VLXSEGPseudo *P = RISCV::getVLXSEGPseudo(\n      NF, IsMasked, IsOrdered, IndexLog2EEW, static_cast<unsigned>(LMUL),\n      static_cast<unsigned>(IndexLMUL));\n  MachineSDNode *Load =\n      CurDAG->getMachineNode(P->Pseudo, DL, MVT::Untyped, MVT::Other, Operands);\n\n  if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n    CurDAG->setNodeMemRefs(Load, {MemOp->getMemOperand()});\n\n  SDValue SuperReg = SDValue(Load, 0);\n  for (unsigned I = 0; I < NF; ++I) {\n    unsigned SubRegIdx = RISCVTargetLowering::getSubregIndexByMVT(VT, I);\n    ReplaceUses(SDValue(Node, I),\n                CurDAG->getTargetExtractSubreg(SubRegIdx, DL, VT, SuperReg));\n  }\n\n  ReplaceUses(SDValue(Node, NF), SDValue(Load, 1));\n  CurDAG->RemoveDeadNode(Node);\n}",
      "start_line": 422,
      "end_line": 471,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMachineNode",
        "createTuple",
        "getLMUL",
        "getTargetExtractSubreg",
        "getSimpleValueType",
        "Regs",
        "Log2_32",
        "setNodeMemRefs",
        "op_begin",
        "addVectorLoadStoreOperands",
        "getSubregIndexByMVT",
        "getVectorElementCount",
        "push_back",
        "getNumValues",
        "SDValue",
        "RemoveDeadNode",
        "getVLXSEGPseudo",
        "ReplaceUses",
        "DL",
        "report_fatal_error"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSSEG",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        },
        {
          "type": "bool",
          "name": "IsMasked"
        },
        {
          "type": "bool",
          "name": "IsStrided"
        }
      ],
      "body": "{\n  SDLoc DL(Node);\n  unsigned NF = Node->getNumOperands() - 4;\n  if (IsStrided)\n    NF--;\n  if (IsMasked)\n    NF--;\n  MVT VT = Node->getOperand(2)->getSimpleValueType(0);\n  unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n  RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n  SmallVector<SDValue, 8> Regs(Node->op_begin() + 2, Node->op_begin() + 2 + NF);\n  SDValue StoreVal = createTuple(*CurDAG, Regs, NF, LMUL);\n\n  SmallVector<SDValue, 8> Operands;\n  Operands.push_back(StoreVal);\n  unsigned CurOp = 2 + NF;\n\n  addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked, IsStrided,\n                             Operands);\n\n  const RISCV::VSSEGPseudo *P = RISCV::getVSSEGPseudo(\n      NF, IsMasked, IsStrided, Log2SEW, static_cast<unsigned>(LMUL));\n  MachineSDNode *Store =\n      CurDAG->getMachineNode(P->Pseudo, DL, Node->getValueType(0), Operands);\n\n  if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n    CurDAG->setNodeMemRefs(Store, {MemOp->getMemOperand()});\n\n  ReplaceNode(Node, Store);\n}",
      "start_line": 473,
      "end_line": 503,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "setNodeMemRefs",
        "getMachineNode",
        "getNumOperands",
        "createTuple",
        "getLMUL",
        "op_begin",
        "ReplaceNode",
        "getSimpleValueType",
        "addVectorLoadStoreOperands",
        "getOperand",
        "DL",
        "Regs",
        "getVSSEGPseudo",
        "push_back",
        "Log2_32"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSXSEG",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        },
        {
          "type": "bool",
          "name": "IsMasked"
        },
        {
          "type": "bool",
          "name": "IsOrdered"
        }
      ],
      "body": "{\n  SDLoc DL(Node);\n  unsigned NF = Node->getNumOperands() - 5;\n  if (IsMasked)\n    --NF;\n  MVT VT = Node->getOperand(2)->getSimpleValueType(0);\n  unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n  RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n  SmallVector<SDValue, 8> Regs(Node->op_begin() + 2, Node->op_begin() + 2 + NF);\n  SDValue StoreVal = createTuple(*CurDAG, Regs, NF, LMUL);\n\n  SmallVector<SDValue, 8> Operands;\n  Operands.push_back(StoreVal);\n  unsigned CurOp = 2 + NF;\n\n  MVT IndexVT;\n  addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked,\n                             /*IsStridedOrIndexed*/ true, Operands,\n                             /*IsLoad=*/false, &IndexVT);\n\n  assert(VT.getVectorElementCount() == IndexVT.getVectorElementCount() &&\n         \"Element count mismatch\");\n\n  RISCVII::VLMUL IndexLMUL = RISCVTargetLowering::getLMUL(IndexVT);\n  unsigned IndexLog2EEW = Log2_32(IndexVT.getScalarSizeInBits());\n  if (IndexLog2EEW == 6 && !Subtarget->is64Bit()) {\n    report_fatal_error(\"The V extension does not support EEW=64 for index \"\n                       \"values when XLEN=32\");\n  }\n  const RISCV::VSXSEGPseudo *P = RISCV::getVSXSEGPseudo(\n      NF, IsMasked, IsOrdered, IndexLog2EEW, static_cast<unsigned>(LMUL),\n      static_cast<unsigned>(IndexLMUL));\n  MachineSDNode *Store =\n      CurDAG->getMachineNode(P->Pseudo, DL, Node->getValueType(0), Operands);\n\n  if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n    CurDAG->setNodeMemRefs(Store, {MemOp->getMemOperand()});\n\n  ReplaceNode(Node, Store);\n}",
      "start_line": 505,
      "end_line": 545,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "setNodeMemRefs",
        "getVSXSEGPseudo",
        "getMachineNode",
        "getNumOperands",
        "createTuple",
        "getLMUL",
        "op_begin",
        "ReplaceNode",
        "getSimpleValueType",
        "addVectorLoadStoreOperands",
        "getOperand",
        "DL",
        "getVectorElementCount",
        "Regs",
        "report_fatal_error",
        "push_back",
        "Log2_32"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSETVLI",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        }
      ],
      "body": "{\n  if (!Subtarget->hasVInstructions())\n    return;\n\n  assert(Node->getOpcode() == ISD::INTRINSIC_WO_CHAIN && \"Unexpected opcode\");\n\n  SDLoc DL(Node);\n  MVT XLenVT = Subtarget->getXLenVT();\n\n  unsigned IntNo = Node->getConstantOperandVal(0);\n\n  assert((IntNo == Intrinsic::riscv_vsetvli ||\n          IntNo == Intrinsic::riscv_vsetvlimax) &&\n         \"Unexpected vsetvli intrinsic\");\n\n  bool VLMax = IntNo == Intrinsic::riscv_vsetvlimax;\n  unsigned Offset = (VLMax ? 1 : 2);\n\n  assert(Node->getNumOperands() == Offset + 2 &&\n         \"Unexpected number of operands\");\n\n  unsigned SEW =\n      RISCVVType::decodeVSEW(Node->getConstantOperandVal(Offset) & 0x7);\n  RISCVII::VLMUL VLMul = static_cast<RISCVII::VLMUL>(\n      Node->getConstantOperandVal(Offset + 1) & 0x7);\n\n  unsigned VTypeI = RISCVVType::encodeVTYPE(VLMul, SEW, /*TailAgnostic*/ true,\n                                            /*MaskAgnostic*/ true);\n  SDValue VTypeIOp = CurDAG->getTargetConstant(VTypeI, DL, XLenVT);\n\n  SDValue VLOperand;\n  unsigned Opcode = RISCV::PseudoVSETVLI;\n  if (auto *C = dyn_cast<ConstantSDNode>(Node->getOperand(1))) {\n    const unsigned VLEN = Subtarget->getRealMinVLen();\n    if (VLEN == Subtarget->getRealMaxVLen())\n      if (VLEN / RISCVVType::getSEWLMULRatio(SEW, VLMul) == C->getZExtValue())\n        VLMax = true;\n  }\n  if (VLMax || isAllOnesConstant(Node->getOperand(1))) {\n    VLOperand = CurDAG->getRegister(RISCV::X0, XLenVT);\n    Opcode = RISCV::PseudoVSETVLIX0;\n  } else {\n    VLOperand = Node->getOperand(1);\n\n    if (auto *C = dyn_cast<ConstantSDNode>(VLOperand)) {\n      uint64_t AVL = C->getZExtValue();\n      if (isUInt<5>(AVL)) {\n        SDValue VLImm = CurDAG->getTargetConstant(AVL, DL, XLenVT);\n        ReplaceNode(Node, CurDAG->getMachineNode(RISCV::PseudoVSETIVLI, DL,\n                                                 XLenVT, VLImm, VTypeIOp));\n        return;\n      }\n    }\n  }\n\n  ReplaceNode(Node,\n              CurDAG->getMachineNode(Opcode, DL, XLenVT, VLOperand, VTypeIOp));\n}",
      "start_line": 547,
      "end_line": 604,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getZExtValue",
        "getConstantOperandVal",
        "encodeVTYPE",
        "getRealMinVLen",
        "ReplaceNode",
        "getRegister",
        "getOperand",
        "DL",
        "getXLenVT",
        "decodeVSEW",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "tryShrinkShlLogicImm",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        }
      ],
      "body": "{\n  MVT VT = Node->getSimpleValueType(0);\n  unsigned Opcode = Node->getOpcode();\n  assert((Opcode == ISD::AND || Opcode == ISD::OR || Opcode == ISD::XOR) &&\n         \"Unexpected opcode\");\n  SDLoc DL(Node);\n\n  // For operations of the form (x << C1) op C2, check if we can use\n  // ANDI/ORI/XORI by transforming it into (x op (C2>>C1)) << C1.\n  SDValue N0 = Node->getOperand(0);\n  SDValue N1 = Node->getOperand(1);\n\n  ConstantSDNode *Cst = dyn_cast<ConstantSDNode>(N1);\n  if (!Cst)\n    return false;\n\n  int64_t Val = Cst->getSExtValue();\n\n  // Check if immediate can already use ANDI/ORI/XORI.\n  if (isInt<12>(Val))\n    return false;\n\n  SDValue Shift = N0;\n\n  // If Val is simm32 and we have a sext_inreg from i32, then the binop\n  // produces at least 33 sign bits. We can peek through the sext_inreg and use\n  // a SLLIW at the end.\n  bool SignExt = false;\n  if (isInt<32>(Val) && N0.getOpcode() == ISD::SIGN_EXTEND_INREG &&\n      N0.hasOneUse() && cast<VTSDNode>(N0.getOperand(1))->getVT() == MVT::i32) {\n    SignExt = true;\n    Shift = N0.getOperand(0);\n  }\n\n  if (Shift.getOpcode() != ISD::SHL || !Shift.hasOneUse())\n    return false;\n\n  ConstantSDNode *ShlCst = dyn_cast<ConstantSDNode>(Shift.getOperand(1));\n  if (!ShlCst)\n    return false;\n\n  uint64_t ShAmt = ShlCst->getZExtValue();\n\n  // Make sure that we don't change the operation by removing bits.\n  // This only matters for OR and XOR, AND is unaffected.\n  uint64_t RemovedBitsMask = maskTrailingOnes<uint64_t>(ShAmt);\n  if (Opcode != ISD::AND && (Val & RemovedBitsMask) != 0)\n    return false;\n\n  int64_t ShiftedVal = Val >> ShAmt;\n  if (!isInt<12>(ShiftedVal))\n    return false;\n\n  // If we peeked through a sext_inreg, make sure the shift is valid for SLLIW.\n  if (SignExt && ShAmt >= 32)\n    return false;\n\n  // Ok, we can reorder to get a smaller immediate.\n  unsigned BinOpc;\n  switch (Opcode) {\n  default: llvm_unreachable(\"Unexpected opcode\");\n  case ISD::AND: BinOpc = RISCV::ANDI; break;\n  case ISD::OR:  BinOpc = RISCV::ORI;  break;\n  case ISD::XOR: BinOpc = RISCV::XORI; break;\n  }\n\n  unsigned ShOpc = SignExt ? RISCV::SLLIW : RISCV::SLLI;\n\n  SDNode *BinOp =\n      CurDAG->getMachineNode(BinOpc, DL, VT, Shift.getOperand(0),\n                             CurDAG->getTargetConstant(ShiftedVal, DL, VT));\n  SDNode *SLLI =\n      CurDAG->getMachineNode(ShOpc, DL, VT, SDValue(BinOp, 0),\n                             CurDAG->getTargetConstant(ShAmt, DL, VT));\n  ReplaceNode(Node, SLLI);\n  return true;\n}",
      "start_line": 606,
      "end_line": 682,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "tryShrinkShlLogicImm",
          "condition": "Opcode",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getZExtValue",
        "hasOneUse",
        "getMachineNode",
        "getOpcode",
        "ReplaceNode",
        "getSimpleValueType",
        "form",
        "into",
        "getVT",
        "getOperand",
        "DL",
        "llvm_unreachable",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "trySignedBitfieldExtract",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        }
      ],
      "body": "{\n  // Only supported with XTHeadBb at the moment.\n  if (!Subtarget->hasVendorXTHeadBb())\n    return false;\n\n  auto *N1C = dyn_cast<ConstantSDNode>(Node->getOperand(1));\n  if (!N1C)\n    return false;\n\n  SDValue N0 = Node->getOperand(0);\n  if (!N0.hasOneUse())\n    return false;\n\n  auto BitfieldExtract = [&](SDValue N0, unsigned Msb, unsigned Lsb, SDLoc DL,\n                             MVT VT) {\n    return CurDAG->getMachineNode(RISCV::TH_EXT, DL, VT, N0.getOperand(0),\n                                  CurDAG->getTargetConstant(Msb, DL, VT),\n                                  CurDAG->getTargetConstant(Lsb, DL, VT));\n  };\n\n  SDLoc DL(Node);\n  MVT VT = Node->getSimpleValueType(0);\n  const unsigned RightShAmt = N1C->getZExtValue();\n\n  // Transform (sra (shl X, C1) C2) with C1 < C2\n  //        -> (TH.EXT X, msb, lsb)\n  if (N0.getOpcode() == ISD::SHL) {\n    auto *N01C = dyn_cast<ConstantSDNode>(N0->getOperand(1));\n    if (!N01C)\n      return false;\n\n    const unsigned LeftShAmt = N01C->getZExtValue();\n    // Make sure that this is a bitfield extraction (i.e., the shift-right\n    // amount can not be less than the left-shift).\n    if (LeftShAmt > RightShAmt)\n      return false;\n\n    const unsigned MsbPlusOne = VT.getSizeInBits() - LeftShAmt;\n    const unsigned Msb = MsbPlusOne - 1;\n    const unsigned Lsb = RightShAmt - LeftShAmt;\n\n    SDNode *TH_EXT = BitfieldExtract(N0, Msb, Lsb, DL, VT);\n    ReplaceNode(Node, TH_EXT);\n    return true;\n  }\n\n  // Transform (sra (sext_inreg X, _), C) ->\n  //           (TH.EXT X, msb, lsb)\n  if (N0.getOpcode() == ISD::SIGN_EXTEND_INREG) {\n    unsigned ExtSize =\n        cast<VTSDNode>(N0.getOperand(1))->getVT().getSizeInBits();\n\n    // ExtSize of 32 should use sraiw via tablegen pattern.\n    if (ExtSize == 32)\n      return false;\n\n    const unsigned Msb = ExtSize - 1;\n    const unsigned Lsb = RightShAmt;\n\n    SDNode *TH_EXT = BitfieldExtract(N0, Msb, Lsb, DL, VT);\n    ReplaceNode(Node, TH_EXT);\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 684,
      "end_line": 749,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getZExtValue",
        "getMachineNode",
        "ReplaceNode",
        "getSimpleValueType",
        "getVT",
        "getOperand",
        "extraction",
        "Transform",
        "DL",
        "getSizeInBits",
        "BitfieldExtract",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "tryIndexedLoad",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        }
      ],
      "body": "{\n  // Target does not support indexed loads.\n  if (!Subtarget->hasVendorXTHeadMemIdx())\n    return false;\n\n  LoadSDNode *Ld = cast<LoadSDNode>(Node);\n  ISD::MemIndexedMode AM = Ld->getAddressingMode();\n  if (AM == ISD::UNINDEXED)\n    return false;\n\n  const ConstantSDNode *C = dyn_cast<ConstantSDNode>(Ld->getOffset());\n  if (!C)\n    return false;\n\n  EVT LoadVT = Ld->getMemoryVT();\n  assert((AM == ISD::PRE_INC || AM == ISD::POST_INC) &&\n         \"Unexpected addressing mode\");\n  bool IsPre = AM == ISD::PRE_INC;\n  bool IsPost = AM == ISD::POST_INC;\n  int64_t Offset = C->getSExtValue();\n\n  // The constants that can be encoded in the THeadMemIdx instructions\n  // are of the form (sign_extend(imm5) << imm2).\n  int64_t Shift;\n  for (Shift = 0; Shift < 4; Shift++)\n    if (isInt<5>(Offset >> Shift) && ((Offset % (1LL << Shift)) == 0))\n      break;\n\n  // Constant cannot be encoded.\n  if (Shift == 4)\n    return false;\n\n  bool IsZExt = (Ld->getExtensionType() == ISD::ZEXTLOAD);\n  unsigned Opcode;\n  if (LoadVT == MVT::i8 && IsPre)\n    Opcode = IsZExt ? RISCV::TH_LBUIB : RISCV::TH_LBIB;\n  else if (LoadVT == MVT::i8 && IsPost)\n    Opcode = IsZExt ? RISCV::TH_LBUIA : RISCV::TH_LBIA;\n  else if (LoadVT == MVT::i16 && IsPre)\n    Opcode = IsZExt ? RISCV::TH_LHUIB : RISCV::TH_LHIB;\n  else if (LoadVT == MVT::i16 && IsPost)\n    Opcode = IsZExt ? RISCV::TH_LHUIA : RISCV::TH_LHIA;\n  else if (LoadVT == MVT::i32 && IsPre)\n    Opcode = IsZExt ? RISCV::TH_LWUIB : RISCV::TH_LWIB;\n  else if (LoadVT == MVT::i32 && IsPost)\n    Opcode = IsZExt ? RISCV::TH_LWUIA : RISCV::TH_LWIA;\n  else if (LoadVT == MVT::i64 && IsPre)\n    Opcode = RISCV::TH_LDIB;\n  else if (LoadVT == MVT::i64 && IsPost)\n    Opcode = RISCV::TH_LDIA;\n  else\n    return false;\n\n  EVT Ty = Ld->getOffset().getValueType();\n  SDValue Ops[] = {Ld->getBasePtr(),\n                   CurDAG->getTargetConstant(Offset >> Shift, SDLoc(Node), Ty),\n                   CurDAG->getTargetConstant(Shift, SDLoc(Node), Ty),\n                   Ld->getChain()};\n  SDNode *New = CurDAG->getMachineNode(Opcode, SDLoc(Node), Ld->getValueType(0),\n                                       Ld->getValueType(1), MVT::Other, Ops);\n\n  MachineMemOperand *MemOp = cast<MemSDNode>(Node)->getMemOperand();\n  CurDAG->setNodeMemRefs(cast<MachineSDNode>(New), {MemOp});\n\n  ReplaceNode(Node, New);\n\n  return true;\n}",
      "start_line": 751,
      "end_line": 818,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "setNodeMemRefs",
        "getMemOperand",
        "getMachineNode",
        "getBasePtr",
        "getOffset",
        "getChain",
        "getMemoryVT",
        "ReplaceNode",
        "form",
        "getExtensionType",
        "getValueType",
        "getAddressingMode",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "Select",
      "return_type": "void",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        }
      ],
      "body": "{\n  // If we have a custom node, we have already selected.\n  if (Node->isMachineOpcode()) {\n    LLVM_DEBUG(dbgs() << \"== \"; Node->dump(CurDAG); dbgs() << \"\\n\");\n    Node->setNodeId(-1);\n    return;\n  }\n\n  // Instruction Selection not handled by the auto-generated tablegen selection\n  // should be handled here.\n  unsigned Opcode = Node->getOpcode();\n  MVT XLenVT = Subtarget->getXLenVT();\n  SDLoc DL(Node);\n  MVT VT = Node->getSimpleValueType(0);\n\n  bool HasBitTest = Subtarget->hasStdExtZbs() || Subtarget->hasVendorXTHeadBs();\n\n  switch (Opcode) {\n  case ISD::Constant: {\n    assert((VT == Subtarget->getXLenVT() || VT == MVT::i32) && \"Unexpected VT\");\n    auto *ConstNode = cast<ConstantSDNode>(Node);\n    if (ConstNode->isZero()) {\n      SDValue New =\n          CurDAG->getCopyFromReg(CurDAG->getEntryNode(), DL, RISCV::X0, VT);\n      ReplaceNode(Node, New.getNode());\n      return;\n    }\n    int64_t Imm = ConstNode->getSExtValue();\n    // If the upper XLen-16 bits are not used, try to convert this to a simm12\n    // by sign extending bit 15.\n    if (isUInt<16>(Imm) && isInt<12>(SignExtend64<16>(Imm)) &&\n        hasAllHUsers(Node))\n      Imm = SignExtend64<16>(Imm);\n    // If the upper 32-bits are not used try to convert this into a simm32 by\n    // sign extending bit 32.\n    if (!isInt<32>(Imm) && isUInt<32>(Imm) && hasAllWUsers(Node))\n      Imm = SignExtend64<32>(Imm);\n\n    ReplaceNode(Node, selectImm(CurDAG, DL, VT, Imm, *Subtarget).getNode());\n    return;\n  }\n  case ISD::ConstantFP: {\n    const APFloat &APF = cast<ConstantFPSDNode>(Node)->getValueAPF();\n    auto [FPImm, NeedsFNeg] =\n        static_cast<const RISCVTargetLowering *>(TLI)->getLegalZfaFPImm(APF,\n                                                                        VT);\n    if (FPImm >= 0) {\n      unsigned Opc;\n      unsigned FNegOpc;\n      switch (VT.SimpleTy) {\n      default:\n        llvm_unreachable(\"Unexpected size\");\n      case MVT::f16:\n        Opc = RISCV::FLI_H;\n        FNegOpc = RISCV::FSGNJN_H;\n        break;\n      case MVT::f32:\n        Opc = RISCV::FLI_S;\n        FNegOpc = RISCV::FSGNJN_S;\n        break;\n      case MVT::f64:\n        Opc = RISCV::FLI_D;\n        FNegOpc = RISCV::FSGNJN_D;\n        break;\n      }\n      SDNode *Res = CurDAG->getMachineNode(\n          Opc, DL, VT, CurDAG->getTargetConstant(FPImm, DL, XLenVT));\n      if (NeedsFNeg)\n        Res = CurDAG->getMachineNode(FNegOpc, DL, VT, SDValue(Res, 0),\n                                     SDValue(Res, 0));\n\n      ReplaceNode(Node, Res);\n      return;\n    }\n\n    bool NegZeroF64 = APF.isNegZero() && VT == MVT::f64;\n    SDValue Imm;\n    // For +0.0 or f64 -0.0 we need to start from X0. For all others, we will\n    // create an integer immediate.\n    if (APF.isPosZero() || NegZeroF64)\n      Imm = CurDAG->getRegister(RISCV::X0, XLenVT);\n    else\n      Imm = selectImm(CurDAG, DL, XLenVT, APF.bitcastToAPInt().getSExtValue(),\n                      *Subtarget);\n\n    bool HasZdinx = Subtarget->hasStdExtZdinx();\n    bool Is64Bit = Subtarget->is64Bit();\n    unsigned Opc;\n    switch (VT.SimpleTy) {\n    default:\n      llvm_unreachable(\"Unexpected size\");\n    case MVT::bf16:\n      assert(Subtarget->hasStdExtZfbfmin());\n      Opc = RISCV::FMV_H_X;\n      break;\n    case MVT::f16:\n      Opc = Subtarget->hasStdExtZhinxmin() ? RISCV::COPY : RISCV::FMV_H_X;\n      break;\n    case MVT::f32:\n      Opc = Subtarget->hasStdExtZfinx() ? RISCV::COPY : RISCV::FMV_W_X;\n      break;\n    case MVT::f64:\n      // For RV32, we can't move from a GPR, we need to convert instead. This\n      // should only happen for +0.0 and -0.0.\n      assert((Subtarget->is64Bit() || APF.isZero()) && \"Unexpected constant\");\n      if (Is64Bit)\n        Opc = HasZdinx ? RISCV::COPY : RISCV::FMV_D_X;\n      else\n        Opc = HasZdinx ? RISCV::FCVT_D_W_IN32X : RISCV::FCVT_D_W;\n      break;\n    }\n\n    SDNode *Res;\n    if (Opc == RISCV::FCVT_D_W_IN32X || Opc == RISCV::FCVT_D_W)\n      Res = CurDAG->getMachineNode(\n          Opc, DL, VT, Imm,\n          CurDAG->getTargetConstant(RISCVFPRndMode::RNE, DL, XLenVT));\n    else\n      Res = CurDAG->getMachineNode(Opc, DL, VT, Imm);\n\n    // For f64 -0.0, we need to insert a fneg.d idiom.\n    if (NegZeroF64) {\n      Opc = RISCV::FSGNJN_D;\n      if (HasZdinx)\n        Opc = Is64Bit ? RISCV::FSGNJN_D_INX : RISCV::FSGNJN_D_IN32X;\n      Res =\n          CurDAG->getMachineNode(Opc, DL, VT, SDValue(Res, 0), SDValue(Res, 0));\n    }\n\n    ReplaceNode(Node, Res);\n    return;\n  }\n  case RISCVISD::SplitF64: {\n    if (!Subtarget->hasStdExtZfa())\n      break;\n    assert(Subtarget->hasStdExtD() && !Subtarget->is64Bit() &&\n           \"Unexpected subtarget\");\n\n    // With Zfa, lower to fmv.x.w and fmvh.x.d.\n    if (!SDValue(Node, 0).use_empty()) {\n      SDNode *Lo = CurDAG->getMachineNode(RISCV::FMV_X_W_FPR64, DL, VT,\n                                          Node->getOperand(0));\n      ReplaceUses(SDValue(Node, 0), SDValue(Lo, 0));\n    }\n    if (!SDValue(Node, 1).use_empty()) {\n      SDNode *Hi = CurDAG->getMachineNode(RISCV::FMVH_X_D, DL, VT,\n                                          Node->getOperand(0));\n      ReplaceUses(SDValue(Node, 1), SDValue(Hi, 0));\n    }\n\n    CurDAG->RemoveDeadNode(Node);\n    return;\n  }\n  case ISD::SHL: {\n    auto *N1C = dyn_cast<ConstantSDNode>(Node->getOperand(1));\n    if (!N1C)\n      break;\n    SDValue N0 = Node->getOperand(0);\n    if (N0.getOpcode() != ISD::AND || !N0.hasOneUse() ||\n        !isa<ConstantSDNode>(N0.getOperand(1)))\n      break;\n    unsigned ShAmt = N1C->getZExtValue();\n    uint64_t Mask = N0.getConstantOperandVal(1);\n\n    // Optimize (shl (and X, C2), C) -> (slli (srliw X, C3), C3+C) where C2 has\n    // 32 leading zeros and C3 trailing zeros.\n    if (ShAmt <= 32 && isShiftedMask_64(Mask)) {\n      unsigned XLen = Subtarget->getXLen();\n      unsigned LeadingZeros = XLen - llvm::bit_width(Mask);\n      unsigned TrailingZeros = llvm::countr_zero(Mask);\n      if (TrailingZeros > 0 && LeadingZeros == 32) {\n        SDNode *SRLIW = CurDAG->getMachineNode(\n            RISCV::SRLIW, DL, VT, N0->getOperand(0),\n            CurDAG->getTargetConstant(TrailingZeros, DL, VT));\n        SDNode *SLLI = CurDAG->getMachineNode(\n            RISCV::SLLI, DL, VT, SDValue(SRLIW, 0),\n            CurDAG->getTargetConstant(TrailingZeros + ShAmt, DL, VT));\n        ReplaceNode(Node, SLLI);\n        return;\n      }\n    }\n    break;\n  }\n  case ISD::SRL: {\n    auto *N1C = dyn_cast<ConstantSDNode>(Node->getOperand(1));\n    if (!N1C)\n      break;\n    SDValue N0 = Node->getOperand(0);\n    if (N0.getOpcode() != ISD::AND || !isa<ConstantSDNode>(N0.getOperand(1)))\n      break;\n    unsigned ShAmt = N1C->getZExtValue();\n    uint64_t Mask = N0.getConstantOperandVal(1);\n\n    // Optimize (srl (and X, C2), C) -> (slli (srliw X, C3), C3-C) where C2 has\n    // 32 leading zeros and C3 trailing zeros.\n    if (isShiftedMask_64(Mask) && N0.hasOneUse()) {\n      unsigned XLen = Subtarget->getXLen();\n      unsigned LeadingZeros = XLen - llvm::bit_width(Mask);\n      unsigned TrailingZeros = llvm::countr_zero(Mask);\n      if (LeadingZeros == 32 && TrailingZeros > ShAmt) {\n        SDNode *SRLIW = CurDAG->getMachineNode(\n            RISCV::SRLIW, DL, VT, N0->getOperand(0),\n            CurDAG->getTargetConstant(TrailingZeros, DL, VT));\n        SDNode *SLLI = CurDAG->getMachineNode(\n            RISCV::SLLI, DL, VT, SDValue(SRLIW, 0),\n            CurDAG->getTargetConstant(TrailingZeros - ShAmt, DL, VT));\n        ReplaceNode(Node, SLLI);\n        return;\n      }\n    }\n\n    // Optimize (srl (and X, C2), C) ->\n    //          (srli (slli X, (XLen-C3), (XLen-C3) + C)\n    // Where C2 is a mask with C3 trailing ones.\n    // Taking into account that the C2 may have had lower bits unset by\n    // SimplifyDemandedBits. This avoids materializing the C2 immediate.\n    // This pattern occurs when type legalizing right shifts for types with\n    // less than XLen bits.\n    Mask |= maskTrailingOnes<uint64_t>(ShAmt);\n    if (!isMask_64(Mask))\n      break;\n    unsigned TrailingOnes = llvm::countr_one(Mask);\n    if (ShAmt >= TrailingOnes)\n      break;\n    // If the mask has 32 trailing ones, use SRLI on RV32 or SRLIW on RV64.\n    if (TrailingOnes == 32) {\n      SDNode *SRLI = CurDAG->getMachineNode(\n          Subtarget->is64Bit() ? RISCV::SRLIW : RISCV::SRLI, DL, VT,\n          N0->getOperand(0), CurDAG->getTargetConstant(ShAmt, DL, VT));\n      ReplaceNode(Node, SRLI);\n      return;\n    }\n\n    // Only do the remaining transforms if the AND has one use.\n    if (!N0.hasOneUse())\n      break;\n\n    // If C2 is (1 << ShAmt) use bexti or th.tst if possible.\n    if (HasBitTest && ShAmt + 1 == TrailingOnes) {\n      SDNode *BEXTI = CurDAG->getMachineNode(\n          Subtarget->hasStdExtZbs() ? RISCV::BEXTI : RISCV::TH_TST, DL, VT,\n          N0->getOperand(0), CurDAG->getTargetConstant(ShAmt, DL, VT));\n      ReplaceNode(Node, BEXTI);\n      return;\n    }\n\n    unsigned LShAmt = Subtarget->getXLen() - TrailingOnes;\n    SDNode *SLLI =\n        CurDAG->getMachineNode(RISCV::SLLI, DL, VT, N0->getOperand(0),\n                               CurDAG->getTargetConstant(LShAmt, DL, VT));\n    SDNode *SRLI = CurDAG->getMachineNode(\n        RISCV::SRLI, DL, VT, SDValue(SLLI, 0),\n        CurDAG->getTargetConstant(LShAmt + ShAmt, DL, VT));\n    ReplaceNode(Node, SRLI);\n    return;\n  }\n  case ISD::SRA: {\n    if (trySignedBitfieldExtract(Node))\n      return;\n\n    // Optimize (sra (sext_inreg X, i16), C) ->\n    //          (srai (slli X, (XLen-16), (XLen-16) + C)\n    // And      (sra (sext_inreg X, i8), C) ->\n    //          (srai (slli X, (XLen-8), (XLen-8) + C)\n    // This can occur when Zbb is enabled, which makes sext_inreg i16/i8 legal.\n    // This transform matches the code we get without Zbb. The shifts are more\n    // compressible, and this can help expose CSE opportunities in the sdiv by\n    // constant optimization.\n    auto *N1C = dyn_cast<ConstantSDNode>(Node->getOperand(1));\n    if (!N1C)\n      break;\n    SDValue N0 = Node->getOperand(0);\n    if (N0.getOpcode() != ISD::SIGN_EXTEND_INREG || !N0.hasOneUse())\n      break;\n    unsigned ShAmt = N1C->getZExtValue();\n    unsigned ExtSize =\n        cast<VTSDNode>(N0.getOperand(1))->getVT().getSizeInBits();\n    // ExtSize of 32 should use sraiw via tablegen pattern.\n    if (ExtSize >= 32 || ShAmt >= ExtSize)\n      break;\n    unsigned LShAmt = Subtarget->getXLen() - ExtSize;\n    SDNode *SLLI =\n        CurDAG->getMachineNode(RISCV::SLLI, DL, VT, N0->getOperand(0),\n                               CurDAG->getTargetConstant(LShAmt, DL, VT));\n    SDNode *SRAI = CurDAG->getMachineNode(\n        RISCV::SRAI, DL, VT, SDValue(SLLI, 0),\n        CurDAG->getTargetConstant(LShAmt + ShAmt, DL, VT));\n    ReplaceNode(Node, SRAI);\n    return;\n  }\n  case ISD::OR:\n  case ISD::XOR:\n    if (tryShrinkShlLogicImm(Node))\n      return;\n\n    break;\n  case ISD::AND: {\n    auto *N1C = dyn_cast<ConstantSDNode>(Node->getOperand(1));\n    if (!N1C)\n      break;\n    uint64_t C1 = N1C->getZExtValue();\n    const bool isC1Mask = isMask_64(C1);\n    const bool isC1ANDI = isInt<12>(C1);\n\n    SDValue N0 = Node->getOperand(0);\n\n    auto tryUnsignedBitfieldExtract = [&](SDNode *Node, SDLoc DL, MVT VT,\n                                          SDValue X, unsigned Msb,\n                                          unsigned Lsb) {\n      if (!Subtarget->hasVendorXTHeadBb())\n        return false;\n\n      SDNode *TH_EXTU = CurDAG->getMachineNode(\n          RISCV::TH_EXTU, DL, VT, X, CurDAG->getTargetConstant(Msb, DL, VT),\n          CurDAG->getTargetConstant(Lsb, DL, VT));\n      ReplaceNode(Node, TH_EXTU);\n      return true;\n    };\n\n    bool LeftShift = N0.getOpcode() == ISD::SHL;\n    if (LeftShift || N0.getOpcode() == ISD::SRL) {\n      auto *C = dyn_cast<ConstantSDNode>(N0.getOperand(1));\n      if (!C)\n        break;\n      unsigned C2 = C->getZExtValue();\n      unsigned XLen = Subtarget->getXLen();\n      assert((C2 > 0 && C2 < XLen) && \"Unexpected shift amount!\");\n\n      // Keep track of whether this is a c.andi. If we can't use c.andi, the\n      // shift pair might offer more compression opportunities.\n      // TODO: We could check for C extension here, but we don't have many lit\n      // tests with the C extension enabled so not checking gets better\n      // coverage.\n      // TODO: What if ANDI faster than shift?\n      bool IsCANDI = isInt<6>(N1C->getSExtValue());\n\n      // Clear irrelevant bits in the mask.\n      if (LeftShift)\n        C1 &= maskTrailingZeros<uint64_t>(C2);\n      else\n        C1 &= maskTrailingOnes<uint64_t>(XLen - C2);\n\n      // Some transforms should only be done if the shift has a single use or\n      // the AND would become (srli (slli X, 32), 32)\n      bool OneUseOrZExtW = N0.hasOneUse() || C1 == UINT64_C(0xFFFFFFFF);\n\n      SDValue X = N0.getOperand(0);\n\n      // Turn (and (srl x, c2) c1) -> (srli (slli x, c3-c2), c3) if c1 is a mask\n      // with c3 leading zeros.\n      if (!LeftShift && isC1Mask) {\n        unsigned Leading = XLen - llvm::bit_width(C1);\n        if (C2 < Leading) {\n          // If the number of leading zeros is C2+32 this can be SRLIW.\n          if (C2 + 32 == Leading) {\n            SDNode *SRLIW = CurDAG->getMachineNode(\n                RISCV::SRLIW, DL, VT, X, CurDAG->getTargetConstant(C2, DL, VT));\n            ReplaceNode(Node, SRLIW);\n            return;\n          }\n\n          // (and (srl (sexti32 Y), c2), c1) -> (srliw (sraiw Y, 31), c3 - 32)\n          // if c1 is a mask with c3 leading zeros and c2 >= 32 and c3-c2==1.\n          //\n          // This pattern occurs when (i32 (srl (sra 31), c3 - 32)) is type\n          // legalized and goes through DAG combine.\n          if (C2 >= 32 && (Leading - C2) == 1 && N0.hasOneUse() &&\n              X.getOpcode() == ISD::SIGN_EXTEND_INREG &&\n              cast<VTSDNode>(X.getOperand(1))->getVT() == MVT::i32) {\n            SDNode *SRAIW =\n                CurDAG->getMachineNode(RISCV::SRAIW, DL, VT, X.getOperand(0),\n                                       CurDAG->getTargetConstant(31, DL, VT));\n            SDNode *SRLIW = CurDAG->getMachineNode(\n                RISCV::SRLIW, DL, VT, SDValue(SRAIW, 0),\n                CurDAG->getTargetConstant(Leading - 32, DL, VT));\n            ReplaceNode(Node, SRLIW);\n            return;\n          }\n\n          // Try to use an unsigned bitfield extract (e.g., th.extu) if\n          // available.\n          // Transform (and (srl x, C2), C1)\n          //        -> (<bfextract> x, msb, lsb)\n          //\n          // Make sure to keep this below the SRLIW cases, as we always want to\n          // prefer the more common instruction.\n          const unsigned Msb = llvm::bit_width(C1) + C2 - 1;\n          const unsigned Lsb = C2;\n          if (tryUnsignedBitfieldExtract(Node, DL, VT, X, Msb, Lsb))\n            return;\n\n          // (srli (slli x, c3-c2), c3).\n          // Skip if we could use (zext.w (sraiw X, C2)).\n          bool Skip = Subtarget->hasStdExtZba() && Leading == 32 &&\n                      X.getOpcode() == ISD::SIGN_EXTEND_INREG &&\n                      cast<VTSDNode>(X.getOperand(1))->getVT() == MVT::i32;\n          // Also Skip if we can use bexti or th.tst.\n          Skip |= HasBitTest && Leading == XLen - 1;\n          if (OneUseOrZExtW && !Skip) {\n            SDNode *SLLI = CurDAG->getMachineNode(\n                RISCV::SLLI, DL, VT, X,\n                CurDAG->getTargetConstant(Leading - C2, DL, VT));\n            SDNode *SRLI = CurDAG->getMachineNode(\n                RISCV::SRLI, DL, VT, SDValue(SLLI, 0),\n                CurDAG->getTargetConstant(Leading, DL, VT));\n            ReplaceNode(Node, SRLI);\n            return;\n          }\n        }\n      }\n\n      // Turn (and (shl x, c2), c1) -> (srli (slli c2+c3), c3) if c1 is a mask\n      // shifted by c2 bits with c3 leading zeros.\n      if (LeftShift && isShiftedMask_64(C1)) {\n        unsigned Leading = XLen - llvm::bit_width(C1);\n\n        if (C2 + Leading < XLen &&\n            C1 == (maskTrailingOnes<uint64_t>(XLen - (C2 + Leading)) << C2)) {\n          // Use slli.uw when possible.\n          if ((XLen - (C2 + Leading)) == 32 && Subtarget->hasStdExtZba()) {\n            SDNode *SLLI_UW =\n                CurDAG->getMachineNode(RISCV::SLLI_UW, DL, VT, X,\n                                       CurDAG->getTargetConstant(C2, DL, VT));\n            ReplaceNode(Node, SLLI_UW);\n            return;\n          }\n\n          // (srli (slli c2+c3), c3)\n          if (OneUseOrZExtW && !IsCANDI) {\n            SDNode *SLLI = CurDAG->getMachineNode(\n                RISCV::SLLI, DL, VT, X,\n                CurDAG->getTargetConstant(C2 + Leading, DL, VT));\n            SDNode *SRLI = CurDAG->getMachineNode(\n                RISCV::SRLI, DL, VT, SDValue(SLLI, 0),\n                CurDAG->getTargetConstant(Leading, DL, VT));\n            ReplaceNode(Node, SRLI);\n            return;\n          }\n        }\n      }\n\n      // Turn (and (shr x, c2), c1) -> (slli (srli x, c2+c3), c3) if c1 is a\n      // shifted mask with c2 leading zeros and c3 trailing zeros.\n      if (!LeftShift && isShiftedMask_64(C1)) {\n        unsigned Leading = XLen - llvm::bit_width(C1);\n        unsigned Trailing = llvm::countr_zero(C1);\n        if (Leading == C2 && C2 + Trailing < XLen && OneUseOrZExtW &&\n            !IsCANDI) {\n          unsigned SrliOpc = RISCV::SRLI;\n          // If the input is zexti32 we should use SRLIW.\n          if (X.getOpcode() == ISD::AND &&\n              isa<ConstantSDNode>(X.getOperand(1)) &&\n              X.getConstantOperandVal(1) == UINT64_C(0xFFFFFFFF)) {\n            SrliOpc = RISCV::SRLIW;\n            X = X.getOperand(0);\n          }\n          SDNode *SRLI = CurDAG->getMachineNode(\n              SrliOpc, DL, VT, X,\n              CurDAG->getTargetConstant(C2 + Trailing, DL, VT));\n          SDNode *SLLI = CurDAG->getMachineNode(\n              RISCV::SLLI, DL, VT, SDValue(SRLI, 0),\n              CurDAG->getTargetConstant(Trailing, DL, VT));\n          ReplaceNode(Node, SLLI);\n          return;\n        }\n        // If the leading zero count is C2+32, we can use SRLIW instead of SRLI.\n        if (Leading > 32 && (Leading - 32) == C2 && C2 + Trailing < 32 &&\n            OneUseOrZExtW && !IsCANDI) {\n          SDNode *SRLIW = CurDAG->getMachineNode(\n              RISCV::SRLIW, DL, VT, X,\n              CurDAG->getTargetConstant(C2 + Trailing, DL, VT));\n          SDNode *SLLI = CurDAG->getMachineNode(\n              RISCV::SLLI, DL, VT, SDValue(SRLIW, 0),\n              CurDAG->getTargetConstant(Trailing, DL, VT));\n          ReplaceNode(Node, SLLI);\n          return;\n        }\n      }\n\n      // Turn (and (shl x, c2), c1) -> (slli (srli x, c3-c2), c3) if c1 is a\n      // shifted mask with no leading zeros and c3 trailing zeros.\n      if (LeftShift && isShiftedMask_64(C1)) {\n        unsigned Leading = XLen - llvm::bit_width(C1);\n        unsigned Trailing = llvm::countr_zero(C1);\n        if (Leading == 0 && C2 < Trailing && OneUseOrZExtW && !IsCANDI) {\n          SDNode *SRLI = CurDAG->getMachineNode(\n              RISCV::SRLI, DL, VT, X,\n              CurDAG->getTargetConstant(Trailing - C2, DL, VT));\n          SDNode *SLLI = CurDAG->getMachineNode(\n              RISCV::SLLI, DL, VT, SDValue(SRLI, 0),\n              CurDAG->getTargetConstant(Trailing, DL, VT));\n          ReplaceNode(Node, SLLI);\n          return;\n        }\n        // If we have (32-C2) leading zeros, we can use SRLIW instead of SRLI.\n        if (C2 < Trailing && Leading + C2 == 32 && OneUseOrZExtW && !IsCANDI) {\n          SDNode *SRLIW = CurDAG->getMachineNode(\n              RISCV::SRLIW, DL, VT, X,\n              CurDAG->getTargetConstant(Trailing - C2, DL, VT));\n          SDNode *SLLI = CurDAG->getMachineNode(\n              RISCV::SLLI, DL, VT, SDValue(SRLIW, 0),\n              CurDAG->getTargetConstant(Trailing, DL, VT));\n          ReplaceNode(Node, SLLI);\n          return;\n        }\n      }\n    }\n\n    // If C1 masks off the upper bits only (but can't be formed as an\n    // ANDI), use an unsigned bitfield extract (e.g., th.extu), if\n    // available.\n    // Transform (and x, C1)\n    //        -> (<bfextract> x, msb, lsb)\n    if (isC1Mask && !isC1ANDI) {\n      const unsigned Msb = llvm::bit_width(C1) - 1;\n      if (tryUnsignedBitfieldExtract(Node, DL, VT, N0, Msb, 0))\n        return;\n    }\n\n    if (tryShrinkShlLogicImm(Node))\n      return;\n\n    break;\n  }\n  case ISD::MUL: {\n    // Special case for calculating (mul (and X, C2), C1) where the full product\n    // fits in XLen bits. We can shift X left by the number of leading zeros in\n    // C2 and shift C1 left by XLen-lzcnt(C2). This will ensure the final\n    // product has XLen trailing zeros, putting it in the output of MULHU. This\n    // can avoid materializing a constant in a register for C2.\n\n    // RHS should be a constant.\n    auto *N1C = dyn_cast<ConstantSDNode>(Node->getOperand(1));\n    if (!N1C || !N1C->hasOneUse())\n      break;\n\n    // LHS should be an AND with constant.\n    SDValue N0 = Node->getOperand(0);\n    if (N0.getOpcode() != ISD::AND || !isa<ConstantSDNode>(N0.getOperand(1)))\n      break;\n\n    uint64_t C2 = N0.getConstantOperandVal(1);\n\n    // Constant should be a mask.\n    if (!isMask_64(C2))\n      break;\n\n    // If this can be an ANDI or ZEXT.H, don't do this if the ANDI/ZEXT has\n    // multiple users or the constant is a simm12. This prevents inserting a\n    // shift and still have uses of the AND/ZEXT. Shifting a simm12 will likely\n    // make it more costly to materialize. Otherwise, using a SLLI might allow\n    // it to be compressed.\n    bool IsANDIOrZExt =\n        isInt<12>(C2) ||\n        (C2 == UINT64_C(0xFFFF) && Subtarget->hasStdExtZbb());\n    // With XTHeadBb, we can use TH.EXTU.\n    IsANDIOrZExt |= C2 == UINT64_C(0xFFFF) && Subtarget->hasVendorXTHeadBb();\n    if (IsANDIOrZExt && (isInt<12>(N1C->getSExtValue()) || !N0.hasOneUse()))\n      break;\n    // If this can be a ZEXT.w, don't do this if the ZEXT has multiple users or\n    // the constant is a simm32.\n    bool IsZExtW = C2 == UINT64_C(0xFFFFFFFF) && Subtarget->hasStdExtZba();\n    // With XTHeadBb, we can use TH.EXTU.\n    IsZExtW |= C2 == UINT64_C(0xFFFFFFFF) && Subtarget->hasVendorXTHeadBb();\n    if (IsZExtW && (isInt<32>(N1C->getSExtValue()) || !N0.hasOneUse()))\n      break;\n\n    // We need to shift left the AND input and C1 by a total of XLen bits.\n\n    // How far left do we need to shift the AND input?\n    unsigned XLen = Subtarget->getXLen();\n    unsigned LeadingZeros = XLen - llvm::bit_width(C2);\n\n    // The constant gets shifted by the remaining amount unless that would\n    // shift bits out.\n    uint64_t C1 = N1C->getZExtValue();\n    unsigned ConstantShift = XLen - LeadingZeros;\n    if (ConstantShift > (XLen - llvm::bit_width(C1)))\n      break;\n\n    uint64_t ShiftedC1 = C1 << ConstantShift;\n    // If this RV32, we need to sign extend the constant.\n    if (XLen == 32)\n      ShiftedC1 = SignExtend64<32>(ShiftedC1);\n\n    // Create (mulhu (slli X, lzcnt(C2)), C1 << (XLen - lzcnt(C2))).\n    SDNode *Imm = selectImm(CurDAG, DL, VT, ShiftedC1, *Subtarget).getNode();\n    SDNode *SLLI =\n        CurDAG->getMachineNode(RISCV::SLLI, DL, VT, N0.getOperand(0),\n                               CurDAG->getTargetConstant(LeadingZeros, DL, VT));\n    SDNode *MULHU = CurDAG->getMachineNode(RISCV::MULHU, DL, VT,\n                                           SDValue(SLLI, 0), SDValue(Imm, 0));\n    ReplaceNode(Node, MULHU);\n    return;\n  }\n  case ISD::LOAD: {\n    if (tryIndexedLoad(Node))\n      return;\n    break;\n  }\n  case ISD::INTRINSIC_WO_CHAIN: {\n    unsigned IntNo = Node->getConstantOperandVal(0);\n    switch (IntNo) {\n      // By default we do not custom select any intrinsic.\n    default:\n      break;\n    case Intrinsic::riscv_vmsgeu:\n    case Intrinsic::riscv_vmsge: {\n      SDValue Src1 = Node->getOperand(1);\n      SDValue Src2 = Node->getOperand(2);\n      bool IsUnsigned = IntNo == Intrinsic::riscv_vmsgeu;\n      bool IsCmpUnsignedZero = false;\n      // Only custom select scalar second operand.\n      if (Src2.getValueType() != XLenVT)\n        break;\n      // Small constants are handled with patterns.\n      if (auto *C = dyn_cast<ConstantSDNode>(Src2)) {\n        int64_t CVal = C->getSExtValue();\n        if (CVal >= -15 && CVal <= 16) {\n          if (!IsUnsigned || CVal != 0)\n            break;\n          IsCmpUnsignedZero = true;\n        }\n      }\n      MVT Src1VT = Src1.getSimpleValueType();\n      unsigned VMSLTOpcode, VMNANDOpcode, VMSetOpcode;\n      switch (RISCVTargetLowering::getLMUL(Src1VT)) {\n      default:\n        llvm_unreachable(\"Unexpected LMUL!\");\n#define CASE_VMSLT_VMNAND_VMSET_OPCODES(lmulenum, suffix, suffix_b)            \\\n  case RISCVII::VLMUL::lmulenum:                                               \\\n    VMSLTOpcode = IsUnsigned ? RISCV::PseudoVMSLTU_VX_##suffix                 \\\n                             : RISCV::PseudoVMSLT_VX_##suffix;                 \\\n    VMNANDOpcode = RISCV::PseudoVMNAND_MM_##suffix;                            \\\n    VMSetOpcode = RISCV::PseudoVMSET_M_##suffix_b;                             \\\n    break;\n        CASE_VMSLT_VMNAND_VMSET_OPCODES(LMUL_F8, MF8, B1)\n        CASE_VMSLT_VMNAND_VMSET_OPCODES(LMUL_F4, MF4, B2)\n        CASE_VMSLT_VMNAND_VMSET_OPCODES(LMUL_F2, MF2, B4)\n        CASE_VMSLT_VMNAND_VMSET_OPCODES(LMUL_1, M1, B8)\n        CASE_VMSLT_VMNAND_VMSET_OPCODES(LMUL_2, M2, B16)\n        CASE_VMSLT_VMNAND_VMSET_OPCODES(LMUL_4, M4, B32)\n        CASE_VMSLT_VMNAND_VMSET_OPCODES(LMUL_8, M8, B64)\n#undef CASE_VMSLT_VMNAND_VMSET_OPCODES\n      }\n      SDValue SEW = CurDAG->getTargetConstant(\n          Log2_32(Src1VT.getScalarSizeInBits()), DL, XLenVT);\n      SDValue VL;\n      selectVLOp(Node->getOperand(3), VL);\n\n      // If vmsgeu with 0 immediate, expand it to vmset.\n      if (IsCmpUnsignedZero) {\n        ReplaceNode(Node, CurDAG->getMachineNode(VMSetOpcode, DL, VT, VL, SEW));\n        return;\n      }\n\n      // Expand to\n      // vmslt{u}.vx vd, va, x; vmnand.mm vd, vd, vd\n      SDValue Cmp = SDValue(\n          CurDAG->getMachineNode(VMSLTOpcode, DL, VT, {Src1, Src2, VL, SEW}),\n          0);\n      ReplaceNode(Node, CurDAG->getMachineNode(VMNANDOpcode, DL, VT,\n                                               {Cmp, Cmp, VL, SEW}));\n      return;\n    }\n    case Intrinsic::riscv_vmsgeu_mask:\n    case Intrinsic::riscv_vmsge_mask: {\n      SDValue Src1 = Node->getOperand(2);\n      SDValue Src2 = Node->getOperand(3);\n      bool IsUnsigned = IntNo == Intrinsic::riscv_vmsgeu_mask;\n      bool IsCmpUnsignedZero = false;\n      // Only custom select scalar second operand.\n      if (Src2.getValueType() != XLenVT)\n        break;\n      // Small constants are handled with patterns.\n      if (auto *C = dyn_cast<ConstantSDNode>(Src2)) {\n        int64_t CVal = C->getSExtValue();\n        if (CVal >= -15 && CVal <= 16) {\n          if (!IsUnsigned || CVal != 0)\n            break;\n          IsCmpUnsignedZero = true;\n        }\n      }\n      MVT Src1VT = Src1.getSimpleValueType();\n      unsigned VMSLTOpcode, VMSLTMaskOpcode, VMXOROpcode, VMANDNOpcode,\n          VMOROpcode;\n      switch (RISCVTargetLowering::getLMUL(Src1VT)) {\n      default:\n        llvm_unreachable(\"Unexpected LMUL!\");\n#define CASE_VMSLT_OPCODES(lmulenum, suffix, suffix_b)                         \\\n  case RISCVII::VLMUL::lmulenum:                                               \\\n    VMSLTOpcode = IsUnsigned ? RISCV::PseudoVMSLTU_VX_##suffix                 \\\n                             : RISCV::PseudoVMSLT_VX_##suffix;                 \\\n    VMSLTMaskOpcode = IsUnsigned ? RISCV::PseudoVMSLTU_VX_##suffix##_MASK      \\\n                                 : RISCV::PseudoVMSLT_VX_##suffix##_MASK;      \\\n    break;\n        CASE_VMSLT_OPCODES(LMUL_F8, MF8, B1)\n        CASE_VMSLT_OPCODES(LMUL_F4, MF4, B2)\n        CASE_VMSLT_OPCODES(LMUL_F2, MF2, B4)\n        CASE_VMSLT_OPCODES(LMUL_1, M1, B8)\n        CASE_VMSLT_OPCODES(LMUL_2, M2, B16)\n        CASE_VMSLT_OPCODES(LMUL_4, M4, B32)\n        CASE_VMSLT_OPCODES(LMUL_8, M8, B64)\n#undef CASE_VMSLT_OPCODES\n      }\n      // Mask operations use the LMUL from the mask type.\n      switch (RISCVTargetLowering::getLMUL(VT)) {\n      default:\n        llvm_unreachable(\"Unexpected LMUL!\");\n#define CASE_VMXOR_VMANDN_VMOR_OPCODES(lmulenum, suffix)                       \\\n  case RISCVII::VLMUL::lmulenum:                                               \\\n    VMXOROpcode = RISCV::PseudoVMXOR_MM_##suffix;                              \\\n    VMANDNOpcode = RISCV::PseudoVMANDN_MM_##suffix;                            \\\n    VMOROpcode = RISCV::PseudoVMOR_MM_##suffix;                                \\\n    break;\n        CASE_VMXOR_VMANDN_VMOR_OPCODES(LMUL_F8, MF8)\n        CASE_VMXOR_VMANDN_VMOR_OPCODES(LMUL_F4, MF4)\n        CASE_VMXOR_VMANDN_VMOR_OPCODES(LMUL_F2, MF2)\n        CASE_VMXOR_VMANDN_VMOR_OPCODES(LMUL_1, M1)\n        CASE_VMXOR_VMANDN_VMOR_OPCODES(LMUL_2, M2)\n        CASE_VMXOR_VMANDN_VMOR_OPCODES(LMUL_4, M4)\n        CASE_VMXOR_VMANDN_VMOR_OPCODES(LMUL_8, M8)\n#undef CASE_VMXOR_VMANDN_VMOR_OPCODES\n      }\n      SDValue SEW = CurDAG->getTargetConstant(\n          Log2_32(Src1VT.getScalarSizeInBits()), DL, XLenVT);\n      SDValue MaskSEW = CurDAG->getTargetConstant(0, DL, XLenVT);\n      SDValue VL;\n      selectVLOp(Node->getOperand(5), VL);\n      SDValue MaskedOff = Node->getOperand(1);\n      SDValue Mask = Node->getOperand(4);\n\n      // If vmsgeu_mask with 0 immediate, expand it to vmor mask, maskedoff.\n      if (IsCmpUnsignedZero) {\n        // We don't need vmor if the MaskedOff and the Mask are the same\n        // value.\n        if (Mask == MaskedOff) {\n          ReplaceUses(Node, Mask.getNode());\n          return;\n        }\n        ReplaceNode(Node,\n                    CurDAG->getMachineNode(VMOROpcode, DL, VT,\n                                           {Mask, MaskedOff, VL, MaskSEW}));\n        return;\n      }\n\n      // If the MaskedOff value and the Mask are the same value use\n      // vmslt{u}.vx vt, va, x;  vmandn.mm vd, vd, vt\n      // This avoids needing to copy v0 to vd before starting the next sequence.\n      if (Mask == MaskedOff) {\n        SDValue Cmp = SDValue(\n            CurDAG->getMachineNode(VMSLTOpcode, DL, VT, {Src1, Src2, VL, SEW}),\n            0);\n        ReplaceNode(Node, CurDAG->getMachineNode(VMANDNOpcode, DL, VT,\n                                                 {Mask, Cmp, VL, MaskSEW}));\n        return;\n      }\n\n      // Mask needs to be copied to V0.\n      SDValue Chain = CurDAG->getCopyToReg(CurDAG->getEntryNode(), DL,\n                                           RISCV::V0, Mask, SDValue());\n      SDValue Glue = Chain.getValue(1);\n      SDValue V0 = CurDAG->getRegister(RISCV::V0, VT);\n\n      // Otherwise use\n      // vmslt{u}.vx vd, va, x, v0.t; vmxor.mm vd, vd, v0\n      // The result is mask undisturbed.\n      // We use the same instructions to emulate mask agnostic behavior, because\n      // the agnostic result can be either undisturbed or all 1.\n      SDValue Cmp = SDValue(\n          CurDAG->getMachineNode(VMSLTMaskOpcode, DL, VT,\n                                 {MaskedOff, Src1, Src2, V0, VL, SEW, Glue}),\n          0);\n      // vmxor.mm vd, vd, v0 is used to update active value.\n      ReplaceNode(Node, CurDAG->getMachineNode(VMXOROpcode, DL, VT,\n                                               {Cmp, Mask, VL, MaskSEW}));\n      return;\n    }\n    case Intrinsic::riscv_vsetvli:\n    case Intrinsic::riscv_vsetvlimax:\n      return selectVSETVLI(Node);\n    }\n    break;\n  }\n  case ISD::INTRINSIC_W_CHAIN: {\n    unsigned IntNo = Node->getConstantOperandVal(1);\n    switch (IntNo) {\n      // By default we do not custom select any intrinsic.\n    default:\n      break;\n    case Intrinsic::riscv_vlseg2:\n    case Intrinsic::riscv_vlseg3:\n    case Intrinsic::riscv_vlseg4:\n    case Intrinsic::riscv_vlseg5:\n    case Intrinsic::riscv_vlseg6:\n    case Intrinsic::riscv_vlseg7:\n    case Intrinsic::riscv_vlseg8: {\n      selectVLSEG(Node, /*IsMasked*/ false, /*IsStrided*/ false);\n      return;\n    }\n    case Intrinsic::riscv_vlseg2_mask:\n    case Intrinsic::riscv_vlseg3_mask:\n    case Intrinsic::riscv_vlseg4_mask:\n    case Intrinsic::riscv_vlseg5_mask:\n    case Intrinsic::riscv_vlseg6_mask:\n    case Intrinsic::riscv_vlseg7_mask:\n    case Intrinsic::riscv_vlseg8_mask: {\n      selectVLSEG(Node, /*IsMasked*/ true, /*IsStrided*/ false);\n      return;\n    }\n    case Intrinsic::riscv_vlsseg2:\n    case Intrinsic::riscv_vlsseg3:\n    case Intrinsic::riscv_vlsseg4:\n    case Intrinsic::riscv_vlsseg5:\n    case Intrinsic::riscv_vlsseg6:\n    case Intrinsic::riscv_vlsseg7:\n    case Intrinsic::riscv_vlsseg8: {\n      selectVLSEG(Node, /*IsMasked*/ false, /*IsStrided*/ true);\n      return;\n    }\n    case Intrinsic::riscv_vlsseg2_mask:\n    case Intrinsic::riscv_vlsseg3_mask:\n    case Intrinsic::riscv_vlsseg4_mask:\n    case Intrinsic::riscv_vlsseg5_mask:\n    case Intrinsic::riscv_vlsseg6_mask:\n    case Intrinsic::riscv_vlsseg7_mask:\n    case Intrinsic::riscv_vlsseg8_mask: {\n      selectVLSEG(Node, /*IsMasked*/ true, /*IsStrided*/ true);\n      return;\n    }\n    case Intrinsic::riscv_vloxseg2:\n    case Intrinsic::riscv_vloxseg3:\n    case Intrinsic::riscv_vloxseg4:\n    case Intrinsic::riscv_vloxseg5:\n    case Intrinsic::riscv_vloxseg6:\n    case Intrinsic::riscv_vloxseg7:\n    case Intrinsic::riscv_vloxseg8:\n      selectVLXSEG(Node, /*IsMasked*/ false, /*IsOrdered*/ true);\n      return;\n    case Intrinsic::riscv_vluxseg2:\n    case Intrinsic::riscv_vluxseg3:\n    case Intrinsic::riscv_vluxseg4:\n    case Intrinsic::riscv_vluxseg5:\n    case Intrinsic::riscv_vluxseg6:\n    case Intrinsic::riscv_vluxseg7:\n    case Intrinsic::riscv_vluxseg8:\n      selectVLXSEG(Node, /*IsMasked*/ false, /*IsOrdered*/ false);\n      return;\n    case Intrinsic::riscv_vloxseg2_mask:\n    case Intrinsic::riscv_vloxseg3_mask:\n    case Intrinsic::riscv_vloxseg4_mask:\n    case Intrinsic::riscv_vloxseg5_mask:\n    case Intrinsic::riscv_vloxseg6_mask:\n    case Intrinsic::riscv_vloxseg7_mask:\n    case Intrinsic::riscv_vloxseg8_mask:\n      selectVLXSEG(Node, /*IsMasked*/ true, /*IsOrdered*/ true);\n      return;\n    case Intrinsic::riscv_vluxseg2_mask:\n    case Intrinsic::riscv_vluxseg3_mask:\n    case Intrinsic::riscv_vluxseg4_mask:\n    case Intrinsic::riscv_vluxseg5_mask:\n    case Intrinsic::riscv_vluxseg6_mask:\n    case Intrinsic::riscv_vluxseg7_mask:\n    case Intrinsic::riscv_vluxseg8_mask:\n      selectVLXSEG(Node, /*IsMasked*/ true, /*IsOrdered*/ false);\n      return;\n    case Intrinsic::riscv_vlseg8ff:\n    case Intrinsic::riscv_vlseg7ff:\n    case Intrinsic::riscv_vlseg6ff:\n    case Intrinsic::riscv_vlseg5ff:\n    case Intrinsic::riscv_vlseg4ff:\n    case Intrinsic::riscv_vlseg3ff:\n    case Intrinsic::riscv_vlseg2ff: {\n      selectVLSEGFF(Node, /*IsMasked*/ false);\n      return;\n    }\n    case Intrinsic::riscv_vlseg8ff_mask:\n    case Intrinsic::riscv_vlseg7ff_mask:\n    case Intrinsic::riscv_vlseg6ff_mask:\n    case Intrinsic::riscv_vlseg5ff_mask:\n    case Intrinsic::riscv_vlseg4ff_mask:\n    case Intrinsic::riscv_vlseg3ff_mask:\n    case Intrinsic::riscv_vlseg2ff_mask: {\n      selectVLSEGFF(Node, /*IsMasked*/ true);\n      return;\n    }\n    case Intrinsic::riscv_vloxei:\n    case Intrinsic::riscv_vloxei_mask:\n    case Intrinsic::riscv_vluxei:\n    case Intrinsic::riscv_vluxei_mask: {\n      bool IsMasked = IntNo == Intrinsic::riscv_vloxei_mask ||\n                      IntNo == Intrinsic::riscv_vluxei_mask;\n      bool IsOrdered = IntNo == Intrinsic::riscv_vloxei ||\n                       IntNo == Intrinsic::riscv_vloxei_mask;\n\n      MVT VT = Node->getSimpleValueType(0);\n      unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n\n      unsigned CurOp = 2;\n      SmallVector<SDValue, 8> Operands;\n      Operands.push_back(Node->getOperand(CurOp++));\n\n      MVT IndexVT;\n      addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked,\n                                 /*IsStridedOrIndexed*/ true, Operands,\n                                 /*IsLoad=*/true, &IndexVT);\n\n      assert(VT.getVectorElementCount() == IndexVT.getVectorElementCount() &&\n             \"Element count mismatch\");\n\n      RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n      RISCVII::VLMUL IndexLMUL = RISCVTargetLowering::getLMUL(IndexVT);\n      unsigned IndexLog2EEW = Log2_32(IndexVT.getScalarSizeInBits());\n      if (IndexLog2EEW == 6 && !Subtarget->is64Bit()) {\n        report_fatal_error(\"The V extension does not support EEW=64 for index \"\n                           \"values when XLEN=32\");\n      }\n      const RISCV::VLX_VSXPseudo *P = RISCV::getVLXPseudo(\n          IsMasked, IsOrdered, IndexLog2EEW, static_cast<unsigned>(LMUL),\n          static_cast<unsigned>(IndexLMUL));\n      MachineSDNode *Load =\n          CurDAG->getMachineNode(P->Pseudo, DL, Node->getVTList(), Operands);\n\n      if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n        CurDAG->setNodeMemRefs(Load, {MemOp->getMemOperand()});\n\n      ReplaceNode(Node, Load);\n      return;\n    }\n    case Intrinsic::riscv_vlm:\n    case Intrinsic::riscv_vle:\n    case Intrinsic::riscv_vle_mask:\n    case Intrinsic::riscv_vlse:\n    case Intrinsic::riscv_vlse_mask: {\n      bool IsMasked = IntNo == Intrinsic::riscv_vle_mask ||\n                      IntNo == Intrinsic::riscv_vlse_mask;\n      bool IsStrided =\n          IntNo == Intrinsic::riscv_vlse || IntNo == Intrinsic::riscv_vlse_mask;\n\n      MVT VT = Node->getSimpleValueType(0);\n      unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n\n      // The riscv_vlm intrinsic are always tail agnostic and no passthru\n      // operand at the IR level.  In pseudos, they have both policy and\n      // passthru operand. The passthru operand is needed to track the\n      // \"tail undefined\" state, and the policy is there just for\n      // for consistency - it will always be \"don't care\" for the\n      // unmasked form.\n      bool HasPassthruOperand = IntNo != Intrinsic::riscv_vlm;\n      unsigned CurOp = 2;\n      SmallVector<SDValue, 8> Operands;\n      if (HasPassthruOperand)\n        Operands.push_back(Node->getOperand(CurOp++));\n      else {\n        // We eagerly lower to implicit_def (instead of undef), as we\n        // otherwise fail to select nodes such as: nxv1i1 = undef\n        SDNode *Passthru =\n          CurDAG->getMachineNode(TargetOpcode::IMPLICIT_DEF, DL, VT);\n        Operands.push_back(SDValue(Passthru, 0));\n      }\n      addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked, IsStrided,\n                                 Operands, /*IsLoad=*/true);\n\n      RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n      const RISCV::VLEPseudo *P =\n          RISCV::getVLEPseudo(IsMasked, IsStrided, /*FF*/ false, Log2SEW,\n                              static_cast<unsigned>(LMUL));\n      MachineSDNode *Load =\n          CurDAG->getMachineNode(P->Pseudo, DL, Node->getVTList(), Operands);\n\n      if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n        CurDAG->setNodeMemRefs(Load, {MemOp->getMemOperand()});\n\n      ReplaceNode(Node, Load);\n      return;\n    }\n    case Intrinsic::riscv_vleff:\n    case Intrinsic::riscv_vleff_mask: {\n      bool IsMasked = IntNo == Intrinsic::riscv_vleff_mask;\n\n      MVT VT = Node->getSimpleValueType(0);\n      unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n\n      unsigned CurOp = 2;\n      SmallVector<SDValue, 7> Operands;\n      Operands.push_back(Node->getOperand(CurOp++));\n      addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked,\n                                 /*IsStridedOrIndexed*/ false, Operands,\n                                 /*IsLoad=*/true);\n\n      RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n      const RISCV::VLEPseudo *P =\n          RISCV::getVLEPseudo(IsMasked, /*Strided*/ false, /*FF*/ true,\n                              Log2SEW, static_cast<unsigned>(LMUL));\n      MachineSDNode *Load = CurDAG->getMachineNode(\n          P->Pseudo, DL, Node->getVTList(), Operands);\n      if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n        CurDAG->setNodeMemRefs(Load, {MemOp->getMemOperand()});\n\n      ReplaceNode(Node, Load);\n      return;\n    }\n    }\n    break;\n  }\n  case ISD::INTRINSIC_VOID: {\n    unsigned IntNo = Node->getConstantOperandVal(1);\n    switch (IntNo) {\n    case Intrinsic::riscv_vsseg2:\n    case Intrinsic::riscv_vsseg3:\n    case Intrinsic::riscv_vsseg4:\n    case Intrinsic::riscv_vsseg5:\n    case Intrinsic::riscv_vsseg6:\n    case Intrinsic::riscv_vsseg7:\n    case Intrinsic::riscv_vsseg8: {\n      selectVSSEG(Node, /*IsMasked*/ false, /*IsStrided*/ false);\n      return;\n    }\n    case Intrinsic::riscv_vsseg2_mask:\n    case Intrinsic::riscv_vsseg3_mask:\n    case Intrinsic::riscv_vsseg4_mask:\n    case Intrinsic::riscv_vsseg5_mask:\n    case Intrinsic::riscv_vsseg6_mask:\n    case Intrinsic::riscv_vsseg7_mask:\n    case Intrinsic::riscv_vsseg8_mask: {\n      selectVSSEG(Node, /*IsMasked*/ true, /*IsStrided*/ false);\n      return;\n    }\n    case Intrinsic::riscv_vssseg2:\n    case Intrinsic::riscv_vssseg3:\n    case Intrinsic::riscv_vssseg4:\n    case Intrinsic::riscv_vssseg5:\n    case Intrinsic::riscv_vssseg6:\n    case Intrinsic::riscv_vssseg7:\n    case Intrinsic::riscv_vssseg8: {\n      selectVSSEG(Node, /*IsMasked*/ false, /*IsStrided*/ true);\n      return;\n    }\n    case Intrinsic::riscv_vssseg2_mask:\n    case Intrinsic::riscv_vssseg3_mask:\n    case Intrinsic::riscv_vssseg4_mask:\n    case Intrinsic::riscv_vssseg5_mask:\n    case Intrinsic::riscv_vssseg6_mask:\n    case Intrinsic::riscv_vssseg7_mask:\n    case Intrinsic::riscv_vssseg8_mask: {\n      selectVSSEG(Node, /*IsMasked*/ true, /*IsStrided*/ true);\n      return;\n    }\n    case Intrinsic::riscv_vsoxseg2:\n    case Intrinsic::riscv_vsoxseg3:\n    case Intrinsic::riscv_vsoxseg4:\n    case Intrinsic::riscv_vsoxseg5:\n    case Intrinsic::riscv_vsoxseg6:\n    case Intrinsic::riscv_vsoxseg7:\n    case Intrinsic::riscv_vsoxseg8:\n      selectVSXSEG(Node, /*IsMasked*/ false, /*IsOrdered*/ true);\n      return;\n    case Intrinsic::riscv_vsuxseg2:\n    case Intrinsic::riscv_vsuxseg3:\n    case Intrinsic::riscv_vsuxseg4:\n    case Intrinsic::riscv_vsuxseg5:\n    case Intrinsic::riscv_vsuxseg6:\n    case Intrinsic::riscv_vsuxseg7:\n    case Intrinsic::riscv_vsuxseg8:\n      selectVSXSEG(Node, /*IsMasked*/ false, /*IsOrdered*/ false);\n      return;\n    case Intrinsic::riscv_vsoxseg2_mask:\n    case Intrinsic::riscv_vsoxseg3_mask:\n    case Intrinsic::riscv_vsoxseg4_mask:\n    case Intrinsic::riscv_vsoxseg5_mask:\n    case Intrinsic::riscv_vsoxseg6_mask:\n    case Intrinsic::riscv_vsoxseg7_mask:\n    case Intrinsic::riscv_vsoxseg8_mask:\n      selectVSXSEG(Node, /*IsMasked*/ true, /*IsOrdered*/ true);\n      return;\n    case Intrinsic::riscv_vsuxseg2_mask:\n    case Intrinsic::riscv_vsuxseg3_mask:\n    case Intrinsic::riscv_vsuxseg4_mask:\n    case Intrinsic::riscv_vsuxseg5_mask:\n    case Intrinsic::riscv_vsuxseg6_mask:\n    case Intrinsic::riscv_vsuxseg7_mask:\n    case Intrinsic::riscv_vsuxseg8_mask:\n      selectVSXSEG(Node, /*IsMasked*/ true, /*IsOrdered*/ false);\n      return;\n    case Intrinsic::riscv_vsoxei:\n    case Intrinsic::riscv_vsoxei_mask:\n    case Intrinsic::riscv_vsuxei:\n    case Intrinsic::riscv_vsuxei_mask: {\n      bool IsMasked = IntNo == Intrinsic::riscv_vsoxei_mask ||\n                      IntNo == Intrinsic::riscv_vsuxei_mask;\n      bool IsOrdered = IntNo == Intrinsic::riscv_vsoxei ||\n                       IntNo == Intrinsic::riscv_vsoxei_mask;\n\n      MVT VT = Node->getOperand(2)->getSimpleValueType(0);\n      unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n\n      unsigned CurOp = 2;\n      SmallVector<SDValue, 8> Operands;\n      Operands.push_back(Node->getOperand(CurOp++)); // Store value.\n\n      MVT IndexVT;\n      addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked,\n                                 /*IsStridedOrIndexed*/ true, Operands,\n                                 /*IsLoad=*/false, &IndexVT);\n\n      assert(VT.getVectorElementCount() == IndexVT.getVectorElementCount() &&\n             \"Element count mismatch\");\n\n      RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n      RISCVII::VLMUL IndexLMUL = RISCVTargetLowering::getLMUL(IndexVT);\n      unsigned IndexLog2EEW = Log2_32(IndexVT.getScalarSizeInBits());\n      if (IndexLog2EEW == 6 && !Subtarget->is64Bit()) {\n        report_fatal_error(\"The V extension does not support EEW=64 for index \"\n                           \"values when XLEN=32\");\n      }\n      const RISCV::VLX_VSXPseudo *P = RISCV::getVSXPseudo(\n          IsMasked, IsOrdered, IndexLog2EEW,\n          static_cast<unsigned>(LMUL), static_cast<unsigned>(IndexLMUL));\n      MachineSDNode *Store =\n          CurDAG->getMachineNode(P->Pseudo, DL, Node->getVTList(), Operands);\n\n      if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n        CurDAG->setNodeMemRefs(Store, {MemOp->getMemOperand()});\n\n      ReplaceNode(Node, Store);\n      return;\n    }\n    case Intrinsic::riscv_vsm:\n    case Intrinsic::riscv_vse:\n    case Intrinsic::riscv_vse_mask:\n    case Intrinsic::riscv_vsse:\n    case Intrinsic::riscv_vsse_mask: {\n      bool IsMasked = IntNo == Intrinsic::riscv_vse_mask ||\n                      IntNo == Intrinsic::riscv_vsse_mask;\n      bool IsStrided =\n          IntNo == Intrinsic::riscv_vsse || IntNo == Intrinsic::riscv_vsse_mask;\n\n      MVT VT = Node->getOperand(2)->getSimpleValueType(0);\n      unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n\n      unsigned CurOp = 2;\n      SmallVector<SDValue, 8> Operands;\n      Operands.push_back(Node->getOperand(CurOp++)); // Store value.\n\n      addVectorLoadStoreOperands(Node, Log2SEW, DL, CurOp, IsMasked, IsStrided,\n                                 Operands);\n\n      RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n      const RISCV::VSEPseudo *P = RISCV::getVSEPseudo(\n          IsMasked, IsStrided, Log2SEW, static_cast<unsigned>(LMUL));\n      MachineSDNode *Store =\n          CurDAG->getMachineNode(P->Pseudo, DL, Node->getVTList(), Operands);\n      if (auto *MemOp = dyn_cast<MemSDNode>(Node))\n        CurDAG->setNodeMemRefs(Store, {MemOp->getMemOperand()});\n\n      ReplaceNode(Node, Store);\n      return;\n    }\n    }\n    break;\n  }\n  case ISD::BITCAST: {\n    MVT SrcVT = Node->getOperand(0).getSimpleValueType();\n    // Just drop bitcasts between vectors if both are fixed or both are\n    // scalable.\n    if ((VT.isScalableVector() && SrcVT.isScalableVector()) ||\n        (VT.isFixedLengthVector() && SrcVT.isFixedLengthVector())) {\n      ReplaceUses(SDValue(Node, 0), Node->getOperand(0));\n      CurDAG->RemoveDeadNode(Node);\n      return;\n    }\n    break;\n  }\n  case ISD::INSERT_SUBVECTOR: {\n    SDValue V = Node->getOperand(0);\n    SDValue SubV = Node->getOperand(1);\n    SDLoc DL(SubV);\n    auto Idx = Node->getConstantOperandVal(2);\n    MVT SubVecVT = SubV.getSimpleValueType();\n\n    const RISCVTargetLowering &TLI = *Subtarget->getTargetLowering();\n    MVT SubVecContainerVT = SubVecVT;\n    // Establish the correct scalable-vector types for any fixed-length type.\n    if (SubVecVT.isFixedLengthVector())\n      SubVecContainerVT = TLI.getContainerForFixedLengthVector(SubVecVT);\n    if (VT.isFixedLengthVector())\n      VT = TLI.getContainerForFixedLengthVector(VT);\n\n    const auto *TRI = Subtarget->getRegisterInfo();\n    unsigned SubRegIdx;\n    std::tie(SubRegIdx, Idx) =\n        RISCVTargetLowering::decomposeSubvectorInsertExtractToSubRegs(\n            VT, SubVecContainerVT, Idx, TRI);\n\n    // If the Idx hasn't been completely eliminated then this is a subvector\n    // insert which doesn't naturally align to a vector register. These must\n    // be handled using instructions to manipulate the vector registers.\n    if (Idx != 0)\n      break;\n\n    RISCVII::VLMUL SubVecLMUL = RISCVTargetLowering::getLMUL(SubVecContainerVT);\n    bool IsSubVecPartReg = SubVecLMUL == RISCVII::VLMUL::LMUL_F2 ||\n                           SubVecLMUL == RISCVII::VLMUL::LMUL_F4 ||\n                           SubVecLMUL == RISCVII::VLMUL::LMUL_F8;\n    (void)IsSubVecPartReg; // Silence unused variable warning without asserts.\n    assert((!IsSubVecPartReg || V.isUndef()) &&\n           \"Expecting lowering to have created legal INSERT_SUBVECTORs when \"\n           \"the subvector is smaller than a full-sized register\");\n\n    // If we haven't set a SubRegIdx, then we must be going between\n    // equally-sized LMUL groups (e.g. VR -> VR). This can be done as a copy.\n    if (SubRegIdx == RISCV::NoSubRegister) {\n      unsigned InRegClassID = RISCVTargetLowering::getRegClassIDForVecVT(VT);\n      assert(RISCVTargetLowering::getRegClassIDForVecVT(SubVecContainerVT) ==\n                 InRegClassID &&\n             \"Unexpected subvector extraction\");\n      SDValue RC = CurDAG->getTargetConstant(InRegClassID, DL, XLenVT);\n      SDNode *NewNode = CurDAG->getMachineNode(TargetOpcode::COPY_TO_REGCLASS,\n                                               DL, VT, SubV, RC);\n      ReplaceNode(Node, NewNode);\n      return;\n    }\n\n    SDValue Insert = CurDAG->getTargetInsertSubreg(SubRegIdx, DL, VT, V, SubV);\n    ReplaceNode(Node, Insert.getNode());\n    return;\n  }\n  case ISD::EXTRACT_SUBVECTOR: {\n    SDValue V = Node->getOperand(0);\n    auto Idx = Node->getConstantOperandVal(1);\n    MVT InVT = V.getSimpleValueType();\n    SDLoc DL(V);\n\n    const RISCVTargetLowering &TLI = *Subtarget->getTargetLowering();\n    MVT SubVecContainerVT = VT;\n    // Establish the correct scalable-vector types for any fixed-length type.\n    if (VT.isFixedLengthVector())\n      SubVecContainerVT = TLI.getContainerForFixedLengthVector(VT);\n    if (InVT.isFixedLengthVector())\n      InVT = TLI.getContainerForFixedLengthVector(InVT);\n\n    const auto *TRI = Subtarget->getRegisterInfo();\n    unsigned SubRegIdx;\n    std::tie(SubRegIdx, Idx) =\n        RISCVTargetLowering::decomposeSubvectorInsertExtractToSubRegs(\n            InVT, SubVecContainerVT, Idx, TRI);\n\n    // If the Idx hasn't been completely eliminated then this is a subvector\n    // extract which doesn't naturally align to a vector register. These must\n    // be handled using instructions to manipulate the vector registers.\n    if (Idx != 0)\n      break;\n\n    // If we haven't set a SubRegIdx, then we must be going between\n    // equally-sized LMUL types (e.g. VR -> VR). This can be done as a copy.\n    if (SubRegIdx == RISCV::NoSubRegister) {\n      unsigned InRegClassID = RISCVTargetLowering::getRegClassIDForVecVT(InVT);\n      assert(RISCVTargetLowering::getRegClassIDForVecVT(SubVecContainerVT) ==\n                 InRegClassID &&\n             \"Unexpected subvector extraction\");\n      SDValue RC = CurDAG->getTargetConstant(InRegClassID, DL, XLenVT);\n      SDNode *NewNode =\n          CurDAG->getMachineNode(TargetOpcode::COPY_TO_REGCLASS, DL, VT, V, RC);\n      ReplaceNode(Node, NewNode);\n      return;\n    }\n\n    SDValue Extract = CurDAG->getTargetExtractSubreg(SubRegIdx, DL, VT, V);\n    ReplaceNode(Node, Extract.getNode());\n    return;\n  }\n  case RISCVISD::VMV_S_X_VL:\n  case RISCVISD::VFMV_S_F_VL:\n  case RISCVISD::VMV_V_X_VL:\n  case RISCVISD::VFMV_V_F_VL: {\n    // Try to match splat of a scalar load to a strided load with stride of x0.\n    bool IsScalarMove = Node->getOpcode() == RISCVISD::VMV_S_X_VL ||\n                        Node->getOpcode() == RISCVISD::VFMV_S_F_VL;\n    if (!Node->getOperand(0).isUndef())\n      break;\n    SDValue Src = Node->getOperand(1);\n    auto *Ld = dyn_cast<LoadSDNode>(Src);\n    // Can't fold load update node because the second\n    // output is used so that load update node can't be removed.\n    if (!Ld || Ld->isIndexed())\n      break;\n    EVT MemVT = Ld->getMemoryVT();\n    // The memory VT should be the same size as the element type.\n    if (MemVT.getStoreSize() != VT.getVectorElementType().getStoreSize())\n      break;\n    if (!IsProfitableToFold(Src, Node, Node) ||\n        !IsLegalToFold(Src, Node, Node, TM.getOptLevel()))\n      break;\n\n    SDValue VL;\n    if (IsScalarMove) {\n      // We could deal with more VL if we update the VSETVLI insert pass to\n      // avoid introducing more VSETVLI.\n      if (!isOneConstant(Node->getOperand(2)))\n        break;\n      selectVLOp(Node->getOperand(2), VL);\n    } else\n      selectVLOp(Node->getOperand(2), VL);\n\n    unsigned Log2SEW = Log2_32(VT.getScalarSizeInBits());\n    SDValue SEW = CurDAG->getTargetConstant(Log2SEW, DL, XLenVT);\n\n    // If VL=1, then we don't need to do a strided load and can just do a\n    // regular load.\n    bool IsStrided = !isOneConstant(VL);\n\n    // Only do a strided load if we have optimized zero-stride vector load.\n    if (IsStrided && !Subtarget->hasOptimizedZeroStrideLoad())\n      break;\n\n    SmallVector<SDValue> Operands = {\n        SDValue(CurDAG->getMachineNode(TargetOpcode::IMPLICIT_DEF, DL, VT), 0),\n        Ld->getBasePtr()};\n    if (IsStrided)\n      Operands.push_back(CurDAG->getRegister(RISCV::X0, XLenVT));\n    uint64_t Policy = RISCVII::MASK_AGNOSTIC | RISCVII::TAIL_AGNOSTIC;\n    SDValue PolicyOp = CurDAG->getTargetConstant(Policy, DL, XLenVT);\n    Operands.append({VL, SEW, PolicyOp, Ld->getChain()});\n\n    RISCVII::VLMUL LMUL = RISCVTargetLowering::getLMUL(VT);\n    const RISCV::VLEPseudo *P = RISCV::getVLEPseudo(\n        /*IsMasked*/ false, IsStrided, /*FF*/ false,\n        Log2SEW, static_cast<unsigned>(LMUL));\n    MachineSDNode *Load =\n        CurDAG->getMachineNode(P->Pseudo, DL, {VT, MVT::Other}, Operands);\n    // Update the chain.\n    ReplaceUses(Src.getValue(1), SDValue(Load, 1));\n    // Record the mem-refs\n    CurDAG->setNodeMemRefs(Load, {Ld->getMemOperand()});\n    // Replace the splat with the vlse.\n    ReplaceNode(Node, Load);\n    return;\n  }\n  case ISD::PREFETCH:\n    unsigned Locality = Node->getConstantOperandVal(3);\n    if (Locality > 2)\n      break;\n\n    if (auto *LoadStoreMem = dyn_cast<MemSDNode>(Node)) {\n      MachineMemOperand *MMO = LoadStoreMem->getMemOperand();\n      MMO->setFlags(MachineMemOperand::MONonTemporal);\n\n      int NontemporalLevel = 0;\n      switch (Locality) {\n      case 0:\n        NontemporalLevel = 3; // NTL.ALL\n        break;\n      case 1:\n        NontemporalLevel = 1; // NTL.PALL\n        break;\n      case 2:\n        NontemporalLevel = 0; // NTL.P1\n        break;\n      default:\n        llvm_unreachable(\"unexpected locality value.\");\n      }\n\n      if (NontemporalLevel & 0b1)\n        MMO->setFlags(MONontemporalBit0);\n      if (NontemporalLevel & 0b10)\n        MMO->setFlags(MONontemporalBit1);\n    }\n    break;\n  }\n\n  // Select the default instruction.\n  SelectCode(Node);\n}",
      "start_line": 820,
      "end_line": 2192,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "Select",
          "condition": "Opcode",
          "cases": [
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "for calculating (mul (and X, C2), C1) where the full product\n    // fits in XLen bits. We can shift X left by the number of leading zeros in\n    // C2 and shift C1 left by XLen-lzcnt(C2). This will ensure the final\n    // product has XLen trailing zeros, putting it in the output of MULHU. This\n    // can avoid materializing a constant in a register for C2.\n\n    // RHS should be a constant.\n    auto *N1C = dyn_cast<ConstantSDNode>(Node->getOperand(1));\n    if (!N1C || !N1C->hasOneUse())\n      break;\n\n    // LHS should be an AND with constant.\n    SDValue N0 = Node->getOperand(0);\n    if (N0.getOpcode() != ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "ISD",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "0",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "1",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "2",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "Select",
          "condition": "VT.SimpleTy",
          "cases": [
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "Select",
          "condition": "VT.SimpleTy",
          "cases": [
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "MVT",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "Select",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "Select",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "Select",
          "condition": "IntNo",
          "cases": [
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "Intrinsic",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        },
        {
          "function": "Select",
          "condition": "Locality",
          "cases": [
            {
              "label": "0",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "1",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "2",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getLegalZfaFPImm",
        "selectVLXSEG",
        "getLMUL",
        "getTargetExtractSubreg",
        "getVLEPseudo",
        "is64Bit",
        "hasStdExtZbs",
        "getSizeInBits",
        "dbgs",
        "hasStdExtZba",
        "srliw",
        "have",
        "hasAllHUsers",
        "getCopyToReg",
        "RemoveDeadNode",
        "getRegisterInfo",
        "getXLen",
        "getBasePtr",
        "ReplaceUses",
        "types",
        "Create",
        "hasStdExtZbb",
        "selectVLSEGFF",
        "report_fatal_error",
        "CASE_VMXOR_VMANDN_VMOR_OPCODES",
        "getTargetConstant",
        "getMachineNode",
        "isMask_64",
        "hasStdExtZhinxmin",
        "countr_zero",
        "slli",
        "UINT64_C",
        "getVSEPseudo",
        "hasStdExtZdinx",
        "when",
        "getVT",
        "setFlags",
        "become",
        "is",
        "getVectorElementCount",
        "use",
        "Turn",
        "push_back",
        "isFixedLengthVector",
        "extract",
        "LLVM_DEBUG",
        "getValue",
        "selectVSETVLI",
        "getOpcode",
        "getContainerForFixedLengthVector",
        "getOperand",
        "getStoreSize",
        "selectVLSEG",
        "decomposeSubvectorInsertExtractToSubRegs",
        "setNodeId",
        "getVSXPseudo",
        "Transform",
        "implicit_def",
        "Optimize",
        "getNode",
        "hasAllWUsers",
        "getMemoryVT",
        "isScalableVector",
        "Log2_32",
        "getCopyFromReg",
        "setNodeMemRefs",
        "groups",
        "srai",
        "isOneConstant",
        "getValueAPF",
        "isUndef",
        "CASE_VMSLT_VMNAND_VMSET_OPCODES",
        "getZExtValue",
        "SDValue",
        "getMemOperand",
        "getRegister",
        "isZero",
        "isNegZero",
        "append",
        "SelectCode",
        "hasStdExtZfinx",
        "IsLegalToFold",
        "CASE_VMSLT_OPCODES",
        "getTargetLowering",
        "DL",
        "getXLenVT",
        "getSExtValue",
        "selectVSXSEG",
        "and",
        "calculating",
        "getSimpleValueType",
        "hasVendorXTHeadBs",
        "getRegClassIDForVecVT",
        "getConstantOperandVal",
        "addVectorLoadStoreOperands",
        "getVectorElementType",
        "selectImm",
        "only",
        "selectVLOp",
        "tie",
        "lzcnt",
        "getTargetInsertSubreg",
        "hasVendorXTHeadBb",
        "hasOneUse",
        "dump",
        "bit_width",
        "countr_one",
        "And",
        "selectVSSEG",
        "use_empty",
        "getVLXPseudo",
        "llvm_unreachable",
        "srli",
        "ReplaceNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SelectInlineAsmMemoryOperand",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const SDValue",
          "name": "&Op"
        },
        {
          "type": "InlineAsm::ConstraintCode",
          "name": "ConstraintID"
        },
        {
          "type": "std::vector<SDValue>",
          "name": "&OutOps"
        }
      ],
      "body": "{\n  // Always produce a register and immediate operand, as expected by\n  // RISCVAsmPrinter::PrintAsmMemoryOperand.\n  switch (ConstraintID) {\n  case InlineAsm::ConstraintCode::o:\n  case InlineAsm::ConstraintCode::m: {\n    SDValue Op0, Op1;\n    bool Found = SelectAddrRegImm(Op, Op0, Op1);\n    assert(Found && \"SelectAddrRegImm should always succeed\");\n    (void)Found;\n    OutOps.push_back(Op0);\n    OutOps.push_back(Op1);\n    return false;\n  }\n  case InlineAsm::ConstraintCode::A:\n    OutOps.push_back(Op);\n    OutOps.push_back(\n        CurDAG->getTargetConstant(0, SDLoc(Op), Subtarget->getXLenVT()));\n    return false;\n  default:\n    report_fatal_error(\"Unexpected asm memory constraint \" +\n                       InlineAsm::getMemConstraintName(ConstraintID));\n  }\n\n  return true;\n}",
      "start_line": 2194,
      "end_line": 2221,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "SelectInlineAsmMemoryOperand",
          "condition": "ConstraintID",
          "cases": [
            {
              "label": "InlineAsm",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "InlineAsm",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "InlineAsm",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getXLenVT",
        "report_fatal_error",
        "SelectAddrRegImm",
        "push_back"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SelectAddrFrameIndex",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Addr"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        }
      ],
      "body": "{\n  if (auto *FIN = dyn_cast<FrameIndexSDNode>(Addr)) {\n    Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), Subtarget->getXLenVT());\n    Offset = CurDAG->getTargetConstant(0, SDLoc(Addr), Subtarget->getXLenVT());\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 2223,
      "end_line": 2232,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getXLenVT",
        "getTargetFrameIndex",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SelectFrameAddrRegImm",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Addr"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        }
      ],
      "body": "{\n  if (SelectAddrFrameIndex(Addr, Base, Offset))\n    return true;\n\n  if (!CurDAG->isBaseWithConstantOffset(Addr))\n    return false;\n\n  if (auto *FIN = dyn_cast<FrameIndexSDNode>(Addr.getOperand(0))) {\n    int64_t CVal = cast<ConstantSDNode>(Addr.getOperand(1))->getSExtValue();\n    if (isInt<12>(CVal)) {\n      Base = CurDAG->getTargetFrameIndex(FIN->getIndex(),\n                                         Subtarget->getXLenVT());\n      Offset = CurDAG->getTargetConstant(CVal, SDLoc(Addr),\n                                         Subtarget->getXLenVT());\n      return true;\n    }\n  }\n\n  return false;\n}",
      "start_line": 2235,
      "end_line": 2255,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getTargetFrameIndex",
        "getOperand",
        "getXLenVT",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectConstantAddr",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SelectionDAG",
          "name": "*CurDAG"
        },
        {
          "type": "const SDLoc",
          "name": "&DL"
        },
        {
          "type": "const MVT",
          "name": "VT"
        },
        {
          "type": "const RISCVSubtarget",
          "name": "*Subtarget"
        },
        {
          "type": "SDValue",
          "name": "Addr"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        },
        {
          "type": "bool IsPrefetch =",
          "name": "false"
        }
      ],
      "body": "{\n  if (!isa<ConstantSDNode>(Addr))\n    return false;\n\n  int64_t CVal = cast<ConstantSDNode>(Addr)->getSExtValue();\n\n  // If the constant is a simm12, we can fold the whole constant and use X0 as\n  // the base. If the constant can be materialized with LUI+simm12, use LUI as\n  // the base. We can't use generateInstSeq because it favors LUI+ADDIW.\n  int64_t Lo12 = SignExtend64<12>(CVal);\n  int64_t Hi = (uint64_t)CVal - (uint64_t)Lo12;\n  if (!Subtarget->is64Bit() || isInt<32>(Hi)) {\n    if (IsPrefetch && (Lo12 & 0b11111) != 0)\n      return false;\n\n    if (Hi) {\n      int64_t Hi20 = (Hi >> 12) & 0xfffff;\n      Base = SDValue(\n          CurDAG->getMachineNode(RISCV::LUI, DL, VT,\n                                 CurDAG->getTargetConstant(Hi20, DL, VT)),\n          0);\n    } else {\n      Base = CurDAG->getRegister(RISCV::X0, VT);\n    }\n    Offset = CurDAG->getTargetConstant(Lo12, DL, VT);\n    return true;\n  }\n\n  // Ask how constant materialization would handle this constant.\n  RISCVMatInt::InstSeq Seq = RISCVMatInt::generateInstSeq(CVal, *Subtarget);\n\n  // If the last instruction would be an ADDI, we can fold its immediate and\n  // emit the rest of the sequence as the base.\n  if (Seq.back().getOpcode() != RISCV::ADDI)\n    return false;\n  Lo12 = Seq.back().getImm();\n  if (IsPrefetch && (Lo12 & 0b11111) != 0)\n    return false;\n\n  // Drop the last instruction.\n  Seq.pop_back();\n  assert(!Seq.empty() && \"Expected more instructions in sequence\");\n\n  Base = selectImmSeq(CurDAG, DL, VT, Seq);\n  Offset = CurDAG->getTargetConstant(Lo12, DL, VT);\n  return true;\n}",
      "start_line": 2258,
      "end_line": 2307,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "generateInstSeq",
        "SDValue",
        "getImm",
        "back",
        "getOpcode",
        "getRegister",
        "pop_back",
        "selectImmSeq",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isWorthFoldingAdd",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Add"
        }
      ],
      "body": "{\n  for (auto *Use : Add->uses()) {\n    if (Use->getOpcode() != ISD::LOAD && Use->getOpcode() != ISD::STORE &&\n        Use->getOpcode() != ISD::ATOMIC_LOAD &&\n        Use->getOpcode() != ISD::ATOMIC_STORE)\n      return false;\n    EVT VT = cast<MemSDNode>(Use)->getMemoryVT();\n    if (!VT.isScalarInteger() && VT != MVT::f16 && VT != MVT::f32 &&\n        VT != MVT::f64)\n      return false;\n    // Don't allow stores of the value. It must be used as the address.\n    if (Use->getOpcode() == ISD::STORE &&\n        cast<StoreSDNode>(Use)->getValue() == Add)\n      return false;\n    if (Use->getOpcode() == ISD::ATOMIC_STORE &&\n        cast<AtomicSDNode>(Use)->getVal() == Add)\n      return false;\n  }\n\n  return true;\n}",
      "start_line": 2311,
      "end_line": 2331,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMemoryVT",
        "getVal",
        "getOpcode",
        "getValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SelectAddrRegRegScale",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Addr"
        },
        {
          "type": "unsigned",
          "name": "MaxShiftAmount"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Index"
        },
        {
          "type": "SDValue",
          "name": "&Scale"
        }
      ],
      "body": "{\n  EVT VT = Addr.getSimpleValueType();\n  auto UnwrapShl = [this, VT, MaxShiftAmount](SDValue N, SDValue &Index,\n                                              SDValue &Shift) {\n    uint64_t ShiftAmt = 0;\n    Index = N;\n\n    if (N.getOpcode() == ISD::SHL && isa<ConstantSDNode>(N.getOperand(1))) {\n      // Only match shifts by a value in range [0, MaxShiftAmount].\n      if (N.getConstantOperandVal(1) <= MaxShiftAmount) {\n        Index = N.getOperand(0);\n        ShiftAmt = N.getConstantOperandVal(1);\n      }\n    }\n\n    Shift = CurDAG->getTargetConstant(ShiftAmt, SDLoc(N), VT);\n    return ShiftAmt != 0;\n  };\n\n  if (Addr.getOpcode() == ISD::ADD) {\n    if (auto *C1 = dyn_cast<ConstantSDNode>(Addr.getOperand(1))) {\n      SDValue AddrB = Addr.getOperand(0);\n      if (AddrB.getOpcode() == ISD::ADD &&\n          UnwrapShl(AddrB.getOperand(0), Index, Scale) &&\n          !isa<ConstantSDNode>(AddrB.getOperand(1)) &&\n          isInt<12>(C1->getSExtValue())) {\n        // (add (add (shl A C2) B) C1) -> (add (add B C1) (shl A C2))\n        SDValue C1Val =\n            CurDAG->getTargetConstant(C1->getZExtValue(), SDLoc(Addr), VT);\n        Base = SDValue(CurDAG->getMachineNode(RISCV::ADDI, SDLoc(Addr), VT,\n                                              AddrB.getOperand(1), C1Val),\n                       0);\n        return true;\n      }\n    } else if (UnwrapShl(Addr.getOperand(0), Index, Scale)) {\n      Base = Addr.getOperand(1);\n      return true;\n    } else {\n      UnwrapShl(Addr.getOperand(1), Index, Scale);\n      Base = Addr.getOperand(0);\n      return true;\n    }\n  } else if (UnwrapShl(Addr, Index, Scale)) {\n    EVT VT = Addr.getValueType();\n    Base = CurDAG->getRegister(RISCV::X0, VT);\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 2333,
      "end_line": 2385,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "getConstantOperandVal",
        "add",
        "getSimpleValueType",
        "getValueType",
        "getRegister",
        "getOperand",
        "SDLoc",
        "UnwrapShl",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SelectAddrRegImm",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Addr"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        },
        {
          "type": "bool",
          "name": "IsINX"
        }
      ],
      "body": "{\n  if (SelectAddrFrameIndex(Addr, Base, Offset))\n    return true;\n\n  SDLoc DL(Addr);\n  MVT VT = Addr.getSimpleValueType();\n\n  if (Addr.getOpcode() == RISCVISD::ADD_LO) {\n    Base = Addr.getOperand(0);\n    Offset = Addr.getOperand(1);\n    return true;\n  }\n\n  int64_t RV32ZdinxRange = IsINX ? 4 : 0;\n  if (CurDAG->isBaseWithConstantOffset(Addr)) {\n    int64_t CVal = cast<ConstantSDNode>(Addr.getOperand(1))->getSExtValue();\n    if (isInt<12>(CVal) && isInt<12>(CVal + RV32ZdinxRange)) {\n      Base = Addr.getOperand(0);\n      if (Base.getOpcode() == RISCVISD::ADD_LO) {\n        SDValue LoOperand = Base.getOperand(1);\n        if (auto *GA = dyn_cast<GlobalAddressSDNode>(LoOperand)) {\n          // If the Lo in (ADD_LO hi, lo) is a global variable's address\n          // (its low part, really), then we can rely on the alignment of that\n          // variable to provide a margin of safety before low part can overflow\n          // the 12 bits of the load/store offset. Check if CVal falls within\n          // that margin; if so (low part + CVal) can't overflow.\n          const DataLayout &DL = CurDAG->getDataLayout();\n          Align Alignment = commonAlignment(\n              GA->getGlobal()->getPointerAlignment(DL), GA->getOffset());\n          if (CVal == 0 || Alignment > CVal) {\n            int64_t CombinedOffset = CVal + GA->getOffset();\n            Base = Base.getOperand(0);\n            Offset = CurDAG->getTargetGlobalAddress(\n                GA->getGlobal(), SDLoc(LoOperand), LoOperand.getValueType(),\n                CombinedOffset, GA->getTargetFlags());\n            return true;\n          }\n        }\n      }\n\n      if (auto *FIN = dyn_cast<FrameIndexSDNode>(Base))\n        Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), VT);\n      Offset = CurDAG->getTargetConstant(CVal, DL, VT);\n      return true;\n    }\n  }\n\n  // Handle ADD with large immediates.\n  if (Addr.getOpcode() == ISD::ADD && isa<ConstantSDNode>(Addr.getOperand(1))) {\n    int64_t CVal = cast<ConstantSDNode>(Addr.getOperand(1))->getSExtValue();\n    assert(!(isInt<12>(CVal) && isInt<12>(CVal + RV32ZdinxRange)) &&\n           \"simm12 not already handled?\");\n\n    // Handle immediates in the range [-4096,-2049] or [2048, 4094]. We can use\n    // an ADDI for part of the offset and fold the rest into the load/store.\n    // This mirrors the AddiPair PatFrag in RISCVInstrInfo.td.\n    if (isInt<12>(CVal / 2) && isInt<12>(CVal - CVal / 2)) {\n      int64_t Adj = CVal < 0 ? -2048 : 2047;\n      Base = SDValue(\n          CurDAG->getMachineNode(RISCV::ADDI, DL, VT, Addr.getOperand(0),\n                                 CurDAG->getTargetConstant(Adj, DL, VT)),\n          0);\n      Offset = CurDAG->getTargetConstant(CVal - Adj, DL, VT);\n      return true;\n    }\n\n    // For larger immediates, we might be able to save one instruction from\n    // constant materialization by folding the Lo12 bits of the immediate into\n    // the address. We should only do this if the ADD is only used by loads and\n    // stores that can fold the lo12 bits. Otherwise, the ADD will get iseled\n    // separately with the full materialized immediate creating extra\n    // instructions.\n    if (isWorthFoldingAdd(Addr) &&\n        selectConstantAddr(CurDAG, DL, VT, Subtarget, Addr.getOperand(1), Base,\n                           Offset)) {\n      // Insert an ADD instruction with the materialized Hi52 bits.\n      Base = SDValue(\n          CurDAG->getMachineNode(RISCV::ADD, DL, VT, Addr.getOperand(0), Base),\n          0);\n      return true;\n    }\n  }\n\n  if (selectConstantAddr(CurDAG, DL, VT, Subtarget, Addr, Base, Offset))\n    return true;\n\n  Base = Addr;\n  Offset = CurDAG->getTargetConstant(0, DL, VT);\n  return true;\n}",
      "start_line": 2387,
      "end_line": 2477,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getTargetFrameIndex",
        "SDValue",
        "in",
        "getOffset",
        "so",
        "getSimpleValueType",
        "getTargetGlobalAddress",
        "getDataLayout",
        "getValueType",
        "getOperand",
        "getTargetFlags",
        "SDLoc",
        "DL",
        "commonAlignment",
        "selectConstantAddr",
        "getPointerAlignment",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "SelectAddrRegImmLsb00000",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "Addr"
        },
        {
          "type": "SDValue",
          "name": "&Base"
        },
        {
          "type": "SDValue",
          "name": "&Offset"
        }
      ],
      "body": "{\n  if (SelectAddrFrameIndex(Addr, Base, Offset))\n    return true;\n\n  SDLoc DL(Addr);\n  MVT VT = Addr.getSimpleValueType();\n\n  if (CurDAG->isBaseWithConstantOffset(Addr)) {\n    int64_t CVal = cast<ConstantSDNode>(Addr.getOperand(1))->getSExtValue();\n    if (isInt<12>(CVal)) {\n      Base = Addr.getOperand(0);\n\n      // Early-out if not a valid offset.\n      if ((CVal & 0b11111) != 0) {\n        Base = Addr;\n        Offset = CurDAG->getTargetConstant(0, DL, VT);\n        return true;\n      }\n\n      if (auto *FIN = dyn_cast<FrameIndexSDNode>(Base))\n        Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), VT);\n      Offset = CurDAG->getTargetConstant(CVal, DL, VT);\n      return true;\n    }\n  }\n\n  // Handle ADD with large immediates.\n  if (Addr.getOpcode() == ISD::ADD && isa<ConstantSDNode>(Addr.getOperand(1))) {\n    int64_t CVal = cast<ConstantSDNode>(Addr.getOperand(1))->getSExtValue();\n    assert(!(isInt<12>(CVal) && isInt<12>(CVal)) &&\n           \"simm12 not already handled?\");\n\n    // Handle immediates in the range [-4096,-2049] or [2017, 4065]. We can save\n    // one instruction by folding adjustment (-2048 or 2016) into the address.\n    if ((-2049 >= CVal && CVal >= -4096) || (4065 >= CVal && CVal >= 2017)) {\n      int64_t Adj = CVal < 0 ? -2048 : 2016;\n      int64_t AdjustedOffset = CVal - Adj;\n      Base = SDValue(CurDAG->getMachineNode(\n                         RISCV::ADDI, DL, VT, Addr.getOperand(0),\n                         CurDAG->getTargetConstant(AdjustedOffset, DL, VT)),\n                     0);\n      Offset = CurDAG->getTargetConstant(Adj, DL, VT);\n      return true;\n    }\n\n    if (selectConstantAddr(CurDAG, DL, VT, Subtarget, Addr.getOperand(1), Base,\n                           Offset, true)) {\n      // Insert an ADD instruction with the materialized Hi52 bits.\n      Base = SDValue(\n          CurDAG->getMachineNode(RISCV::ADD, DL, VT, Addr.getOperand(0), Base),\n          0);\n      return true;\n    }\n  }\n\n  if (selectConstantAddr(CurDAG, DL, VT, Subtarget, Addr, Base, Offset, true))\n    return true;\n\n  Base = Addr;\n  Offset = CurDAG->getTargetConstant(0, DL, VT);\n  return true;\n}",
      "start_line": 2481,
      "end_line": 2543,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getTargetFrameIndex",
        "SDValue",
        "getSimpleValueType",
        "getOperand",
        "DL",
        "getSExtValue",
        "adjustment",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectShiftMask",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "unsigned",
          "name": "ShiftWidth"
        },
        {
          "type": "SDValue",
          "name": "&ShAmt"
        }
      ],
      "body": "{\n  ShAmt = N;\n\n  // Peek through zext.\n  if (ShAmt->getOpcode() == ISD::ZERO_EXTEND)\n    ShAmt = ShAmt.getOperand(0);\n\n  // Shift instructions on RISC-V only read the lower 5 or 6 bits of the shift\n  // amount. If there is an AND on the shift amount, we can bypass it if it\n  // doesn't affect any of those bits.\n  if (ShAmt.getOpcode() == ISD::AND &&\n      isa<ConstantSDNode>(ShAmt.getOperand(1))) {\n    const APInt &AndMask = ShAmt.getConstantOperandAPInt(1);\n\n    // Since the max shift amount is a power of 2 we can subtract 1 to make a\n    // mask that covers the bits needed to represent all shift amounts.\n    assert(isPowerOf2_32(ShiftWidth) && \"Unexpected max shift amount!\");\n    APInt ShMask(AndMask.getBitWidth(), ShiftWidth - 1);\n\n    if (ShMask.isSubsetOf(AndMask)) {\n      ShAmt = ShAmt.getOperand(0);\n    } else {\n      // SimplifyDemandedBits may have optimized the mask so try restoring any\n      // bits that are known zero.\n      KnownBits Known = CurDAG->computeKnownBits(ShAmt.getOperand(0));\n      if (!ShMask.isSubsetOf(AndMask | Known.Zero))\n        return true;\n      ShAmt = ShAmt.getOperand(0);\n    }\n  }\n\n  if (ShAmt.getOpcode() == ISD::ADD &&\n      isa<ConstantSDNode>(ShAmt.getOperand(1))) {\n    uint64_t Imm = ShAmt.getConstantOperandVal(1);\n    // If we are shifting by X+N where N == 0 mod Size, then just shift by X\n    // to avoid the ADD.\n    if (Imm != 0 && Imm % ShiftWidth == 0) {\n      ShAmt = ShAmt.getOperand(0);\n      return true;\n    }\n  } else if (ShAmt.getOpcode() == ISD::SUB &&\n             isa<ConstantSDNode>(ShAmt.getOperand(0))) {\n    uint64_t Imm = ShAmt.getConstantOperandVal(0);\n    // If we are shifting by N-X where N == 0 mod Size, then just shift by -X to\n    // generate a NEG instead of a SUB of a constant.\n    if (Imm != 0 && Imm % ShiftWidth == 0) {\n      SDLoc DL(ShAmt);\n      EVT VT = ShAmt.getValueType();\n      SDValue Zero = CurDAG->getRegister(RISCV::X0, VT);\n      unsigned NegOpc = VT == MVT::i64 ? RISCV::SUBW : RISCV::SUB;\n      MachineSDNode *Neg = CurDAG->getMachineNode(NegOpc, DL, VT, Zero,\n                                                  ShAmt.getOperand(1));\n      ShAmt = SDValue(Neg, 0);\n      return true;\n    }\n    // If we are shifting by N-X where N == -1 mod Size, then just shift by ~X\n    // to generate a NOT instead of a SUB of a constant.\n    if (Imm % ShiftWidth == ShiftWidth - 1) {\n      SDLoc DL(ShAmt);\n      EVT VT = ShAmt.getValueType();\n      MachineSDNode *Not =\n          CurDAG->getMachineNode(RISCV::XORI, DL, VT, ShAmt.getOperand(1),\n                                 CurDAG->getTargetConstant(-1, DL, VT));\n      ShAmt = SDValue(Not, 0);\n      return true;\n    }\n  }\n\n  return true;\n}",
      "start_line": 2545,
      "end_line": 2615,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getConstantOperandAPInt",
        "SDValue",
        "getMachineNode",
        "getConstantOperandVal",
        "ShMask",
        "getRegister",
        "getValueType",
        "getOperand",
        "DL",
        "computeKnownBits",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectSETCC",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "ISD::CondCode",
          "name": "ExpectedCCVal"
        },
        {
          "type": "SDValue",
          "name": "&Val"
        }
      ],
      "body": "{\n  assert(ISD::isIntEqualitySetCC(ExpectedCCVal) &&\n         \"Unexpected condition code!\");\n\n  // We're looking for a setcc.\n  if (N->getOpcode() != ISD::SETCC)\n    return false;\n\n  // Must be an equality comparison.\n  ISD::CondCode CCVal = cast<CondCodeSDNode>(N->getOperand(2))->get();\n  if (CCVal != ExpectedCCVal)\n    return false;\n\n  SDValue LHS = N->getOperand(0);\n  SDValue RHS = N->getOperand(1);\n\n  if (!LHS.getValueType().isScalarInteger())\n    return false;\n\n  // If the RHS side is 0, we don't need any extra instructions, return the LHS.\n  if (isNullConstant(RHS)) {\n    Val = LHS;\n    return true;\n  }\n\n  SDLoc DL(N);\n\n  if (auto *C = dyn_cast<ConstantSDNode>(RHS)) {\n    int64_t CVal = C->getSExtValue();\n    // If the RHS is -2048, we can use xori to produce 0 if the LHS is -2048 and\n    // non-zero otherwise.\n    if (CVal == -2048) {\n      Val =\n          SDValue(CurDAG->getMachineNode(\n                      RISCV::XORI, DL, N->getValueType(0), LHS,\n                      CurDAG->getTargetConstant(CVal, DL, N->getValueType(0))),\n                  0);\n      return true;\n    }\n    // If the RHS is [-2047,2048], we can use addi with -RHS to produce 0 if the\n    // LHS is equal to the RHS and non-zero otherwise.\n    if (isInt<12>(CVal) || CVal == 2048) {\n      Val =\n          SDValue(CurDAG->getMachineNode(\n                      RISCV::ADDI, DL, N->getValueType(0), LHS,\n                      CurDAG->getTargetConstant(-CVal, DL, N->getValueType(0))),\n                  0);\n      return true;\n    }\n  }\n\n  // If nothing else we can XOR the LHS and RHS to produce zero if they are\n  // equal and a non-zero value if they aren't.\n  Val = SDValue(\n      CurDAG->getMachineNode(RISCV::XOR, DL, N->getValueType(0), LHS, RHS), 0);\n  return true;\n}",
      "start_line": 2622,
      "end_line": 2679,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "isScalarInteger",
        "get",
        "getOperand",
        "DL",
        "getSExtValue",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectSExtBits",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "unsigned",
          "name": "Bits"
        },
        {
          "type": "SDValue",
          "name": "&Val"
        }
      ],
      "body": "{\n  if (N.getOpcode() == ISD::SIGN_EXTEND_INREG &&\n      cast<VTSDNode>(N.getOperand(1))->getVT().getSizeInBits() == Bits) {\n    Val = N.getOperand(0);\n    return true;\n  }\n\n  auto UnwrapShlSra = [](SDValue N, unsigned ShiftAmt) {\n    if (N.getOpcode() != ISD::SRA || !isa<ConstantSDNode>(N.getOperand(1)))\n      return N;\n\n    SDValue N0 = N.getOperand(0);\n    if (N0.getOpcode() == ISD::SHL && isa<ConstantSDNode>(N0.getOperand(1)) &&\n        N.getConstantOperandVal(1) == ShiftAmt &&\n        N0.getConstantOperandVal(1) == ShiftAmt)\n      return N0.getOperand(0);\n\n    return N;\n  };\n\n  MVT VT = N.getSimpleValueType();\n  if (CurDAG->ComputeNumSignBits(N) > (VT.getSizeInBits() - Bits)) {\n    Val = UnwrapShlSra(N, VT.getSizeInBits() - Bits);\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 2681,
      "end_line": 2708,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getConstantOperandVal",
        "UnwrapShlSra",
        "getSimpleValueType",
        "getVT",
        "getOperand",
        "getSizeInBits"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectZExtBits",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "unsigned",
          "name": "Bits"
        },
        {
          "type": "SDValue",
          "name": "&Val"
        }
      ],
      "body": "{\n  if (N.getOpcode() == ISD::AND) {\n    auto *C = dyn_cast<ConstantSDNode>(N.getOperand(1));\n    if (C && C->getZExtValue() == maskTrailingOnes<uint64_t>(Bits)) {\n      Val = N.getOperand(0);\n      return true;\n    }\n  }\n  MVT VT = N.getSimpleValueType();\n  APInt Mask = APInt::getBitsSetFrom(VT.getSizeInBits(), Bits);\n  if (CurDAG->MaskedValueIsZero(N, Mask)) {\n    Val = N;\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 2710,
      "end_line": 2726,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getOperand",
        "getSimpleValueType",
        "getBitsSetFrom"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectSHXADDOp",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "unsigned",
          "name": "ShAmt"
        },
        {
          "type": "SDValue",
          "name": "&Val"
        }
      ],
      "body": "{\n  if (N.getOpcode() == ISD::AND && isa<ConstantSDNode>(N.getOperand(1))) {\n    SDValue N0 = N.getOperand(0);\n\n    bool LeftShift = N0.getOpcode() == ISD::SHL;\n    if ((LeftShift || N0.getOpcode() == ISD::SRL) &&\n        isa<ConstantSDNode>(N0.getOperand(1))) {\n      uint64_t Mask = N.getConstantOperandVal(1);\n      unsigned C2 = N0.getConstantOperandVal(1);\n\n      unsigned XLen = Subtarget->getXLen();\n      if (LeftShift)\n        Mask &= maskTrailingZeros<uint64_t>(C2);\n      else\n        Mask &= maskTrailingOnes<uint64_t>(XLen - C2);\n\n      // Look for (and (shl y, c2), c1) where c1 is a shifted mask with no\n      // leading zeros and c3 trailing zeros. We can use an SRLI by c2+c3\n      // followed by a SHXADD with c3 for the X amount.\n      if (isShiftedMask_64(Mask)) {\n        unsigned Leading = XLen - llvm::bit_width(Mask);\n        unsigned Trailing = llvm::countr_zero(Mask);\n        if (LeftShift && Leading == 0 && C2 < Trailing && Trailing == ShAmt) {\n          SDLoc DL(N);\n          EVT VT = N.getValueType();\n          Val = SDValue(CurDAG->getMachineNode(\n                            RISCV::SRLI, DL, VT, N0.getOperand(0),\n                            CurDAG->getTargetConstant(Trailing - C2, DL, VT)),\n                        0);\n          return true;\n        }\n        // Look for (and (shr y, c2), c1) where c1 is a shifted mask with c2\n        // leading zeros and c3 trailing zeros. We can use an SRLI by C3\n        // followed by a SHXADD using c3 for the X amount.\n        if (!LeftShift && Leading == C2 && Trailing == ShAmt) {\n          SDLoc DL(N);\n          EVT VT = N.getValueType();\n          Val = SDValue(\n              CurDAG->getMachineNode(\n                  RISCV::SRLI, DL, VT, N0.getOperand(0),\n                  CurDAG->getTargetConstant(Leading + Trailing, DL, VT)),\n              0);\n          return true;\n        }\n      }\n    }\n  }\n\n  bool LeftShift = N.getOpcode() == ISD::SHL;\n  if ((LeftShift || N.getOpcode() == ISD::SRL) &&\n      isa<ConstantSDNode>(N.getOperand(1))) {\n    SDValue N0 = N.getOperand(0);\n    if (N0.getOpcode() == ISD::AND && N0.hasOneUse() &&\n        isa<ConstantSDNode>(N0.getOperand(1))) {\n      uint64_t Mask = N0.getConstantOperandVal(1);\n      if (isShiftedMask_64(Mask)) {\n        unsigned C1 = N.getConstantOperandVal(1);\n        unsigned XLen = Subtarget->getXLen();\n        unsigned Leading = XLen - llvm::bit_width(Mask);\n        unsigned Trailing = llvm::countr_zero(Mask);\n        // Look for (shl (and X, Mask), C1) where Mask has 32 leading zeros and\n        // C3 trailing zeros. If C1+C3==ShAmt we can use SRLIW+SHXADD.\n        if (LeftShift && Leading == 32 && Trailing > 0 &&\n            (Trailing + C1) == ShAmt) {\n          SDLoc DL(N);\n          EVT VT = N.getValueType();\n          Val = SDValue(CurDAG->getMachineNode(\n                            RISCV::SRLIW, DL, VT, N0.getOperand(0),\n                            CurDAG->getTargetConstant(Trailing, DL, VT)),\n                        0);\n          return true;\n        }\n        // Look for (srl (and X, Mask), C1) where Mask has 32 leading zeros and\n        // C3 trailing zeros. If C3-C1==ShAmt we can use SRLIW+SHXADD.\n        if (!LeftShift && Leading == 32 && Trailing > C1 &&\n            (Trailing - C1) == ShAmt) {\n          SDLoc DL(N);\n          EVT VT = N.getValueType();\n          Val = SDValue(CurDAG->getMachineNode(\n                            RISCV::SRLIW, DL, VT, N0.getOperand(0),\n                            CurDAG->getTargetConstant(Trailing, DL, VT)),\n                        0);\n          return true;\n        }\n      }\n    }\n  }\n\n  return false;\n}",
      "start_line": 2731,
      "end_line": 2821,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "SDValue",
        "hasOneUse",
        "countr_zero",
        "getConstantOperandVal",
        "bit_width",
        "getOpcode",
        "getValueType",
        "getOperand",
        "getXLen",
        "DL",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectSHXADD_UWOp",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "unsigned",
          "name": "ShAmt"
        },
        {
          "type": "SDValue",
          "name": "&Val"
        }
      ],
      "body": "{\n  if (N.getOpcode() == ISD::AND && isa<ConstantSDNode>(N.getOperand(1)) &&\n      N.hasOneUse()) {\n    SDValue N0 = N.getOperand(0);\n    if (N0.getOpcode() == ISD::SHL && isa<ConstantSDNode>(N0.getOperand(1)) &&\n        N0.hasOneUse()) {\n      uint64_t Mask = N.getConstantOperandVal(1);\n      unsigned C2 = N0.getConstantOperandVal(1);\n\n      Mask &= maskTrailingZeros<uint64_t>(C2);\n\n      // Look for (and (shl y, c2), c1) where c1 is a shifted mask with\n      // 32-ShAmt leading zeros and c2 trailing zeros. We can use SLLI by\n      // c2-ShAmt followed by SHXADD_UW with ShAmt for the X amount.\n      if (isShiftedMask_64(Mask)) {\n        unsigned Leading = llvm::countl_zero(Mask);\n        unsigned Trailing = llvm::countr_zero(Mask);\n        if (Leading == 32 - ShAmt && Trailing == C2 && Trailing > ShAmt) {\n          SDLoc DL(N);\n          EVT VT = N.getValueType();\n          Val = SDValue(CurDAG->getMachineNode(\n                            RISCV::SLLI, DL, VT, N0.getOperand(0),\n                            CurDAG->getTargetConstant(C2 - ShAmt, DL, VT)),\n                        0);\n          return true;\n        }\n      }\n    }\n  }\n\n  return false;\n}",
      "start_line": 2826,
      "end_line": 2858,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "countl_zero",
        "SDValue",
        "hasOneUse",
        "countr_zero",
        "getConstantOperandVal",
        "getValueType",
        "getOperand",
        "DL",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "vectorPseudoHasAllNBitUsers",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*User"
        },
        {
          "type": "unsigned",
          "name": "UserOpNo"
        },
        {
          "type": "unsigned",
          "name": "Bits"
        },
        {
          "type": "const TargetInstrInfo",
          "name": "*TII"
        }
      ],
      "body": "{\n  unsigned MCOpcode = RISCV::getRVVMCOpcode(User->getMachineOpcode());\n\n  if (!MCOpcode)\n    return false;\n\n  const MCInstrDesc &MCID = TII->get(User->getMachineOpcode());\n  const uint64_t TSFlags = MCID.TSFlags;\n  if (!RISCVII::hasSEWOp(TSFlags))\n    return false;\n  assert(RISCVII::hasVLOp(TSFlags));\n\n  bool HasGlueOp = User->getGluedNode() != nullptr;\n  unsigned ChainOpIdx = User->getNumOperands() - HasGlueOp - 1;\n  bool HasChainOp = User->getOperand(ChainOpIdx).getValueType() == MVT::Other;\n  bool HasVecPolicyOp = RISCVII::hasVecPolicyOp(TSFlags);\n  unsigned VLIdx =\n      User->getNumOperands() - HasVecPolicyOp - HasChainOp - HasGlueOp - 2;\n  const unsigned Log2SEW = User->getConstantOperandVal(VLIdx + 1);\n\n  if (UserOpNo == VLIdx)\n    return false;\n\n  auto NumDemandedBits =\n      RISCV::getVectorLowDemandedScalarBits(MCOpcode, Log2SEW);\n  return NumDemandedBits && Bits >= *NumDemandedBits;\n}",
      "start_line": 2860,
      "end_line": 2888,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getRVVMCOpcode",
        "getGluedNode",
        "getConstantOperandVal",
        "getVectorLowDemandedScalarBits",
        "getNumOperands",
        "get",
        "getValueType",
        "getOperand",
        "hasVecPolicyOp"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasAllNBitUsers",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*Node"
        },
        {
          "type": "unsigned",
          "name": "Bits"
        },
        {
          "type": "const unsigned",
          "name": "Depth"
        }
      ],
      "body": "{\n  assert((Node->getOpcode() == ISD::ADD || Node->getOpcode() == ISD::SUB ||\n          Node->getOpcode() == ISD::MUL || Node->getOpcode() == ISD::SHL ||\n          Node->getOpcode() == ISD::SRL || Node->getOpcode() == ISD::AND ||\n          Node->getOpcode() == ISD::OR || Node->getOpcode() == ISD::XOR ||\n          Node->getOpcode() == ISD::SIGN_EXTEND_INREG ||\n          isa<ConstantSDNode>(Node) || Depth != 0) &&\n         \"Unexpected opcode\");\n\n  if (Depth >= SelectionDAG::MaxRecursionDepth)\n    return false;\n\n  // The PatFrags that call this may run before RISCVGenDAGISel.inc has checked\n  // the VT. Ensure the type is scalar to avoid wasting time on vectors.\n  if (Depth == 0 && !Node->getValueType(0).isScalarInteger())\n    return false;\n\n  for (auto UI = Node->use_begin(), UE = Node->use_end(); UI != UE; ++UI) {\n    SDNode *User = *UI;\n    // Users of this node should have already been instruction selected\n    if (!User->isMachineOpcode())\n      return false;\n\n    // TODO: Add more opcodes?\n    switch (User->getMachineOpcode()) {\n    default:\n      if (vectorPseudoHasAllNBitUsers(User, UI.getOperandNo(), Bits, TII))\n        break;\n      return false;\n    case RISCV::ADDW:\n    case RISCV::ADDIW:\n    case RISCV::SUBW:\n    case RISCV::MULW:\n    case RISCV::SLLW:\n    case RISCV::SLLIW:\n    case RISCV::SRAW:\n    case RISCV::SRAIW:\n    case RISCV::SRLW:\n    case RISCV::SRLIW:\n    case RISCV::DIVW:\n    case RISCV::DIVUW:\n    case RISCV::REMW:\n    case RISCV::REMUW:\n    case RISCV::ROLW:\n    case RISCV::RORW:\n    case RISCV::RORIW:\n    case RISCV::CLZW:\n    case RISCV::CTZW:\n    case RISCV::CPOPW:\n    case RISCV::SLLI_UW:\n    case RISCV::FMV_W_X:\n    case RISCV::FCVT_H_W:\n    case RISCV::FCVT_H_WU:\n    case RISCV::FCVT_S_W:\n    case RISCV::FCVT_S_WU:\n    case RISCV::FCVT_D_W:\n    case RISCV::FCVT_D_WU:\n    case RISCV::TH_REVW:\n    case RISCV::TH_SRRIW:\n      if (Bits < 32)\n        return false;\n      break;\n    case RISCV::SLL:\n    case RISCV::SRA:\n    case RISCV::SRL:\n    case RISCV::ROL:\n    case RISCV::ROR:\n    case RISCV::BSET:\n    case RISCV::BCLR:\n    case RISCV::BINV:\n      // Shift amount operands only use log2(Xlen) bits.\n      if (UI.getOperandNo() != 1 || Bits < Log2_32(Subtarget->getXLen()))\n        return false;\n      break;\n    case RISCV::SLLI:\n      // SLLI only uses the lower (XLen - ShAmt) bits.\n      if (Bits < Subtarget->getXLen() - User->getConstantOperandVal(1))\n        return false;\n      break;\n    case RISCV::ANDI:\n      if (Bits >= (unsigned)llvm::bit_width(User->getConstantOperandVal(1)))\n        break;\n      goto RecCheck;\n    case RISCV::ORI: {\n      uint64_t Imm = cast<ConstantSDNode>(User->getOperand(1))->getSExtValue();\n      if (Bits >= (unsigned)llvm::bit_width<uint64_t>(~Imm))\n        break;\n      [[fallthrough]];\n    }\n    case RISCV::AND:\n    case RISCV::OR:\n    case RISCV::XOR:\n    case RISCV::XORI:\n    case RISCV::ANDN:\n    case RISCV::ORN:\n    case RISCV::XNOR:\n    case RISCV::SH1ADD:\n    case RISCV::SH2ADD:\n    case RISCV::SH3ADD:\n    RecCheck:\n      if (hasAllNBitUsers(User, Bits, Depth + 1))\n        break;\n      return false;\n    case RISCV::SRLI: {\n      unsigned ShAmt = User->getConstantOperandVal(1);\n      // If we are shifting right by less than Bits, and users don't demand any\n      // bits that were shifted into [Bits-1:0], then we can consider this as an\n      // N-Bit user.\n      if (Bits > ShAmt && hasAllNBitUsers(User, Bits - ShAmt, Depth + 1))\n        break;\n      return false;\n    }\n    case RISCV::SEXT_B:\n    case RISCV::PACKH:\n      if (Bits < 8)\n        return false;\n      break;\n    case RISCV::SEXT_H:\n    case RISCV::FMV_H_X:\n    case RISCV::ZEXT_H_RV32:\n    case RISCV::ZEXT_H_RV64:\n    case RISCV::PACKW:\n      if (Bits < 16)\n        return false;\n      break;\n    case RISCV::PACK:\n      if (Bits < (Subtarget->getXLen() / 2))\n        return false;\n      break;\n    case RISCV::ADD_UW:\n    case RISCV::SH1ADD_UW:\n    case RISCV::SH2ADD_UW:\n    case RISCV::SH3ADD_UW:\n      // The first operand to add.uw/shXadd.uw is implicitly zero extended from\n      // 32 bits.\n      if (UI.getOperandNo() != 0 || Bits < 32)\n        return false;\n      break;\n    case RISCV::SB:\n      if (UI.getOperandNo() != 0 || Bits < 8)\n        return false;\n      break;\n    case RISCV::SH:\n      if (UI.getOperandNo() != 0 || Bits < 16)\n        return false;\n      break;\n    case RISCV::SW:\n      if (UI.getOperandNo() != 0 || Bits < 32)\n        return false;\n      break;\n    }\n  }\n\n  return true;\n}",
      "start_line": 2899,
      "end_line": 3054,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getConstantOperandVal",
        "bit_width",
        "getOpcode",
        "log2",
        "isScalarInteger",
        "use_end",
        "getOperand",
        "lower",
        "getSExtValue",
        "Log2_32"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectSimm5Shl2",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&Simm5"
        },
        {
          "type": "SDValue",
          "name": "&Shl2"
        }
      ],
      "body": "{\n  if (auto *C = dyn_cast<ConstantSDNode>(N)) {\n    int64_t Offset = C->getSExtValue();\n    int64_t Shift;\n    for (Shift = 0; Shift < 4; Shift++)\n      if (isInt<5>(Offset >> Shift) && ((Offset % (1LL << Shift)) == 0))\n        break;\n\n    // Constant cannot be encoded.\n    if (Shift == 4)\n      return false;\n\n    EVT Ty = N->getValueType(0);\n    Simm5 = CurDAG->getTargetConstant(Offset >> Shift, SDLoc(N), Ty);\n    Shl2 = CurDAG->getTargetConstant(Shift, SDLoc(N), Ty);\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 3057,
      "end_line": 3077,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getTargetConstant",
        "getSExtValue",
        "getValueType"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVLOp",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&VL"
        }
      ],
      "body": "{\n  auto *C = dyn_cast<ConstantSDNode>(N);\n  if (C && isUInt<5>(C->getZExtValue())) {\n    VL = CurDAG->getTargetConstant(C->getZExtValue(), SDLoc(N),\n                                   N->getValueType(0));\n  } else if (C && C->isAllOnes()) {\n    // Treat all ones as VLMax.\n    VL = CurDAG->getTargetConstant(RISCV::VLMaxSentinel, SDLoc(N),\n                                   N->getValueType(0));\n  } else if (isa<RegisterSDNode>(N) &&\n             cast<RegisterSDNode>(N)->getReg() == RISCV::X0) {\n    // All our VL operands use an operand that allows GPRNoX0 or an immediate\n    // as the register class. Convert X0 to a special immediate to pass the\n    // MachineVerifier. This is recognized specially by the vsetvli insertion\n    // pass.\n    VL = CurDAG->getTargetConstant(RISCV::VLMaxSentinel, SDLoc(N),\n                                   N->getValueType(0));\n  } else {\n    VL = N;\n  }\n\n  return true;\n}",
      "start_line": 3081,
      "end_line": 3103,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getReg",
        "SDLoc",
        "getValueType",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "findVSplat",
      "return_type": "SDValue",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        }
      ],
      "body": "{\n  if (N.getOpcode() == ISD::INSERT_SUBVECTOR) {\n    if (!N.getOperand(0).isUndef())\n      return SDValue();\n    N = N.getOperand(1);\n  }\n  SDValue Splat = N;\n  if ((Splat.getOpcode() != RISCVISD::VMV_V_X_VL &&\n       Splat.getOpcode() != RISCVISD::VMV_S_X_VL) ||\n      !Splat.getOperand(0).isUndef())\n    return SDValue();\n  assert(Splat.getNumOperands() == 3 && \"Unexpected number of operands\");\n  return Splat;\n}",
      "start_line": 3105,
      "end_line": 3118,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "isUndef",
        "SDValue",
        "getOperand",
        "getOpcode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplat",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&SplatVal"
        }
      ],
      "body": "{\n  SDValue Splat = findVSplat(N);\n  if (!Splat)\n    return false;\n\n  SplatVal = Splat.getOperand(1);\n  return true;\n}",
      "start_line": 3120,
      "end_line": 3127,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getOperand",
        "findVSplat"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatSimm5",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&SplatVal"
        }
      ],
      "body": "{\n  return selectVSplatImmHelper(N, SplatVal, *CurDAG, *Subtarget,\n                               [](int64_t Imm) { return isInt<5>(Imm); });\n}",
      "start_line": 3159,
      "end_line": 3162,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "selectVSplatImmHelper"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatImmHelper",
      "return_type": "return",
      "parameters": [
        {
          "type": "N",
          "name": ""
        },
        {
          "type": "SplatVal",
          "name": ""
        },
        {
          "type": "*CurDAG",
          "name": ""
        },
        {
          "type": "*Subtarget",
          "name": ""
        },
        {
          "type": "[](int64_t",
          "name": "Imm"
        }
      ],
      "body": "{ return isInt<5>(Imm); }",
      "start_line": 3160,
      "end_line": 3161,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatSimm5Plus1",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&SplatVal"
        }
      ],
      "body": "{\n  return selectVSplatImmHelper(\n      N, SplatVal, *CurDAG, *Subtarget,\n      [](int64_t Imm) { return (isInt<5>(Imm) && Imm != -16) || Imm == 16; });\n}",
      "start_line": 3164,
      "end_line": 3168,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "selectVSplatImmHelper"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatImmHelper",
      "return_type": "return",
      "parameters": [
        {
          "type": "N",
          "name": ""
        },
        {
          "type": "SplatVal",
          "name": ""
        },
        {
          "type": "*CurDAG",
          "name": ""
        },
        {
          "type": "*Subtarget",
          "name": ""
        },
        {
          "type": "[](int64_t",
          "name": "Imm"
        }
      ],
      "body": "{ return (isInt<5>(Imm) && Imm != -16) || Imm == 16; }",
      "start_line": 3165,
      "end_line": 3167,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatSimm5Plus1NonZero",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&SplatVal"
        }
      ],
      "body": "{\n  return selectVSplatImmHelper(\n      N, SplatVal, *CurDAG, *Subtarget, [](int64_t Imm) {\n        return Imm != 0 && ((isInt<5>(Imm) && Imm != -16) || Imm == 16);\n      });\n}",
      "start_line": 3170,
      "end_line": 3176,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "selectVSplatImmHelper"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatImmHelper",
      "return_type": "return",
      "parameters": [
        {
          "type": "N",
          "name": ""
        },
        {
          "type": "SplatVal",
          "name": ""
        },
        {
          "type": "*CurDAG",
          "name": ""
        },
        {
          "type": "*Subtarget",
          "name": ""
        },
        {
          "type": "[](int64_t",
          "name": "Imm"
        }
      ],
      "body": "{\n        return Imm != 0 && ((isInt<5>(Imm) && Imm != -16) || Imm == 16);\n      }",
      "start_line": 3172,
      "end_line": 3175,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatUimm",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "unsigned",
          "name": "Bits"
        },
        {
          "type": "SDValue",
          "name": "&SplatVal"
        }
      ],
      "body": "{\n  return selectVSplatImmHelper(\n      N, SplatVal, *CurDAG, *Subtarget,\n      [Bits](int64_t Imm) { return isUIntN(Bits, Imm); });\n}",
      "start_line": 3178,
      "end_line": 3183,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "selectVSplatImmHelper",
        "isUIntN"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectVSplatImmHelper",
      "return_type": "return",
      "parameters": [
        {
          "type": "N",
          "name": ""
        },
        {
          "type": "SplatVal",
          "name": ""
        },
        {
          "type": "*CurDAG",
          "name": ""
        },
        {
          "type": "*Subtarget",
          "name": ""
        },
        {
          "type": "[Bits](int64_t",
          "name": "Imm"
        }
      ],
      "body": "{ return isUIntN(Bits, Imm); }",
      "start_line": 3180,
      "end_line": 3182,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "isUIntN"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectLow8BitsVSplat",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&SplatVal"
        }
      ],
      "body": "{\n  // Truncates are custom lowered during legalization.\n  auto IsTrunc = [this](SDValue N) {\n    if (N->getOpcode() != RISCVISD::TRUNCATE_VECTOR_VL)\n      return false;\n    SDValue VL;\n    selectVLOp(N->getOperand(2), VL);\n    // Any vmset_vl is ok, since any bits past VL are undefined and we can\n    // assume they are set.\n    return N->getOperand(1).getOpcode() == RISCVISD::VMSET_VL &&\n           isa<ConstantSDNode>(VL) &&\n           cast<ConstantSDNode>(VL)->getSExtValue() == RISCV::VLMaxSentinel;\n  };\n\n  // We can have multiple nested truncates, so unravel them all if needed.\n  while (N->getOpcode() == ISD::SIGN_EXTEND ||\n         N->getOpcode() == ISD::ZERO_EXTEND || IsTrunc(N)) {\n    if (!N.hasOneUse() ||\n        N.getValueType().getSizeInBits().getKnownMinValue() < 8)\n      return false;\n    N = N->getOperand(0);\n  }\n\n  return selectVSplat(N, SplatVal);\n}",
      "start_line": 3185,
      "end_line": 3209,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "selectVLOp",
        "IsTrunc",
        "getOpcode",
        "getValueType",
        "getOperand",
        "selectVSplat",
        "getSizeInBits",
        "getKnownMinValue",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectFPImm",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "SDValue",
          "name": "&Imm"
        }
      ],
      "body": "{\n  ConstantFPSDNode *CFP = dyn_cast<ConstantFPSDNode>(N.getNode());\n  if (!CFP)\n    return false;\n  const APFloat &APF = CFP->getValueAPF();\n  // td can handle +0.0 already.\n  if (APF.isPosZero())\n    return false;\n\n  MVT VT = CFP->getSimpleValueType(0);\n\n  // Even if this FPImm requires an additional FNEG (i.e. the second element of\n  // the returned pair is true) we still prefer FLI + FNEG over immediate\n  // materialization as the latter might generate a longer instruction sequence.\n  if (static_cast<const RISCVTargetLowering *>(TLI)\n          ->getLegalZfaFPImm(APF, VT)\n          .first >= 0)\n    return false;\n\n  MVT XLenVT = Subtarget->getXLenVT();\n  if (VT == MVT::f64 && !Subtarget->is64Bit()) {\n    assert(APF.isNegZero() && \"Unexpected constant.\");\n    return false;\n  }\n  SDLoc DL(N);\n  Imm = selectImm(CurDAG, DL, XLenVT, APF.bitcastToAPInt().getSExtValue(),\n                  *Subtarget);\n  return true;\n}",
      "start_line": 3211,
      "end_line": 3239,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getLegalZfaFPImm",
        "getSimpleValueType",
        "getValueAPF",
        "FNEG",
        "DL",
        "selectImm",
        "getXLenVT",
        "getSExtValue",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "selectRVVSimm5",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "N"
        },
        {
          "type": "unsigned",
          "name": "Width"
        },
        {
          "type": "SDValue",
          "name": "&Imm"
        }
      ],
      "body": "{\n  if (auto *C = dyn_cast<ConstantSDNode>(N)) {\n    int64_t ImmVal = SignExtend64(C->getSExtValue(), Width);\n\n    if (!isInt<5>(ImmVal))\n      return false;\n\n    Imm = CurDAG->getTargetConstant(ImmVal, SDLoc(N), Subtarget->getXLenVT());\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 3241,
      "end_line": 3254,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getXLenVT",
        "SignExtend64",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "doPeepholeSExtW",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        }
      ],
      "body": "{\n  // Look for the sext.w pattern, addiw rd, rs1, 0.\n  if (N->getMachineOpcode() != RISCV::ADDIW ||\n      !isNullConstant(N->getOperand(1)))\n    return false;\n\n  SDValue N0 = N->getOperand(0);\n  if (!N0.isMachineOpcode())\n    return false;\n\n  switch (N0.getMachineOpcode()) {\n  default:\n    break;\n  case RISCV::ADD:\n  case RISCV::ADDI:\n  case RISCV::SUB:\n  case RISCV::MUL:\n  case RISCV::SLLI: {\n    // Convert sext.w+add/sub/mul to their W instructions. This will create\n    // a new independent instruction. This improves latency.\n    unsigned Opc;\n    switch (N0.getMachineOpcode()) {\n    default:\n      llvm_unreachable(\"Unexpected opcode!\");\n    case RISCV::ADD:  Opc = RISCV::ADDW;  break;\n    case RISCV::ADDI: Opc = RISCV::ADDIW; break;\n    case RISCV::SUB:  Opc = RISCV::SUBW;  break;\n    case RISCV::MUL:  Opc = RISCV::MULW;  break;\n    case RISCV::SLLI: Opc = RISCV::SLLIW; break;\n    }\n\n    SDValue N00 = N0.getOperand(0);\n    SDValue N01 = N0.getOperand(1);\n\n    // Shift amount needs to be uimm5.\n    if (N0.getMachineOpcode() == RISCV::SLLI &&\n        !isUInt<5>(cast<ConstantSDNode>(N01)->getSExtValue()))\n      break;\n\n    SDNode *Result =\n        CurDAG->getMachineNode(Opc, SDLoc(N), N->getValueType(0),\n                               N00, N01);\n    ReplaceUses(N, Result);\n    return true;\n  }\n  case RISCV::ADDW:\n  case RISCV::ADDIW:\n  case RISCV::SUBW:\n  case RISCV::MULW:\n  case RISCV::SLLIW:\n  case RISCV::PACKW:\n  case RISCV::TH_MULAW:\n  case RISCV::TH_MULAH:\n  case RISCV::TH_MULSW:\n  case RISCV::TH_MULSH:\n    if (N0.getValueType() == MVT::i32)\n      break;\n\n    // Result is already sign extended just remove the sext.w.\n    // NOTE: We only handle the nodes that are selected with hasAllWUsers.\n    ReplaceUses(N, N0.getNode());\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 3258,
      "end_line": 3323,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMachineNode",
        "ReplaceUses",
        "getValueType",
        "getOperand",
        "isNullConstant",
        "llvm_unreachable",
        "getSExtValue"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "usesAllOnesMask",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "MaskOp"
        },
        {
          "type": "SDValue",
          "name": "GlueOp"
        }
      ],
      "body": "{\n  // Check that we're using V0 as a mask register.\n  if (!isa<RegisterSDNode>(MaskOp) ||\n      cast<RegisterSDNode>(MaskOp)->getReg() != RISCV::V0)\n    return false;\n\n  // The glued user defines V0.\n  const auto *Glued = GlueOp.getNode();\n\n  if (!Glued || Glued->getOpcode() != ISD::CopyToReg)\n    return false;\n\n  // Check that we're defining V0 as a mask register.\n  if (!isa<RegisterSDNode>(Glued->getOperand(1)) ||\n      cast<RegisterSDNode>(Glued->getOperand(1))->getReg() != RISCV::V0)\n    return false;\n\n  // Check the instruction defining V0; it needs to be a VMSET pseudo.\n  SDValue MaskSetter = Glued->getOperand(2);\n\n  // Sometimes the VMSET is wrapped in a COPY_TO_REGCLASS, e.g. if the mask came\n  // from an extract_subvector or insert_subvector.\n  if (MaskSetter->isMachineOpcode() &&\n      MaskSetter->getMachineOpcode() == RISCV::COPY_TO_REGCLASS)\n    MaskSetter = MaskSetter->getOperand(0);\n\n  const auto IsVMSet = [](unsigned Opc) {\n    return Opc == RISCV::PseudoVMSET_M_B1 || Opc == RISCV::PseudoVMSET_M_B16 ||\n           Opc == RISCV::PseudoVMSET_M_B2 || Opc == RISCV::PseudoVMSET_M_B32 ||\n           Opc == RISCV::PseudoVMSET_M_B4 || Opc == RISCV::PseudoVMSET_M_B64 ||\n           Opc == RISCV::PseudoVMSET_M_B8;\n  };\n\n  // TODO: Check that the VMSET is the expected bitwidth? The pseudo has\n  // undefined behaviour if it's the wrong bitwidth, so we could choose to\n  // assume that it's all-ones? Same applies to its VL.\n  return MaskSetter->isMachineOpcode() &&\n         IsVMSet(MaskSetter.getMachineOpcode());\n}",
      "start_line": 3325,
      "end_line": 3363,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "IsVMSet",
        "getMachineOpcode",
        "isMachineOpcode",
        "getOperand",
        "getReg",
        "getNode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "usesAllOnesMask",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        },
        {
          "type": "unsigned",
          "name": "MaskOpIdx"
        }
      ],
      "body": "{\n  return usesAllOnesMask(N->getOperand(MaskOpIdx),\n                         N->getOperand(N->getNumOperands() - 1));\n}",
      "start_line": 3366,
      "end_line": 3369,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getOperand",
        "usesAllOnesMask"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isImplicitDef",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDValue",
          "name": "V"
        }
      ],
      "body": "{\n  return V.isMachineOpcode() &&\n         V.getMachineOpcode() == TargetOpcode::IMPLICIT_DEF;\n}",
      "start_line": 3371,
      "end_line": 3374,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getMachineOpcode",
        "isMachineOpcode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "doPeepholeMaskedRVV",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MachineSDNode",
          "name": "*N"
        }
      ],
      "body": "{\n  const RISCV::RISCVMaskedPseudoInfo *I =\n      RISCV::getMaskedPseudoInfo(N->getMachineOpcode());\n  if (!I)\n    return false;\n\n  unsigned MaskOpIdx = I->MaskOpIdx;\n  if (!usesAllOnesMask(N, MaskOpIdx))\n    return false;\n\n  // There are two classes of pseudos in the table - compares and\n  // everything else.  See the comment on RISCVMaskedPseudo for details.\n  const unsigned Opc = I->UnmaskedPseudo;\n  const MCInstrDesc &MCID = TII->get(Opc);\n  const bool UseTUPseudo = RISCVII::hasVecPolicyOp(MCID.TSFlags);\n#ifndef NDEBUG\n  const MCInstrDesc &MaskedMCID = TII->get(N->getMachineOpcode());\n  assert(RISCVII::hasVecPolicyOp(MaskedMCID.TSFlags) ==\n         RISCVII::hasVecPolicyOp(MCID.TSFlags) &&\n         \"Masked and unmasked pseudos are inconsistent\");\n  const bool HasTiedDest = RISCVII::isFirstDefTiedToFirstUse(MCID);\n  assert(UseTUPseudo == HasTiedDest && \"Unexpected pseudo structure\");\n#endif\n\n  SmallVector<SDValue, 8> Ops;\n  // Skip the merge operand at index 0 if !UseTUPseudo.\n  for (unsigned I = !UseTUPseudo, E = N->getNumOperands(); I != E; I++) {\n    // Skip the mask, and the Glue.\n    SDValue Op = N->getOperand(I);\n    if (I == MaskOpIdx || Op.getValueType() == MVT::Glue)\n      continue;\n    Ops.push_back(Op);\n  }\n\n  // Transitively apply any node glued to our new node.\n  const auto *Glued = N->getGluedNode();\n  if (auto *TGlued = Glued->getGluedNode())\n    Ops.push_back(SDValue(TGlued, TGlued->getNumValues() - 1));\n\n  MachineSDNode *Result =\n      CurDAG->getMachineNode(Opc, SDLoc(N), N->getVTList(), Ops);\n\n  if (!N->memoperands_empty())\n    CurDAG->setNodeMemRefs(Result, N->memoperands());\n\n  Result->setFlags(N->getFlags());\n  ReplaceUses(N, Result);\n\n  return true;\n}",
      "start_line": 3380,
      "end_line": 3429,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "setNodeMemRefs",
        "getMachineNode",
        "getGluedNode",
        "getVTList",
        "ReplaceUses",
        "get",
        "setFlags",
        "getOperand",
        "getMaskedPseudoInfo",
        "hasVecPolicyOp",
        "push_back",
        "isFirstDefTiedToFirstUse"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "IsVMerge",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        }
      ],
      "body": "{\n  return RISCV::getRVVMCOpcode(N->getMachineOpcode()) == RISCV::VMERGE_VVM;\n}",
      "start_line": 3431,
      "end_line": 3433,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getRVVMCOpcode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "IsVMv",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        }
      ],
      "body": "{\n  return RISCV::getRVVMCOpcode(N->getMachineOpcode()) == RISCV::VMV_V_V;\n}",
      "start_line": 3435,
      "end_line": 3437,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getRVVMCOpcode"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "GetVMSetForLMul",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "RISCVII::VLMUL",
          "name": "LMUL"
        }
      ],
      "body": "{\n  switch (LMUL) {\n  case RISCVII::LMUL_F8:\n    return RISCV::PseudoVMSET_M_B1;\n  case RISCVII::LMUL_F4:\n    return RISCV::PseudoVMSET_M_B2;\n  case RISCVII::LMUL_F2:\n    return RISCV::PseudoVMSET_M_B4;\n  case RISCVII::LMUL_1:\n    return RISCV::PseudoVMSET_M_B8;\n  case RISCVII::LMUL_2:\n    return RISCV::PseudoVMSET_M_B16;\n  case RISCVII::LMUL_4:\n    return RISCV::PseudoVMSET_M_B32;\n  case RISCVII::LMUL_8:\n    return RISCV::PseudoVMSET_M_B64;\n  case RISCVII::LMUL_RESERVED:\n    llvm_unreachable(\"Unexpected LMUL\");\n  }\n  llvm_unreachable(\"Unknown VLMUL enum\");\n}",
      "start_line": 3439,
      "end_line": 3459,
      "is_virtual": false,
      "is_const": false,
      "switches": [
        {
          "function": "GetVMSetForLMul",
          "condition": "LMUL",
          "cases": [
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "RISCVII",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "performCombineVMergeAndVOps",
      "return_type": "bool",
      "parameters": [
        {
          "type": "SDNode",
          "name": "*N"
        }
      ],
      "body": "{\n  SDValue Merge, False, True, VL, Mask, Glue;\n  // A vmv.v.v is equivalent to a vmerge with an all-ones mask.\n  if (IsVMv(N)) {\n    Merge = N->getOperand(0);\n    False = N->getOperand(0);\n    True = N->getOperand(1);\n    VL = N->getOperand(2);\n    // A vmv.v.v won't have a Mask or Glue, instead we'll construct an all-ones\n    // mask later below.\n  } else {\n    assert(IsVMerge(N));\n    Merge = N->getOperand(0);\n    False = N->getOperand(1);\n    True = N->getOperand(2);\n    Mask = N->getOperand(3);\n    VL = N->getOperand(4);\n    // We always have a glue node for the mask at v0.\n    Glue = N->getOperand(N->getNumOperands() - 1);\n  }\n  assert(!Mask || cast<RegisterSDNode>(Mask)->getReg() == RISCV::V0);\n  assert(!Glue || Glue.getValueType() == MVT::Glue);\n\n  // We require that either merge and false are the same, or that merge\n  // is undefined.\n  if (Merge != False && !isImplicitDef(Merge))\n    return false;\n\n  assert(True.getResNo() == 0 &&\n         \"Expect True is the first output of an instruction.\");\n\n  // Need N is the exactly one using True.\n  if (!True.hasOneUse())\n    return false;\n\n  if (!True.isMachineOpcode())\n    return false;\n\n  unsigned TrueOpc = True.getMachineOpcode();\n  const MCInstrDesc &TrueMCID = TII->get(TrueOpc);\n  uint64_t TrueTSFlags = TrueMCID.TSFlags;\n  bool HasTiedDest = RISCVII::isFirstDefTiedToFirstUse(TrueMCID);\n\n  bool IsMasked = false;\n  const RISCV::RISCVMaskedPseudoInfo *Info =\n      RISCV::lookupMaskedIntrinsicByUnmasked(TrueOpc);\n  if (!Info && HasTiedDest) {\n    Info = RISCV::getMaskedPseudoInfo(TrueOpc);\n    IsMasked = true;\n  }\n\n  if (!Info)\n    return false;\n\n  // When Mask is not a true mask, this transformation is illegal for some\n  // operations whose results are affected by mask, like viota.m.\n  if (Info->MaskAffectsResult && Mask && !usesAllOnesMask(Mask, Glue))\n    return false;\n\n  if (HasTiedDest && !isImplicitDef(True->getOperand(0))) {\n    // The vmerge instruction must be TU.\n    // FIXME: This could be relaxed, but we need to handle the policy for the\n    // resulting op correctly.\n    if (isImplicitDef(Merge))\n      return false;\n    SDValue MergeOpTrue = True->getOperand(0);\n    // Both the vmerge instruction and the True instruction must have the same\n    // merge operand.\n    if (False != MergeOpTrue)\n      return false;\n  }\n\n  if (IsMasked) {\n    assert(HasTiedDest && \"Expected tied dest\");\n    // The vmerge instruction must be TU.\n    if (isImplicitDef(Merge))\n      return false;\n    // The vmerge instruction must have an all 1s mask since we're going to keep\n    // the mask from the True instruction.\n    // FIXME: Support mask agnostic True instruction which would have an\n    // undef merge operand.\n    if (Mask && !usesAllOnesMask(Mask, Glue))\n      return false;\n  }\n\n  // Skip if True has side effect.\n  // TODO: Support vleff and vlsegff.\n  if (TII->get(TrueOpc).hasUnmodeledSideEffects())\n    return false;\n\n  // The last operand of a masked instruction may be glued.\n  bool HasGlueOp = True->getGluedNode() != nullptr;\n\n  // The chain operand may exist either before the glued operands or in the last\n  // position.\n  unsigned TrueChainOpIdx = True.getNumOperands() - HasGlueOp - 1;\n  bool HasChainOp =\n      True.getOperand(TrueChainOpIdx).getValueType() == MVT::Other;\n\n  if (HasChainOp) {\n    // Avoid creating cycles in the DAG. We must ensure that none of the other\n    // operands depend on True through it's Chain.\n    SmallVector<const SDNode *, 4> LoopWorklist;\n    SmallPtrSet<const SDNode *, 16> Visited;\n    LoopWorklist.push_back(False.getNode());\n    if (Mask)\n      LoopWorklist.push_back(Mask.getNode());\n    LoopWorklist.push_back(VL.getNode());\n    if (Glue)\n      LoopWorklist.push_back(Glue.getNode());\n    if (SDNode::hasPredecessorHelper(True.getNode(), Visited, LoopWorklist))\n      return false;\n  }\n\n  // The vector policy operand may be present for masked intrinsics\n  bool HasVecPolicyOp = RISCVII::hasVecPolicyOp(TrueTSFlags);\n  unsigned TrueVLIndex =\n      True.getNumOperands() - HasVecPolicyOp - HasChainOp - HasGlueOp - 2;\n  SDValue TrueVL = True.getOperand(TrueVLIndex);\n  SDValue SEW = True.getOperand(TrueVLIndex + 1);\n\n  auto GetMinVL = [](SDValue LHS, SDValue RHS) {\n    if (LHS == RHS)\n      return LHS;\n    if (isAllOnesConstant(LHS))\n      return RHS;\n    if (isAllOnesConstant(RHS))\n      return LHS;\n    auto *CLHS = dyn_cast<ConstantSDNode>(LHS);\n    auto *CRHS = dyn_cast<ConstantSDNode>(RHS);\n    if (!CLHS || !CRHS)\n      return SDValue();\n    return CLHS->getZExtValue() <= CRHS->getZExtValue() ? LHS : RHS;\n  };\n\n  // Because N and True must have the same merge operand (or True's operand is\n  // implicit_def), the \"effective\" body is the minimum of their VLs.\n  SDValue OrigVL = VL;\n  VL = GetMinVL(TrueVL, VL);\n  if (!VL)\n    return false;\n\n  // If we end up changing the VL or mask of True, then we need to make sure it\n  // doesn't raise any observable fp exceptions, since changing the active\n  // elements will affect how fflags is set.\n  if (TrueVL != VL || !IsMasked)\n    if (mayRaiseFPException(True.getNode()) &&\n        !True->getFlags().hasNoFPExcept())\n      return false;\n\n  SDLoc DL(N);\n\n  // From the preconditions we checked above, we know the mask and thus glue\n  // for the result node will be taken from True.\n  if (IsMasked) {\n    Mask = True->getOperand(Info->MaskOpIdx);\n    Glue = True->getOperand(True->getNumOperands() - 1);\n    assert(Glue.getValueType() == MVT::Glue);\n  }\n  // If we end up using the vmerge mask the vmerge is actually a vmv.v.v, create\n  // an all-ones mask to use.\n  else if (IsVMv(N)) {\n    unsigned TSFlags = TII->get(N->getMachineOpcode()).TSFlags;\n    unsigned VMSetOpc = GetVMSetForLMul(RISCVII::getLMul(TSFlags));\n    ElementCount EC = N->getValueType(0).getVectorElementCount();\n    MVT MaskVT = MVT::getVectorVT(MVT::i1, EC);\n\n    SDValue AllOnesMask =\n        SDValue(CurDAG->getMachineNode(VMSetOpc, DL, MaskVT, VL, SEW), 0);\n    SDValue MaskCopy = CurDAG->getCopyToReg(CurDAG->getEntryNode(), DL,\n                                            RISCV::V0, AllOnesMask, SDValue());\n    Mask = CurDAG->getRegister(RISCV::V0, MaskVT);\n    Glue = MaskCopy.getValue(1);\n  }\n\n  unsigned MaskedOpc = Info->MaskedPseudo;\n#ifndef NDEBUG\n  const MCInstrDesc &MaskedMCID = TII->get(MaskedOpc);\n  assert(RISCVII::hasVecPolicyOp(MaskedMCID.TSFlags) &&\n         \"Expected instructions with mask have policy operand.\");\n  assert(MaskedMCID.getOperandConstraint(MaskedMCID.getNumDefs(),\n                                         MCOI::TIED_TO) == 0 &&\n         \"Expected instructions with mask have a tied dest.\");\n#endif\n\n  // Use a tumu policy, relaxing it to tail agnostic provided that the merge\n  // operand is undefined.\n  //\n  // However, if the VL became smaller than what the vmerge had originally, then\n  // elements past VL that were previously in the vmerge's body will have moved\n  // to the tail. In that case we always need to use tail undisturbed to\n  // preserve them.\n  bool MergeVLShrunk = VL != OrigVL;\n  uint64_t Policy = (isImplicitDef(Merge) && !MergeVLShrunk)\n                        ? RISCVII::TAIL_AGNOSTIC\n                        : /*TUMU*/ 0;\n  SDValue PolicyOp =\n    CurDAG->getTargetConstant(Policy, DL, Subtarget->getXLenVT());\n\n\n  SmallVector<SDValue, 8> Ops;\n  Ops.push_back(False);\n\n  const bool HasRoundingMode = RISCVII::hasRoundModeOp(TrueTSFlags);\n  const unsigned NormalOpsEnd = TrueVLIndex - IsMasked - HasRoundingMode;\n  assert(!IsMasked || NormalOpsEnd == Info->MaskOpIdx);\n  Ops.append(True->op_begin() + HasTiedDest, True->op_begin() + NormalOpsEnd);\n\n  Ops.push_back(Mask);\n\n  // For unmasked \"VOp\" with rounding mode operand, that is interfaces like\n  // (..., rm, vl) or (..., rm, vl, policy).\n  // Its masked version is (..., vm, rm, vl, policy).\n  // Check the rounding mode pseudo nodes under RISCVInstrInfoVPseudos.td\n  if (HasRoundingMode)\n    Ops.push_back(True->getOperand(TrueVLIndex - 1));\n\n  Ops.append({VL, SEW, PolicyOp});\n\n  // Result node should have chain operand of True.\n  if (HasChainOp)\n    Ops.push_back(True.getOperand(TrueChainOpIdx));\n\n  // Add the glue for the CopyToReg of mask->v0.\n  Ops.push_back(Glue);\n\n  MachineSDNode *Result =\n      CurDAG->getMachineNode(MaskedOpc, DL, True->getVTList(), Ops);\n  Result->setFlags(True->getFlags());\n\n  if (!cast<MachineSDNode>(True)->memoperands_empty())\n    CurDAG->setNodeMemRefs(Result, cast<MachineSDNode>(True)->memoperands());\n\n  // Replace vmerge.vvm node by Result.\n  ReplaceUses(SDValue(N, 0), SDValue(Result, 0));\n\n  // Replace another value of True. E.g. chain and VL.\n  for (unsigned Idx = 1; Idx < True->getNumValues(); ++Idx)\n    ReplaceUses(True.getValue(Idx), SDValue(Result, Idx));\n\n  return true;\n}",
      "start_line": 3474,
      "end_line": 3715,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getReg",
        "getMachineNode",
        "operand",
        "isFirstDefTiedToFirstUse",
        "memoperands_empty",
        "setNodeMemRefs",
        "hasNoFPExcept",
        "getNumOperands",
        "getMachineOpcode",
        "isImplicitDef",
        "op_begin",
        "getFlags",
        "setFlags",
        "getVectorVT",
        "hasRoundModeOp",
        "getVectorElementCount",
        "or",
        "is",
        "GetMinVL",
        "push_back",
        "getZExtValue",
        "SDValue",
        "getCopyToReg",
        "getGluedNode",
        "getValue",
        "getRegister",
        "getValueType",
        "append",
        "memoperands",
        "getOperand",
        "hasVecPolicyOp",
        "hasUnmodeledSideEffects",
        "ReplaceUses",
        "GetVMSetForLMul",
        "get",
        "lookupMaskedIntrinsicByUnmasked",
        "DL",
        "getMaskedPseudoInfo",
        "getTargetConstant"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "doPeepholeMergeVVMFold",
      "return_type": "bool",
      "parameters": [],
      "body": "{\n  bool MadeChange = false;\n  SelectionDAG::allnodes_iterator Position = CurDAG->allnodes_end();\n\n  while (Position != CurDAG->allnodes_begin()) {\n    SDNode *N = &*--Position;\n    if (N->use_empty() || !N->isMachineOpcode())\n      continue;\n\n    if (IsVMerge(N) || IsVMv(N))\n      MadeChange |= performCombineVMergeAndVOps(N);\n  }\n  return MadeChange;\n}",
      "start_line": 3717,
      "end_line": 3730,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "IsVMv",
        "isMachineOpcode",
        "performCombineVMergeAndVOps",
        "allnodes_end"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "doPeepholeNoRegPassThru",
      "return_type": "bool",
      "parameters": [],
      "body": "{\n  bool MadeChange = false;\n  SelectionDAG::allnodes_iterator Position = CurDAG->allnodes_end();\n\n  while (Position != CurDAG->allnodes_begin()) {\n    SDNode *N = &*--Position;\n    if (N->use_empty() || !N->isMachineOpcode())\n      continue;\n\n    const unsigned Opc = N->getMachineOpcode();\n    if (!RISCVVPseudosTable::getPseudoInfo(Opc) ||\n        !RISCVII::isFirstDefTiedToFirstUse(TII->get(Opc)) ||\n        !isImplicitDef(N->getOperand(0)))\n      continue;\n\n    SmallVector<SDValue> Ops;\n    Ops.push_back(CurDAG->getRegister(RISCV::NoRegister, N->getValueType(0)));\n    for (unsigned I = 1, E = N->getNumOperands(); I != E; I++) {\n      SDValue Op = N->getOperand(I);\n      Ops.push_back(Op);\n    }\n\n    MachineSDNode *Result =\n      CurDAG->getMachineNode(Opc, SDLoc(N), N->getVTList(), Ops);\n    Result->setFlags(N->getFlags());\n    CurDAG->setNodeMemRefs(Result, cast<MachineSDNode>(N)->memoperands());\n    ReplaceUses(N, Result);\n    MadeChange = true;\n  }\n  return MadeChange;\n}",
      "start_line": 3737,
      "end_line": 3767,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "setNodeMemRefs",
        "getMachineNode",
        "getVTList",
        "ReplaceUses",
        "isImplicitDef",
        "getMachineOpcode",
        "memoperands",
        "isMachineOpcode",
        "getOperand",
        "setFlags",
        "allnodes_end",
        "isFirstDefTiedToFirstUse",
        "push_back"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getABIStackAlignment",
      "return_type": "Align",
      "parameters": [
        {
          "type": "RISCVABI::ABI",
          "name": "ABI"
        }
      ],
      "body": "{\n  if (ABI == RISCVABI::ABI_ILP32E)\n    return Align(4);\n  if (ABI == RISCVABI::ABI_LP64E)\n    return Align(8);\n  return Align(16);\n}",
      "start_line": 30,
      "end_line": 36,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "Align"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "emitSCSPrologue",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::iterator",
          "name": "MI"
        },
        {
          "type": "const DebugLoc",
          "name": "&DL"
        }
      ],
      "body": "{\n  if (!MF.getFunction().hasFnAttribute(Attribute::ShadowCallStack))\n    return;\n\n  const auto &STI = MF.getSubtarget<RISCVSubtarget>();\n  const llvm::RISCVRegisterInfo *TRI = STI.getRegisterInfo();\n  Register RAReg = TRI->getRARegister();\n\n  // Do not save RA to the SCS if it's not saved to the regular stack,\n  // i.e. RA is not at risk of being overwritten.\n  std::vector<CalleeSavedInfo> &CSI = MF.getFrameInfo().getCalleeSavedInfo();\n  if (llvm::none_of(\n          CSI, [&](CalleeSavedInfo &CSR) { return CSR.getReg() == RAReg; }))\n    return;\n\n  Register SCSPReg = RISCVABI::getSCSPReg();\n\n  const RISCVInstrInfo *TII = STI.getInstrInfo();\n  bool IsRV64 = STI.hasFeature(RISCV::Feature64Bit);\n  int64_t SlotSize = STI.getXLen() / 8;\n  // Store return address to shadow call stack\n  // addi    gp, gp, [4|8]\n  // s[w|d]  ra, -[4|8](gp)\n  BuildMI(MBB, MI, DL, TII->get(RISCV::ADDI))\n      .addReg(SCSPReg, RegState::Define)\n      .addReg(SCSPReg)\n      .addImm(SlotSize)\n      .setMIFlag(MachineInstr::FrameSetup);\n  BuildMI(MBB, MI, DL, TII->get(IsRV64 ? RISCV::SD : RISCV::SW))\n      .addReg(RAReg)\n      .addReg(SCSPReg)\n      .addImm(-SlotSize)\n      .setMIFlag(MachineInstr::FrameSetup);\n\n  // Emit a CFI instruction that causes SlotSize to be subtracted from the value\n  // of the shadow stack pointer when unwinding past this frame.\n  char DwarfSCSReg = TRI->getDwarfRegNum(SCSPReg, /*IsEH*/ true);\n  assert(DwarfSCSReg < 32 && \"SCS Register should be < 32 (X3).\");\n\n  char Offset = static_cast<char>(-SlotSize) & 0x7f;\n  const char CFIInst[] = {\n      dwarf::DW_CFA_val_expression,\n      DwarfSCSReg, // register\n      2,           // length\n      static_cast<char>(unsigned(dwarf::DW_OP_breg0 + DwarfSCSReg)),\n      Offset, // addend (sleb128)\n  };\n\n  unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createEscape(\n      nullptr, StringRef(CFIInst, sizeof(CFIInst))));\n  BuildMI(MBB, MI, DL, TII->get(TargetOpcode::CFI_INSTRUCTION))\n      .addCFIIndex(CFIIndex)\n      .setMIFlag(MachineInstr::FrameSetup);\n}",
      "start_line": 52,
      "end_line": 107,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "addFrameInst",
        "addend",
        "unsigned",
        "setMIFlag",
        "hasFeature",
        "BuildMI",
        "getFrameInfo",
        "addImm",
        "getDwarfRegNum",
        "getRARegister",
        "addReg",
        "getSCSPReg",
        "getRegisterInfo",
        "getXLen",
        "getInstrInfo",
        "getCalleeSavedInfo",
        "hasFnAttribute",
        "addCFIIndex",
        "getReg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "emitSCSEpilogue",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::iterator",
          "name": "MI"
        },
        {
          "type": "const DebugLoc",
          "name": "&DL"
        }
      ],
      "body": "{\n  if (!MF.getFunction().hasFnAttribute(Attribute::ShadowCallStack))\n    return;\n\n  const auto &STI = MF.getSubtarget<RISCVSubtarget>();\n  Register RAReg = STI.getRegisterInfo()->getRARegister();\n\n  // See emitSCSPrologue() above.\n  std::vector<CalleeSavedInfo> &CSI = MF.getFrameInfo().getCalleeSavedInfo();\n  if (llvm::none_of(\n          CSI, [&](CalleeSavedInfo &CSR) { return CSR.getReg() == RAReg; }))\n    return;\n\n  Register SCSPReg = RISCVABI::getSCSPReg();\n\n  const RISCVInstrInfo *TII = STI.getInstrInfo();\n  bool IsRV64 = STI.hasFeature(RISCV::Feature64Bit);\n  int64_t SlotSize = STI.getXLen() / 8;\n  // Load return address from shadow call stack\n  // l[w|d]  ra, -[4|8](gp)\n  // addi    gp, gp, -[4|8]\n  BuildMI(MBB, MI, DL, TII->get(IsRV64 ? RISCV::LD : RISCV::LW))\n      .addReg(RAReg, RegState::Define)\n      .addReg(SCSPReg)\n      .addImm(-SlotSize)\n      .setMIFlag(MachineInstr::FrameDestroy);\n  BuildMI(MBB, MI, DL, TII->get(RISCV::ADDI))\n      .addReg(SCSPReg, RegState::Define)\n      .addReg(SCSPReg)\n      .addImm(-SlotSize)\n      .setMIFlag(MachineInstr::FrameDestroy);\n  // Restore the SCS pointer\n  unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createRestore(\n      nullptr, STI.getRegisterInfo()->getDwarfRegNum(SCSPReg, /*IsEH*/ true)));\n  BuildMI(MBB, MI, DL, TII->get(TargetOpcode::CFI_INSTRUCTION))\n      .addCFIIndex(CFIIndex)\n      .setMIFlags(MachineInstr::FrameDestroy);\n}",
      "start_line": 109,
      "end_line": 148,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "addFrameInst",
        "emitSCSPrologue",
        "setMIFlag",
        "hasFeature",
        "BuildMI",
        "getFrameInfo",
        "addImm",
        "getDwarfRegNum",
        "getRARegister",
        "addReg",
        "getSCSPReg",
        "getRegisterInfo",
        "getXLen",
        "getInstrInfo",
        "getCalleeSavedInfo",
        "hasFnAttribute",
        "addCFIIndex",
        "setMIFlags",
        "getReg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getLibCallID",
      "return_type": "int",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "const std::vector<CalleeSavedInfo>",
          "name": "&CSI"
        }
      ],
      "body": "{\n  const auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n\n  if (CSI.empty() || !RVFI->useSaveRestoreLibCalls(MF))\n    return -1;\n\n  Register MaxReg = RISCV::NoRegister;\n  for (auto &CS : CSI)\n    // RISCVRegisterInfo::hasReservedSpillSlot assigns negative frame indexes to\n    // registers which can be saved by libcall.\n    if (CS.getFrameIdx() < 0)\n      MaxReg = std::max(MaxReg.id(), CS.getReg().id());\n\n  if (MaxReg == RISCV::NoRegister)\n    return -1;\n\n  switch (MaxReg) {\n  default:\n    llvm_unreachable(\"Something has gone wrong!\");\n  case /*s11*/ RISCV::X27: return 12;\n  case /*s10*/ RISCV::X26: return 11;\n  case /*s9*/  RISCV::X25: return 10;\n  case /*s8*/  RISCV::X24: return 9;\n  case /*s7*/  RISCV::X23: return 8;\n  case /*s6*/  RISCV::X22: return 7;\n  case /*s5*/  RISCV::X21: return 6;\n  case /*s4*/  RISCV::X20: return 5;\n  case /*s3*/  RISCV::X19: return 4;\n  case /*s2*/  RISCV::X18: return 3;\n  case /*s1*/  RISCV::X9:  return 2;\n  case /*s0*/  RISCV::X8:  return 1;\n  case /*ra*/  RISCV::X1:  return 0;\n  }\n}",
      "start_line": 154,
      "end_line": 188,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "getLibCallID",
          "condition": "MaxReg",
          "cases": [
            {
              "label": "/*s11*/ RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s10*/ RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s9*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s8*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s7*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s6*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s5*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s4*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s3*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s2*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s1*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*s0*/  RISCV",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "/*ra*/  RISCV",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getReg",
        "useSaveRestoreLibCalls",
        "id",
        "max",
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getSpillLibCallName",
      "return_type": "char *",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "const std::vector<CalleeSavedInfo>",
          "name": "&CSI"
        }
      ],
      "body": "{\n  static const char *const SpillLibCalls[] = {\n    \"__riscv_save_0\",\n    \"__riscv_save_1\",\n    \"__riscv_save_2\",\n    \"__riscv_save_3\",\n    \"__riscv_save_4\",\n    \"__riscv_save_5\",\n    \"__riscv_save_6\",\n    \"__riscv_save_7\",\n    \"__riscv_save_8\",\n    \"__riscv_save_9\",\n    \"__riscv_save_10\",\n    \"__riscv_save_11\",\n    \"__riscv_save_12\"\n  };\n\n  int LibCallID = getLibCallID(MF, CSI);\n  if (LibCallID == -1)\n    return nullptr;\n  return SpillLibCalls[LibCallID];\n}",
      "start_line": 192,
      "end_line": 215,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLibCallID"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRestoreLibCallName",
      "return_type": "char *",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "const std::vector<CalleeSavedInfo>",
          "name": "&CSI"
        }
      ],
      "body": "{\n  static const char *const RestoreLibCalls[] = {\n    \"__riscv_restore_0\",\n    \"__riscv_restore_1\",\n    \"__riscv_restore_2\",\n    \"__riscv_restore_3\",\n    \"__riscv_restore_4\",\n    \"__riscv_restore_5\",\n    \"__riscv_restore_6\",\n    \"__riscv_restore_7\",\n    \"__riscv_restore_8\",\n    \"__riscv_restore_9\",\n    \"__riscv_restore_10\",\n    \"__riscv_restore_11\",\n    \"__riscv_restore_12\"\n  };\n\n  int LibCallID = getLibCallID(MF, CSI);\n  if (LibCallID == -1)\n    return nullptr;\n  return RestoreLibCalls[LibCallID];\n}",
      "start_line": 219,
      "end_line": 242,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getLibCallID"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getMaxPushPopReg",
      "return_type": "Register",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "const std::vector<CalleeSavedInfo>",
          "name": "&CSI"
        }
      ],
      "body": "{\n  Register MaxPushPopReg = RISCV::NoRegister;\n  for (auto &CS : CSI) {\n    // RISCVRegisterInfo::hasReservedSpillSlot assigns negative frame indices to\n    // registers which can be saved by Zcmp Push.\n    if (CS.getFrameIdx() < 0)\n      MaxPushPopReg = std::max(MaxPushPopReg.id(), CS.getReg().id());\n  }\n  // if rlist is {rs, s0-s10}, then s11 will also be included\n  if (MaxPushPopReg == RISCV::X26)\n    MaxPushPopReg = RISCV::X27;\n  return MaxPushPopReg;\n}",
      "start_line": 280,
      "end_line": 293,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "max",
        "id",
        "getReg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasFP",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  const TargetRegisterInfo *RegInfo = MF.getSubtarget().getRegisterInfo();\n\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  return MF.getTarget().Options.DisableFramePointerElim(MF) ||\n         RegInfo->hasStackRealignment(MF) || MFI.hasVarSizedObjects() ||\n         MFI.isFrameAddressTaken();\n}",
      "start_line": 299,
      "end_line": 306,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isFrameAddressTaken",
        "getSubtarget",
        "DisableFramePointerElim",
        "getFrameInfo",
        "getTarget",
        "hasVarSizedObjects",
        "getRegisterInfo",
        "hasStackRealignment"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasBP",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  const TargetRegisterInfo *TRI = STI.getRegisterInfo();\n\n  // If we do not reserve stack space for outgoing arguments in prologue,\n  // we will adjust the stack pointer before call instruction. After the\n  // adjustment, we can not use SP to access the stack objects for the\n  // arguments. Instead, use BP to access these stack objects.\n  return (MFI.hasVarSizedObjects() ||\n          (!hasReservedCallFrame(MF) && (!MFI.isMaxCallFrameSizeComputed() ||\n                                         MFI.getMaxCallFrameSize() != 0))) &&\n         TRI->hasStackRealignment(MF);\n}",
      "start_line": 308,
      "end_line": 320,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasReservedCallFrame",
        "getFrameInfo",
        "getMaxCallFrameSize",
        "getRegisterInfo",
        "isMaxCallFrameSizeComputed",
        "hasStackRealignment"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "determineFrameLayout",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n\n  // Get the number of bytes to allocate from the FrameInfo.\n  uint64_t FrameSize = MFI.getStackSize();\n\n  // Get the alignment.\n  Align StackAlign = getStackAlign();\n\n  // Make sure the frame is aligned.\n  FrameSize = alignTo(FrameSize, StackAlign);\n\n  // Update frame info.\n  MFI.setStackSize(FrameSize);\n\n  // When using SP or BP to access stack objects, we may require extra padding\n  // to ensure the bottom of the RVV stack is correctly aligned within the main\n  // stack. We calculate this as the amount required to align the scalar local\n  // variable section up to the RVV alignment.\n  const TargetRegisterInfo *TRI = STI.getRegisterInfo();\n  if (RVFI->getRVVStackSize() && (!hasFP(MF) || TRI->hasStackRealignment(MF))) {\n    int ScalarLocalVarSize = FrameSize - RVFI->getCalleeSavedStackSize() -\n                             RVFI->getVarArgsSaveSize();\n    if (auto RVVPadding =\n            offsetToAlignment(ScalarLocalVarSize, RVFI->getRVVStackAlign()))\n      RVFI->setRVVPadding(RVVPadding);\n  }\n}",
      "start_line": 323,
      "end_line": 351,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasFP",
        "setRVVPadding",
        "getStackSize",
        "setStackSize",
        "getFrameInfo",
        "getRegisterInfo",
        "getCalleeSavedStackSize",
        "getVarArgsSaveSize",
        "alignTo",
        "hasStackRealignment",
        "getStackAlign"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getStackSizeWithRVVPadding",
      "return_type": "uint64_t",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n  return alignTo(MFI.getStackSize() + RVFI->getRVVPadding(), getStackAlign());\n}",
      "start_line": 355,
      "end_line": 360,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "alignTo",
        "getStackAlign",
        "getRVVPadding",
        "getFrameInfo"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getFPReg",
      "return_type": "Register",
      "parameters": [
        {
          "type": "const RISCVSubtarget",
          "name": "&STI"
        }
      ],
      "body": "{ return RISCV::X8; }",
      "start_line": 363,
      "end_line": 363,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getSPReg",
      "return_type": "Register",
      "parameters": [
        {
          "type": "const RISCVSubtarget",
          "name": "&STI"
        }
      ],
      "body": "{ return RISCV::X2; }",
      "start_line": 366,
      "end_line": 366,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "adjustStackForRVV",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::iterator",
          "name": "MBBI"
        },
        {
          "type": "const DebugLoc",
          "name": "&DL"
        },
        {
          "type": "int64_t",
          "name": "Amount"
        },
        {
          "type": "MachineInstr::MIFlag",
          "name": "Flag"
        }
      ],
      "body": "{\n  assert(Amount != 0 && \"Did not need to adjust stack pointer for RVV.\");\n\n  const Register SPReg = getSPReg(STI);\n\n  // Optimize compile time offset case\n  StackOffset Offset = StackOffset::getScalable(Amount);\n  if (STI.getRealMinVLen() == STI.getRealMaxVLen()) {\n    // 1. Multiply the number of v-slots by the (constant) length of register\n    const int64_t VLENB = STI.getRealMinVLen() / 8;\n    assert(Amount % 8 == 0 &&\n           \"Reserve the stack by the multiple of one vector size.\");\n    const int64_t NumOfVReg = Amount / 8;\n    const int64_t FixedOffset = NumOfVReg * VLENB;\n    if (!isInt<32>(FixedOffset)) {\n      report_fatal_error(\n        \"Frame size outside of the signed 32-bit range not supported\");\n    }\n    Offset = StackOffset::getFixed(FixedOffset);\n  }\n\n  const RISCVRegisterInfo &RI = *STI.getRegisterInfo();\n  // We must keep the stack pointer aligned through any intermediate\n  // updates.\n  RI.adjustReg(MBB, MBBI, DL, SPReg, SPReg, Offset,\n               Flag, getStackAlign());\n}",
      "start_line": 383,
      "end_line": 413,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "the",
        "getRealMaxVLen",
        "getRealMinVLen",
        "getScalable",
        "getFixed",
        "adjustReg",
        "getSPReg",
        "getRegisterInfo",
        "report_fatal_error"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "createDefCFAExpression",
      "return_type": "MCCFIInstruction",
      "parameters": [
        {
          "type": "const TargetRegisterInfo",
          "name": "&TRI"
        },
        {
          "type": "Register",
          "name": "Reg"
        },
        {
          "type": "uint64_t",
          "name": "FixedOffset"
        },
        {
          "type": "uint64_t",
          "name": "ScalableOffset"
        }
      ],
      "body": "{\n  assert(ScalableOffset != 0 && \"Did not need to adjust CFA for RVV\");\n  SmallString<64> Expr;\n  std::string CommentBuffer;\n  llvm::raw_string_ostream Comment(CommentBuffer);\n  // Build up the expression (Reg + FixedOffset + ScalableOffset * VLENB).\n  unsigned DwarfReg = TRI.getDwarfRegNum(Reg, true);\n  Expr.push_back((uint8_t)(dwarf::DW_OP_breg0 + DwarfReg));\n  Expr.push_back(0);\n  if (Reg == RISCV::X2)\n    Comment << \"sp\";\n  else\n    Comment << printReg(Reg, &TRI);\n\n  uint8_t buffer[16];\n  if (FixedOffset) {\n    Expr.push_back(dwarf::DW_OP_consts);\n    Expr.append(buffer, buffer + encodeSLEB128(FixedOffset, buffer));\n    Expr.push_back((uint8_t)dwarf::DW_OP_plus);\n    Comment << \" + \" << FixedOffset;\n  }\n\n  Expr.push_back((uint8_t)dwarf::DW_OP_consts);\n  Expr.append(buffer, buffer + encodeSLEB128(ScalableOffset, buffer));\n\n  unsigned DwarfVlenb = TRI.getDwarfRegNum(RISCV::VLENB, true);\n  Expr.push_back((uint8_t)dwarf::DW_OP_bregx);\n  Expr.append(buffer, buffer + encodeULEB128(DwarfVlenb, buffer));\n  Expr.push_back(0);\n\n  Expr.push_back((uint8_t)dwarf::DW_OP_mul);\n  Expr.push_back((uint8_t)dwarf::DW_OP_plus);\n\n  Comment << \" + \" << ScalableOffset << \" * vlenb\";\n\n  SmallString<64> DefCfaExpr;\n  DefCfaExpr.push_back(dwarf::DW_CFA_def_cfa_expression);\n  DefCfaExpr.append(buffer, buffer + encodeULEB128(Expr.size(), buffer));\n  DefCfaExpr.append(Expr.str());\n\n  return MCCFIInstruction::createEscape(nullptr, DefCfaExpr.str(), SMLoc(),\n                                        Comment.str());\n}",
      "start_line": 415,
      "end_line": 460,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "SMLoc",
        "Comment",
        "append",
        "str",
        "printReg",
        "push_back",
        "expression",
        "getDwarfRegNum",
        "createEscape"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "emitPrologue",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        }
      ],
      "body": "{\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n  const RISCVRegisterInfo *RI = STI.getRegisterInfo();\n  const RISCVInstrInfo *TII = STI.getInstrInfo();\n  MachineBasicBlock::iterator MBBI = MBB.begin();\n\n  Register FPReg = getFPReg(STI);\n  Register SPReg = getSPReg(STI);\n  Register BPReg = RISCVABI::getBPReg();\n\n  // Debug location must be unknown since the first debug location is used\n  // to determine the end of the prologue.\n  DebugLoc DL;\n\n  // All calls are tail calls in GHC calling conv, and functions have no\n  // prologue/epilogue.\n  if (MF.getFunction().getCallingConv() == CallingConv::GHC)\n    return;\n\n  // Emit prologue for shadow call stack.\n  emitSCSPrologue(MF, MBB, MBBI, DL);\n\n  auto FirstFrameSetup = MBBI;\n\n  // Since spillCalleeSavedRegisters may have inserted a libcall, skip past\n  // any instructions marked as FrameSetup\n  while (MBBI != MBB.end() && MBBI->getFlag(MachineInstr::FrameSetup))\n    ++MBBI;\n\n  // Determine the correct frame layout\n  determineFrameLayout(MF);\n\n  // If libcalls are used to spill and restore callee-saved registers, the frame\n  // has two sections; the opaque section managed by the libcalls, and the\n  // section managed by MachineFrameInfo which can also hold callee saved\n  // registers in fixed stack slots, both of which have negative frame indices.\n  // This gets even more complicated when incoming arguments are passed via the\n  // stack, as these too have negative frame indices. An example is detailed\n  // below:\n  //\n  //  | incoming arg | <- FI[-3]\n  //  | libcallspill |\n  //  | calleespill  | <- FI[-2]\n  //  | calleespill  | <- FI[-1]\n  //  | this_frame   | <- FI[0]\n  //\n  // For negative frame indices, the offset from the frame pointer will differ\n  // depending on which of these groups the frame index applies to.\n  // The following calculates the correct offset knowing the number of callee\n  // saved registers spilt by the two methods.\n  if (int LibCallRegs = getLibCallID(MF, MFI.getCalleeSavedInfo()) + 1) {\n    // Calculate the size of the frame managed by the libcall. The stack\n    // alignment of these libcalls should be the same as how we set it in\n    // getABIStackAlignment.\n    unsigned LibCallFrameSize =\n        alignTo((STI.getXLen() / 8) * LibCallRegs, getStackAlign());\n    RVFI->setLibCallStackSize(LibCallFrameSize);\n  }\n\n  // FIXME (note copied from Lanai): This appears to be overallocating.  Needs\n  // investigation. Get the number of bytes to allocate from the FrameInfo.\n  uint64_t StackSize = getStackSizeWithRVVPadding(MF);\n  uint64_t RealStackSize = StackSize + RVFI->getReservedSpillsSize();\n  uint64_t RVVStackSize = RVFI->getRVVStackSize();\n\n  // Early exit if there is no need to allocate on the stack\n  if (RealStackSize == 0 && !MFI.adjustsStack() && RVVStackSize == 0)\n    return;\n\n  // If the stack pointer has been marked as reserved, then produce an error if\n  // the frame requires stack allocation\n  if (STI.isRegisterReservedByUser(SPReg))\n    MF.getFunction().getContext().diagnose(DiagnosticInfoUnsupported{\n        MF.getFunction(), \"Stack pointer required, but has been reserved.\"});\n\n  uint64_t FirstSPAdjustAmount = getFirstSPAdjustAmount(MF);\n  // Split the SP adjustment to reduce the offsets of callee saved spill.\n  if (FirstSPAdjustAmount) {\n    StackSize = FirstSPAdjustAmount;\n    RealStackSize = FirstSPAdjustAmount;\n  }\n\n  if (RVFI->isPushable(MF) && FirstFrameSetup != MBB.end() &&\n      FirstFrameSetup->getOpcode() == RISCV::CM_PUSH) {\n    // Use available stack adjustment in push instruction to allocate additional\n    // stack space.\n    uint64_t Spimm = std::min(StackSize, (uint64_t)48);\n    FirstFrameSetup->getOperand(1).setImm(Spimm);\n    StackSize -= Spimm;\n  }\n\n  if (StackSize != 0) {\n    // Allocate space on the stack if necessary.\n    RI->adjustReg(MBB, MBBI, DL, SPReg, SPReg,\n                  StackOffset::getFixed(-StackSize), MachineInstr::FrameSetup,\n                  getStackAlign());\n  }\n\n  // Emit \".cfi_def_cfa_offset RealStackSize\"\n  unsigned CFIIndex = MF.addFrameInst(\n      MCCFIInstruction::cfiDefCfaOffset(nullptr, RealStackSize));\n  BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::CFI_INSTRUCTION))\n      .addCFIIndex(CFIIndex)\n      .setMIFlag(MachineInstr::FrameSetup);\n\n  const auto &CSI = MFI.getCalleeSavedInfo();\n\n  // The frame pointer is callee-saved, and code has been generated for us to\n  // save it to the stack. We need to skip over the storing of callee-saved\n  // registers as the frame pointer must be modified after it has been saved\n  // to the stack, not before.\n  // FIXME: assumes exactly one instruction is used to save each callee-saved\n  // register.\n  std::advance(MBBI, getUnmanagedCSI(MF, CSI).size());\n\n  // Iterate over list of callee-saved registers and emit .cfi_offset\n  // directives.\n  for (const auto &Entry : CSI) {\n    int FrameIdx = Entry.getFrameIdx();\n    int64_t Offset;\n    // Offsets for objects with fixed locations (IE: those saved by libcall) are\n    // simply calculated from the frame index.\n    if (FrameIdx < 0) {\n      if (RVFI->isPushable(MF)) {\n        // Callee-saved register stored by Zcmp push is in reverse order.\n        Offset = -(FrameIdx + RVFI->getRVPushRegs() + 1) *\n                 (int64_t)STI.getXLen() / 8;\n      } else {\n        Offset = FrameIdx * (int64_t)STI.getXLen() / 8;\n      }\n    } else {\n      Offset = MFI.getObjectOffset(FrameIdx) - RVFI->getReservedSpillsSize();\n    }\n    Register Reg = Entry.getReg();\n    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createOffset(\n        nullptr, RI->getDwarfRegNum(Reg, true), Offset));\n    BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::CFI_INSTRUCTION))\n        .addCFIIndex(CFIIndex)\n        .setMIFlag(MachineInstr::FrameSetup);\n  }\n\n  // Generate new FP.\n  if (hasFP(MF)) {\n    if (STI.isRegisterReservedByUser(FPReg))\n      MF.getFunction().getContext().diagnose(DiagnosticInfoUnsupported{\n          MF.getFunction(), \"Frame pointer required, but has been reserved.\"});\n    // The frame pointer does need to be reserved from register allocation.\n    assert(MF.getRegInfo().isReserved(FPReg) && \"FP not reserved\");\n\n    RI->adjustReg(MBB, MBBI, DL, FPReg, SPReg,\n                  StackOffset::getFixed(RealStackSize - RVFI->getVarArgsSaveSize()),\n                  MachineInstr::FrameSetup, getStackAlign());\n\n    // Emit \".cfi_def_cfa $fp, RVFI->getVarArgsSaveSize()\"\n    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::cfiDefCfa(\n        nullptr, RI->getDwarfRegNum(FPReg, true), RVFI->getVarArgsSaveSize()));\n    BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::CFI_INSTRUCTION))\n        .addCFIIndex(CFIIndex)\n        .setMIFlag(MachineInstr::FrameSetup);\n  }\n\n  // Emit the second SP adjustment after saving callee saved registers.\n  if (FirstSPAdjustAmount) {\n    uint64_t SecondSPAdjustAmount =\n        getStackSizeWithRVVPadding(MF) - FirstSPAdjustAmount;\n    assert(SecondSPAdjustAmount > 0 &&\n           \"SecondSPAdjustAmount should be greater than zero\");\n    RI->adjustReg(MBB, MBBI, DL, SPReg, SPReg,\n                  StackOffset::getFixed(-SecondSPAdjustAmount),\n                  MachineInstr::FrameSetup, getStackAlign());\n\n    // If we are using a frame-pointer, and thus emitted \".cfi_def_cfa fp, 0\",\n    // don't emit an sp-based .cfi_def_cfa_offset\n    if (!hasFP(MF)) {\n      // Emit \".cfi_def_cfa_offset StackSize\"\n      unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::cfiDefCfaOffset(\n          nullptr, getStackSizeWithRVVPadding(MF)));\n      BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::CFI_INSTRUCTION))\n          .addCFIIndex(CFIIndex)\n          .setMIFlag(MachineInstr::FrameSetup);\n    }\n  }\n\n  if (RVVStackSize) {\n    adjustStackForRVV(MF, MBB, MBBI, DL, -RVVStackSize,\n                      MachineInstr::FrameSetup);\n    if (!hasFP(MF)) {\n      // Emit .cfi_def_cfa_expression \"sp + StackSize + RVVStackSize * vlenb\".\n      unsigned CFIIndex = MF.addFrameInst(createDefCFAExpression(\n          *RI, SPReg, getStackSizeWithRVVPadding(MF), RVVStackSize / 8));\n      BuildMI(MBB, MBBI, DL, TII->get(TargetOpcode::CFI_INSTRUCTION))\n          .addCFIIndex(CFIIndex)\n          .setMIFlag(MachineInstr::FrameSetup);\n    }\n  }\n\n  if (hasFP(MF)) {\n    // Realign Stack\n    const RISCVRegisterInfo *RI = STI.getRegisterInfo();\n    if (RI->hasStackRealignment(MF)) {\n      Align MaxAlignment = MFI.getMaxAlign();\n\n      const RISCVInstrInfo *TII = STI.getInstrInfo();\n      if (isInt<12>(-(int)MaxAlignment.value())) {\n        BuildMI(MBB, MBBI, DL, TII->get(RISCV::ANDI), SPReg)\n            .addReg(SPReg)\n            .addImm(-(int)MaxAlignment.value())\n            .setMIFlag(MachineInstr::FrameSetup);\n      } else {\n        unsigned ShiftAmount = Log2(MaxAlignment);\n        Register VR =\n            MF.getRegInfo().createVirtualRegister(&RISCV::GPRRegClass);\n        BuildMI(MBB, MBBI, DL, TII->get(RISCV::SRLI), VR)\n            .addReg(SPReg)\n            .addImm(ShiftAmount)\n            .setMIFlag(MachineInstr::FrameSetup);\n        BuildMI(MBB, MBBI, DL, TII->get(RISCV::SLLI), SPReg)\n            .addReg(VR)\n            .addImm(ShiftAmount)\n            .setMIFlag(MachineInstr::FrameSetup);\n      }\n      // FP will be used to restore the frame in the epilogue, so we need\n      // another base register BP to record SP after re-alignment. SP will\n      // track the current stack after allocating variable sized objects.\n      if (hasBP(MF)) {\n        // move BP, SP\n        BuildMI(MBB, MBBI, DL, TII->get(RISCV::ADDI), BPReg)\n            .addReg(SPReg)\n            .addImm(0)\n            .setMIFlag(MachineInstr::FrameSetup);\n      }\n    }\n  }\n}",
      "start_line": 462,
      "end_line": 697,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getContext",
        "addFrameInst",
        "getFirstSPAdjustAmount",
        "FIXME",
        "getMaxAlign",
        "adjustReg",
        "adjustStackForRVV",
        "locations",
        "emitSCSPrologue",
        "setMIFlag",
        "alignTo",
        "getVarArgsSaveSize",
        "getReservedSpillsSize",
        "setLibCallStackSize",
        "BuildMI",
        "createVirtualRegister",
        "diagnose",
        "min",
        "getFrameInfo",
        "end",
        "isReserved",
        "begin",
        "addImm",
        "size",
        "getStackAlign",
        "getObjectOffset",
        "determineFrameLayout",
        "value",
        "getRVVStackSize",
        "getOpcode",
        "addReg",
        "getBPReg",
        "getOperand",
        "getRegisterInfo",
        "getXLen",
        "getRVPushRegs",
        "getFlag",
        "getInstrInfo",
        "getCalleeSavedInfo",
        "setImm",
        "addCFIIndex",
        "getRegInfo",
        "getFPReg",
        "getCallingConv",
        "advance",
        "getSPReg",
        "Log2",
        "getStackSizeWithRVVPadding",
        "getReg",
        "getFrameIdx",
        "getFunction"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "emitEpilogue",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        }
      ],
      "body": "{\n  const RISCVRegisterInfo *RI = STI.getRegisterInfo();\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n  Register FPReg = getFPReg(STI);\n  Register SPReg = getSPReg(STI);\n\n  // All calls are tail calls in GHC calling conv, and functions have no\n  // prologue/epilogue.\n  if (MF.getFunction().getCallingConv() == CallingConv::GHC)\n    return;\n\n  // Get the insert location for the epilogue. If there were no terminators in\n  // the block, get the last instruction.\n  MachineBasicBlock::iterator MBBI = MBB.end();\n  DebugLoc DL;\n  if (!MBB.empty()) {\n    MBBI = MBB.getLastNonDebugInstr();\n    if (MBBI != MBB.end())\n      DL = MBBI->getDebugLoc();\n\n    MBBI = MBB.getFirstTerminator();\n\n    // If callee-saved registers are saved via libcall, place stack adjustment\n    // before this call.\n    while (MBBI != MBB.begin() &&\n           std::prev(MBBI)->getFlag(MachineInstr::FrameDestroy))\n      --MBBI;\n  }\n\n  const auto &CSI = getUnmanagedCSI(MF, MFI.getCalleeSavedInfo());\n\n  // Skip to before the restores of callee-saved registers\n  // FIXME: assumes exactly one instruction is used to restore each\n  // callee-saved register.\n  auto LastFrameDestroy = MBBI;\n  if (!CSI.empty())\n    LastFrameDestroy = std::prev(MBBI, CSI.size());\n\n  uint64_t StackSize = getStackSizeWithRVVPadding(MF);\n  uint64_t RealStackSize = StackSize + RVFI->getReservedSpillsSize();\n  uint64_t FPOffset = RealStackSize - RVFI->getVarArgsSaveSize();\n  uint64_t RVVStackSize = RVFI->getRVVStackSize();\n\n  // Restore the stack pointer using the value of the frame pointer. Only\n  // necessary if the stack pointer was modified, meaning the stack size is\n  // unknown.\n  //\n  // In order to make sure the stack point is right through the EH region,\n  // we also need to restore stack pointer from the frame pointer if we\n  // don't preserve stack space within prologue/epilogue for outgoing variables,\n  // normally it's just checking the variable sized object is present or not\n  // is enough, but we also don't preserve that at prologue/epilogue when\n  // have vector objects in stack.\n  if (RI->hasStackRealignment(MF) || MFI.hasVarSizedObjects() ||\n      !hasReservedCallFrame(MF)) {\n    assert(hasFP(MF) && \"frame pointer should not have been eliminated\");\n    RI->adjustReg(MBB, LastFrameDestroy, DL, SPReg, FPReg,\n                  StackOffset::getFixed(-FPOffset),\n                  MachineInstr::FrameDestroy, getStackAlign());\n  } else {\n    if (RVVStackSize)\n      adjustStackForRVV(MF, MBB, LastFrameDestroy, DL, RVVStackSize,\n                        MachineInstr::FrameDestroy);\n  }\n\n  uint64_t FirstSPAdjustAmount = getFirstSPAdjustAmount(MF);\n  if (FirstSPAdjustAmount) {\n    uint64_t SecondSPAdjustAmount =\n        getStackSizeWithRVVPadding(MF) - FirstSPAdjustAmount;\n    assert(SecondSPAdjustAmount > 0 &&\n           \"SecondSPAdjustAmount should be greater than zero\");\n\n    RI->adjustReg(MBB, LastFrameDestroy, DL, SPReg, SPReg,\n                  StackOffset::getFixed(SecondSPAdjustAmount),\n                  MachineInstr::FrameDestroy, getStackAlign());\n  }\n\n  if (FirstSPAdjustAmount)\n    StackSize = FirstSPAdjustAmount;\n\n  if (RVFI->isPushable(MF) && MBBI != MBB.end() &&\n      MBBI->getOpcode() == RISCV::CM_POP) {\n    // Use available stack adjustment in pop instruction to deallocate stack\n    // space.\n    uint64_t Spimm = std::min(StackSize, (uint64_t)48);\n    MBBI->getOperand(1).setImm(Spimm);\n    StackSize -= Spimm;\n  }\n\n  // Deallocate stack\n  if (StackSize != 0) {\n    RI->adjustReg(MBB, MBBI, DL, SPReg, SPReg, StackOffset::getFixed(StackSize),\n                  MachineInstr::FrameDestroy, getStackAlign());\n  }\n\n  // Emit epilogue for shadow call stack.\n  emitSCSEpilogue(MF, MBB, MBBI, DL);\n}",
      "start_line": 699,
      "end_line": 798,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getFirstSPAdjustAmount",
        "adjustReg",
        "adjustStackForRVV",
        "getVarArgsSaveSize",
        "getReservedSpillsSize",
        "min",
        "getFrameInfo",
        "end",
        "prev",
        "hasVarSizedObjects",
        "getFirstTerminator",
        "emitSCSEpilogue",
        "getStackAlign",
        "getRVVStackSize",
        "getOpcode",
        "getDebugLoc",
        "getOperand",
        "getRegisterInfo",
        "getFlag",
        "getUnmanagedCSI",
        "setImm",
        "hasReservedCallFrame",
        "getFPReg",
        "getCallingConv",
        "getSPReg",
        "getStackSizeWithRVVPadding",
        "getLastNonDebugInstr"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "stack",
      "parameters": [
        {
          "type": "StackSize !=",
          "name": "0"
        }
      ],
      "body": "{\n    RI->adjustReg(MBB, MBBI, DL, SPReg, SPReg, StackOffset::getFixed(StackSize),\n                  MachineInstr::FrameDestroy, getStackAlign());\n  }",
      "start_line": 790,
      "end_line": 794,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "adjustReg",
        "getStackAlign"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getFrameIndexReference",
      "return_type": "StackOffset",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "int",
          "name": "FI"
        },
        {
          "type": "Register",
          "name": "&FrameReg"
        }
      ],
      "body": "{\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  const TargetRegisterInfo *RI = MF.getSubtarget().getRegisterInfo();\n  const auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n\n  // Callee-saved registers should be referenced relative to the stack\n  // pointer (positive offset), otherwise use the frame pointer (negative\n  // offset).\n  const auto &CSI = getUnmanagedCSI(MF, MFI.getCalleeSavedInfo());\n  int MinCSFI = 0;\n  int MaxCSFI = -1;\n  StackOffset Offset;\n  auto StackID = MFI.getStackID(FI);\n\n  assert((StackID == TargetStackID::Default ||\n          StackID == TargetStackID::ScalableVector) &&\n         \"Unexpected stack ID for the frame object.\");\n  if (StackID == TargetStackID::Default) {\n    Offset =\n        StackOffset::getFixed(MFI.getObjectOffset(FI) - getOffsetOfLocalArea() +\n                              MFI.getOffsetAdjustment());\n  } else if (StackID == TargetStackID::ScalableVector) {\n    Offset = StackOffset::getScalable(MFI.getObjectOffset(FI));\n  }\n\n  uint64_t FirstSPAdjustAmount = getFirstSPAdjustAmount(MF);\n\n  if (CSI.size()) {\n    MinCSFI = CSI[0].getFrameIdx();\n    MaxCSFI = CSI[CSI.size() - 1].getFrameIdx();\n  }\n\n  if (FI >= MinCSFI && FI <= MaxCSFI) {\n    FrameReg = RISCV::X2;\n\n    if (FirstSPAdjustAmount)\n      Offset += StackOffset::getFixed(FirstSPAdjustAmount);\n    else\n      Offset += StackOffset::getFixed(getStackSizeWithRVVPadding(MF));\n    return Offset;\n  }\n\n  if (RI->hasStackRealignment(MF) && !MFI.isFixedObjectIndex(FI)) {\n    // If the stack was realigned, the frame pointer is set in order to allow\n    // SP to be restored, so we need another base register to record the stack\n    // after realignment.\n    // |--------------------------| -- <-- FP\n    // | callee-allocated save    | | <----|\n    // | area for register varargs| |      |\n    // |--------------------------| |      |\n    // | callee-saved registers   | |      |\n    // |--------------------------| --     |\n    // | realignment (the size of | |      |\n    // | this area is not counted | |      |\n    // | in MFI.getStackSize())   | |      |\n    // |--------------------------| --     |-- MFI.getStackSize()\n    // | RVV alignment padding    | |      |\n    // | (not counted in          | |      |\n    // | MFI.getStackSize() but   | |      |\n    // | counted in               | |      |\n    // | RVFI.getRVVStackSize())  | |      |\n    // |--------------------------| --     |\n    // | RVV objects              | |      |\n    // | (not counted in          | |      |\n    // | MFI.getStackSize())      | |      |\n    // |--------------------------| --     |\n    // | padding before RVV       | |      |\n    // | (not counted in          | |      |\n    // | MFI.getStackSize() or in | |      |\n    // | RVFI.getRVVStackSize())  | |      |\n    // |--------------------------| --     |\n    // | scalar local variables   | | <----'\n    // |--------------------------| -- <-- BP (if var sized objects present)\n    // | VarSize objects          | |\n    // |--------------------------| -- <-- SP\n    if (hasBP(MF)) {\n      FrameReg = RISCVABI::getBPReg();\n    } else {\n      // VarSize objects must be empty in this case!\n      assert(!MFI.hasVarSizedObjects());\n      FrameReg = RISCV::X2;\n    }\n  } else {\n    FrameReg = RI->getFrameRegister(MF);\n  }\n\n  if (FrameReg == getFPReg(STI)) {\n    Offset += StackOffset::getFixed(RVFI->getVarArgsSaveSize());\n    if (FI >= 0)\n      Offset -= StackOffset::getFixed(RVFI->getReservedSpillsSize());\n    // When using FP to access scalable vector objects, we need to minus\n    // the frame size.\n    //\n    // |--------------------------| -- <-- FP\n    // | callee-allocated save    | |\n    // | area for register varargs| |\n    // |--------------------------| |\n    // | callee-saved registers   | |\n    // |--------------------------| | MFI.getStackSize()\n    // | scalar local variables   | |\n    // |--------------------------| -- (Offset of RVV objects is from here.)\n    // | RVV objects              |\n    // |--------------------------|\n    // | VarSize objects          |\n    // |--------------------------| <-- SP\n    if (MFI.getStackID(FI) == TargetStackID::ScalableVector) {\n      assert(!RI->hasStackRealignment(MF) &&\n             \"Can't index across variable sized realign\");\n      // We don't expect any extra RVV alignment padding, as the stack size\n      // and RVV object sections should be correct aligned in their own\n      // right.\n      assert(MFI.getStackSize() == getStackSizeWithRVVPadding(MF) &&\n             \"Inconsistent stack layout\");\n      Offset -= StackOffset::getFixed(MFI.getStackSize());\n    }\n    return Offset;\n  }\n\n  // This case handles indexing off both SP and BP.\n  // If indexing off SP, there must not be any var sized objects\n  assert(FrameReg == RISCVABI::getBPReg() || !MFI.hasVarSizedObjects());\n\n  // When using SP to access frame objects, we need to add RVV stack size.\n  //\n  // |--------------------------| -- <-- FP\n  // | callee-allocated save    | | <----|\n  // | area for register varargs| |      |\n  // |--------------------------| |      |\n  // | callee-saved registers   | |      |\n  // |--------------------------| --     |\n  // | RVV alignment padding    | |      |\n  // | (not counted in          | |      |\n  // | MFI.getStackSize() but   | |      |\n  // | counted in               | |      |\n  // | RVFI.getRVVStackSize())  | |      |\n  // |--------------------------| --     |\n  // | RVV objects              | |      |-- MFI.getStackSize()\n  // | (not counted in          | |      |\n  // | MFI.getStackSize())      | |      |\n  // |--------------------------| --     |\n  // | padding before RVV       | |      |\n  // | (not counted in          | |      |\n  // | MFI.getStackSize())      | |      |\n  // |--------------------------| --     |\n  // | scalar local variables   | | <----'\n  // |--------------------------| -- <-- BP (if var sized objects present)\n  // | VarSize objects          | |\n  // |--------------------------| -- <-- SP\n  //\n  // The total amount of padding surrounding RVV objects is described by\n  // RVV->getRVVPadding() and it can be zero. It allows us to align the RVV\n  // objects to the required alignment.\n  if (MFI.getStackID(FI) == TargetStackID::Default) {\n    if (MFI.isFixedObjectIndex(FI)) {\n      assert(!RI->hasStackRealignment(MF) &&\n             \"Can't index across variable sized realign\");\n      Offset += StackOffset::get(getStackSizeWithRVVPadding(MF) +\n                                     RVFI->getReservedSpillsSize(),\n                                 RVFI->getRVVStackSize());\n    } else {\n      Offset += StackOffset::getFixed(MFI.getStackSize());\n    }\n  } else if (MFI.getStackID(FI) == TargetStackID::ScalableVector) {\n    // Ensure the base of the RVV stack is correctly aligned: add on the\n    // alignment padding.\n    int ScalarLocalVarSize = MFI.getStackSize() -\n                             RVFI->getCalleeSavedStackSize() -\n                             RVFI->getRVPushStackSize() -\n                             RVFI->getVarArgsSaveSize() + RVFI->getRVVPadding();\n    Offset += StackOffset::get(ScalarLocalVarSize, RVFI->getRVVStackSize());\n  }\n  return Offset;\n}",
      "start_line": 800,
      "end_line": 974,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getFirstSPAdjustAmount",
        "getRVVPadding",
        "getCalleeSavedStackSize",
        "getVarArgsSaveSize",
        "pointer",
        "getReservedSpillsSize",
        "BP",
        "getFixed",
        "getFrameInfo",
        "hasVarSizedObjects",
        "getFrameRegister",
        "size",
        "getRVVStackSize",
        "getSubtarget",
        "getScalable",
        "getStackSize",
        "getBPReg",
        "isFixedObjectIndex",
        "getRegisterInfo",
        "getRVPushStackSize",
        "getUnmanagedCSI",
        "getStackID",
        "get",
        "realignment",
        "getStackSizeWithRVVPadding",
        "getOffsetAdjustment",
        "getOffsetOfLocalArea",
        "getFrameIdx"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "StackID ==",
          "name": "TargetStackID::ScalableVector"
        }
      ],
      "body": "{\n    Offset = StackOffset::getScalable(MFI.getObjectOffset(FI));\n  }",
      "start_line": 823,
      "end_line": 825,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "getScalable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "determineCalleeSaves",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "BitVector",
          "name": "&SavedRegs"
        },
        {
          "type": "RegScavenger",
          "name": "*RS"
        }
      ],
      "body": "{\n  TargetFrameLowering::determineCalleeSaves(MF, SavedRegs, RS);\n  // Unconditionally spill RA and FP only if the function uses a frame\n  // pointer.\n  if (hasFP(MF)) {\n    SavedRegs.set(RISCV::X1);\n    SavedRegs.set(RISCV::X8);\n  }\n  // Mark BP as used if function has dedicated base pointer.\n  if (hasBP(MF))\n    SavedRegs.set(RISCVABI::getBPReg());\n\n  // If interrupt is enabled and there are calls in the handler,\n  // unconditionally save all Caller-saved registers and\n  // all FP registers, regardless whether they are used.\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  auto &Subtarget = MF.getSubtarget<RISCVSubtarget>();\n\n  if (MF.getFunction().hasFnAttribute(\"interrupt\") && MFI.hasCalls()) {\n\n    static const MCPhysReg CSRegs[] = { RISCV::X1,      /* ra */\n      RISCV::X5, RISCV::X6, RISCV::X7,                  /* t0-t2 */\n      RISCV::X10, RISCV::X11,                           /* a0-a1, a2-a7 */\n      RISCV::X12, RISCV::X13, RISCV::X14, RISCV::X15, RISCV::X16, RISCV::X17,\n      RISCV::X28, RISCV::X29, RISCV::X30, RISCV::X31 /* t3-t6 */\n    };\n\n    for (auto Reg : CSRegs)\n      // Only save x0-x15 for RVE.\n      if (Reg < RISCV::X16 || !Subtarget.isRVE())\n        SavedRegs.set(Reg);\n\n    // According to psABI, if ilp32e/lp64e ABIs are used with an ISA that\n    // has any of the registers x16-x31 and f0-f31, then these registers are\n    // considered temporaries, so we should also save x16-x31 here.\n    if (STI.getTargetABI() == RISCVABI::ABI_ILP32E ||\n        STI.getTargetABI() == RISCVABI::ABI_LP64E) {\n      for (MCPhysReg Reg = RISCV::X16; Reg <= RISCV::X31; Reg++)\n        SavedRegs.set(Reg);\n    }\n\n    if (Subtarget.hasStdExtF()) {\n\n      // If interrupt is enabled, this list contains all FP registers.\n      const MCPhysReg * Regs = MF.getRegInfo().getCalleeSavedRegs();\n\n      for (unsigned i = 0; Regs[i]; ++i)\n        if (RISCV::FPR16RegClass.contains(Regs[i]) ||\n            RISCV::FPR32RegClass.contains(Regs[i]) ||\n            RISCV::FPR64RegClass.contains(Regs[i]))\n          SavedRegs.set(Regs[i]);\n    }\n  }\n}",
      "start_line": 976,
      "end_line": 1031,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getRegInfo",
        "contains",
        "getFrameInfo",
        "set",
        "hasCalls",
        "getTargetABI",
        "getCalleeSavedRegs",
        "hasFnAttribute",
        "determineCalleeSaves"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getScavSlotsNumForRVV",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  // For RVV spill, scalable stack offsets computing requires up to two scratch\n  // registers\n  static constexpr unsigned ScavSlotsNumRVVSpillScalableObject = 2;\n\n  // For RVV spill, non-scalable stack offsets computing requires up to one\n  // scratch register.\n  static constexpr unsigned ScavSlotsNumRVVSpillNonScalableObject = 1;\n\n  // ADDI instruction's destination register can be used for computing\n  // offsets. So Scalable stack offsets require up to one scratch register.\n  static constexpr unsigned ScavSlotsADDIScalableObject = 1;\n\n  static constexpr unsigned MaxScavSlotsNumKnown =\n      std::max({ScavSlotsADDIScalableObject, ScavSlotsNumRVVSpillScalableObject,\n                ScavSlotsNumRVVSpillNonScalableObject});\n\n  unsigned MaxScavSlotsNum = 0;\n  if (!MF.getSubtarget<RISCVSubtarget>().hasVInstructions())\n    return false;\n  for (const MachineBasicBlock &MBB : MF)\n    for (const MachineInstr &MI : MBB) {\n      bool IsRVVSpill = RISCV::isRVVSpill(MI);\n      for (auto &MO : MI.operands()) {\n        if (!MO.isFI())\n          continue;\n        bool IsScalableVectorID = MF.getFrameInfo().getStackID(MO.getIndex()) ==\n                                  TargetStackID::ScalableVector;\n        if (IsRVVSpill) {\n          MaxScavSlotsNum = std::max(\n              MaxScavSlotsNum, IsScalableVectorID\n                                   ? ScavSlotsNumRVVSpillScalableObject\n                                   : ScavSlotsNumRVVSpillNonScalableObject);\n        } else if (MI.getOpcode() == RISCV::ADDI && IsScalableVectorID) {\n          MaxScavSlotsNum =\n              std::max(MaxScavSlotsNum, ScavSlotsADDIScalableObject);\n        }\n      }\n      if (MaxScavSlotsNum == MaxScavSlotsNumKnown)\n        return MaxScavSlotsNumKnown;\n    }\n  return MaxScavSlotsNum;\n}",
      "start_line": 1087,
      "end_line": 1129,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "hasVInstructions",
        "getFrameInfo",
        "isRVVSpill",
        "max",
        "getStackID"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasRVVFrameObject",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  // Originally, the function will scan all the stack objects to check whether\n  // if there is any scalable vector object on the stack or not. However, it\n  // causes errors in the register allocator. In issue 53016, it returns false\n  // before RA because there is no RVV stack objects. After RA, it returns true\n  // because there are spilling slots for RVV values during RA. It will not\n  // reserve BP during register allocation and generate BP access in the PEI\n  // pass due to the inconsistent behavior of the function.\n  //\n  // The function is changed to use hasVInstructions() as the return value. It\n  // is not precise, but it can make the register allocation correct.\n  //\n  // FIXME: Find a better way to make the decision or revisit the solution in\n  // D103622.\n  //\n  // Refer to https://github.com/llvm/llvm-project/issues/53016.\n  return MF.getSubtarget<RISCVSubtarget>().hasVInstructions();\n}",
      "start_line": 1131,
      "end_line": 1148,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasVInstructions"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "estimateFunctionSizeInBytes",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "const RISCVInstrInfo",
          "name": "&TII"
        }
      ],
      "body": "{\n  unsigned FnSize = 0;\n  for (auto &MBB : MF) {\n    for (auto &MI : MBB) {\n      // Far branches over 20-bit offset will be relaxed in branch relaxation\n      // pass. In the worst case, conditional branches will be relaxed into\n      // the following instruction sequence. Unconditional branches are\n      // relaxed in the same way, with the exception that there is no first\n      // branch instruction.\n      //\n      //        foo\n      //        bne     t5, t6, .rev_cond # `TII->getInstSizeInBytes(MI)` bytes\n      //        sd      s11, 0(sp)        # 4 bytes, or 2 bytes in RVC\n      //        jump    .restore, s11     # 8 bytes\n      // .rev_cond\n      //        bar\n      //        j       .dest_bb          # 4 bytes, or 2 bytes in RVC\n      // .restore:\n      //        ld      s11, 0(sp)        # 4 bytes, or 2 bytes in RVC\n      // .dest:\n      //        baz\n      if (MI.isConditionalBranch())\n        FnSize += TII.getInstSizeInBytes(MI);\n      if (MI.isConditionalBranch() || MI.isUnconditionalBranch()) {\n        if (MF.getSubtarget<RISCVSubtarget>().hasStdExtC())\n          FnSize += 2 + 8 + 2 + 2;\n        else\n          FnSize += 4 + 8 + 4 + 4;\n        continue;\n      }\n\n      FnSize += TII.getInstSizeInBytes(MI);\n    }\n  }\n  return FnSize;\n}",
      "start_line": 1150,
      "end_line": 1186,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getInstSizeInBytes",
        "hasStdExtC",
        "isUnconditionalBranch",
        "0"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "processFunctionBeforeFrameFinalized",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "RegScavenger",
          "name": "*RS"
        }
      ],
      "body": "{\n  const RISCVRegisterInfo *RegInfo =\n      MF.getSubtarget<RISCVSubtarget>().getRegisterInfo();\n  const RISCVInstrInfo *TII = MF.getSubtarget<RISCVSubtarget>().getInstrInfo();\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  const TargetRegisterClass *RC = &RISCV::GPRRegClass;\n  auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n\n  int64_t RVVStackSize;\n  Align RVVStackAlign;\n  std::tie(RVVStackSize, RVVStackAlign) = assignRVVStackObjectOffsets(MF);\n\n  RVFI->setRVVStackSize(RVVStackSize);\n  RVFI->setRVVStackAlign(RVVStackAlign);\n\n  if (hasRVVFrameObject(MF)) {\n    // Ensure the entire stack is aligned to at least the RVV requirement: some\n    // scalable-vector object alignments are not considered by the\n    // target-independent code.\n    MFI.ensureMaxAlignment(RVVStackAlign);\n  }\n\n  unsigned ScavSlotsNum = 0;\n\n  // estimateStackSize has been observed to under-estimate the final stack\n  // size, so give ourselves wiggle-room by checking for stack size\n  // representable an 11-bit signed field rather than 12-bits.\n  if (!isInt<11>(MFI.estimateStackSize(MF)))\n    ScavSlotsNum = 1;\n\n  // Far branches over 20-bit offset require a spill slot for scratch register.\n  bool IsLargeFunction = !isInt<20>(estimateFunctionSizeInBytes(MF, *TII));\n  if (IsLargeFunction)\n    ScavSlotsNum = std::max(ScavSlotsNum, 1u);\n\n  // RVV loads & stores have no capacity to hold the immediate address offsets\n  // so we must always reserve an emergency spill slot if the MachineFunction\n  // contains any RVV spills.\n  ScavSlotsNum = std::max(ScavSlotsNum, getScavSlotsNumForRVV(MF));\n\n  for (unsigned I = 0; I < ScavSlotsNum; I++) {\n    int FI = MFI.CreateStackObject(RegInfo->getSpillSize(*RC),\n                                   RegInfo->getSpillAlign(*RC), false);\n    RS->addScavengingFrameIndex(FI);\n\n    if (IsLargeFunction && RVFI->getBranchRelaxationScratchFrameIndex() == -1)\n      RVFI->setBranchRelaxationScratchFrameIndex(FI);\n  }\n\n  if (MFI.getCalleeSavedInfo().empty() || RVFI->useSaveRestoreLibCalls(MF) ||\n      RVFI->isPushable(MF)) {\n    RVFI->setCalleeSavedStackSize(0);\n    return;\n  }\n\n  unsigned Size = 0;\n  for (const auto &Info : MFI.getCalleeSavedInfo()) {\n    int FrameIdx = Info.getFrameIdx();\n    if (MFI.getStackID(FrameIdx) != TargetStackID::Default)\n      continue;\n\n    Size += MFI.getObjectSize(FrameIdx);\n  }\n  RVFI->setCalleeSavedStackSize(Size);\n}",
      "start_line": 1188,
      "end_line": 1253,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isPushable",
        "addScavengingFrameIndex",
        "assignRVVStackObjectOffsets",
        "ensureMaxAlignment",
        "setRVVStackAlign",
        "useSaveRestoreLibCalls",
        "CreateStackObject",
        "getFrameInfo",
        "getSpillAlign",
        "max",
        "estimateFunctionSizeInBytes",
        "getObjectSize",
        "setCalleeSavedStackSize",
        "getRegisterInfo",
        "tie",
        "getInstrInfo",
        "empty",
        "getFrameIdx",
        "setRVVStackSize",
        "setBranchRelaxationScratchFrameIndex"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasReservedCallFrame",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  return !MF.getFrameInfo().hasVarSizedObjects() &&\n         !(hasFP(MF) && hasRVVFrameObject(MF));\n}",
      "start_line": 1259,
      "end_line": 1262,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasFP",
        "hasRVVFrameObject",
        "getFrameInfo",
        "hasVarSizedObjects"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "eliminateCallFramePseudoInstr",
      "return_type": "iterator",
      "parameters": [
        {
          "type": "MachineFunction",
          "name": "&MF"
        },
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::iterator",
          "name": "MI"
        }
      ],
      "body": "{\n  Register SPReg = RISCV::X2;\n  DebugLoc DL = MI->getDebugLoc();\n\n  if (!hasReservedCallFrame(MF)) {\n    // If space has not been reserved for a call frame, ADJCALLSTACKDOWN and\n    // ADJCALLSTACKUP must be converted to instructions manipulating the stack\n    // pointer. This is necessary when there is a variable length stack\n    // allocation (e.g. alloca), which means it's not possible to allocate\n    // space for outgoing arguments from within the function prologue.\n    int64_t Amount = MI->getOperand(0).getImm();\n\n    if (Amount != 0) {\n      // Ensure the stack remains aligned after adjustment.\n      Amount = alignSPAdjust(Amount);\n\n      if (MI->getOpcode() == RISCV::ADJCALLSTACKDOWN)\n        Amount = -Amount;\n\n      const RISCVRegisterInfo &RI = *STI.getRegisterInfo();\n      RI.adjustReg(MBB, MI, DL, SPReg, SPReg, StackOffset::getFixed(Amount),\n                   MachineInstr::NoFlags, getStackAlign());\n    }\n  }\n\n  return MBB.erase(MI);\n}",
      "start_line": 1265,
      "end_line": 1293,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getImm",
        "alignSPAdjust",
        "erase",
        "adjustReg",
        "getDebugLoc",
        "getOperand",
        "getRegisterInfo",
        "allocation",
        "getStackAlign"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getFirstSPAdjustAmount",
      "return_type": "uint64_t",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  const auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  const std::vector<CalleeSavedInfo> &CSI = MFI.getCalleeSavedInfo();\n  uint64_t StackSize = getStackSizeWithRVVPadding(MF);\n\n  // Disable SplitSPAdjust if save-restore libcall is used. The callee-saved\n  // registers will be pushed by the save-restore libcalls, so we don't have to\n  // split the SP adjustment in this case.\n  if (RVFI->getReservedSpillsSize())\n    return 0;\n\n  // Return the FirstSPAdjustAmount if the StackSize can not fit in a signed\n  // 12-bit and there exists a callee-saved register needing to be pushed.\n  if (!isInt<12>(StackSize) && (CSI.size() > 0)) {\n    // FirstSPAdjustAmount is chosen at most as (2048 - StackAlign) because\n    // 2048 will cause sp = sp + 2048 in the epilogue to be split into multiple\n    // instructions. Offsets smaller than 2048 can fit in a single load/store\n    // instruction, and we have to stick with the stack alignment. 2048 has\n    // 16-byte alignment. The stack alignment for RV32 and RV64 is 16 and for\n    // RV32E it is 4. So (2048 - StackAlign) will satisfy the stack alignment.\n    const uint64_t StackAlign = getStackAlign().value();\n\n    // Amount of (2048 - StackAlign) will prevent callee saved and restored\n    // instructions be compressed, so try to adjust the amount to the largest\n    // offset that stack compression instructions accept when target supports\n    // compression instructions.\n    if (STI.hasStdExtCOrZca()) {\n      // The compression extensions may support the following instructions:\n      // riscv32: c.lwsp rd, offset[7:2] => 2^(6 + 2)\n      //          c.swsp rs2, offset[7:2] => 2^(6 + 2)\n      //          c.flwsp rd, offset[7:2] => 2^(6 + 2)\n      //          c.fswsp rs2, offset[7:2] => 2^(6 + 2)\n      // riscv64: c.ldsp rd, offset[8:3] => 2^(6 + 3)\n      //          c.sdsp rs2, offset[8:3] => 2^(6 + 3)\n      //          c.fldsp rd, offset[8:3] => 2^(6 + 3)\n      //          c.fsdsp rs2, offset[8:3] => 2^(6 + 3)\n      const uint64_t RVCompressLen = STI.getXLen() * 8;\n      // Compared with amount (2048 - StackAlign), StackSize needs to\n      // satisfy the following conditions to avoid using more instructions\n      // to adjust the sp after adjusting the amount, such as\n      // StackSize meets the condition (StackSize <= 2048 + RVCompressLen),\n      // case1: Amount is 2048 - StackAlign: use addi + addi to adjust sp.\n      // case2: Amount is RVCompressLen: use addi + addi to adjust sp.\n      auto CanCompress = [&](uint64_t CompressLen) -> bool {\n        if (StackSize <= 2047 + CompressLen ||\n            (StackSize > 2048 * 2 - StackAlign &&\n             StackSize <= 2047 * 2 + CompressLen) ||\n            StackSize > 2048 * 3 - StackAlign)\n          return true;\n\n        return false;\n      };\n      // In the epilogue, addi sp, sp, 496 is used to recover the sp and it\n      // can be compressed(C.ADDI16SP, offset can be [-512, 496]), but\n      // addi sp, sp, 512 can not be compressed. So try to use 496 first.\n      const uint64_t ADDI16SPCompressLen = 496;\n      if (STI.is64Bit() && CanCompress(ADDI16SPCompressLen))\n        return ADDI16SPCompressLen;\n      if (CanCompress(RVCompressLen))\n        return RVCompressLen;\n    }\n    return 2048 - StackAlign;\n  }\n  return 0;\n}",
      "start_line": 1306,
      "end_line": 1372,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "value",
        "So",
        "condition",
        "getStackAlign",
        "compressed",
        "CanCompress",
        "getFrameInfo",
        "of",
        "amount",
        "getStackSizeWithRVVPadding",
        "getXLen",
        "size",
        "as",
        "getCalleeSavedInfo"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "spillCalleeSavedRegisters",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::iterator",
          "name": "MI"
        },
        {
          "type": "ArrayRef<CalleeSavedInfo>",
          "name": "CSI"
        },
        {
          "type": "const TargetRegisterInfo",
          "name": "*TRI"
        }
      ],
      "body": "{\n  if (CSI.empty())\n    return true;\n\n  MachineFunction *MF = MBB.getParent();\n  const TargetInstrInfo &TII = *MF->getSubtarget().getInstrInfo();\n  DebugLoc DL;\n  if (MI != MBB.end() && !MI->isDebugInstr())\n    DL = MI->getDebugLoc();\n\n  // Emit CM.PUSH with base SPimm & evaluate Push stack\n  RISCVMachineFunctionInfo *RVFI = MF->getInfo<RISCVMachineFunctionInfo>();\n  if (RVFI->isPushable(*MF)) {\n    Register MaxReg = getMaxPushPopReg(*MF, CSI);\n    if (MaxReg != RISCV::NoRegister) {\n      auto [RegEnc, PushedRegNum] = getPushPopEncodingAndNum(MaxReg);\n      RVFI->setRVPushRegs(PushedRegNum);\n      RVFI->setRVPushStackSize(alignTo((STI.getXLen() / 8) * PushedRegNum, 16));\n\n      // Use encoded number to represent registers to spill.\n      RVFI->setRVPushRlist(RegEnc);\n      MachineInstrBuilder PushBuilder =\n          BuildMI(MBB, MI, DL, TII.get(RISCV::CM_PUSH))\n              .setMIFlag(MachineInstr::FrameSetup);\n      PushBuilder.addImm((int64_t)RegEnc);\n      PushBuilder.addImm(0);\n\n      for (unsigned i = 0; i < PushedRegNum; i++)\n        PushBuilder.addUse(AllPopRegs[i], RegState::Implicit);\n    }\n  } else if (const char *SpillLibCall = getSpillLibCallName(*MF, CSI)) {\n    // Add spill libcall via non-callee-saved register t0.\n    BuildMI(MBB, MI, DL, TII.get(RISCV::PseudoCALLReg), RISCV::X5)\n        .addExternalSymbol(SpillLibCall, RISCVII::MO_CALL)\n        .setMIFlag(MachineInstr::FrameSetup);\n\n    // Add registers spilled in libcall as liveins.\n    for (auto &CS : CSI)\n      MBB.addLiveIn(CS.getReg());\n  }\n\n  // Manually spill values not spilled by libcall & Push/Pop.\n  const auto &UnmanagedCSI = getUnmanagedCSI(*MF, CSI);\n  for (auto &CS : UnmanagedCSI) {\n    // Insert the spill to the stack frame.\n    Register Reg = CS.getReg();\n    const TargetRegisterClass *RC = TRI->getMinimalPhysRegClass(Reg);\n    TII.storeRegToStackSlot(MBB, MI, Reg, !MBB.isLiveIn(Reg), CS.getFrameIdx(),\n                            RC, TRI, Register());\n  }\n\n  return true;\n}",
      "start_line": 1374,
      "end_line": 1428,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getPushPopEncodingAndNum",
        "addUse",
        "setRVPushRegs",
        "addExternalSymbol",
        "getParent",
        "setMIFlag",
        "setRVPushRlist",
        "BuildMI",
        "getMaxPushPopReg",
        "addImm",
        "storeRegToStackSlot",
        "isDebugInstr",
        "setRVPushStackSize",
        "getSubtarget",
        "addLiveIn",
        "getDebugLoc",
        "Register",
        "getInstrInfo",
        "getUnmanagedCSI",
        "getMinimalPhysRegClass",
        "getReg",
        "getFrameIdx"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "restoreCalleeSavedRegisters",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::iterator",
          "name": "MI"
        },
        {
          "type": "MutableArrayRef<CalleeSavedInfo>",
          "name": "CSI"
        },
        {
          "type": "const TargetRegisterInfo",
          "name": "*TRI"
        }
      ],
      "body": "{\n  if (CSI.empty())\n    return true;\n\n  MachineFunction *MF = MBB.getParent();\n  const TargetInstrInfo &TII = *MF->getSubtarget().getInstrInfo();\n  DebugLoc DL;\n  if (MI != MBB.end() && !MI->isDebugInstr())\n    DL = MI->getDebugLoc();\n\n  // Manually restore values not restored by libcall & Push/Pop.\n  // Keep the same order as in the prologue. There is no need to reverse the\n  // order in the epilogue. In addition, the return address will be restored\n  // first in the epilogue. It increases the opportunity to avoid the\n  // load-to-use data hazard between loading RA and return by RA.\n  // loadRegFromStackSlot can insert multiple instructions.\n  const auto &UnmanagedCSI = getUnmanagedCSI(*MF, CSI);\n  for (auto &CS : UnmanagedCSI) {\n    Register Reg = CS.getReg();\n    const TargetRegisterClass *RC = TRI->getMinimalPhysRegClass(Reg);\n    TII.loadRegFromStackSlot(MBB, MI, Reg, CS.getFrameIdx(), RC, TRI,\n                             Register());\n    assert(MI != MBB.begin() && \"loadRegFromStackSlot didn't insert any code!\");\n  }\n\n  RISCVMachineFunctionInfo *RVFI = MF->getInfo<RISCVMachineFunctionInfo>();\n  if (RVFI->isPushable(*MF)) {\n    int RegEnc = RVFI->getRVPushRlist();\n    if (RegEnc != llvm::RISCVZC::RLISTENCODE::INVALID_RLIST) {\n      MachineInstrBuilder PopBuilder =\n          BuildMI(MBB, MI, DL, TII.get(RISCV::CM_POP))\n              .setMIFlag(MachineInstr::FrameDestroy);\n      // Use encoded number to represent registers to restore.\n      PopBuilder.addImm(RegEnc);\n      PopBuilder.addImm(0);\n\n      for (unsigned i = 0; i < RVFI->getRVPushRegs(); i++)\n        PopBuilder.addDef(AllPopRegs[i], RegState::ImplicitDefine);\n    }\n  } else {\n    const char *RestoreLibCall = getRestoreLibCallName(*MF, CSI);\n    if (RestoreLibCall) {\n      // Add restore libcall via tail call.\n      MachineBasicBlock::iterator NewMI =\n          BuildMI(MBB, MI, DL, TII.get(RISCV::PseudoTAIL))\n              .addExternalSymbol(RestoreLibCall, RISCVII::MO_CALL)\n              .setMIFlag(MachineInstr::FrameDestroy);\n\n      // Remove trailing returns, since the terminator is now a tail call to the\n      // restore function.\n      if (MI != MBB.end() && MI->getOpcode() == RISCV::PseudoRET) {\n        NewMI->copyImplicitOps(*MF, *MI);\n        MI->eraseFromParent();\n      }\n    }\n  }\n  return true;\n}",
      "start_line": 1430,
      "end_line": 1489,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "addDef",
        "addExternalSymbol",
        "getParent",
        "setMIFlag",
        "loadRegFromStackSlot",
        "BuildMI",
        "addImm",
        "getRVPushRlist",
        "isDebugInstr",
        "getRestoreLibCallName",
        "getSubtarget",
        "eraseFromParent",
        "getOpcode",
        "getDebugLoc",
        "Register",
        "copyImplicitOps",
        "getInstrInfo",
        "getUnmanagedCSI",
        "getMinimalPhysRegClass",
        "getReg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "enableShrinkWrapping",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  // Keep the conventional code flow when not optimizing.\n  if (MF.getFunction().hasOptNone())\n    return false;\n\n  return true;\n}",
      "start_line": 1491,
      "end_line": 1497,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasOptNone"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "canUseAsPrologue",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineBasicBlock",
          "name": "&MBB"
        }
      ],
      "body": "{\n  MachineBasicBlock *TmpMBB = const_cast<MachineBasicBlock *>(&MBB);\n  const MachineFunction *MF = MBB.getParent();\n  const auto *RVFI = MF->getInfo<RISCVMachineFunctionInfo>();\n\n  if (!RVFI->useSaveRestoreLibCalls(*MF))\n    return true;\n\n  // Inserting a call to a __riscv_save libcall requires the use of the register\n  // t0 (X5) to hold the return address. Therefore if this register is already\n  // used we can't insert the call.\n\n  RegScavenger RS;\n  RS.enterBasicBlock(*TmpMBB);\n  return !RS.isRegUsed(RISCV::X5);\n}",
      "start_line": 1499,
      "end_line": 1514,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getParent",
        "enterBasicBlock",
        "t0",
        "isRegUsed"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "canUseAsEpilogue",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineBasicBlock",
          "name": "&MBB"
        }
      ],
      "body": "{\n  const MachineFunction *MF = MBB.getParent();\n  MachineBasicBlock *TmpMBB = const_cast<MachineBasicBlock *>(&MBB);\n  const auto *RVFI = MF->getInfo<RISCVMachineFunctionInfo>();\n\n  if (!RVFI->useSaveRestoreLibCalls(*MF))\n    return true;\n\n  // Using the __riscv_restore libcalls to restore CSRs requires a tail call.\n  // This means if we still need to continue executing code within this function\n  // the restore cannot take place in this basic block.\n\n  if (MBB.succ_size() > 1)\n    return false;\n\n  MachineBasicBlock *SuccMBB =\n      MBB.succ_empty() ? TmpMBB->getFallThrough() : *MBB.succ_begin();\n\n  // Doing a tail call should be safe if there are no successors, because either\n  // we have a returning block or the end of the block is unreachable, so the\n  // restore will be eliminated regardless.\n  if (!SuccMBB)\n    return true;\n\n  // The successor can only contain a return, since we would effectively be\n  // replacing the successor with our own tail return at the end of our block.\n  return SuccMBB->isReturnBlock() && SuccMBB->size() == 1;\n}",
      "start_line": 1516,
      "end_line": 1543,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getParent",
        "succ_begin",
        "isReturnBlock",
        "size",
        "succ_empty",
        "getFallThrough"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isSupportedStackID",
      "return_type": "bool",
      "parameters": [
        {
          "type": "TargetStackID::Value",
          "name": "ID"
        }
      ],
      "body": "{\n  switch (ID) {\n  case TargetStackID::Default:\n  case TargetStackID::ScalableVector:\n    return true;\n  case TargetStackID::NoAlloc:\n  case TargetStackID::SGPRSpill:\n  case TargetStackID::WasmLocal:\n    return false;\n  }\n  llvm_unreachable(\"Invalid TargetStackID::Value\");\n}",
      "start_line": 1545,
      "end_line": 1556,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "isSupportedStackID",
          "condition": "ID",
          "cases": [
            {
              "label": "TargetStackID",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "TargetStackID",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "TargetStackID",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "TargetStackID",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "TargetStackID",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getStackIDForScalableVectors",
      "return_type": "Value",
      "parameters": [],
      "body": "{\n  return TargetStackID::ScalableVector;\n}",
      "start_line": 1558,
      "end_line": 1560,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVFrameLowering.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getCalleeSavedRegs",
      "return_type": "MCPhysReg *",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "*MF"
        }
      ],
      "body": "{\n  auto &Subtarget = MF->getSubtarget<RISCVSubtarget>();\n  if (MF->getFunction().getCallingConv() == CallingConv::GHC)\n    return CSR_NoRegs_SaveList;\n  if (MF->getFunction().hasFnAttribute(\"interrupt\")) {\n    if (Subtarget.hasStdExtD())\n      return CSR_XLEN_F64_Interrupt_SaveList;\n    if (Subtarget.hasStdExtF())\n      return Subtarget.isRVE() ? CSR_XLEN_F32_Interrupt_RVE_SaveList\n                               : CSR_XLEN_F32_Interrupt_SaveList;\n    return Subtarget.isRVE() ? CSR_Interrupt_RVE_SaveList\n                             : CSR_Interrupt_SaveList;\n  }\n\n  switch (Subtarget.getTargetABI()) {\n  default:\n    llvm_unreachable(\"Unrecognized ABI\");\n  case RISCVABI::ABI_ILP32E:\n  case RISCVABI::ABI_LP64E:\n    return CSR_ILP32E_LP64E_SaveList;\n  case RISCVABI::ABI_ILP32:\n  case RISCVABI::ABI_LP64:\n    return CSR_ILP32_LP64_SaveList;\n  case RISCVABI::ABI_ILP32F:\n  case RISCVABI::ABI_LP64F:\n    return CSR_ILP32F_LP64F_SaveList;\n  case RISCVABI::ABI_ILP32D:\n  case RISCVABI::ABI_LP64D:\n    return CSR_ILP32D_LP64D_SaveList;\n  }\n}",
      "start_line": 57,
      "end_line": 88,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCallingConv",
        "llvm_unreachable",
        "hasFnAttribute",
        "isRVE"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getReservedRegs",
      "return_type": "BitVector",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  const RISCVFrameLowering *TFI = getFrameLowering(MF);\n  BitVector Reserved(getNumRegs());\n  auto &Subtarget = MF.getSubtarget<RISCVSubtarget>();\n\n  // Mark any registers requested to be reserved as such\n  for (size_t Reg = 0; Reg < getNumRegs(); Reg++) {\n    if (Subtarget.isRegisterReservedByUser(Reg))\n      markSuperRegs(Reserved, Reg);\n  }\n\n  // Use markSuperRegs to ensure any register aliases are also reserved\n  markSuperRegs(Reserved, RISCV::X0); // zero\n  markSuperRegs(Reserved, RISCV::X2); // sp\n  markSuperRegs(Reserved, RISCV::X3); // gp\n  markSuperRegs(Reserved, RISCV::X4); // tp\n  if (TFI->hasFP(MF))\n    markSuperRegs(Reserved, RISCV::X8); // fp\n  // Reserve the base register if we need to realign the stack and allocate\n  // variable-sized objects at runtime.\n  if (TFI->hasBP(MF))\n    markSuperRegs(Reserved, RISCVABI::getBPReg()); // bp\n\n  // Additionally reserve dummy register used to form the register pair\n  // beginning with 'x0' for instructions that take register pairs.\n  markSuperRegs(Reserved, RISCV::DUMMY_REG_PAIR_WITH_X0);\n\n  // There are only 16 GPRs for RVE.\n  if (Subtarget.isRVE())\n    for (MCPhysReg Reg = RISCV::X16; Reg <= RISCV::X31; Reg++)\n      markSuperRegs(Reserved, Reg);\n\n  // V registers for code generation. We handle them manually.\n  markSuperRegs(Reserved, RISCV::VL);\n  markSuperRegs(Reserved, RISCV::VTYPE);\n  markSuperRegs(Reserved, RISCV::VXSAT);\n  markSuperRegs(Reserved, RISCV::VXRM);\n  markSuperRegs(Reserved, RISCV::VLENB); // vlenb (constant)\n\n  // Floating point environment registers.\n  markSuperRegs(Reserved, RISCV::FRM);\n  markSuperRegs(Reserved, RISCV::FFLAGS);\n\n  if (MF.getFunction().getCallingConv() == CallingConv::GRAAL) {\n    if (Subtarget.isRVE())\n      report_fatal_error(\"Graal reserved registers do not exist in RVE\");\n    markSuperRegs(Reserved, RISCV::X23);\n    markSuperRegs(Reserved, RISCV::X27);\n  }\n\n  // Shadow stack pointer.\n  markSuperRegs(Reserved, RISCV::SSP);\n\n  assert(checkAllSuperRegsMarked(Reserved));\n  return Reserved;\n}",
      "start_line": 90,
      "end_line": 145,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getCallingConv",
        "vlenb",
        "Reserved",
        "getFrameLowering",
        "report_fatal_error",
        "markSuperRegs"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isAsmClobberable",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "MCRegister",
          "name": "PhysReg"
        }
      ],
      "body": "{\n  return !MF.getSubtarget<RISCVSubtarget>().isRegisterReservedByUser(PhysReg);\n}",
      "start_line": 147,
      "end_line": 150,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isRegisterReservedByUser"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "hasReservedSpillSlot",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "Register",
          "name": "Reg"
        },
        {
          "type": "int",
          "name": "&FrameIdx"
        }
      ],
      "body": "{\n  const auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();\n  if (!RVFI->useSaveRestoreLibCalls(MF) && !RVFI->isPushable(MF))\n    return false;\n\n  const auto *FII =\n      llvm::find_if(FixedCSRFIMap, [&](auto P) { return P.first == Reg; });\n  if (FII == std::end(FixedCSRFIMap))\n    return false;\n\n  FrameIdx = FII->second;\n  return true;\n}",
      "start_line": 174,
      "end_line": 188,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "isPushable",
        "find_if"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "adjustReg",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineBasicBlock",
          "name": "&MBB"
        },
        {
          "type": "MachineBasicBlock::iterator",
          "name": "II"
        },
        {
          "type": "const DebugLoc",
          "name": "&DL"
        },
        {
          "type": "Register",
          "name": "DestReg"
        },
        {
          "type": "Register",
          "name": "SrcReg"
        },
        {
          "type": "StackOffset",
          "name": "Offset"
        },
        {
          "type": "MachineInstr::MIFlag",
          "name": "Flag"
        },
        {
          "type": "MaybeAlign",
          "name": "RequiredAlign"
        }
      ],
      "body": "{\n\n  if (DestReg == SrcReg && !Offset.getFixed() && !Offset.getScalable())\n    return;\n\n  MachineFunction &MF = *MBB.getParent();\n  MachineRegisterInfo &MRI = MF.getRegInfo();\n  const RISCVSubtarget &ST = MF.getSubtarget<RISCVSubtarget>();\n  const RISCVInstrInfo *TII = ST.getInstrInfo();\n\n  bool KillSrcReg = false;\n\n  if (Offset.getScalable()) {\n    unsigned ScalableAdjOpc = RISCV::ADD;\n    int64_t ScalableValue = Offset.getScalable();\n    if (ScalableValue < 0) {\n      ScalableValue = -ScalableValue;\n      ScalableAdjOpc = RISCV::SUB;\n    }\n    // Get vlenb and multiply vlen with the number of vector registers.\n    Register ScratchReg = DestReg;\n    if (DestReg == SrcReg)\n      ScratchReg = MRI.createVirtualRegister(&RISCV::GPRRegClass);\n    TII->getVLENFactoredAmount(MF, MBB, II, DL, ScratchReg, ScalableValue, Flag);\n    BuildMI(MBB, II, DL, TII->get(ScalableAdjOpc), DestReg)\n      .addReg(SrcReg).addReg(ScratchReg, RegState::Kill)\n      .setMIFlag(Flag);\n    SrcReg = DestReg;\n    KillSrcReg = true;\n  }\n\n  int64_t Val = Offset.getFixed();\n  if (DestReg == SrcReg && Val == 0)\n    return;\n\n  const uint64_t Align = RequiredAlign.valueOrOne().value();\n\n  if (isInt<12>(Val)) {\n    BuildMI(MBB, II, DL, TII->get(RISCV::ADDI), DestReg)\n        .addReg(SrcReg, getKillRegState(KillSrcReg))\n        .addImm(Val)\n        .setMIFlag(Flag);\n    return;\n  }\n\n  // Try to split the offset across two ADDIs. We need to keep the intermediate\n  // result aligned after each ADDI.  We need to determine the maximum value we\n  // can put in each ADDI. In the negative direction, we can use -2048 which is\n  // always sufficiently aligned. In the positive direction, we need to find the\n  // largest 12-bit immediate that is aligned.  Exclude -4096 since it can be\n  // created with LUI.\n  assert(Align < 2048 && \"Required alignment too large\");\n  int64_t MaxPosAdjStep = 2048 - Align;\n  if (Val > -4096 && Val <= (2 * MaxPosAdjStep)) {\n    int64_t FirstAdj = Val < 0 ? -2048 : MaxPosAdjStep;\n    Val -= FirstAdj;\n    BuildMI(MBB, II, DL, TII->get(RISCV::ADDI), DestReg)\n        .addReg(SrcReg, getKillRegState(KillSrcReg))\n        .addImm(FirstAdj)\n        .setMIFlag(Flag);\n    BuildMI(MBB, II, DL, TII->get(RISCV::ADDI), DestReg)\n        .addReg(DestReg, RegState::Kill)\n        .addImm(Val)\n        .setMIFlag(Flag);\n    return;\n  }\n\n  unsigned Opc = RISCV::ADD;\n  if (Val < 0) {\n    Val = -Val;\n    Opc = RISCV::SUB;\n  }\n\n  Register ScratchReg = MRI.createVirtualRegister(&RISCV::GPRRegClass);\n  TII->movImm(MBB, II, DL, ScratchReg, Val, Flag);\n  BuildMI(MBB, II, DL, TII->get(Opc), DestReg)\n      .addReg(SrcReg, getKillRegState(KillSrcReg))\n      .addReg(ScratchReg, RegState::Kill)\n      .setMIFlag(Flag);\n}",
      "start_line": 190,
      "end_line": 274,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "valueOrOne",
        "BuildMI",
        "createVirtualRegister",
        "getRegInfo",
        "value",
        "getScalable",
        "addReg",
        "getFixed",
        "movImm",
        "getParent",
        "setMIFlag",
        "addImm",
        "getInstrInfo",
        "getVLENFactoredAmount"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVSPILL",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineBasicBlock::iterator",
          "name": "II"
        }
      ],
      "body": "{\n  DebugLoc DL = II->getDebugLoc();\n  MachineBasicBlock &MBB = *II->getParent();\n  MachineFunction &MF = *MBB.getParent();\n  MachineRegisterInfo &MRI = MF.getRegInfo();\n  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();\n  const TargetInstrInfo *TII = STI.getInstrInfo();\n  const TargetRegisterInfo *TRI = STI.getRegisterInfo();\n\n  auto ZvlssegInfo = RISCV::isRVVSpillForZvlsseg(II->getOpcode());\n  unsigned NF = ZvlssegInfo->first;\n  unsigned LMUL = ZvlssegInfo->second;\n  assert(NF * LMUL <= 8 && \"Invalid NF/LMUL combinations.\");\n  unsigned Opcode, SubRegIdx;\n  switch (LMUL) {\n  default:\n    llvm_unreachable(\"LMUL must be 1, 2, or 4.\");\n  case 1:\n    Opcode = RISCV::VS1R_V;\n    SubRegIdx = RISCV::sub_vrm1_0;\n    break;\n  case 2:\n    Opcode = RISCV::VS2R_V;\n    SubRegIdx = RISCV::sub_vrm2_0;\n    break;\n  case 4:\n    Opcode = RISCV::VS4R_V;\n    SubRegIdx = RISCV::sub_vrm4_0;\n    break;\n  }\n  static_assert(RISCV::sub_vrm1_7 == RISCV::sub_vrm1_0 + 7,\n                \"Unexpected subreg numbering\");\n  static_assert(RISCV::sub_vrm2_3 == RISCV::sub_vrm2_0 + 3,\n                \"Unexpected subreg numbering\");\n  static_assert(RISCV::sub_vrm4_1 == RISCV::sub_vrm4_0 + 1,\n                \"Unexpected subreg numbering\");\n\n  Register VL = MRI.createVirtualRegister(&RISCV::GPRRegClass);\n  // Optimize for constant VLEN.\n  if (STI.getRealMinVLen() == STI.getRealMaxVLen()) {\n    const int64_t VLENB = STI.getRealMinVLen() / 8;\n    int64_t Offset = VLENB * LMUL;\n    STI.getInstrInfo()->movImm(MBB, II, DL, VL, Offset);\n  } else {\n    BuildMI(MBB, II, DL, TII->get(RISCV::PseudoReadVLENB), VL);\n    uint32_t ShiftAmount = Log2_32(LMUL);\n    if (ShiftAmount != 0)\n      BuildMI(MBB, II, DL, TII->get(RISCV::SLLI), VL)\n          .addReg(VL)\n          .addImm(ShiftAmount);\n  }\n\n  Register SrcReg = II->getOperand(0).getReg();\n  Register Base = II->getOperand(1).getReg();\n  bool IsBaseKill = II->getOperand(1).isKill();\n  Register NewBase = MRI.createVirtualRegister(&RISCV::GPRRegClass);\n  for (unsigned I = 0; I < NF; ++I) {\n    // Adding implicit-use of super register to describe we are using part of\n    // super register, that prevents machine verifier complaining when part of\n    // subreg is undef, see comment in MachineVerifier::checkLiveness for more\n    // detail.\n    BuildMI(MBB, II, DL, TII->get(Opcode))\n        .addReg(TRI->getSubReg(SrcReg, SubRegIdx + I))\n        .addReg(Base, getKillRegState(I == NF - 1))\n        .addMemOperand(*(II->memoperands_begin()))\n        .addReg(SrcReg, RegState::Implicit);\n    if (I != NF - 1)\n      BuildMI(MBB, II, DL, TII->get(RISCV::ADD), NewBase)\n          .addReg(Base, getKillRegState(I != 0 || IsBaseKill))\n          .addReg(VL, getKillRegState(I == NF - 2));\n    Base = NewBase;\n  }\n  II->eraseFromParent();\n}",
      "start_line": 278,
      "end_line": 351,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "lowerVSPILL",
          "condition": "LMUL",
          "cases": [
            {
              "label": "1",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "2",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "4",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getRealMaxVLen",
        "getRealMinVLen",
        "getParent",
        "Log2_32",
        "BuildMI",
        "createVirtualRegister",
        "addImm",
        "addMemOperand",
        "eraseFromParent",
        "addReg",
        "getDebugLoc",
        "getOperand",
        "getRegisterInfo",
        "movImm",
        "getInstrInfo",
        "getRegInfo",
        "static_assert",
        "isKill",
        "getReg",
        "llvm_unreachable",
        "isRVVSpillForZvlsseg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "lowerVRELOAD",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineBasicBlock::iterator",
          "name": "II"
        }
      ],
      "body": "{\n  DebugLoc DL = II->getDebugLoc();\n  MachineBasicBlock &MBB = *II->getParent();\n  MachineFunction &MF = *MBB.getParent();\n  MachineRegisterInfo &MRI = MF.getRegInfo();\n  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();\n  const TargetInstrInfo *TII = STI.getInstrInfo();\n  const TargetRegisterInfo *TRI = STI.getRegisterInfo();\n\n  auto ZvlssegInfo = RISCV::isRVVSpillForZvlsseg(II->getOpcode());\n  unsigned NF = ZvlssegInfo->first;\n  unsigned LMUL = ZvlssegInfo->second;\n  assert(NF * LMUL <= 8 && \"Invalid NF/LMUL combinations.\");\n  unsigned Opcode, SubRegIdx;\n  switch (LMUL) {\n  default:\n    llvm_unreachable(\"LMUL must be 1, 2, or 4.\");\n  case 1:\n    Opcode = RISCV::VL1RE8_V;\n    SubRegIdx = RISCV::sub_vrm1_0;\n    break;\n  case 2:\n    Opcode = RISCV::VL2RE8_V;\n    SubRegIdx = RISCV::sub_vrm2_0;\n    break;\n  case 4:\n    Opcode = RISCV::VL4RE8_V;\n    SubRegIdx = RISCV::sub_vrm4_0;\n    break;\n  }\n  static_assert(RISCV::sub_vrm1_7 == RISCV::sub_vrm1_0 + 7,\n                \"Unexpected subreg numbering\");\n  static_assert(RISCV::sub_vrm2_3 == RISCV::sub_vrm2_0 + 3,\n                \"Unexpected subreg numbering\");\n  static_assert(RISCV::sub_vrm4_1 == RISCV::sub_vrm4_0 + 1,\n                \"Unexpected subreg numbering\");\n\n  Register VL = MRI.createVirtualRegister(&RISCV::GPRRegClass);\n  // Optimize for constant VLEN.\n  if (STI.getRealMinVLen() == STI.getRealMaxVLen()) {\n    const int64_t VLENB = STI.getRealMinVLen() / 8;\n    int64_t Offset = VLENB * LMUL;\n    STI.getInstrInfo()->movImm(MBB, II, DL, VL, Offset);\n  } else {\n    BuildMI(MBB, II, DL, TII->get(RISCV::PseudoReadVLENB), VL);\n    uint32_t ShiftAmount = Log2_32(LMUL);\n    if (ShiftAmount != 0)\n      BuildMI(MBB, II, DL, TII->get(RISCV::SLLI), VL)\n          .addReg(VL)\n          .addImm(ShiftAmount);\n  }\n\n  Register DestReg = II->getOperand(0).getReg();\n  Register Base = II->getOperand(1).getReg();\n  bool IsBaseKill = II->getOperand(1).isKill();\n  Register NewBase = MRI.createVirtualRegister(&RISCV::GPRRegClass);\n  for (unsigned I = 0; I < NF; ++I) {\n    BuildMI(MBB, II, DL, TII->get(Opcode),\n            TRI->getSubReg(DestReg, SubRegIdx + I))\n        .addReg(Base, getKillRegState(I == NF - 1))\n        .addMemOperand(*(II->memoperands_begin()));\n    if (I != NF - 1)\n      BuildMI(MBB, II, DL, TII->get(RISCV::ADD), NewBase)\n          .addReg(Base, getKillRegState(I != 0 || IsBaseKill))\n          .addReg(VL, getKillRegState(I == NF - 2));\n    Base = NewBase;\n  }\n  II->eraseFromParent();\n}",
      "start_line": 355,
      "end_line": 423,
      "is_virtual": false,
      "is_const": true,
      "switches": [
        {
          "function": "lowerVRELOAD",
          "condition": "LMUL",
          "cases": [
            {
              "label": "1",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "2",
              "return": "",
              "fallthrough": true
            },
            {
              "label": "4",
              "return": "",
              "fallthrough": true
            }
          ],
          "default": ""
        }
      ],
      "calls": [
        "getRealMaxVLen",
        "getRealMinVLen",
        "getParent",
        "Log2_32",
        "BuildMI",
        "createVirtualRegister",
        "addImm",
        "getSubReg",
        "addMemOperand",
        "eraseFromParent",
        "addReg",
        "getDebugLoc",
        "getOperand",
        "getRegisterInfo",
        "movImm",
        "getInstrInfo",
        "getRegInfo",
        "static_assert",
        "isKill",
        "getReg",
        "llvm_unreachable",
        "isRVVSpillForZvlsseg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "eliminateFrameIndex",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MachineBasicBlock::iterator",
          "name": "II"
        },
        {
          "type": "int",
          "name": "SPAdj"
        },
        {
          "type": "unsigned",
          "name": "FIOperandNum"
        },
        {
          "type": "RegScavenger",
          "name": "*RS"
        }
      ],
      "body": "{\n  assert(SPAdj == 0 && \"Unexpected non-zero SPAdj value\");\n\n  MachineInstr &MI = *II;\n  MachineFunction &MF = *MI.getParent()->getParent();\n  MachineRegisterInfo &MRI = MF.getRegInfo();\n  const RISCVSubtarget &ST = MF.getSubtarget<RISCVSubtarget>();\n  DebugLoc DL = MI.getDebugLoc();\n\n  int FrameIndex = MI.getOperand(FIOperandNum).getIndex();\n  Register FrameReg;\n  StackOffset Offset =\n      getFrameLowering(MF)->getFrameIndexReference(MF, FrameIndex, FrameReg);\n  bool IsRVVSpill = RISCV::isRVVSpill(MI);\n  if (!IsRVVSpill)\n    Offset += StackOffset::getFixed(MI.getOperand(FIOperandNum + 1).getImm());\n\n  if (Offset.getScalable() &&\n      ST.getRealMinVLen() == ST.getRealMaxVLen()) {\n    // For an exact VLEN value, scalable offsets become constant and thus\n    // can be converted entirely into fixed offsets.\n    int64_t FixedValue = Offset.getFixed();\n    int64_t ScalableValue = Offset.getScalable();\n    assert(ScalableValue % 8 == 0 &&\n           \"Scalable offset is not a multiple of a single vector size.\");\n    int64_t NumOfVReg = ScalableValue / 8;\n    int64_t VLENB = ST.getRealMinVLen() / 8;\n    Offset = StackOffset::getFixed(FixedValue + NumOfVReg * VLENB);\n  }\n\n  if (!isInt<32>(Offset.getFixed())) {\n    report_fatal_error(\n        \"Frame offsets outside of the signed 32-bit range not supported\");\n  }\n\n  if (!IsRVVSpill) {\n    if (MI.getOpcode() == RISCV::ADDI && !isInt<12>(Offset.getFixed())) {\n      // We chose to emit the canonical immediate sequence rather than folding\n      // the offset into the using add under the theory that doing so doesn't\n      // save dynamic instruction count and some target may fuse the canonical\n      // 32 bit immediate sequence.  We still need to clear the portion of the\n      // offset encoded in the immediate.\n      MI.getOperand(FIOperandNum + 1).ChangeToImmediate(0);\n    } else {\n      // We can encode an add with 12 bit signed immediate in the immediate\n      // operand of our user instruction.  As a result, the remaining\n      // offset can by construction, at worst, a LUI and a ADD.\n      int64_t Val = Offset.getFixed();\n      int64_t Lo12 = SignExtend64<12>(Val);\n      if ((MI.getOpcode() == RISCV::PREFETCH_I ||\n           MI.getOpcode() == RISCV::PREFETCH_R ||\n           MI.getOpcode() == RISCV::PREFETCH_W) &&\n          (Lo12 & 0b11111) != 0)\n        MI.getOperand(FIOperandNum + 1).ChangeToImmediate(0);\n      else {\n        MI.getOperand(FIOperandNum + 1).ChangeToImmediate(Lo12);\n        Offset = StackOffset::get((uint64_t)Val - (uint64_t)Lo12,\n                                  Offset.getScalable());\n      }\n    }\n  }\n\n  if (Offset.getScalable() || Offset.getFixed()) {\n    Register DestReg;\n    if (MI.getOpcode() == RISCV::ADDI)\n      DestReg = MI.getOperand(0).getReg();\n    else\n      DestReg = MRI.createVirtualRegister(&RISCV::GPRRegClass);\n    adjustReg(*II->getParent(), II, DL, DestReg, FrameReg, Offset,\n              MachineInstr::NoFlags, std::nullopt);\n    MI.getOperand(FIOperandNum).ChangeToRegister(DestReg, /*IsDef*/false,\n                                                 /*IsImp*/false,\n                                                 /*IsKill*/true);\n  } else {\n    MI.getOperand(FIOperandNum).ChangeToRegister(FrameReg, /*IsDef*/false,\n                                                 /*IsImp*/false,\n                                                 /*IsKill*/false);\n  }\n\n  // If after materializing the adjustment, we have a pointless ADDI, remove it\n  if (MI.getOpcode() == RISCV::ADDI &&\n      MI.getOperand(0).getReg() == MI.getOperand(1).getReg() &&\n      MI.getOperand(2).getImm() == 0) {\n    MI.eraseFromParent();\n    return true;\n  }\n\n  // Handle spill/fill of synthetic register classes for segment operations to\n  // ensure correctness in the edge case one gets spilled. There are many\n  // possible optimizations here, but given the extreme rarity of such spills,\n  // we prefer simplicity of implementation for now.\n  switch (MI.getOpcode()) {\n  case RISCV::PseudoVSPILL2_M1:\n  case RISCV::PseudoVSPILL2_M2:\n  case RISCV::PseudoVSPILL2_M4:\n  case RISCV::PseudoVSPILL3_M1:\n  case RISCV::PseudoVSPILL3_M2:\n  case RISCV::PseudoVSPILL4_M1:\n  case RISCV::PseudoVSPILL4_M2:\n  case RISCV::PseudoVSPILL5_M1:\n  case RISCV::PseudoVSPILL6_M1:\n  case RISCV::PseudoVSPILL7_M1:\n  case RISCV::PseudoVSPILL8_M1:\n    lowerVSPILL(II);\n    return true;\n  case RISCV::PseudoVRELOAD2_M1:\n  case RISCV::PseudoVRELOAD2_M2:\n  case RISCV::PseudoVRELOAD2_M4:\n  case RISCV::PseudoVRELOAD3_M1:\n  case RISCV::PseudoVRELOAD3_M2:\n  case RISCV::PseudoVRELOAD4_M1:\n  case RISCV::PseudoVRELOAD4_M2:\n  case RISCV::PseudoVRELOAD5_M1:\n  case RISCV::PseudoVRELOAD6_M1:\n  case RISCV::PseudoVRELOAD7_M1:\n  case RISCV::PseudoVRELOAD8_M1:\n    lowerVRELOAD(II);\n    return true;\n  }\n\n  return false;\n}",
      "start_line": 425,
      "end_line": 548,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "ChangeToRegister",
        "getRealMaxVLen",
        "getRealMinVLen",
        "adjustReg",
        "ChangeToImmediate",
        "getParent",
        "getIndex",
        "lowerVSPILL",
        "getImm",
        "createVirtualRegister",
        "getFixed",
        "isRVVSpill",
        "getFrameLowering",
        "getScalable",
        "eraseFromParent",
        "getOpcode",
        "getDebugLoc",
        "getOperand",
        "getFrameIndexReference",
        "getRegInfo",
        "get",
        "lowerVRELOAD",
        "getReg",
        "report_fatal_error"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "requiresVirtualBaseRegisters",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  return true;\n}",
      "start_line": 550,
      "end_line": 553,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "needsFrameBaseReg",
      "return_type": "bool",
      "parameters": [
        {
          "type": "MachineInstr",
          "name": "*MI"
        },
        {
          "type": "int64_t",
          "name": "Offset"
        }
      ],
      "body": "{\n  unsigned FIOperandNum = 0;\n  for (; !MI->getOperand(FIOperandNum).isFI(); FIOperandNum++)\n    assert(FIOperandNum < MI->getNumOperands() &&\n           \"Instr doesn't have FrameIndex operand\");\n\n  // For RISC-V, The machine instructions that include a FrameIndex operand\n  // are load/store, ADDI instructions.\n  unsigned MIFrm = RISCVII::getFormat(MI->getDesc().TSFlags);\n  if (MIFrm != RISCVII::InstFormatI && MIFrm != RISCVII::InstFormatS)\n    return false;\n  // We only generate virtual base registers for loads and stores, so\n  // return false for everything else.\n  if (!MI->mayLoad() && !MI->mayStore())\n    return false;\n\n  const MachineFunction &MF = *MI->getMF();\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  const RISCVFrameLowering *TFI = getFrameLowering(MF);\n  const MachineRegisterInfo &MRI = MF.getRegInfo();\n  unsigned CalleeSavedSize = 0;\n  Offset += getFrameIndexInstrOffset(MI, FIOperandNum);\n\n  // Estimate the stack size used to store callee saved registers(\n  // excludes reserved registers).\n  BitVector ReservedRegs = getReservedRegs(MF);\n  for (const MCPhysReg *R = MRI.getCalleeSavedRegs(); MCPhysReg Reg = *R; ++R) {\n    if (!ReservedRegs.test(Reg))\n      CalleeSavedSize += getSpillSize(*getMinimalPhysRegClass(Reg));\n  }\n\n  int64_t MaxFPOffset = Offset - CalleeSavedSize;\n  if (TFI->hasFP(MF) && !shouldRealignStack(MF))\n    return !isFrameOffsetLegal(MI, RISCV::X8, MaxFPOffset);\n\n  // Assume 128 bytes spill slots size to estimate the maximum possible\n  // offset relative to the stack pointer.\n  // FIXME: The 128 is copied from ARM. We should run some statistics and pick a\n  // real one for RISC-V.\n  int64_t MaxSPOffset = Offset + 128;\n  MaxSPOffset += MFI.getLocalFrameSize();\n  return !isFrameOffsetLegal(MI, RISCV::X2, MaxSPOffset);\n}",
      "start_line": 559,
      "end_line": 602,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getMF",
        "getRegInfo",
        "getFormat",
        "getSpillSize",
        "shouldRealignStack",
        "registers",
        "mayStore",
        "getFrameInfo",
        "isFI",
        "getReservedRegs",
        "getFrameIndexInstrOffset",
        "getFrameLowering",
        "isFrameOffsetLegal",
        "getLocalFrameSize"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "isFrameOffsetLegal",
      "return_type": "bool",
      "parameters": [
        {
          "type": "const MachineInstr",
          "name": "*MI"
        },
        {
          "type": "Register",
          "name": "BaseReg"
        },
        {
          "type": "int64_t",
          "name": "Offset"
        }
      ],
      "body": "{\n  unsigned FIOperandNum = 0;\n  while (!MI->getOperand(FIOperandNum).isFI()) {\n    FIOperandNum++;\n    assert(FIOperandNum < MI->getNumOperands() &&\n           \"Instr does not have a FrameIndex operand!\");\n  }\n\n  Offset += getFrameIndexInstrOffset(MI, FIOperandNum);\n  return isInt<12>(Offset);\n}",
      "start_line": 606,
      "end_line": 618,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getFrameIndexInstrOffset",
        "isFI"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "materializeFrameBaseRegister",
      "return_type": "Register",
      "parameters": [
        {
          "type": "MachineBasicBlock",
          "name": "*MBB"
        },
        {
          "type": "int",
          "name": "FrameIdx"
        },
        {
          "type": "int64_t",
          "name": "Offset"
        }
      ],
      "body": "{\n  MachineBasicBlock::iterator MBBI = MBB->begin();\n  DebugLoc DL;\n  if (MBBI != MBB->end())\n    DL = MBBI->getDebugLoc();\n  MachineFunction *MF = MBB->getParent();\n  MachineRegisterInfo &MFI = MF->getRegInfo();\n  const TargetInstrInfo *TII = MF->getSubtarget().getInstrInfo();\n\n  Register BaseReg = MFI.createVirtualRegister(&RISCV::GPRRegClass);\n  BuildMI(*MBB, MBBI, DL, TII->get(RISCV::ADDI), BaseReg)\n      .addFrameIndex(FrameIdx)\n      .addImm(Offset);\n  return BaseReg;\n}",
      "start_line": 623,
      "end_line": 639,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "BuildMI",
        "getRegInfo",
        "createVirtualRegister",
        "getSubtarget",
        "addImm",
        "getDebugLoc",
        "addFrameIndex",
        "getParent",
        "begin",
        "getInstrInfo"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "resolveFrameIndex",
      "return_type": "void",
      "parameters": [
        {
          "type": "MachineInstr",
          "name": "&MI"
        },
        {
          "type": "Register",
          "name": "BaseReg"
        },
        {
          "type": "int64_t",
          "name": "Offset"
        }
      ],
      "body": "{\n  unsigned FIOperandNum = 0;\n  while (!MI.getOperand(FIOperandNum).isFI()) {\n    FIOperandNum++;\n    assert(FIOperandNum < MI.getNumOperands() &&\n           \"Instr does not have a FrameIndex operand!\");\n  }\n\n  Offset += getFrameIndexInstrOffset(&MI, FIOperandNum);\n  // FrameIndex Operands are always represented as a\n  // register followed by an immediate.\n  MI.getOperand(FIOperandNum).ChangeToRegister(BaseReg, false);\n  MI.getOperand(FIOperandNum + 1).ChangeToImmediate(Offset);\n}",
      "start_line": 643,
      "end_line": 657,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "ChangeToRegister",
        "ChangeToImmediate",
        "isFI",
        "getOperand",
        "getFrameIndexInstrOffset"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getFrameIndexInstrOffset",
      "return_type": "int64_t",
      "parameters": [
        {
          "type": "const MachineInstr",
          "name": "*MI"
        },
        {
          "type": "int",
          "name": "Idx"
        }
      ],
      "body": "{\n  assert((RISCVII::getFormat(MI->getDesc().TSFlags) == RISCVII::InstFormatI ||\n          RISCVII::getFormat(MI->getDesc().TSFlags) == RISCVII::InstFormatS) &&\n         \"The MI must be I or S format.\");\n  assert(MI->getOperand(Idx).isFI() && \"The Idx'th operand of MI is not a \"\n                                       \"FrameIndex operand\");\n  return MI->getOperand(Idx + 1).getImm();\n}",
      "start_line": 661,
      "end_line": 669,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getOperand",
        "getFormat",
        "getImm",
        "isFI"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getFrameRegister",
      "return_type": "Register",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  const TargetFrameLowering *TFI = getFrameLowering(MF);\n  return TFI->hasFP(MF) ? RISCV::X8 : RISCV::X2;\n}",
      "start_line": 671,
      "end_line": 674,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasFP",
        "getFrameLowering"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getCallPreservedMask",
      "return_type": "uint32_t *",
      "parameters": [
        {
          "type": "const MachineFunction &",
          "name": "MF"
        },
        {
          "type": "CallingConv::ID",
          "name": "CC"
        }
      ],
      "body": "{\n  auto &Subtarget = MF.getSubtarget<RISCVSubtarget>();\n\n  if (CC == CallingConv::GHC)\n    return CSR_NoRegs_RegMask;\n  switch (Subtarget.getTargetABI()) {\n  default:\n    llvm_unreachable(\"Unrecognized ABI\");\n  case RISCVABI::ABI_ILP32E:\n  case RISCVABI::ABI_LP64E:\n    return CSR_ILP32E_LP64E_RegMask;\n  case RISCVABI::ABI_ILP32:\n  case RISCVABI::ABI_LP64:\n    return CSR_ILP32_LP64_RegMask;\n  case RISCVABI::ABI_ILP32F:\n  case RISCVABI::ABI_LP64F:\n    return CSR_ILP32F_LP64F_RegMask;\n  case RISCVABI::ABI_ILP32D:\n  case RISCVABI::ABI_LP64D:\n    return CSR_ILP32D_LP64D_RegMask;\n  }\n}",
      "start_line": 676,
      "end_line": 699,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "llvm_unreachable"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getLargestLegalSuperClass",
      "return_type": "TargetRegisterClass *",
      "parameters": [
        {
          "type": "const TargetRegisterClass",
          "name": "*RC"
        },
        {
          "type": "const MachineFunction",
          "name": "&"
        }
      ],
      "body": "{\n  if (RC == &RISCV::VMV0RegClass)\n    return &RISCV::VRRegClass;\n  if (RC == &RISCV::VRNoV0RegClass)\n    return &RISCV::VRRegClass;\n  if (RC == &RISCV::VRM2NoV0RegClass)\n    return &RISCV::VRM2RegClass;\n  if (RC == &RISCV::VRM4NoV0RegClass)\n    return &RISCV::VRM4RegClass;\n  if (RC == &RISCV::VRM8NoV0RegClass)\n    return &RISCV::VRM8RegClass;\n  return RC;\n}",
      "start_line": 701,
      "end_line": 715,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getOffsetOpcodes",
      "return_type": "void",
      "parameters": [
        {
          "type": "const StackOffset",
          "name": "&Offset"
        },
        {
          "type": "SmallVectorImpl<uint64_t>",
          "name": "&Ops"
        }
      ],
      "body": "{\n  // VLENB is the length of a vector register in bytes. We use <vscale x 8 x i8>\n  // to represent one vector register. The dwarf offset is\n  // VLENB * scalable_offset / 8.\n  assert(Offset.getScalable() % 8 == 0 && \"Invalid frame offset\");\n\n  // Add fixed-sized offset using existing DIExpression interface.\n  DIExpression::appendOffset(Ops, Offset.getFixed());\n\n  unsigned VLENB = getDwarfRegNum(RISCV::VLENB, true);\n  int64_t VLENBSized = Offset.getScalable() / 8;\n  if (VLENBSized > 0) {\n    Ops.push_back(dwarf::DW_OP_constu);\n    Ops.push_back(VLENBSized);\n    Ops.append({dwarf::DW_OP_bregx, VLENB, 0ULL});\n    Ops.push_back(dwarf::DW_OP_mul);\n    Ops.push_back(dwarf::DW_OP_plus);\n  } else if (VLENBSized < 0) {\n    Ops.push_back(dwarf::DW_OP_constu);\n    Ops.push_back(-VLENBSized);\n    Ops.append({dwarf::DW_OP_bregx, VLENB, 0ULL});\n    Ops.push_back(dwarf::DW_OP_mul);\n    Ops.push_back(dwarf::DW_OP_minus);\n  }\n}",
      "start_line": 717,
      "end_line": 742,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "getScalable",
        "append",
        "appendOffset",
        "push_back",
        "getDwarfRegNum"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "if",
      "return_type": "else",
      "parameters": [
        {
          "type": "VLENBSized <",
          "name": "0"
        }
      ],
      "body": "{\n    Ops.push_back(dwarf::DW_OP_constu);\n    Ops.push_back(-VLENBSized);\n    Ops.append({dwarf::DW_OP_bregx, VLENB, 0ULL});\n    Ops.push_back(dwarf::DW_OP_mul);\n    Ops.push_back(dwarf::DW_OP_minus);\n  }",
      "start_line": 735,
      "end_line": 741,
      "is_virtual": false,
      "is_const": false,
      "switches": [],
      "calls": [
        "push_back",
        "append"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRegisterCostTableIndex",
      "return_type": "unsigned",
      "parameters": [
        {
          "type": "const MachineFunction",
          "name": "&MF"
        }
      ],
      "body": "{\n  return MF.getSubtarget<RISCVSubtarget>().hasStdExtCOrZca() ? 1 : 0;\n}",
      "start_line": 744,
      "end_line": 747,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "hasStdExtCOrZca"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    },
    {
      "name": "getRegAllocationHints",
      "return_type": "bool",
      "parameters": [
        {
          "type": "Register",
          "name": "VirtReg"
        },
        {
          "type": "ArrayRef<MCPhysReg>",
          "name": "Order"
        },
        {
          "type": "SmallVectorImpl<MCPhysReg>",
          "name": "&Hints"
        },
        {
          "type": "const MachineFunction",
          "name": "&MF"
        },
        {
          "type": "const VirtRegMap",
          "name": "*VRM"
        },
        {
          "type": "const LiveRegMatrix",
          "name": "*Matrix"
        }
      ],
      "body": "{\n  const MachineRegisterInfo *MRI = &MF.getRegInfo();\n  auto &Subtarget = MF.getSubtarget<RISCVSubtarget>();\n\n  bool BaseImplRetVal = TargetRegisterInfo::getRegAllocationHints(\n      VirtReg, Order, Hints, MF, VRM, Matrix);\n\n  if (!VRM || DisableRegAllocHints)\n    return BaseImplRetVal;\n\n  // Add any two address hints after any copy hints.\n  SmallSet<Register, 4> TwoAddrHints;\n\n  auto tryAddHint = [&](const MachineOperand &VRRegMO, const MachineOperand &MO,\n                        bool NeedGPRC) -> void {\n    Register Reg = MO.getReg();\n    Register PhysReg = Reg.isPhysical() ? Reg : Register(VRM->getPhys(Reg));\n    if (PhysReg && (!NeedGPRC || RISCV::GPRCRegClass.contains(PhysReg))) {\n      assert(!MO.getSubReg() && !VRRegMO.getSubReg() && \"Unexpected subreg!\");\n      if (!MRI->isReserved(PhysReg) && !is_contained(Hints, PhysReg))\n        TwoAddrHints.insert(PhysReg);\n    }\n  };\n\n  // This is all of the compressible binary instructions. If an instruction\n  // needs GPRC register class operands \\p NeedGPRC will be set to true.\n  auto isCompressible = [&Subtarget](const MachineInstr &MI, bool &NeedGPRC) {\n    NeedGPRC = false;\n    switch (MI.getOpcode()) {\n    default:\n      return false;\n    case RISCV::AND:\n    case RISCV::OR:\n    case RISCV::XOR:\n    case RISCV::SUB:\n    case RISCV::ADDW:\n    case RISCV::SUBW:\n      NeedGPRC = true;\n      return true;\n    case RISCV::ANDI: {\n      NeedGPRC = true;\n      if (!MI.getOperand(2).isImm())\n        return false;\n      int64_t Imm = MI.getOperand(2).getImm();\n      if (isInt<6>(Imm))\n        return true;\n      // c.zext.b\n      return Subtarget.hasStdExtZcb() && Imm == 255;\n    }\n    case RISCV::SRAI:\n    case RISCV::SRLI:\n      NeedGPRC = true;\n      return true;\n    case RISCV::ADD:\n    case RISCV::SLLI:\n      return true;\n    case RISCV::ADDI:\n    case RISCV::ADDIW:\n      return MI.getOperand(2).isImm() && isInt<6>(MI.getOperand(2).getImm());\n    case RISCV::MUL:\n    case RISCV::SEXT_B:\n    case RISCV::SEXT_H:\n    case RISCV::ZEXT_H_RV32:\n    case RISCV::ZEXT_H_RV64:\n      // c.mul, c.sext.b, c.sext.h, c.zext.h\n      NeedGPRC = true;\n      return Subtarget.hasStdExtZcb();\n    case RISCV::ADD_UW:\n      // c.zext.w\n      NeedGPRC = true;\n      return Subtarget.hasStdExtZcb() && MI.getOperand(2).isReg() &&\n             MI.getOperand(2).getReg() == RISCV::X0;\n    case RISCV::XORI:\n      // c.not\n      NeedGPRC = true;\n      return Subtarget.hasStdExtZcb() && MI.getOperand(2).isImm() &&\n             MI.getOperand(2).getImm() == -1;\n    }\n  };\n\n  // Returns true if this operand is compressible. For non-registers it always\n  // returns true. Immediate range was already checked in isCompressible.\n  // For registers, it checks if the register is a GPRC register. reg-reg\n  // instructions that require GPRC need all register operands to be GPRC.\n  auto isCompressibleOpnd = [&](const MachineOperand &MO) {\n    if (!MO.isReg())\n      return true;\n    Register Reg = MO.getReg();\n    Register PhysReg = Reg.isPhysical() ? Reg : Register(VRM->getPhys(Reg));\n    return PhysReg && RISCV::GPRCRegClass.contains(PhysReg);\n  };\n\n  for (auto &MO : MRI->reg_nodbg_operands(VirtReg)) {\n    const MachineInstr &MI = *MO.getParent();\n    unsigned OpIdx = MO.getOperandNo();\n    bool NeedGPRC;\n    if (isCompressible(MI, NeedGPRC)) {\n      if (OpIdx == 0 && MI.getOperand(1).isReg()) {\n        if (!NeedGPRC || MI.getNumExplicitOperands() < 3 ||\n            MI.getOpcode() == RISCV::ADD_UW ||\n            isCompressibleOpnd(MI.getOperand(2)))\n          tryAddHint(MO, MI.getOperand(1), NeedGPRC);\n        if (MI.isCommutable() && MI.getOperand(2).isReg() &&\n            (!NeedGPRC || isCompressibleOpnd(MI.getOperand(1))))\n          tryAddHint(MO, MI.getOperand(2), NeedGPRC);\n      } else if (OpIdx == 1 && (!NeedGPRC || MI.getNumExplicitOperands() < 3 ||\n                                isCompressibleOpnd(MI.getOperand(2)))) {\n        tryAddHint(MO, MI.getOperand(0), NeedGPRC);\n      } else if (MI.isCommutable() && OpIdx == 2 &&\n                 (!NeedGPRC || isCompressibleOpnd(MI.getOperand(1)))) {\n        tryAddHint(MO, MI.getOperand(0), NeedGPRC);\n      }\n    }\n  }\n\n  for (MCPhysReg OrderReg : Order)\n    if (TwoAddrHints.count(OrderReg))\n      Hints.push_back(OrderReg);\n\n  return BaseImplRetVal;\n}",
      "start_line": 751,
      "end_line": 874,
      "is_virtual": false,
      "is_const": true,
      "switches": [],
      "calls": [
        "is_contained",
        "getParent",
        "isImm",
        "tryAddHint",
        "getImm",
        "getRegAllocationHints",
        "isReg",
        "isCompressibleOpnd",
        "hasStdExtZcb",
        "push_back",
        "getSubReg",
        "contains",
        "getOpcode",
        "getOperand",
        "Register",
        "insert",
        "isPhysical",
        "getRegInfo",
        "getOperandNo",
        "getReg"
      ],
      "source_file": "llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp",
      "backend": "RISCV"
    }
  ]
}