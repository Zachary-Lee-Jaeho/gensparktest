#!/usr/bin/env python3
"""
ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸.

ì„¸ ê°€ì§€ ëª¨ë¸ í¬ê¸°(small, base, large)ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ëª¨ë“  ëª¨ë¸ ë¹„êµ
    python scripts/compare_models.py
    
    # íŠ¹ì • ìƒ˜í”Œ í¬ê¸°ë¡œ ë¹„êµ
    python scripts/compare_models.py --sample-size 100
    
    # íŠ¹ì • ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
    python scripts/compare_models.py --models small base
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent))


@dataclass
class ModelTestResult:
    """ë‹¨ì¼ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼."""
    model_name: str
    model_path: str
    model_loaded: bool = False
    load_time_seconds: float = 0.0
    total_tests: int = 0
    successful_repairs: int = 0
    failed_repairs: int = 0
    errors: int = 0
    total_time_seconds: float = 0.0
    avg_inference_time_seconds: float = 0.0
    repair_accuracy: float = 0.0
    error_messages: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "model_path": self.model_path,
            "model_loaded": self.model_loaded,
            "load_time_seconds": self.load_time_seconds,
            "total_tests": self.total_tests,
            "successful_repairs": self.successful_repairs,
            "failed_repairs": self.failed_repairs,
            "errors": self.errors,
            "total_time_seconds": self.total_time_seconds,
            "avg_inference_time_seconds": self.avg_inference_time_seconds,
            "repair_accuracy": self.repair_accuracy,
        }


def check_model_exists(model_path: str) -> bool:
    """ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸."""
    path = Path(model_path)
    if not path.exists():
        return False
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    required_files = ["config.json"]
    optional_model_files = ["model.safetensors", "pytorch_model.bin"]
    
    has_config = (path / "config.json").exists()
    has_model = any((path / f).exists() for f in optional_model_files)
    
    return has_config and has_model


def load_model(model_path: str, model_name: str, device: str = "cpu"):
    """ëª¨ë¸ ë¡œë“œ."""
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
    import torch
    
    print(f"  ë¡œë”© ì¤‘: {model_path}")
    start = time.time()
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    
    if device == "cuda" and torch.cuda.is_available():
        model = model.cuda()
    
    model.eval()
    load_time = time.time() - start
    print(f"  ë¡œë“œ ì™„ë£Œ: {load_time:.2f}ì´ˆ")
    
    return tokenizer, model, load_time


def run_inference(model, tokenizer, buggy_code: str, device: str = "cpu") -> str:
    """ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰."""
    import torch
    
    prompt = f"fix bug: {buggy_code}"
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    if device == "cuda" and torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=512,
            num_beams=5,
            early_stopping=True
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def get_test_cases() -> List[Dict[str, str]]:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±."""
    return [
        {
            "name": "missing_case_1",
            "buggy": '''switch (Kind) {
    case FK_Data_4: return R_X86_64_32;
    default: return R_X86_64_NONE;
}''',
            "expected_fix": "FK_Data_8",  # ì´ ì¼€ì´ìŠ¤ê°€ ì¶”ê°€ë˜ì–´ì•¼ í•¨
        },
        {
            "name": "wrong_return_1",
            "buggy": '''switch (Kind) {
    case FK_Data_1: return R_X86_64_16;
    default: return R_X86_64_NONE;
}''',
            "expected_fix": "R_X86_64_8",  # ë°˜í™˜ê°’ì´ ìˆ˜ì •ë˜ì–´ì•¼ í•¨
        },
        {
            "name": "missing_case_2",
            "buggy": '''switch (Kind) {
    case FK_PCRel_1: return R_X86_64_PC8;
    case FK_PCRel_2: return R_X86_64_PC16;
    default: return R_X86_64_NONE;
}''',
            "expected_fix": "FK_PCRel_4",  # ì´ ì¼€ì´ìŠ¤ê°€ ì¶”ê°€ë˜ì–´ì•¼ í•¨
        },
        {
            "name": "wrong_return_2",
            "buggy": '''unsigned getRelocType(MCFixupKind Kind) {
    switch (Kind) {
        case RISCV::fixup_riscv_hi20: return ELF::R_RISCV_HI20;
        case RISCV::fixup_riscv_lo12_i: return ELF::R_RISCV_LO12_S;
        default: return ELF::R_RISCV_NONE;
    }
}''',
            "expected_fix": "R_RISCV_LO12_I",  # Së¥¼ Ië¡œ ìˆ˜ì •
        },
        {
            "name": "missing_null_check",
            "buggy": '''void process(Value *V) {
    auto *User = V->getUser();
    User->doSomething();
}''',
            "expected_fix": "if",  # null ì²´í¬ê°€ ì¶”ê°€ë˜ì–´ì•¼ í•¨
        },
    ]


def test_model(
    model_name: str,
    model_path: str,
    test_cases: List[Dict],
    device: str = "cpu",
    verbose: bool = False
) -> ModelTestResult:
    """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸."""
    result = ModelTestResult(model_name=model_name, model_path=model_path)
    
    # ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if not check_model_exists(model_path):
        result.error_messages.append(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
        print(f"  âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
        return result
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        tokenizer, model, load_time = load_model(model_path, model_name, device)
        result.model_loaded = True
        result.load_time_seconds = load_time
    except Exception as e:
        result.error_messages.append(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return result
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result.total_tests = len(test_cases)
    inference_times = []
    
    for i, tc in enumerate(test_cases):
        try:
            start = time.time()
            repaired = run_inference(model, tokenizer, tc["buggy"], device)
            inference_time = time.time() - start
            inference_times.append(inference_time)
            
            # ê°„ë‹¨í•œ ì„±ê³µ íŒë‹¨ (expected_fix ë¬¸ìì—´ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì„±ê³µ)
            if tc["expected_fix"].lower() in repaired.lower():
                result.successful_repairs += 1
                status = "âœ…"
            else:
                result.failed_repairs += 1
                status = "âŒ"
            
            if verbose:
                print(f"    [{i+1}/{len(test_cases)}] {tc['name']}: {status} ({inference_time:.2f}s)")
                
        except Exception as e:
            result.errors += 1
            result.error_messages.append(f"{tc['name']}: {e}")
            if verbose:
                print(f"    [{i+1}/{len(test_cases)}] {tc['name']}: âš ï¸ Error: {e}")
    
    # í†µê³„ ê³„ì‚°
    result.total_time_seconds = sum(inference_times)
    result.avg_inference_time_seconds = (
        result.total_time_seconds / len(inference_times) if inference_times else 0
    )
    result.repair_accuracy = (
        result.successful_repairs / result.total_tests if result.total_tests > 0 else 0
    )
    
    return result


def main():
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ë¹„êµ ì‹¤í—˜")
    parser.add_argument(
        "--models", nargs="+", 
        choices=["small", "base", "large"],
        default=["small", "base", "large"],
        help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (ê¸°ë³¸: ëª¨ë‘)"
    )
    parser.add_argument(
        "--device", type=str, default="cpu",
        choices=["cpu", "cuda", "auto"],
        help="ì¶”ë¡  ì¥ì¹˜"
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true",
        help="ìƒì„¸ ì¶œë ¥"
    )
    parser.add_argument(
        "--output", "-o", type=str, default="results/model_comparison.json",
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )
    
    args = parser.parse_args()
    
    # ì¥ì¹˜ ì„¤ì •
    import torch
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    print("=" * 60)
    print("VEGA-Verified ëª¨ë¸ ë¹„êµ ì‹¤í—˜")
    print("=" * 60)
    print(f"ì¥ì¹˜: {device}")
    print(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸: {args.models}")
    print("=" * 60)
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_paths = {
        "small": "models/repair_model_small/final",
        "base": "models/repair_model_base/final",
        "large": "models/repair_model_large/final",
    }
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
    test_cases = get_test_cases()
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
    print()
    
    # ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸
    results = {}
    for model_name in args.models:
        print(f"\n>>> {model_name.upper()} ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        result = test_model(
            model_name=model_name,
            model_path=model_paths[model_name],
            test_cases=test_cases,
            device=device,
            verbose=args.verbose
        )
        results[model_name] = result
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n  ê²°ê³¼:")
        print(f"    ë¡œë“œë¨: {result.model_loaded}")
        if result.model_loaded:
            print(f"    ë¡œë“œ ì‹œê°„: {result.load_time_seconds:.2f}ì´ˆ")
            print(f"    ì„±ê³µ: {result.successful_repairs}/{result.total_tests}")
            print(f"    ì‹¤íŒ¨: {result.failed_repairs}/{result.total_tests}")
            print(f"    ì—ëŸ¬: {result.errors}")
            print(f"    ì •í™•ë„: {result.repair_accuracy*100:.1f}%")
            print(f"    í‰ê·  ì¶”ë¡  ì‹œê°„: {result.avg_inference_time_seconds:.2f}ì´ˆ")
    
    # ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    print("\n" + "=" * 60)
    print("ê²°ê³¼ ë¹„êµ")
    print("=" * 60)
    print(f"{'ëª¨ë¸':<10} {'ë¡œë“œ':<6} {'ì •í™•ë„':<10} {'í‰ê· ì‹œê°„':<10} {'ìƒíƒœ'}")
    print("-" * 60)
    
    for model_name in args.models:
        r = results[model_name]
        if r.model_loaded:
            status = "âœ…" if r.repair_accuracy >= 0.5 else "ğŸŸ¡"
            print(f"{model_name:<10} {'ì˜ˆ':<6} {r.repair_accuracy*100:>6.1f}%    {r.avg_inference_time_seconds:>6.2f}s    {status}")
        else:
            print(f"{model_name:<10} {'ì•„ë‹ˆì˜¤':<6} {'N/A':<10} {'N/A':<10} âŒ ëª¨ë¸ ì—†ìŒ")
    
    # ê²°ê³¼ ì €ì¥
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    output_data = {
        "timestamp": datetime.now().isoformat(),
        "device": device,
        "test_cases_count": len(test_cases),
        "results": {name: r.to_dict() for name, r in results.items()},
    }
    
    with open(output_path, "w") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nê²°ê³¼ ì €ì¥ë¨: {output_path}")
    
    # ì¶”ì²œ
    print("\n" + "=" * 60)
    print("ì¶”ì²œ")
    print("=" * 60)
    
    loaded_models = [name for name, r in results.items() if r.model_loaded]
    
    if not loaded_models:
        print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("\nëª¨ë¸ì„ í•™ìŠµí•˜ê³  ë‹¤ìŒ ê²½ë¡œì— ë³µì‚¬í•˜ì„¸ìš”:")
        for name, path in model_paths.items():
            print(f"  - {name}: {path}")
    else:
        best_accuracy = max(results[name].repair_accuracy for name in loaded_models)
        best_models = [name for name in loaded_models 
                       if results[name].repair_accuracy == best_accuracy]
        
        print(f"âœ… ìµœê³  ì •í™•ë„ ëª¨ë¸: {', '.join(best_models)} ({best_accuracy*100:.1f}%)")
        
        if best_accuracy < 0.5:
            print("\nâš ï¸ ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:")
            print("  - í•™ìŠµ ë°ì´í„° ì¦ê°€ (--train-size 5000)")
            print("  - ì—í­ ì¦ê°€ (--epochs 20)")
            print("  - ë” í° ëª¨ë¸ ì‚¬ìš© (large)")


if __name__ == "__main__":
    main()
